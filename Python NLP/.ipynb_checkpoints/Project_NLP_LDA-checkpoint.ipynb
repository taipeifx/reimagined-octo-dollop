{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYCDSA Blog Post Data- Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "1. Load and Clean Dataset for a WordCloud.\n",
    "2. Use Latent Dirichlet Allocation (LDA) to group the blog posts into topics, not necessarily their original categories but maybe topics such as politics, sports, technology etc.\n",
    "3. Further Study: Perform Text Classification to see if it is possible to group a blog post from category \"Student Works\" into a more specific category such as R, Web Scraping, Machine Learning etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The NYCDSA Blog Post dataset used here \"lda.csv\" was web scraped from https://nycdatascience.com/blog/ on Oct 26, 2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 2. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA is an unsupervised method that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    " \n",
    "### Steps:\n",
    "- Extract term frequency feature from the corpus (NYCDSA Blog Data).\n",
    "- Fit the LDA model, return the topics, and then inspect each topic (a topic is a common latent variable produced by LDA and used to characterize a document)\n",
    "- Finally we inspect the relation between the topics and the particular documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following was done in python 2\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #counting object\n",
    "from sklearn.decomposition import LatentDirichletAllocation #train lda\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words): #print most freq words\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print\n",
    "        \n",
    "#from before, df = pd.read_csv('lda.csv') \n",
    "df = pd.read_csv('lda.csv')  \n",
    "dataset = df \n",
    "data_samples = dataset.post\n",
    "\n",
    "n_features = 1000\n",
    "#n_topics, number of topics = 10  \n",
    "#n_top_words, top words defining each topic = 20\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,  #specify rules\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples) \n",
    "\n",
    "type(tf) #sparse matrix, matrix with a lot of 0's <class 'scipy.sparse.csr.csr_matrix'>, only record value of non-0s positions\n",
    "type(tf.toarray()) #<type 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1215L, 1000L)\n",
      "----------------------------------------------------------------------------------------\n",
      "[1 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 3 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0\n",
      " 0 0 0 3 0 0 0 0 0 0 1 1 0 0 4 0 0 0 0 0 2 5 5 0 2 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "----------------------------------------------------------------------------------------\n",
      "[u'american', u'ams', u'analysis', u'analytics', u'analyze', u'analyzing', u'annual', u'answer', u'api', u'app', u'appear', u'appears', u'application', u'applications', u'applied', u'apply', u'approach', u'approximately', u'apps', u'april', u'area', u'areas', u'article', u'articles', u'artists']\n",
      "[u'went', u'white', u'wide', u'win', u'wine', u'wines', u'winning', u'women', u'word', u'words', u'work', u'worked', u'working', u'works', u'workshop', u'world', u'worth', u'xgboost', u'yc', u'year', u'years', u'yelp', u'york', u'zero', u'zip']\n"
     ]
    }
   ],
   "source": [
    "print tf.toarray().shape #each row is a document , each column is a word (top 1000 words)\n",
    "print '-'*88\n",
    "\n",
    "print tf.toarray()[958, 0:200] #shows article 958, words 0:200, after applying tf_vectorizer.fit_transform()\n",
    "print '-'*88\n",
    "\n",
    "print tf_vectorizer.get_feature_names()[50: 75] #words 50 - 75, found by vectorizor\n",
    "print tf_vectorizer.get_feature_names()[975: 1000] #words 975 - 1000, when we specified max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taiwan Voting Data: Exploratory Visualization and Shiny Project - You will build an interactive Shiny application that can create visual representations...: You will build an interactive Shiny application that can create visual representations of data insights and trends.Visit my project here: : This was the first project I did at the YC Data Science Academy (Fall Cohort 2018). I browsed my way to  where they store a large variety of publicly shared data. I wanted to select a data set suitable for charting and mapping purposes. I also preferred to work with data which I had to translate from Mandarin Chinese'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[958][0:615] #my R Shiny Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=10, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we start to fit the LDA model:\n",
    "n_topics = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "#lda.components_ \n",
    "\n",
    "#lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10L, 1000L)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lda.components_ \n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "Topic #0:\n",
      "data science learning work job time use project academy people like yc using bootcamp python students machine really want just\n",
      "\n",
      "Topic #1:\n",
      "reviews rating ratings movie shows wine average movies music data review score number user artists complaints scores songs different higher\n",
      "\n",
      "Topic #2:\n",
      "data number time year years countries city ew population york world map people states state analysis country different shows dataset\n",
      "\n",
      "Topic #3:\n",
      "data app shiny user time information number map code based different tab health used project average job application chart salary\n",
      "\n",
      "Topic #4:\n",
      "price market sales prices fig airlines flight brands project category number brand average popular categories industry shot days apps projects\n",
      "\n",
      "Topic #5:\n",
      "stock games data performance analysis game correlation time model number used plot companies trading significant variables news stocks values words\n",
      "\n",
      "Topic #6:\n",
      "data crime school schools loan education rates car rate high house loans students price grade years higher credit student time\n",
      "\n",
      "Topic #7:\n",
      "team player players data game teams season win winning match statistics games field points performance use percentage home let results\n",
      "\n",
      "Topic #8:\n",
      "data words user used using users information project scraping time web page word reviews analysis number text website use restaurants\n",
      "\n",
      "Topic #9:\n",
      "model data models features variables set regression used feature training values learning dataset number test random using machine score variable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 20\n",
    "print(\"\\nTopics in LDA model:\\n\") #print_top_words\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "#each post will fall into a topic, read the topic words to see what the post might be about!\n",
    "#FYI, without stopwords: Topic #0: that we this on it for but are as movies user more be reviews with by there analysis rate can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan Voting Data: Exploratory Visualization and Shiny Project - You will build an interactive Shiny application that can create visual representations...: You will build an interactive Shiny application that can create visual representations of data insights and trends.Visit my project here: : This was the first project I did at the YC Data Science Academy (Fall Cohort 2018). I browsed my way to  where they store a large variety of publicly shared data. I wanted to select a data set suitable for charting and mapping purposes. I also preferred to work with data which I had to translate from Mandarin Chinese\n",
      "----------------------------------------------------------------------------------------\n",
      "[[1.36030417e-01 1.96982420e-02 7.70709540e-02 7.65025867e-01\n",
      "  3.62418627e-04 3.62425841e-04 3.62396375e-04 3.62411829e-04\n",
      "  3.62429431e-04 3.62438249e-04]]\n"
     ]
    }
   ],
   "source": [
    "text = data_samples[958]\n",
    "\n",
    "print text[0:615]\n",
    "print '-'*88\n",
    "print lda.transform(tf[958,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Matches for my blog post \"Taiwan Voting Data\":\n",
    "- Topic #3: 0.765 : data app shiny user time information number map code based different tab health used project average job application chart salary\n",
    "\n",
    "- Topic #0: 0.136 : data science learning work job time use project academy people like yc using bootcamp python students machine really want just\n",
    "\n",
    "- Topic #2: 0.077 : data number time year years countries city ew population york world map people states state analysis country different shows dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further study: Text Classification\n",
    "- LDA topics contain words like \"shiny\" and \"scraping\". Might it be possible to use the topics to group unknown articles into categories?\n",
    "- Either that or I would use Text Classification (sentiment analysis but with categories such as R, Python, Web Scraping, R Shiny). This would be a supervised learning method to teach the computer to recognize different blog post categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "textclass = []\n",
    "for i in range(0,1215): #1216   \n",
    "    #text = data_samples[i]\n",
    "    #print text[0:615]\n",
    "    #print '-'*88\n",
    "    a = lda.transform(tf[i,:])\n",
    "    b = a.tolist()\n",
    "    textclass.extend(b)\n",
    "\n",
    "with open('csvfile.csv','wb') as file:\n",
    "    file.writelines([\"%s\\n\" % item  for item in textclass])\n",
    "#a = lda.transform(tf[i,:])\n",
    "#type(a)\n",
    "#a.tolist()[0][0]\n",
    "#a.tolist()[0][1]\n",
    "#a.tolist()[0][2]\n",
    "#a.tolist()[0][3]\n",
    "#a.tolist()[0][4]\n",
    "#a.tolist()[0][5]\n",
    "#a.tolist()[0][6]\n",
    "#a.tolist()[0][7]\n",
    "#a.tolist()[0][8]\n",
    "#a.tolist()[0][9]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
