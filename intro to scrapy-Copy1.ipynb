{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\tdef parse(self, response):\n",
    "\t\tfor num in [1,2]:\n",
    "            rows = response.xpath('//*[@id=\"template-home-v2\"]/div[1]/div[4]/div/div[2]/div[1]/div[%d]'%num) #for row in rows, [1] to [9] #this gets the first row\n",
    "            pattern = './a[2]/@title' #or a/@title\n",
    "            for row in rows:\n",
    "                title = row.xpath(pattern).extract_first()\n",
    "                print (title)\n",
    "\n",
    "\t\titem = NycdsaItem()\n",
    "\t\titem['title'] = title\t\n",
    "\t\tyield item \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WikiItem(scrapy.Item):\n",
    "    film = scrapy.Field()\n",
    "    year = scrapy.Field()\n",
    "    awards = scrapy.Field()\n",
    "    nominations = scrapy.Field()\n",
    "#__init__.py is for packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the wiki_spider.py document\n",
    "#import scrapy.Spider (from which our spider will inherit), \n",
    "#and the item class we defined in items.py\n",
    "from scrapy import Spider\n",
    "from wiki.wiki.items import WikiItem\n",
    "\n",
    "# define our spider class, and give it its 3 must have/ required attributes\n",
    "class WikiSpider(Spider):\n",
    "    name = 'wiki_spider'\n",
    "    allowed_urls = ['https://en.wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Find all the table rows\n",
    "        rows = response.xpath('//*[@id=\"mw-content-text\"]/div/table/tbody/tr')\n",
    "\n",
    "        # The movie title could be of different styles so we need to provide all the possibilities.\n",
    "        patterns = ['./td[1]/i/a/text()', './td[1]/i/b/a/text()', './td[1]/i/span[2]//text()', './td[1]/i/b/span/text()']\n",
    "        for row in rows:\n",
    "            # extract() will return a Python list, extract_first() will return the first element in the list\n",
    "            # If you know the first element is what you want, you can use extract_first()\n",
    "            for pattern in patterns:\n",
    "            film = row.xpath(pattern).extract_first()\n",
    "                if film:\n",
    "                    break\n",
    "            # If the movie title is missing, then we just skip it.\n",
    "            if not film: # non-type object \n",
    "                continue # back to the for loop for rows in rows \n",
    "            # Relative xpath for all the other columns\n",
    "            year = row.xpath('./td[2]/a/text()').extract_first()\n",
    "            awards = row.xpath('./td[3]/text()').extract_first()\n",
    "            nominations = row.xpath('./td[4]/text()').extract_first().strip()\n",
    "\n",
    "            # Initialize a new WikiItem instance for each movie.\n",
    "            item = WikiItem()\n",
    "            item['film'] = film\n",
    "            item['year'] = year\n",
    "            item['awards'] = awards\n",
    "            item['nominations'] = nominations\n",
    "            yield item\n",
    "\n",
    "# after yielding item from spider, next is pipeline, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://doc.scrapy.org/en/latest/topics/selectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //div[@class=\"mine\"] selects all the <div> tags having an attribute class=\"mine\"\n",
    "# class is an attribute\n",
    "\n",
    "#https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films\n",
    "<table class =\"wikitable sortable jquery-tablesorter\"> \n",
    "#right click, copy, XPath :\n",
    "/html/body/div[3]/div[3]/div[4]/div/table #firefox\n",
    "//*[@id=\"mw-content-text\"]/div/table #chrome Xpath for the entire table\n",
    "# / tbody /tr #append tbody and tr to Xpath\n",
    "\n",
    "##########################################\n",
    "<thead>...</thead>\n",
    "<tbody>\n",
    "\n",
    "<tr>...</tr> #each tr is a different movie\n",
    "<tr>...</tr>\n",
    "<tr>...</tr>\n",
    "<tr>...</tr>\n",
    "<tr>...</tr>\n",
    "\n",
    "</tbody>\n",
    "<tfoot></tfoot>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## START SCRAPY SHELL IN ANACONDA IN PY3 ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right click anaconda, start second terminal\n",
    "# cd into project folder\n",
    "\n",
    "cd C:\\Users\\asus\\NYC Data Science Academy\\NYCDSA Unit 5 Data Analysis with Python\\Project 2 - Web Scraping\n",
    "\n",
    "scrapy shell \"https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films\"\n",
    "           # \"url_of_the_page_you're_scraping\"\n",
    "\n",
    "rows = response.xpath('//*[@id=\"mw-content-text\"]/div/table/tbody/tr') #rows, points to the entire table\n",
    "row = rows[3] #define row\n",
    "\n",
    "In [9]: print (row)\n",
    "<Selector xpath='//*[@id=\"mw-content-text\"]/div/table/tbody/tr' data='<tr>\\n<td><i><a href=\"/wiki/Three_Billboa'>\n",
    "\n",
    "In [18]: xtext = row.xpath('./td[1]/i/a/text()') #define xtext through row.xpath\n",
    "\n",
    "In [19]: xtext\n",
    "Out[19]: [<Selector xpath='./td[1]/i/a/text()' data='Three Billboards Outside Ebbing, Missour'>]\n",
    "#In [20]: print (xtext)\n",
    "#[<Selector xpath='./td[1]/i/a/text()' data='Three Billboards Outside Ebbing, Missour'>]\n",
    "\n",
    "In [21]: xtext.extract() #use .extract() to print value\n",
    "Out[21]: ['Three Billboards Outside Ebbing, Missouri']\n",
    "    #extract_first() returns none if extracting from empty list. \n",
    "    \n",
    "film = row.xpath('./td[1]/i/a/text()').extract_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelines.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "from scrapy.exceptions import DropItem\n",
    "from scrapy.exporters import CsvItemExporter\n",
    "\n",
    "class ValidateItemPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if not all(item.values()):\n",
    "            raise DropItem(\"Missing values!\")\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "class WriteItemPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.filename = 'academy_awards.csv'\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.csvfile = open(self.filename, 'wb')\n",
    "        self.exporter = CsvItemExporter(self.csvfile)\n",
    "        self.exporter.start_exporting()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.csvfile.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.exporter.export_item(item)\n",
    "        return item\n",
    "#next edit settings.py\n",
    "#then, ready to deploy spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for wiki project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://doc.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'wiki'\n",
    "\n",
    "SPIDER_MODULES = ['wiki.spiders']\n",
    "NEWSPIDER_MODULE = 'wiki.spiders'\n",
    "\n",
    "ITEM_PIPELINES = {'wiki.pipelines.ValidateItemPipeline': 100, #smaller number -> higher priority\n",
    "\t\t\t\t\t'wiki.pipelines.WriteItemPipeline': 200}\n",
    "#validate first, then write item to file\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'wiki (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'wiki.middlewares.WikiSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'wiki.middlewares.WikiDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://doc.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'wiki.pipelines.WikiPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy spider\n",
    "#open new anaconda prompt, activate py3, cd to folder with scrapy.cfg\n",
    "# (ipykernel_py3) C:\\Users\\asus\\NYC Data Science Academy\\\n",
    "# NYCDSA Unit 5 Data Analysis with Python\\Project 2 - Web Scraping\\wiki\n",
    "scrapy crawl wiki_spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# NEW PROJECT ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject budget #create new scrapy project\n",
    "#can drag folder into file editor to see structure\n",
    "\n",
    "#create files\n",
    "#update items.py\n",
    "import scrapy\n",
    "class BudgetItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    RDate = scrapy.Field()\n",
    "    Title = scrapy.Field()\n",
    "    PBudget = scrapy.Field()\n",
    "    DomesticG = scrapy.Field()\n",
    "    WorldwideG = scrapy.Field()\n",
    "    \n",
    "#create budget_spider.py under budget/spiders/\n",
    "#Definitely the first thing to do is to import \n",
    "#the module and the budget item we defined in items.py\n",
    "from scrapy import Spider\n",
    "from budget.items import BudgetItem\n",
    "\n",
    "#define spider called BudgetSpider, give it a name called budget_spider\n",
    "#use budget_spider to run a crawl on terminal\n",
    "\n",
    "class BudgetSpider(Spider):\n",
    "    name = \"budget_spider\"\n",
    "    allowed_urls = ['https://www.the-numbers.com/']\n",
    "    start_urls = ['https://www.the-numbers.com/movie/budgets/all']\n",
    "    def parse(self, response):\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ctrl+d to quit scrapy shell\n",
    "#start new scrapy shell in py3 env\n",
    "scrapy shell \"https://www.the-numbers.com/movie/budgets/all\"\n",
    "#403 forbidden\n",
    "#add header to request for 200 \n",
    "scrapy shell -s USER_AGENT=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36\" \"https://www.the-numbers.com/movie/budgets/all\"\n",
    "#success\n",
    "#In [1]: response\n",
    "#Out[1]: <200 https://www.the-numbers.com/movie/budgets/all>\n",
    "\n",
    "rows = response.xpath('//*[@id=\"page_filling_chart\"]/center/table/tbody/tr')\n",
    "#doesnt work, use:\n",
    "rows = response.xpath('//*[@id=\"page_filling_chart\"]/center/table//tr') #tbody not in code, added in by google chrome\n",
    "#view page source to see actual code\n",
    "\n",
    "#Release Date:\n",
    "In [11]: rows[1].xpath('./td[2]/a/text()').extract_first()\n",
    "Out[11]: '12/18/2009'\n",
    "\n",
    "#Movie Title:\n",
    "In [12]: rows[1].xpath('./td[3]/b/a/text()').extract_first()\n",
    "Out[12]: 'Avatar'\n",
    "\n",
    "#3 more rows, PBudget, DomesticG, WorldwideG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAGE 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ctrl+d to quit scrapy shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls to clear screen #ctrl+L to clear screen in scrapy shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
