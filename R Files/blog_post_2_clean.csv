link,page,post,shares,title
https://nycdatascience.com/blog/student-works/does-apples-fixed-iphone-release-schedule-hurt-its-customer-satisfaction/,1,"As a selfproclaimed techenthusiast I've been following the tech review community especially on YouTube for quite a while. During that time I recognized a certain pattern emerge after every new iPhone release: highly popular videos (as well as articles) would be released criticizing initial problems with the new iPhone.However Apple's sales numbers do not seem to be impacted by this negative atmosphere around its release date. This made me wonder who is most impacted by these videos and articles and whether or not they affect Apple's customer satisfaction. Should these reviews in fact impact Apple's customers a more loosely defined release schedule giving Apple more time to refine new features of the iPhone might increase Apple's customer satisfaction.I chose to scrape amazon.co.uk for several reasons. Firstly Europe besides China is arguably the . Thus customer reviews from the UK present relevant information for Apple. Secondly Amazon enabled me to not only gather information regarding the reviews themselves (Rating Title Text Helpful Votes) but also regarding the Amazon user that published the review. By clicking on the username I was able to collect information about each user such as the total number of helpful votes and reviews as well as all published reviews.In conducting my research I chose to exclusively focus on reviews for the iPhone X from November 2017 to September 2018 to only gather reviews for the newest iPhone at that point in time. Future research could apply the same concept to older iPhones.I used Selenium to scrape amazon.co.uk mainly because of its flexibility in navigating between different websites. After having scraped the data I prepared and cleaned the data mostly using Pandas NumPy and RE. This step included identifying and appropriately dealing with missing values adapting my code and reformatting the gathered data to enable further processing and analysis.Then I manipulated and analyzed the data breaking it down into subgroups and comparing their characteristics.Finally I visualized the results of my analysis with matplotlib seaborn and wordcloud.The first part of my analysis focused on the number of reviews per month from the iPhone's first reviews in November 2017 until September 2018. Against my initial expectations there were relatively little reviews around the time of the iPhone's release. Then around December 2017 the number of reviews started significantly increasing reaching its peak in March/April of 2018.My suspicion regarding the relatively low number of reviews around the release is that firstly the iPhone is released later in Europe than it is in the US possibly causing a delay in reviews. Furthermore customers in the UK might have waited to purchase the iPhone until Christmas which could explain the increase in December.Next I split the reviews into two categories: verified and unverified reviews. Generally verified reviews clearly outnumbered unverified reviews. However the only moment at which the number of unverified reviews exceeded the number of verified reviews was November 2017 directly after the iPhone's release. This finding fueled my suspicion that the generally negative atmosphere on the Internet at the time of the iPhone's release is not created by Apple customers but by people disliking Apple as a company.Furthermore this line plot shows that the previously identified increase in reviews is almost exclusively driven by verified reviews while unverified reviews decrease after November 2017 and never really increase again.Having proven that the substantial difference between the number of verified and unverified reviews I decided to dig deeper and compare the average rating of these two groups of reviews. The following box plot visualizes the result:This box plot shows that the two types of reviews not only tremendously differ when it comes to the number of reviews but also the average rating. While the average ratings for verified reviews are clustered between 4.50 and 4.75 with the 1st and 3rd quartile being extremely close to each other the average ratings of unverified reviews exhibit substantial variability. On top the median of unverified average ratings at approximately 3.50 provides further evidence for my suspicion that actual Apple customers are generally very satisfied with their iPhone's and not affected by negative reviews on the Internet.To confirm these findings I took a closer look at reviews published in November 2017 grouped by the type of review:While the relatively low number of reviews does not allow for very reliable conclusions there still is a visible difference between the verified and unverified ratings with the verified ratings generally being more favorable than the unverified ratings.In my next step I exclusively focused on the review texts of verified reviews and after preparing the data generated a word cloud that allows for a quick overview of the general sentiment among verified reviews:Again the majority of words in this word cloud are positive and express satisfaction with the iPhone X (""happy"" ""excellent"" ""best""). In addition the delivery seems to have been very important for customers which is not necessarily an insight for Apple but definitely for Amazon. Finding negative words in this word cloud requires either very good eyes or a magnifying glass. If you possess either you might be able to spot ""crashed"" or ""smashed"" however the size and rarity of negative words provide further evidence for the high satisfaction of actual Apple customers.At this point I was almost ready to wrap up my project and include a few more describing visualizations to prove my point. However after sampling some of the reviews I started having doubts as to how many reviews were published by people actually having purchased the product. Some reviews even verified reviews seemed very suspicious to me. In order to decrease the uncertainty introduced by not knowing which reviews are real I came up with something that I named the .To incorporate the Fake Review Index into my research I decided to go back to scraping and gathered not only reviews but also relevant information regarding each user. Then I used this information to calculate the Fake Review Index based on the following 6 factors:I calculated the Fake Review Index by assigning weights and categorizing several scenarios for each of these factors. For instance the highest score for the  factor is 10. One scenario for this factor would be the following: if the result of this calculation for a given user returns a number smaller than or equal to 1 ( the user received less than or equal to 1 helpful votes for all of his posts on average) the user's Fake Review Index gets increased by 10. Therefore  the Fake Review Index  it is that the review is genuine.While the Fake Review Index is not extremely sophisticated (yet) I am very confident it is able to at least identify the most obvious fake reviews and filter them out. To illustrate the Fake Review Index I included two scores of two different users that have published a review for the iPhone X:This user received a Fake Review Index of 74.5. As evident from his reviews he barely receives helpful votes and his reviews do not appear to be genuine whatsoever. Furthermore this user mainly uses 5star ratings and publishes several reviews for different products on the same day. Thus he receives a high Fake Review Index Score.The second user received a Fake Review Index of 27 making his reviews likely to be genuine. Indeed this profile seems to belong to an active member of the Amazon review community as his reviews actually detail his experience with the product do not only consist of 5star ratings and receive more helpful votes:Finally after grouping the reviews by their Fake Review Index I made an interesting observation: the more likely a review is genuine the lower the average rating. While the decrease is not extremely large it is still significant as the average rating for reviews that are very likely genuine is approximately 3.8. This rating is still very high and proves that Apple's customers are very satisfied nevertheless it is not as astronomically high as the previously discussed 4.5  4.75 average for verified reviews.In summary Apple's customer satisfaction is still very high. The negative atmosphere on the Internet at the time of the iPhone's release can mainly be attributed to noncustomers. Apple's customer satisfaction however is not as high as one might suspect at first glance when considering how genuine the reviews are. To answer my initial research question there does not seem to be an urgent need for Apple to implement a more loosely defined release schedule.Future extensions of this project would include a more refined and sophisticated fake review index that could then be universally applied to different review websites to enable companies to filter out the most relevant reviews and trends among their customers.",NA,Does Apple’s Fixed iPhone Release Schedule Hurt Its Customer Satisfaction?
https://nycdatascience.com/blog/student-works/variation-in-hospital-charges-and-medicare-payments-for-inpatient-procedures-in-the-united-states/,1,U.S. healthcare costs have been on the rise over the past several years outpacing the growth of the economy overall. The Centers for Medicare and Medicaid Services (CMS) estimates that American healthcare spending increased by 4.6% in 2017 to reach $3.5 trillion. The increases in medical care are  driven by the Medicaid expansion the private insurance market as well as the aging population. Consequently h,NA,Variation in Hospital Charges and Medicare Payments for Inpatient Procedures in the United States
https://nycdatascience.com/blog/student-works/housing-price-prediction-using-advanced-regression-analysis/,1,Intuitively which of the four houses in the picture do you think is the most expensive?Most people will say the blue one on the right because it is the biggest and the newest. However you might have a different answer after reading this blog post and discover a more precise approach to predicting prices. In this blog post we discuss how we use machine learning techniques to predict house prices.  The dataset can be found on. The dataset is divided into the training and test datasets. In total there are about 2600 rows and 79 columns which contain descriptive information on different houses (e.g. number of bedrooms square feet of the first floor etc.). The training dataset contains the actual house prices while the test dataset doesn’t. The house prices are rightskewed with a mean and a median around $200000. Most houses are in the range of 100k to 250k; the high end is around 550k to 750k with a sparse distribution.Most of the variables in the dataset (51 out of 79) are categorical.  They include things like the neighborhood of the house the overall quality the house style etc. The most predictive variables for the sale price are the quality variables. For example the overall quality turns out to be the strongest predictor for the sale price. Quality on particular aspect of the house like the pool quality the garage quality and the basement quality also show high correlation with the sale price.The numeric variables in the dataset are mostly the area of the house including the firstfloor area pool area number of bedrooms garage area etc. Most of the variables show a correlation with the sale price.One challenge of this dataset is the missing data. For missing data such as pool quality and pool area  where a missing value means there is no pool in this house  we replace the missing value with 0 for numeric variables and “None” for categorical variables. However for missing data that are missing at random we use other variables to impute the value. Dealing with a large number of dirty features is always a challenge. This section focuses on the feature engineering (creating and dropping variables) and feature transformation (dummifying variables removing skewness etc.) tasks. GarageYrBlt (year the garage was built) and YrBlt (year the house was built) had a very strong positive correlation of 0.83. In fact more than 75.8% of these values were exactly the same. Hence we decided to drop GarageYrBlt since it had many missing values which could be compensated by YrBlt.Because we have to work with so many variables we introduced the use of regularization techniques to address the issue of multicollinearity found in our correlation matrix and the possibility of overfitting using the multiple linear regression model. We address that in the exploratory data analysis section. The great thing about regularization is that it reduces the model complexity as it automatically does the feature selection for you. All the regularization models penalize for extra features.Regularization models include (Lasso Ridge and Elastic Net). The lasso model will set coefficients to zero while the ridge model will minimize the coefficients making some of them very close to zero. Elastic net is a hybrid of both the lasso and ridge model. It groups correlated variables together and if one of the variables in the group is a strong predictor then it will include the entire group into the model. The next step is to tune the hyperparameters of each model through the use of crossvalidation. We choose alpha  .0005 for the Lasso model and alpha  2.8 for the Ridge model. We choose alpha  .0005 and L1_Ratio  0.9 for Elastic Net. Because Elastic Net with a L1_Ratio of 0.9 is very similar to the Lasso model which has a default L1_Ratio of 1 we do not depict it here.Positive coefficients for Sale Price: Above Grade Living Area Overall Condition and the Neighborhoods (Stone Bridge North Ridge and Crawford).Negative coefficients for Sale Price: MS Zoning Neighborhood Edwards and Above Ground Kitchen.Positive coefficients for Sale Price: General Living Area Roofing Material (Wood Shingle) Overall Condition.Negative coefficients for Sale Price: The General Zoning requirements Proximity to Main Road or Railroad and the Pool Quality being in Good condition.The two graphs below show how accurate our model prediction is for the sales price vs the actual price. Dots closer to or on the red line show how accurate the model prediction was. There are some outliers that we should investigate as future work on the model. Gradient Boosting Regressor was one of our best performing algorithms. We first trained gradient boosting machine using the entire set of features (baseline model). We performed crossvalidation with parameter tuning using GridSearchCV function from scikitlearn package for Python. Our best model parameters were: learning rate of 0.05 2000 estimators and max depth of 3.  We created a relative importance chart to visualize feature importance in gradient boosting. Feature importance scores indicate how useful each feature is in the construction of the boosted decision tree. Above Grade Living Area Square Feet Kitchen Quality Total Square Feet of Basement Area and Size of Garage in Car Capacity were among most valuable features.  We then attempted to improve our baseline model performance by reducing the feature dimensionality. High dimensional data can be sparse or spread out which makes it harder for certain algorithms to train effective models. In general predictive algorithms benefit from optimal nonredundant subset of features that improve the rate of training as well as enhance interpretability and generalization. We managed our machine learning workflows with scikitlearn Pipelines. Scikitlearn Pipeline class allows us to apply a series of data transformations followed by the application of an estimator. We built several pipelines each with different estimator (e.g. Gradient Boosting Regressor Linear Regression etc.) For Gradient Boosting Machine our pipeline included:After we completed feature engineering we had over 200 features and about 1500 rows in our training set. We decided to keep 150 principal components after examining cumulative percentage of variance chart. 150 components accounted for over 85% of variance of our data. Variance measures how spread out the dataset is. Not all tweaks improve results. After we implemented PCA our cross validation scores did not improve. In fact our scores deteriorated (cross validation score declined to 0.87 from 0.91 for baseline model). We believe that reducing dimensions caused loss of some important information. PCA not only removed random noise in our data but also some valuable inputs. For Multivariate Linear Regression our pipeline included:Using PCA with Multivariate Linear Regression did not produce good results as well. Our cross validation score decreased as compared to a baseline model (training Multivariate Linear Regression using the entire set of features).  Using single isolated models gives us a decent result. But usually all real life problems do not have a direct linear or nonlinear relationship with the target variable that can be captured alone by a single model. An ensemble of conservative and aggressive linear and nonlinear models best describes the housing price prediction problem. To begin with we tried a simple ensemble model of XGBoost (nonlinear) and ENet (linear) with a 5050 weightage. Next following the standard stacking approach we stacked different models to see if we could do better. Our stacked model consisted of the linear ENet model a conservative random forest of short depth a fullygrown aggressive random forest a conservative gradient boosting of short depth and finally a fullygrown aggressive gradient boosting model. The performance has been recorded below:The below correlation heatmap shows our predicted sale prices for some of the models used. We can see that Elastic Net Lasso and Ridge were very similar in nature whilst Ensembling and Stacking was also very similar. The one standalone model with distinctly different results was the XGBoost one.,NA,Predicting House Prices with Machine Learning Algorithms
https://nycdatascience.com/blog/community/hiring-partner-event-at-nyc-data-science-academy-september-26th-2018/,1,On September 26th our bootcamp graduates had the opportunity to have a glimpse into their future. Over 50 HR and hiring managers from various companies engaged in conversation with them regarding career opportunities in the data science field at our Hiring Partner Event. Through our bootcamp students have the opportunity to work on realworld data science projects for companies like the ones present at this event. From small startups to large corporations there was no limit to the diversity of companies present. Officials from Spotify American Express VICE Media Morgan Stanley Spectrum and many more attended this occasion.Some of the hiring partners also happened to be alumni seeking grads to join their teams. These alumni include Michael Goldman (Digitas) Chao Shi (National Grid) Kweku Ulzen (Nielsen) Sam Marks (VICE Media) Andrew Dodd (Well Woven) and Nelson Chen (Work Fusion). It was great seeing alumni coming back helping out their fellow grads!Are you interested in participating in our future hiring events or recruiting NYC Data Science Academy graduates? Contact us at.,NA,"Hiring Partner Event at NYC Data Science Academy, September 26th 2018"
https://nycdatascience.com/blog/student-works/will-the-nba-players-salary-contribute-to-the-teams-win/,1,"Which team will win the championship this season? Who will win the MVP?   Many sports stars surprise people with huge amounts of contracts.
However So are their salaries contributing to the team's victory?
My topic is to learn about the relationship between NBA games (WL) and salary using Web Scrap.  
Modern basketball is evolving right now.  Because of the unique nature of basketball we can not simply say that the salary contributes to the team win.  There are also many invisible indicators such as mental and leadership for victory. It is difficult to relate salary and victory but I think it is also fun to watch sports.",NA,Will the NBA Player's salary contribute to the team’s Win?
https://nycdatascience.com/blog/student-works/residential-property-investment-visualization-and-analysis-shiny-app/,1,"The purpose of this Shiny web app is to better advise clients in residential property investments in New York City by visualizing sales and rental market trends in major areas.Check out the app  and find the code .Sales and rental data are compiled from . The dataset used in this web app is combined using: Borough Neighborhood Date Condo Listing Median Price Condo Sale Median Price Rental Studio Median Price Rental One Bedroom Median Price Rental Two Bedroom Median Price Rental Three+ Bedroom Median PriceBoroughs in consideration are Manhattan Brooklyn Queens.The date of the data point ranges from Jan 2014 to Aug 2018 aggregated monthly.Boundary data of the neighborhoods are acquired from .Start by clicking on the ""Visualization"" tab on the side menu to go to the visualization page. Beware that it takes a little time to load so don't be startled by the error messages. Voilà!First choose a borough and neighborhood from the right side of the page.Then you will see that the neighborhood you selected will be highlighted on the map at the top of the page.On the bottom left there will be a brief summary of the market trend of property sales in your chosen neighborhood whether it has been increasing or decreasing. And it will provide a rough estimate of the percentage change in property value and value prediction for 2019 and 2023 (5 years). You can also opt to see an interactive chart of Listing and Sale price over the years.On the bottom right there is a chart showing the rental prices for different properties types whether it is a Studio One Bedroom or even Three+ Bedroom.You will notice a slider and selector below the location selectors it is used to set a budget and a room count so that the tool will calculate a rough estimate of your mortgage payment given 20% down payment  30year term and an annual rate of 5.04%.",NA,Residential Property Investment Visualization and Analysis Shiny App
https://nycdatascience.com/blog/student-works/which-industry-to-work-for/,1,"There are many factors that are important when choosing an industry to work in. Career growth opportunities worklife balance salary packages working hours and job security to name a few. But in any case It’s important to consider jobs that are indemand and recessionproof.Starting your career in growing industries will make it easier to find gainful employment build a valuable skillset and help you climb up that career ladder.In my work I tried to find most growing industries by analyzing PR News.What is PR(public relations) and PR News? According to the Public Relations Society of America (PRSA) PR is ""Public relations is a strategic communication process that builds mutually beneficial relationships between organizations and their publics.""The aim of public relations is to inform the public prospective customers investors partners employees and other stakeholders and ultimately persuade them to maintain a positive or favorable view about the company organization or its leadership. The most used tools in this field is news release. And That is called “PR News”.Growing industries creates more PR News than dying industries. So If I can count the number of PR News for various industries over the same time period I can find fast growing industries. And arguably that's the industry to work for.I chose  to achieve my goal since it covers an extensive numbers of industries and all the news in it is very well categorized.My scrapy spider crawled like as follows 1. collect name and recent news url in 3rd level(which is lowest level) of industry category2. for each collected industry go the the recent news list and get news title and release date.3. follow the previous news link and get the news title and release date until there is no news left.4. jump to next industry category and do the same.and results are as follows.This was my first and most import question. If not my entire scraping would be meaningless.And simple news count against industries plot showssignificant differences between industries.In top industries are Computer softwares Medical pharmaceutical Aerospace & Defenseand in the bottomThere are public safety Office products and Supermarkets.Result looks reasonable. Thus I can proceed to next analysis.To find best industry to work for I needed to find industries with upward trend of news count in time series. To get the time series data for each industries I had to unstack and resample the data.and simple plot of news count against date (example is blockchain industry) showsAt a glance I could not see any trend But there was periodic zero news count.  So I had looked into data and found thatObviously News is not released in weekend. After removing weekendsand trying different samplingI could clearly see that There is a trend. To get the trend for each all industries I used polyfit function from numpy.thrend_line function takes time series data frame as an argument and returns slope. If slope is greater than 0 then the time series data have an upward trend. After applying trend_line function to all industries I could plot trend against industries.Again I could see top industries (which means industry with most upward trend) and bottom industries. But What I really needed to see is industries which have relatively high news count and relatively high trend at the same time.To see the whole picture I merged dataframes for my question 1 and 2 and plotted it in 3d area.and it shows...If you watch 3d plot in realtime you can clearly distinguish that those 2 dots have high PR Count and High Trend.Those 2 dots are as follows.Those are the best industries to work for. It's very interesting that data analytics is the winner.I could find best industry based on my simple theory. But how credible is the result? I can not say ""highly"" because I could get data only just after april this year. For some reason PR News sites does not keep their news for a long time. So To accomplish analysis based on precise long term data Daily batch crawler preferably with database storage is needed.And from this project I could get 95459 rows of data which consists of category title and url for contents. Hopefully These data will be useful for my machine learning NLP study. Thank you for taking the time to read about my project!",NA,Which industry to work for?
https://nycdatascience.com/blog/student-works/spilling-the-beans-on-trade-coffee/,1, curated collection of seasonal freshly roastedtoorder coffees. I was curious what this new homedelivery service had to offer me and thought perhaps I could further investigate.470 products down to 452.An NLP analysis on the product descriptions and flavor profiles gave a picture on what products one might find on Trade Coffee. the NLTK package was used to process the words from all the product descriptions and flavor profiles. In order to get an informative word cloud I added words such as 'coffee' 'flavor' 'taste' 'varietal' into the dictionary of stop words that remove redundant and noninformative words from the description content.,NA,Spilling the Beans on Trade Coffee
https://nycdatascience.com/blog/student-works/which-new-yorkers-are-not-licensing-their-dogs/,1,The same information of the previous two maps can also be presented in a histogram where each bar is the frequency of dogs for each zip code colored by borough and the dark points represent the number of dogs per capita (100K people).,NA,Which New Yorkers are not licensing their dogs?
https://nycdatascience.com/blog/student-works/capstone/restaurant-recommendation-app/,2,"Modern consumers are overwhelmed with dining choices. With all the information and services available at a consumer’s fingertips it takes a lot for restaurants to stand out. Recommendation systems offer an effective way for lesserknown restaurants to come to the attention of consumers. Such systems allow consumers to explore the abundance of choices at hand while still catering to their particular interests. As a result they can enhance a consumer’s satisfaction and loyalty. Considering this we wanted to explore the two broad groups of recommendation systems: contentbased and collaborative filtering methods. Contentbased systems use the attributes of items and users to recommend items similar to those liked by the users in the past. In contrast collaborative filtering systems recommend items that were liked by people who were identified as having similar tastes as the user. We have also combined both approaches into a hybrid recommendation system using restaurant attributes and the users’ review history.Additionally we have developed an evaluation system for the userbased recommendation systems in order to be able compare them and choose the best approach. The chosen recommendation system was implemented in our app “OkFoodie!” that is now online to provide indecisive customers recommendations for two within the city of Las Vegas.For this project our team wanted to explore data provided by Yelp to create a recommendation system for two people with different tastes. The only task more difficult than one person picking a place to go to dinner is when two people need to choose a restaurant.The  is a subset of Yelp’s businesses reviews and user data from several cities. The data consists of:We chose to focus on Las Vegas because it had a large proportion of the businesses in the dataset with over 4000 restaurants.Our exploratory data analysis started with a thorough understanding of the Yelp users. The average rating given for businesses is 3.8 stars. Out of all the reviewers only 20% are writing the majority of the reviews most reviewers write fewer than 5 reviews.The Yelp Elite status is a way for the site to recognize users who are active in the Yelp community. Eliteworthiness is based on wellwritten reviews and high quality tips. Our team felt the reviews from Yelp Elite users should receive more weight when creating a recommendation system.The general process for the app as shown in the diagram below requires two users to interact with the app by providing their location and each person’s restaurant preferences from the local restaurant list. The model utilizes the reviews written for each restaurant and runs the text through a natural language processing (NLP) model to find the most similar restaurants based on their reviews. Next the similar restaurants are filtered based on location and other criteria taken from the businesses’ information contained in the dataset. Those recommendations are then ranked and finally returned back to the user within the app along with their locations displayed on a map. When you think about Yelp users you probably think of the users who write the reviews. However according a phenomenon in social media known as the 90/9/1 Rule the users that write reviews are only a small percentage of the overall population using Yelp. According to this rule only 1 percent of users will actively create content. Another 9 percent are users that observe and occasionally contribute. The other 90 percent observe without responding. In other words most people on Yelp don’t write reviews; they merely read them. Those Yelp lurkers represent the majority of the prospective users for our app.The first approach we chose was to offer recommendations to these users using a contentbased recommendation system. This approach uses a series of discrete characteristics of an item liked by the user in order to recommend additional items with similar properties. In our case each user would choose a restaurant that they like to form the basis of the recommendation. Examining the text of the reviews of the two selected restaurants would bring to light details that we would use to find other restaurants with similar characteristics.A precedent for this contentbased approach can be found in . The music site uses the properties of a song or artist to create a ""station"" that plays music with similar properties. The approach we followed to rank the restaurants for our recommendation system was to find restaurants with similar reviews to the chosen restaurants. In order to be able to compare the similarity between the reviews we have to follow the following steps: preprocessing vectorization and document similarity. The preprocessing of the text mainly includes:Once the text is preprocessed we vectorized the text (or make a numerical representation of it) by using word embeddings. The types of word embeddings can be classified into two categories: (1) frequencybased embeddings and (2) predictionbased embeddings.Finally once we have our vectorized text for each review we measure the document similarities by calculating the cosine similarity between the reviews of the business “liked” by our users and the reviews of all the other restaurants. When calculating cosine similarity the dimensions are the terms or words. The similarity depends on the orientation of the vectors. Two similar vectors with the same orientation would have a cosine similarity of 1 and two vectors oriented 90degrees to each other would have a cosine similarity of 0. Using the cosine similarity between the reviews the corresponding restaurants are ranked in descending order.For our recommendation system we have generated the TFIDF vectors and also generated a more sophisticated text document vector by multiplying the matrix of TFIDF weights by a cooccurrence matrix to obtain a TFIDF weighted sum of embedding vectors. In the resulting matrix the number of rows corresponds to the number of documents and the number of columns corresponds to the number of terms which were the top 300 words in each review. In order to be able to use these vectors as inputs in our collaborative filtering model we decided to reduce dimensionality. With PCA (Principal Component Analysis) we reduced it to 8 dimensions. We further used tSNE (Tdistributed Stochastic Neighbor Embedding) to project the vectors in 2D obtaining this graph in which each document is a dot. Orange represents good reviews and blue represents the bad reviews. Even though there is one large cluster we can see how the bad reviews are mostly located in the upper part of the cluster. These vectors were further used to feed into our collaborative filtering model along with the vectors generated by the Doc2Vec model.Another approach we used for analyzing the Yelp reviews in order to recommend restaurants was with a neural networkbased algorithm called Doc2Vec. This method allows us to compare the reviews based on context as well as the word representations.Doc2Vec an unsupervised NLP model generates vectors for a larger piece of text such as a paragraph or document independent of the text’s length. It is based on the Word2Vec algorithm that defines a vector representation for each word in the document based on its neighboring words to infer additional relationships between them such as antonyms or analogies. These word vectors are then combined with a paragraph or document vector that represents the overall contextual meaning of the document. Besides comparing the words contained within each review which may give information about menu items cuisine and service quality one advantage of the document vectors is that they can help infer other useful information about each restaurant such as its uniqueness reputation or customer base leading to more insightful recommendations for the two users.The process for comparing the Yelp restaurant reviews using the GenSim Doc2Vec module is fairly straightforward. All reviews are preprocessed to tokenize the text and punctuation is removed. Each review is tagged with its ID before training with the Doc2Vec model where 200dimension vectors are constructed for each review. Once they are trained we are able to use the vectors from the set of reviews for each restaurant to find similarities between the two original choices and the remaining restaurants in the chosen area. Using the median cosine similarities to remove any unusual reviews or outliers we can then rank the restaurants and provide personalized recommendations for the users. The paragraph vectors are also used in the collaborative filtering model as discussed in the following section.Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization techniques are much more effective than user and itembased systems because it uncovers the latent (hidden) features and underlying interactions which describe the relationships between users and items. In mathematics factorization or factoring consists of writing a number or another mathematical object as a product of several factors usually smaller or simpler objects of the same type. Matrix factorization algorithms work the same way by decomposing the useritem interaction matrix into the products of matrices with lower dimensionality. Let R be the matrix of size U (Users) x I (ItemsRestaurants) that contains all the ratings that the users have assigned to the items. Now the latent features would be discovered. Our task then is to find two matrices P (U – Users x К – Latent Features) and Q (I – Items x К – Latent Features) such that their product approximately equals R given by:R ≈ P x QT  ȒOur team used the Graphlab library in Python to build a model where you can feed various features pertaining to the users and items. The Factorization recommender for this library trains a model capable of predicting a score for each possible combination of users and items. The internal coefficients of the model are learned from the known score of users and items. The dataset used for training the model must contain a column of user IDs and a column of item IDs. Each row represents an observed interaction between the user and item. The user and item pairs are stored with the model so that they can later be excluded from recommendations if desired. It can optionally contain a target ratings column. All other columns included are considered side features.The GraphLab factorization model was fed a user table item (restaurant) table and review table. User and items are represented by weights and factors.Our team extracted features such as Elite users user with friends and users that provided at least 5 ratings from the Yelp dataset. The restaurant table included features such as kidfriendly restaurant atmosphere alcohol intimate touristy friendly and about 10 other features that our team felt were relevant to restaurant attributes. The review table included NLP vectors created from Doc2Vec and TFIDF for each review.The factor terms model interactions between users and items. For example if a user tends to love Mexican food and hate Italian food the factor terms attempt to capture these features along with items (restaurants) that are similar to the items that this user likes.The Factorization Machine recommender model approximates target rating values as a weighted combination of user and item latent factors biases side features and their pairwise combinations. The recommender targets rating values by utilizing stochastic gradient descent and alternating least square (ALS). The categorical features were turned into dummy variables and when fed into the model the matrix factorization predicts the user ratings (scores).When utilizing this recommender system for 2 our team generates a list of recommendations for each user with a score and rank. The two lists are then merged on the business ID and the users ranks are summed. The lower the score the stronger the recommendation for that particular user; rank 1 would be the best recommendation. The restaurant with the lowest sum (highest rank) is the restaurant recommended to both users.When implementing our exploratory data analysis and NLP vectors the results shown below demonstrate that adding more information (particularly the weight of the review through NLP) substantially improves the model’s ability to predict user ratings.As we collect more user and restaurant information the model will improve its ability to predict ratings and improve its recommendation capabilities.While the performance of our collaborative filtering models allows for a direct evaluation metric (e.g. RMSE) recommendations based on unsupervised models lack an immediate approach to evaluation. Nonetheless we sought out to identify a way to measure the success of our recommendations for unknown users. Our approach relies on the idea that despite lacking information for such users the selected restaurant(s) allows us to match them with existing users in the Yelp dataset. By matching a new and existing user based on the former’s input to our app we can develop a measure of how likely unknown users are to like the recommendations despite them resulting from unsupervised machine learning models.Our general approach is summarized as follows: Imagine two users who are unknown to Yelp supplying two restaurant choices to our app. At this point all we know about these users is that one user (e.g. User A) likes their choice (e.g. Restaurant 1) and the other user (e.g. User B) likes their choice (e.g. Restaurant 2). Despite lacking additional information about these individuals our project uses these choices to produce a recommendation set for these users. We evaluate the quality of our recommendation set by assuming the preferences of these unknown users can be approximated by existing Yelp users who have registered that like Restaurants A and B.Implementing this idea requires several steps. First we must deal with the sparsity of the existing ratings matrix. As most Yelp users only review a small number of restaurants we use collaborative filtering to predict users’ unknown ratings. Second we must also match new and existing users. As any two given Yelp users may rate restaurants very differently on average we initially demean the ratings matrix at the userlevel. The demeaned ratings are then used to identify the set of users that “really like” (or would really like) Restaurant A and also the set of users that really like Restaurant B. The set of existing users that are matched to the unknown user that selected Restaurant A are those for which this restaurant is among the top 600 restaurants in the ratings matrix. With approximately 3000 restaurants in our main sample this threshold corresponds to approximately a user’s top 20% of restaurants.The ratings of these comparison groups can then be used to approximate the probability that the two unknown users would like the restaurants our app recommends. For any given input from our app which is a pair of restaurants this probability can be estimated by simply computing the probability that the comparison groups would like the set of recommendations produced by the app. In general any given unsupervised approach can then be evaluated by developing a random sample of restaurant pairs and then computing the average probability that the unknown users would like their recommendation set. The probability that our top recommendations would be liked by the users is quite high: ~90%. However we did find that this probability can be influenced considerably by which thresholds are used. For example we found much lower success rates when we computed the probability that a recommendation set of the top 100 restaurants would be liked on average when the comparison group of users were only considered to have liked the restaurant if it were in the top 200 (of ~3000) restaurants. This point illustrates that while the collaborative filtering results can help us gauge the performance of our unsupervised recommendations additional work should be done to determine the optimal parameter choices.The app we nicknamed “” currently utilizes the Doc2Vec NLP model within the Flask interactive framework to provide recommendations for restaurants in Las Vegas. Users can allow the webbased app use their current location or they may enter their zip code so it can provide recommendations within 5 miles of the given location. The two users then enter their preferred restaurants hit Submit and the model returns a list of 5 restaurants based on similarities to both of the users’ original restaurant preferences. While we are able to provide restaurant recommendations based on similarities inferred from the Yelp reviews as well as with preferences assumed for the two users we know that there are many more steps to be taken to enhance the algorithm recommendation quality and user experience. Moving forward we expect that the model would be refined further to incorporate larger samples of the Yelp dataset and possibly the reviewers’ tips that are also available for each restaurant in order to improve accuracy and personalization. We could also begin adding other cities available from the Yelp challenge dataset at which point we also intend to scaleup the model for processing with Spark.Future versions may also include the option for existing Yelp users to log into the site so we may use their profiles to provide more personalized recommendations through the collaborative filtering model. User selections can also help optimize the model performance. Ultimately we anticipate this would be available as a mobile app allowing users to find restaurants when traveling based on restaurants they’ve liked in other locations around the globe.",NA,Recommendation App & Restaurant Decision Tool for Two
https://nycdatascience.com/blog/student-works/capstone/fashion-rec/,2,The goal of our capstone project is to build a clothing recommender system that given a user’s choice of  fashion blogger’s photo our algorithm will return clothes with similar style to that choice in a more affordable price range. For example If a user likes influencer ’s style our recommender system will suggest similar pieces of apparel from selected ecommerce or department stores available on our website ().Influencer marketing is a fastgrowing industry. An influencer is someone on Instagram who commands a lot of attention (likes views comments number of followers) and who has considerable influence over shopping choices made by his or her followers. According to  advertisers paid around $1.6 billion on Instagram advertising alone.Many influencers get paid by the brand that they are wearing when they post a photo on Instagram usually with the post mentioning the brand’s name. This is a common marketing technique deployed by these brands as they expect influencers’ followers to buy their products as well. Our recommender system might seem to work against these marketing campaigns but in reality we are simply bringing more choices to Instagram followers/consumers.We used an agile project pipeline approach to complete our capstone project. This involved training of the machine learning models and the development of flask application simultaneously across group members. Below is the workflow of combining all the components together: We have ~10000 product information from retailers and ~1000 IG information from influencers in our dataset. The median prices for products range from ~$150 to ~$20 across the retailers as shown below. The majority of product items came from ASOS based on the affordable price range as well as the diversity of clothing styles.  The total Instagram posts per influencer collected over the past six month is shown below. On average the frequency of posts range from ~5 posts/month to ~ 20 posts/month.The median number of likes per instagram post reflect the popularity of each Instagram influencer. Note: Kylie Jenner is not included in the graph below in order to better visualize the median likes for other Instagram influencer. Kylie Jenner has ~ 6 million median likes per post.The high number of likes for bloggers in our database indicate many people are attracted to their outfits. The recommender system will be a good platform to let users discover their favorite blogger’s style and find apparels similar to it.NLP analysis was applied on the item descriptions of the webscraped products to uncover the current market clothing trends. The NLP discovered styles were used to partition each blogger’s fashion styles that are currently available in the market.Predictionbased methods and frequencybased methods are two common methods of NLP. Frequencybased methods assume words in a text are independent from each other and only the occurrence of words in the text are taken into account. In contrast predictionbased methods take the cooccurrence of words into consideration which offers a clear  advantage in dealing with text with strong interwords relationship. We performed both predictionbased and frequencybased methodologies  in our project to evaluate which method provides the best result. For the predictionbased method two approaches  Word2Vec and Doc2Vec  were used to generate vector representations for each item description followed by kmean clustering to group the products into different style categories based on vector distance. For Word2Vec analysis word vectors were obtained from a pretrained Word2Vec model (common crawl 42B 1.9M vocab and 300 dimensions https://github.com/stanfordnlp/GloVe). The word vectors within an item description were averaged to generate a single vector that represent a single item description. As for Doc2Vec corresponding vectors for each description was generated based on a Doc2Vec model using Gensim that was trained on words from our item text descriptions. Using vectors from Doc2Vec resulted in item images that appear more similar among the top 10 most similar vectors. As a result we decided to proceed Kmeans clustering with vectors generated from Doc2Vec. The item descriptions were separated into 6 different groups using Kmeans clustering based on cosine distance between vectors. Predictionbased method did not perform well for this case as the tSNE graph from Kmeans clustering did not show clear separation of clusters as shown below. One explanation could be product descriptions are made of key phrases instead of full sentences with strong context.On the other hand a frequencybased method specifically Latent Dirichlet Allocation (LDA) showed better result. As one of the popular topic modeling algorithm LDA takes all the words and their counts in each document as input and attempts to find the structure or topics in this collection of unlabelled documents. Topic modeling assumes that word usage is correlated with topic occurrence. Each topics is a combination of different words with different weights and each document is a mixture of different topics. In our case the documents are item descriptions and topics are different fashion styles described by different keywords. For each product we chose the style with the highest score to represent it. One tricky part of this method is that we do not know how many styles are covered in the retailer data we collected. We tried several different numbers and found 6 might be a reasonable choice when we saw the keywords in the topic. One thing worth noting though  is that this is not an absolute answer for this problem. There are other possible ways to do the clustering and this is the challenge and beauty of unsupervised clustering problems. After we grouped those products we applied tSNE to better visualize the results. tSNE can reduce high dimensional data set into lower dimensions so that we can see it in a 2D graph. In the graph below all the products are separated into 6 groups with different colors. The keywords of each group are shown in the upper left corner. As shown below instead of simply separating them into dresses or tops the algorithm successfully grouped all these clothes into 6 styles by features. For example ‘print floral’ ‘straps bodycon’ and ‘soft touch stripe’. These keywords have more information and better represent the stylish features of those products. We applied the deep Convolutional Neural Network (CNN) algorithm with pretrained imageNet (VGG16) to perform a multicategory classification on the million fashion images with labels provided by the recent Kaggle Competition (iMaterialist Challenge (Fashion) at FGVC5) to obtain our first version of image recognition model (classifier). Combining with the cosine similarity measurement the algorithm can recommend similar items to online shoppers. The training dataset includes images from 228 fashion attribute classes with multiple ground truth labels per image. It includes a total of 1014544 images for training 10586 images for validation and 42590 images for testing. For this project we didn’t use any of the testing images from the Kaggle dataset.The overall ROC and precisionrecall curves on the validation set. Overall performance of the model is pretty good. However there are also some labels didn’t perform well. We are currently looking into those classes and seeking a way to fine tune the algorithm to improve the model performance. Nevertheless we would like to see how does the model perform visually as the next step. That’s why we start to deploy all our models into the Amazon Web Services.The end product of our project is the FashionRec (link goes here) website hosted on AWS using Flask. The first page contains images of influencing fashion bloggers for user to choose(e.g. Chiara Helena Bordon).Once the user clicks on any of them the website will be redirected to another page containing pictures of this specific blogger. The algorithm will map each bloggers’ style into 5 or 6 current available clothing trend group according to the NLP analysis. This will ensure that users can have as many and as diverse choices as possible. For example in the picture below this blogger has six diverse styles like floral dress casual shirt and formal suit. If the user clicks on any of these pictures the website will send back the url of the clicked image to the server and look for similar clothes in our database. The criteria that we are using to quantify the difference/similarity between two clothes is the cosine distance calculated on the CNN model outputs. The higher the similarity score the more similar are the two clothes. For example If you select ‘Style0’above the system will automatically generate the similar dresses that are availables in our product database. The user can continue clicking on items under similar products and the system will keep providing more similar dresses based on user’s preference.To enrich the user’s experience we also provide a list of the most dissimilar clothes to users while they are shopping. That way if they want to try out a different style they can easily add some new clothes in their shopping cart.Please check out our website: ,NA,Fashion Rec.
https://nycdatascience.com/blog/student-works/capstone-mapping-brands-to-transactional-data/,2,Our capstone project aimed to help a debitcard startup which “empowers teens to make the best financial decisions.”  Through an easytouse app teens can gain financial freedom while parents can achieve peace of mind over their kids’ spending habits.  Through this project we helped the company efficiently and accurately predict the merchants used in each teen’s transaction. The ask was to map at least 50% of the data with 90% confidence.  At present merchant data exists in long convoluted strings composed of a mix of brand name address store number and random characters. Predicting the right brand based on the provided string will help this company best serve its clients; it will enable parents to restrict  their teens to selected sellers for purchases provide more granular reporting of specific brands and add features for other prospective clients. Additionally this will save them time through automating brand prediction and improving the data asset of the company.To tackle this problem we sought the best models to handle multiclass classification imbalanced classes and categorical features including text.  Upon cleaning the data engineering features and balancing classes we implemented Naive Bayes and Multinomial Logistic Regression models. The following sections walk through our process to optimize our predictions.Our data contained a limited number of fields three predictors (“merchant string” “merchant category” “network”) and the response variable (“mapped brand”).  We had over 300000 observations which we needed to map into over 500 categories. Many of these categories were infrequent and could have just one entry making imbalanced classes something to address.  On the other hand we had one very frequent class (ATM) comprising roughly 60% of the training data. After ATM the twenty brands that appeared most often amongst labeled data included the ones that appear in the graph below:To clean the merchant strings we first applied common NLP techniques including tokenizing adjusting to lowercase removing stopwords and applying regex on certain number combinations.  For example through this process we converted a string like “CHICKFILA #02241” to “chick fil a”. String cleaning posed a difficult challenge of removing irrelevant portions while preserving patterns important to brand recognition.  We created a cleaning function that easily incorporated various parameters (for instance what to split on what words to remove) to enable simply rerunning to optimize the cleaning for different models.One new feature type we created stemmed from concatenating cleaned merchant string and merchant category (“mcc”).  This feature engineering increased our models’ score from 90% to 95% a significant marginal improvement. Naive Bayes is a probabilistic supervised classification method that is particularly useful in handling various categorical variables with many possible values.  It also is able to pick up on small effects that can add together to have meaningful impact. We utilized multinomial naive bayes to predict brand based on frequency.After cleaning and feature engineering we used the bagofwords approach via CountVectorizer to count word frequency throughout our merchant strings.  We opted to include ngrams as part of the vectorization to increase our model’s ability to recognize brand with multiple words (like “dollar general”).  This process created the sparse matrix which we fed into the model as its predictors.In order to train Naive Bayes on more data we created a dictionary that linked mapped brands to common substrings within the merchant strings and used it to fill in some of the unlabeled data. Once we optimized the dictionary we were able to label 53.5% of the data. The accuracy of these labels on our training set was 99.6%.To view our performance and continue to improve we graphed the Naive Bayes model prediction accuracy (on a holdout set of the labeled data) against the Naive Bayes confidence level.  While we recognize that accuracy is not the best metric of a classification model since it gives equal weight to false positives and false negatives we started by using this metric to improve our model and considered other metrics later in the modelling process.The graph allowed us to isolate jumps where the model accuracy improved greatly as we examined groups of more confident predictions.  When we saw these jumps we isolated the batch of incorrect predictions and improved our processing accordingly (cleaning and/or modeling).  In the example below we added a feature to our data set that indicated if the mcc was not available. This decreased our model's predictability which chose 'ATM' (the most frequent label) over other labels when mcc was not available.  Reprocessing our data ameliorated these misclassifications smoothing the below curve.We also built a multinomial logistic regression model to predict our various labeled brands.  Multinomial logistic regression is a supervised model that fits a softmax function to a multiclass categorical response.  The model produces the probability that each observation belongs to a particular class an advantage over over other classification techniques.  After cleaning the mapped brands we built a function to extract the most common words.  Each word then became a predictor in our model representing whether the merchant string contained that specified word or not.  These words can be visualized below though it was an iterative process to gain the cleanest list:Thereafter we balanced the classes trained our model and predicted the brand corresponding to each observation. We then gauged how correct our predictions were on our test data using a dictionary as a proxy.The accuracy of our Naive Bayes model on the training data set was 97.9%. This decreased to 95.3% when looking at a holdout set of labeled data. We achieved that same 95.3% accuracy level on our Multinomial Logistic model on the training data set. We further examined model performance by looking at ROC curves for some of the top brands and comparing the results of the Naive Bayes to Multinomial Logistic. ROC curves helped us consider sensitivity and specificity which are important metrics beyond accuracy.For both McDonald’s and Starbucks the AUC of the Multinomial Logistic Regression was higher than for Naive Bayes. The reason Naive Bayes performed more poorly with identifying Starbucks transactions could be because when the term “store” was in the merchant string Starbucks was often incorrectly predicted. This is due to the independent nature of Naive Bayes: given the term “store” was often coupled with “Starbucks” the Naive Bayes model would predict Starbucks when “store” appeared. The Multinomial Logistic model on the other hand would consider all the most common words we fed into the model instead of giving an inordinate amount of weight to a single word to predict the mapped brand.The Multinomial Logistic model performed better on predicting “McDonald’s” than the Naive Bayes model for a similar reason. The hanging ‘s of “McDonald’s” was always coupled with “McDonald’s” so Naive Bayes would predict McDonald’s when “ ‘s “ was present. The Multinomial Logistic model gave this attribute less weight.Last we looked at how well the models predicted Target transactions.Both models had an AUC of 0.6. This was one of the lower AUCs among the various brands. It’s possible the model doesn’t predict Target transactions as well because Target is a more generic store with associated  transactions that could easily fit other brands. Consequently some Target transactions are incorrectly labelled as “dollar general” or “paypal”.We provided the company with a model that can take one or more observations in realtime and offer a predicted brand.  We reduced the prediction time to ~2 seconds after storing the fitmodel as an object.The code is packaged into an organized pipeline enabling the company not only to run the prediction quickly but also to easily view the files supporting the Naive Bayes and Logistic Regression model fitting. The predicted output of our final product was an ensemble of both Naive Bayes and Logistic Regression.This tool will achieve the company’s goal of quickly determining the mapped brand of a transaction to enable parental supervision when teens use their debit cards. This will both improve the customer experience and save the company a significant amount of time by reducing the need to physically look up each transaction and map it to a brand.,NA,Capstone: Mapping Brands to Transactional Data
https://nycdatascience.com/blog/student-works/forecasting-cryptocurrencies-price-trends/,2,"A cryptocurrency's price is mainly influenced by security problems of the blockchain technology new policies of governments (for regulation or boosting) and public opinion from news and forums. The plot below describes that the price trends of Bitcoin (the first cryptocurrency and the one with largest market capitalization) and the frequencies of key words from news. You can see the period from late 2017 to early 2018 when the price of Bitcoin steeply went up and suddenly collapsed.  The cryptocurrencies are increasingly adopted as a means of payment in real life. Thus each government has been considering various regulations in different ways.  In case of US government was basically in the position to regulate cryptocurrencies within the framework of existing financial regulations. As several large asset managers consider investments in the cryptocurrency market the government is trying to tighten regulations for financial supervision. In case of Japan cryptocurrency transactions and exchanges were prevalent during early days of cryptocurrency. So the government has created policies and refined them since 2014. Similarly Singapore government also has refined policies about cryptocurrency. The government defined the cryptocurrency as “Good purchased product for purchasing goods”. And they created specific tax policy imposing on transactions of cryptocurrencies since 2014. On the other hand China and South Korea have considered several regulations such as prohibition of exchange of cryptocurrency and some of the regulations have been implemented. Of course the complete prohibition was impossible.
Currently the market capitalization of cryptocurrency is 199 billion dollars. (for comparison the 2017 US Defense Budget was 590 billion dollars)
 The cryptocurrency has not only negative side but also has positive side. On the positive side it can be an alternative to the existing financial system. In case of XRP (as known as Ripple) They entered remittance business by taking advantage of the fact that there is no commission fee for oversea transactions. On the negative side the cryptocurrency still has security problems. One of the largest exchange Bithumb was hacked and 31 million dollars were stolen. Bithumb is an exchange ranked 6 in the world.Then the centers (mean) of each of the 3 clusters were computed and their relative distances were evaluated by cosine function: 1 cos(𝞱) taking values in [0 2]. Here we can see from the heated map that each of the 3 clusters is perfectly collinear with itself (distance  0) cluster 1 and 2 are mostly anticollinear (antiproportional) while cluster 0 is mixed. Then we labeled 3 clusters as D (down) M (mixed) and U (up) based on the corresponding position of the cluster center then computed the possibilities of D/M/U of the next day based on today’s label. We can conclude that for each case the prediction of M is always the majority case.Then we continue to use PCA to visualize the division of the 3 clusters as shown above including the compositions (weights) of the first and second principle components (PC) of each of the 8 features (cryptocurrencies). The scatter plot shows how the ~3000 points (days) in colors are determined by the 1st and 2nd PC. The boundary is not parallel to either axes indicating both PC’s are import.We considered two different architectures for the LSTM network. The first one considered the entire Bitcoin price history as a long chain: the LSTM network would remember all the intermediate states. The second one only considered rolling windows of a fixed size: the LSTM network would start over with clean states for each window. The graphic representations of the architectures could be found below:For interested readers our codes and notebooks could be found .",NA,Forecasting Cryptocurrency Price Trends
https://nycdatascience.com/blog/student-works/predicting-house-prices-in-ames-iowa-using-machine-learning/,3,"There are several factors involved with the sale price of a house. Some things that come to mind are size number of bedrooms year built neighborhood proximity to grocery store etc. In the Kaggle Competition: “House Prices: Advanced Regression Techniques” created by Dr. Dean De Cock our team explored a multitude of variables to further understand what influences the value of a house and how we can use these variables to predict the sales price given specific characteristics. 
The goals for this project were:The first thing after getting this dataset is to explore the data. This dataset contains all kinds of data type: numeric ordinal categorical and binomial. The entire data set (training + test) has 34 variables with missing data 13965 values out of 115340 values (12%). Some of the variables such as ‘PoolQC’ ‘MscFeature’ ‘Alley’ and ‘Fence’ have more than 80% data missing. The imputation of those columns could be a huge challenge without any leads. The natural way is to find how the original data set says about those variables. The data description has specific definitions for many of the ‘NA’s in the data set for example ‘NA’s in ‘Alley’ column simply indicates that there is no alley for the house and ‘GarageArea’ would be ‘NA’ too if there is no garage in the house. Therefore by looking at the data description 15 categorical and 10 numerical features can be imputed by either ‘None’ or number 0 depending on the original data type of the feature.After this round we only have 3425  9 columns left to be imputed. If we look at the numbers of the missing data among those features 8 of them are only missing less than 4 values which can be imputed by the most frequent values in their corresponding columns. Now the only feature left here is ‘LotFrontage’ the linear feet of street connected to property. Since there are 486 (17%) data missing a cautious way to handle it is to use kNN to help determine the values with the consideration of all other features including ‘Neighborhood’ and ‘LandContour’ that may have bigger influence on the street around the house. However in order to use kNN we have to wait until all other categorical features to be properly converted (labeled) with numbers.There are 79 features (36 numerical + 43 categorical). We examined each of them and divided them into 4 cases: A B C and D to handle them differently. Case A indicate 33 numeric features that do not need any further transformation. They are mainly area in square feet or quality levels that are already in ordinal numbers. Case B however are 3 numerical features that require closer look. By definition ‘MSSubClass’ is using integer numbers to indicate certain house types. Therefore they are actually categorical values. We converted this feature to categorical > numerical by sorting out their corresponding average ‘SalePrice’. ‘SalePrice’ does not have monotonic dependence on ‘MoSold’ or ‘YrSold’ either. So we also converted them in a similar way as ‘MSSubClass’.Case C consists 25 categorical features that are evaluating qualities or conditions. They can be intuitively labeled by ordinal integer numbers. Some of them such as LandSlope’ ‘LandContour’ and ‘Functional’ are also showing specific trends regarding certain house conditions. So they were also labeled based on their direct meanings.Case D includes the last 18 features with categorical values that are not showing obvious dependence with ‘SalePrice’ by the first look. For example it is hard to tell how the ‘RoofMat1’ ‘GarageType’ ‘SaleType’ or ‘Neighborhood’ would influence the house price. Instead of ‘onehotlabeling’ or ranking them based on value frequency we again use their corresponding average ‘SalePrice’ to sort and give them integer labels as we did to Case B.With all the other 78 columns filled and properly converted to numerical values it is time to revisit the last column with missing data. We chose to use Manhattan distance to impute ‘LotFrontage’ to reflect more subtle details from other data points and to reduce the influence from larger distance dominating the imputation results. kNeighbor  5 was determined by the R^2 resulted by a series tests of k numbers.
Three linear models were tested and used to predicted on the training data. While Ridge Regression tend to keep all the 79 features Lasso Regression dropped 24 features as unimportant. After the grid search Elastic Net Regression consisted 91% of Lasso. In all cases the ‘OverallQual’ ‘OverallCond’ and ‘GrLivArea’ are the three features that show most import positive influences on the ‘SalePrice’.Besides linear models two tree models Gradient Boosting and Random Forest as well as a nonlinear model Kernel Ridge Regression were also tested and used for house price prediction. The corresponding training error and Kaggle Public Board error are shown below. The training and PB errors show that overall our Gradient Boosting and Kernel Ridge models are making the best prediction linear models show slightly larger errors and that the Random Forest model produces largest errors. Therefore we chose Gradient Boosting (GBoost) Elastic Net (ENet) and Kernel Ridge (KRR) Regression for further model stacking.Before running into model stacking we examined the actual comparison between the predicted and true ‘SalePrice’ values along the y  x line for our three best models. ENet and KRR show similar data distribution features despite the fact that ENet has bigger errors at high price region mainly because they are both using l2squared error in the loss function. One of the main sources of our linear models is rooted in the multicollinearity: We didn’t combine strongly correlated living/basement area features. Comparing with the other two GBoost did better overall predictions but it still made several predictions that are far deviated from the y  x line. Therefore further model stacking is needed to help adjust the differences between models to give more accurate results. A simple averaging between the predicted ‘SalePrice’ among three models has already reduce the training and PB error significantly.Our stacking approach followed simple “greedy manner”. Since we only have three starting models there are only three choices for the metamodel when picking two as bases. Our results shows that the metamodel of ENet trained by stacking GBoost & KRR gave the lowest errors which are lower than the simple averaging results. For the model ensembling we again apply the greedy forward approach: picking up the best model linear Lasso Regression among other candidates. With a simple ensembling formula of 3 quarters of metamodel + 1 quarter of Lasso our final RMSLE PB error was further lowered down to 0.11718 among the top 600 results. Since the main source of our linear models came from multicollinearity our future improvement in the data preprocessing part will be including:Meanwhile for the modeling part:",NA,"Predicting House Prices in Ames, Iowa Using Machine Learning"
https://nycdatascience.com/blog/student-works/predicting-housing-prices-in-ames-iowa-2/,3,     As a first step to finding a suitable model to relate the sale price of houses to the other variables we explored the data to get a better sense of the different features present and how to handle them. One of the first things that we noticed was the histogram of the target variable sale price when plotted is clearly not normally distributed.     The plot has a distinct rightward skew. Such a skew is not surprising since you would expect there would be more houses with higher than mean prices than otherwise. Given this information if we were to do a linear fit of the sale price we should log transform the data first so that the column is actually normally distributed.     Next we investigated the correlation between the various features and sale price. The top ten features with a correlation of 0.5 or higher with sale price were plotted and we noticed many of these features were not only highly correlated with sale price but also with each other. As we do feature selection and engineering later on this is something we should keep in mind.     Continuing our investigation of the dataset we looked for missing values and figured out how to deal with them. The first figure is a graph showing percentage that each column is filled(not null) and the second is the actual number of missing values for each column. As we can see there are quite a lot of missing values in several columns. In general we classified them based on what the missing value represent and impute them accordingly.     The majority of the missing data correspond to “No such Feature” and we impute ‘None’ or 0 for them.  With the other missing values we took a more granular approach. For the missing numerical features we grouped the data according to its corresponding Neighborhood and then imputed the mean or median for the Neighborhood whichever seem more appropriate. This is reasonable as we expect houses in each neighborhood should share similar features. For the missing categorical features we grouped the data similarly but imputed the mode of the Neighborhood instead. With these steps we were able to take care of the majority of the missing values. There were a few special cases which we handled individually such as the “Garage Year Built” which we imputed the same year as the house built year and kitchen quality which we scaled according to its overall house quality instead of going by neighborhood average.     For feature engineering we began with the simplification of features.  Because many features are related to each other and are highly correlated as shown in the previous chart we condensed many columns into one.  Altogether we built five new features:      After noting that several of the variables such as Ground Living Area showed a mostly linear relation to sale price we decided we were going to use linear models to fit the dataset. We were unsure whether it will be the best method but we wanted to give it a try.  Due to this fact we need to scale our data as well as transform our categorical values to dummy variables. For this we simply used ScikitLearn’s “Standard Scaler” method to scale the data (subtract the mean and divide by the standard deviation) and Pandas “Get Dummies” method to one hot encode our categorical features.  As mentioned earlier we also performed a log transformation on the target variable to normalize it.Below is a list of methods we used:The results are summarised in table below. A detailed discussion of each model will followed.     The linear models we tried were regularized models such as Ridge Lasso and the ElasticNet regressions. Based on the general linear trend among the target and the predictors as mentioned earlier we expected the linear models to work fine with the data. Since the data had more than 200 features and we do not have an exact way to choose them according to their importance for predicting the house price it would be difficult to use the general linear regression models. We decided to directly try the regularized regression models so we can select the meaningful features for the prediction mitigate overfitting and overcome multicollinearity problems at the same time.      Because Lasso and Ridge regressions put constraints on the size of the coefficients associated to each variable which depend on the magnitude of each variable standardization as we mentioned before was necessary. Second we removed the outliers which were significantly far off from the linear relationship between the target and some of the main predictors as shown in the plots below. Although the outlier removal might have caused information loss we saw that it did improved the performance of the models when comparing the results from before and after their removal.     In the plots comparing our prediction from the Ridge/Lasso models to the original target all the models seemed to agree pretty well. All the models got R     We also used the Lasso method to generate the coefficient plots below showing the importance of the different variables. In the plot the later a variable turns to zero the more it affects the target. The two variables that most influence the model are Total Square Ft. and Overall Quality. Other important variables are the Ground Living Area Year Built and Overall Condition.     Another way to view which feature importance is shown below. The top20 variables ranked by magnitude of the coefficients from our best lasso model is plotted showing the same variables Total Square Ft Overall Quality etc. affect the house sale price the most. One should note that in general the size of the coefficients may not be an indicator of feature importance. But since we have scaled all our variables we can use this metric as measure of feature importance more readily.     After trying the Ridge / Lasso based linear models we tried the SVM based regression to see if we can use a different model to get results that are just as good or better by using parameter tuning methods such as grid search and crossvalidation (CV). For experimental purposes we first tested SVR without parameter tuning then obtained benchmark results with parameter tuning. We recorded the RMSLE benchmark using a 5fold CV done on the training set Kaggle leaderboard score and finally the computational time of each configuration. Below is a table summary:     The results we obtained through SVR showed us several key points. First the choice of kernel in the SVR model plays a critical role in all three statistics. The linear kernel produced extremely small RMSLE even before parameter tuning indicative of severe overfitting. While the Gaussian kernel had relatively large RMSLE but actually showing an improved Kaggle score over the linear kernel.     The next trend we observed was that in general SVR training time increases by at least one order of magnitude when we used GridSearchCV as a parameter tuning framework. This suggests that in projects involving larger datasets one is advised to first run the model without parameter tuning as a benchmark as model performance based on different kernels correspond well with performance after parameter tuning. For example the RBF (Gaussian) kernel achieved the best Kaggle leaderboard result both before and after tuning. Conversely Poly and Sigmoid kernels performed poorly both before tuning and after tuning.     The last conclusion we can draw is that the 5fold CV benchmark on the training set for different model kernels is a good indicator for the Kaggle performance of the kernels. If a model performs well under the 5fold CV benchmark it is likely to perform well in the test set as well.     Due to the high number of categorical features we felt the next best course of action would be to train a Random Forest model because of its inherent resiliency to nonscaled and categorical features. It would also allowed us to Even though it may not be the most time efficient process we implemented a Grid Search Cross Validation method to tune for the best hyperparameters.  We started with a fairly coarse grid search tuning over large gaps in the parameters and ended with a very fine search to hone in on the best parameters.       To test the usefulness of these hyperparameters we also modeled a base random forest estimator using just 10 trees and the rest as default settings.  With this base estimator we achieved an accuracy of 99.20% with an average error rate of 0.0954. Our tuned model achieved an accuracy of 99.26% with an average error rate of 0.0888.  With only a 0.06% increase in model accuracy in most cases it would not have been worth it to spend the time tuning especially for large datasets. This shows that our hyperparameter optimization process is not as efficient as it could be.      As we completed our analysis of the dataset we thought of ways that we can improve our model. One idea that we discussed but did not have time to implement was to perform some sort of classification before doing the modeling. We could add our own classes or groupings as variables and check feature importance to see if and how our models changed based on this new variable. These classification can even be done with unsupervised methods such as clustering to discover hidden groupings within the data and utilize them as new variables. Finally we could have use ensemble methods to combine our models to obtain the best results.      In conclusion this is a basic analysis of the dataset using relatively rudimentary modeling techniques. Given the relatively simplicity of the data despite the large number of features it is not surprising that we obtained the best results with our linear models. With more time and now a greater understanding of what other modeling processes are out there we feel that a much more in depth analysis and subsequent modeling process can be done.,NA,"Predicting Housing Prices in Ames, Iowa"
https://nycdatascience.com/blog/student-works/kaggle-competition-the-strength-of-linear-models-in-predicting-housing-prices-in-ames-iowa/,3,The Ames Housing Dataset was introduced by Professor Dean De Cock in 2011. It contains 2919 observations of housing sales in Ames Iowa between 2006 and 2010. There are 23 nominal 23 ordinal 14 discrete and 20 continuous features describing each house’s size quality area age and other miscellaneous attributes. For this project our objective was to apply machine learning techniques to predict the sale price of houses based on their features. Here is the procedure we followed to tackle the project.We coded everything in Python using classes. This made it each for group members to add methods into classes. Also this kept our code very organized.Let’s first go into how we handled our missing values in this dataset. Below see two visualizations that show which parameters had missing values. The nullity correlation matrix on the right allows us to see which values are missing in correlation with each other (and not the correlation of their actual values). For most of the missing we imputed “None” or zero. These were generally descriptive parameters for parts of the house that these particular houses didn’t have. For the more random missing variables we imputed either the mode or the mean. The mode we used because these variables generally had a distribution like Electrical shown below where a huge portion of the observations fall into one category. We used the mean for unfinished square feet in the basement because it is a continuous variable.Caption: A visualization of the missing values (left) a nullity correlation matrix (center) the distribution of categories in the parameter Electrical on of many parameters in which we imputed the mode into the missing values since 80% of the values fell into one category (top right).The last parameter for whose missing values were imputed was ‘LotFrontage’ or the linear feet of street connected to the property which was missing in about 17% of the houses. For this variable we tried two different methods: extrapolate based on its linear relationship  with ‘LotArea’ (sq ft) and extrapolate the median ‘LotFrontage’ from the neighborhood of each missing value (See the graphs below).Both the log lot area and the median by neighborhood methods improved our model by an equivalent amount. We finally chose the latter for our models. To start off our EDA we looked at how housing prices changed over time. One might expect for example a dip in prices in 2008 based on the financial crisis in that year. Looking below we can see that there were slightly fewer homes sold in 2008 compared to other years. However on the right we can see that the median sale price remained pretty constant over the five years. We also looked at the influence the month of the year (averaged over all five years). You can see there are many more houses sold in the summer months but again the median house price stays the same over these months.Caption: Because we had no values after July 2010 we calculated 2010 sales as if the sales continued on the same trend as before July 2010.Feature engineering is a crucial step in the machinelearning pipeline. Starting with categorical features we used  to perform “binarization” of the categories. This method worked best with linear models since it doesn’t introduce false relationships between the categorical features.  The second method was  which assigns a unique ID to each category based on the number of values within each category. This method worked best for treebased models since it doesn’t increase the dimensionality of the data set. The third method used to deal with categorical features was applied only to features with ordinal nature where we assigned a numbered dictionary to preserve the unique relationships of the categories. We decided to keep the combination of OneHot Encoding and ordinal features because it improved both our linear models and our combined models.For all 19 numerical features with skewness greater than 0.75 we performed BoxCox transformation. As for our target variable sale price it was right skewed. We performed log transformation on the sale price in order to improve the normality of the distribution. These transformation were especially effective for linear models.Area is a feature with big influence on house pricing so it was important to create a new area feature by adding the living area and the basement area. As the chart shows the new feature had good correlation with the sale price. Both marked points represents houses with large areas and low sale price. Upon further investigation both of these observations were houses located in the Edwards neighbourhood one of the lowpriced neighborhoods. About 50% of their square footage was from their basement area and both of these houses had unfinished basements. It’s possible that if their basements were finished they would sell for 50% more given the increase in usable living area. Based on this information we decided to remove these two observations to prevent the outliers from interfering with the modeln results. Removing there two outliers improved the scores of our models.To evaluate the fit of our models we used root mean squared error (RMSE) for the log of the sale price the metric used by Kaggle to evaluate submitted predictions. As a validation strategy we did cross validation with K5 using 4 parts of our data for fitting our models and one part to obtain predictions. For our individual models we trained the following: Multiple Linear Regression Kernel Ridge Regression (KRR) Lasso and Elastic Net(ENet) as linear models then Random Forest Gboost and XGBoost as treebased models. Finally we combined our individual models by stacking and ensembling. First we tried the simplest stacking approach by averaging base models. Then we added a “meta” model. In this approach we used the out of folds prediction of the base models to train our metamodel (in this case Lasso). Next we added XGBoost and Random Forest to the previous stacked averaged models by performing a weighted average calculating the beta coefficient with a Multiple Linear Regression.Considering that regularized linear models give less weight to less important features by assigning them a lower coefficient and treebased models by setting  a lower splitting preference we decided to use all the features to train our models allowing them to select the most predictive features themselves. We then noticed that while adding features some times improved our scores removing features usually reduced our scores. Therefore we decided to keep the previously described feature “Total SF”. Interestingly exploring the feature importance of our Random Forest we could see that Overall Quality (“OverallQuall”) has the highest weight by far even though it is not the last feature to shrink its coefficient to 0 in a Lasso model when increasing lambda. The same hold for other features such as “TotalSF” (feature created by us) and “GarageCars” confirming that tree models and linear models have different preferences when it comes to feature importance which is why we did not use any of these models to “prescreen” the features for the other models. In the figure below the left panel shows the top 20 features ordered by feature importance in our Random Forest model the middle panel shows the coefficient of the top 20 features correlated with Price Sold variable in our Lasso model and the right panel shows the coefficient of the basement related features with increasing lambda in our Lasso model. The red line is our chosen value for “lambda” selected by Bayesian optimization.Overall our models had values between 0.11697 and 0.12425 RMSE (Kaggle score) the highest crossvalidation RMSE (or worst score) was for Random Forest followed by Multiple Linear Regression. To our surprise our linear models performed very well. The Lasso one delivered  our highest score for individual models surpassing the boosted tree models Gboost and XGBoost. A simple average of the prediction of Gboost Enet and Lasso improved the score of the individual models taking us from 0.12109 to 0.11697. Adding ENet as a meta model the base models Lasso Gboost and KRR gave us a slightly lower score (0.11702) compared to the simple averaging of models. Finally adding to this stacked model two extra models (Random Forest and XGBoost) lowered our scored to 0.11858. Interestingly our best performing combined model was a simple average of the predictions which ended up being our best score  (0.11697 top 14% of the public leaderboard as of today)To summarize this project demonstrated the strength of linear model over treebased models when the target variable is linearly related to the predictors. We have shown how onehot encoding ordinal encoding boxcox transformation and parameter tuning improved our linear models. As a result Lasso model was our higher scored individual model. A simple average of the prediction of our best simple models gave us our best score in exchange of losing model interpretability. Depending on the specific need of the project simple or combined models have different advantages. For a Kaggle competition where a 0.0001 score improvement is an great achievement combined models work very well taking advantage of the strengths of different models. However when we build a model both to make a prediction and to get information about the response variable linear models have an extraordinary predictive power that can be explained and understood.,NA,"Kaggle Competition: The Strength of Linear Models in Predicting Housing Prices in Ames, Iowa"
https://nycdatascience.com/blog/student-works/predictive-modeling-on-house-prices-ames-iowa/,3,The goal of this project was aimed to utilize supervised machine learning techniques to predict the price of houses located in Ames Iowa. This dataset was provided by  a very popular website for data scientist come to compete and test their skill and knowledge. This dataset provided around 80 different features including multiple aspects of the house that would help or may not help predict the fluctuation of the house prices. The strategic approach our team adopted was to derive meaning from the dataset through various analytical graphs statistical methods and then to apply different supervised machinelearning algorithms to predict the house sale price.OutlineThe dataset contained two csv files (train.csv test.csv).The training set had 1460 observations and the test set had 1459 observations. The only key difference between the two sets was the absence of the column of sale prices in the test set. In order to predict the sale price of a house we began by looking at factors like Neighborhood and Overall Quality of the house. As you can see below both these categories play important roles in housing prices. Neighborhood plays into the old saying about real estate being all about location location location. But the house condition also indicates that the higher the quality the higher the overall price for the house.Below is a schematic of the data cleaning and feature engineering process that was performed on the datasetsData cleaning and feature engineering were performed to construct additional explanatory variables that can help predict the housing sale price. For this process we combined both training and test datasets together after dropping the sales price. We first assessed features with missing values. Columns with missing values were imputed as shown in the table below. Next we engineered additional features by creating additional columns that would help predict the sale price or combine columns that included redundant information. Three main types of feature engineering were performed:Transforming the data was important due to the skewness of the overall data. The first transformation we performed was on the sale price of the dataset. The data below visually displays how skewed the data is and how log transformations provide a more normal distribution of the sale prices. Another transformation we used was the boxcox transformation on every predictor variable to ensure predictors were normally distributed.Different types of encoding were performed on the categorical predictors because machine learning algorithms are incapable of processing strings or plain text in their raw form. Three approaches of encoding were performed: onehot encoding labelencoding and ordinal encoding. Ordinal encoding was performed before the boxcox transformation step on predictors with inherent rankings.  After boxcox transformation the remaining predictors were either all one hot encoded or all label encoded. One hot encoded was used for regression techniques and label encoding was used for decision tree techniques. We tried out various types of models to understand which one worked best for our dataset. We started off using multiple lasso (L1) ridge (L2) and finally elastic net regressions. Afterward we used Decision Trees Random Forest Gradient Boost and XG boost.The regression techniques were initially used because they were fairly easy to implement to our dataset and didn’t require much maintenance to configure and optimize the results. We found that different regression techniques provided very different results. The one regression technique that provided the best results was lasso regression. We believe that L1 was the best due to the recognition of our one hot encoding which was able to use coefficients that heavily leaned on the correlation of the sales price. Our team assumed that using an Elastic Net would provide better results than the lasso due to its combination of using L1 and L2; however this was not the case and provided an even worse CV score. The plot below shows the positive and negative 20 coefficients in the linear regression model.After our regression techniques we switched over and implemented a number of treebased models such as RF XG and XGboost. The initial tree based model we performed was the decision tree which had a poor result. We used the decision tree results as a baseline to compare against more complex treebased models. Random Forest Gradient Boost and XG Boost had significantly better results  with XG Boost showing the best result. Below is the feature importance plot from the XG Boost model. The main reason for attempting so many modeling techniques was to try to get  an ensemble method to produce the best result possible. However since lasso regression yields the best results consistently through validation and Kaggle scores ensembling of all the models was not necessary to gain marginal improvement on scores at the cost of losing model interpretability. The results below showed that our Kaggle score and RMSE score from crossvalidation follows the same trend and are similar in range. None of our models were overfitting and lasso regression model and XGboost had the best results.A few notes we had in mind to further improve our model accuracy would be to explore the possibility of stacking or ensembling individual models. In addition we also would like to further explore the neighborhood effects on couple important features we identified using hierarchical linear regression. We had a theory that the clustering of the neighborhoods had a bigger role on the house price than we initially thought and using hierarchical linear regression would have helped prove our theory right or wrong. Another interesting topic we had in mind for future improvements was the inclusion of time series event data to be included in our dataset. The recession cycle in 2008 must have played an impact somehow and our group wanted to see how that would have played out in our results. The economic index is something we had in mind we wanted to implement showing things such as economic statuses and the communities living in the area. Since Iowa state university was in the middle Ames Iowa we saw a trend noticing that houses near the college were typically lower in price while those north of the campus were higher in price.,NA,"Predictive Modeling On House Prices (Ames, Iowa)"
https://nycdatascience.com/blog/student-works/key-learnings-from-predicting-housing-prices/,3,"We set out to predict housing prices in Ames Iowa utilizing over eighty features ranging from square footage to number of rooms to kitchen quality.  To effectively optimize our predictions we established a pipeline to easily preprocess the data store auxiliary functions run models and autogenerate output files.  We iteratively implemented model improvements to reduce our prediction error measured here as root mean squared logarithmic error (RMSLE). Through this process we learned a lot about effective pipelines the nuances of regression the value of stacking and the implications for real estate markets.  Here are our top five takeaways.We decided to build a pipeline at the beginning. There are two major advantages for such an approach: on individual level repeated codes are greatly reduced; on team level each individual’s results can be easily reproduced in a standardized way by all other members which provides a common ground for the team to validate and improve the methodologies.Our pipeline contains the following major units:After spending time to carefully understand the feature space we started addressing missingness. The data set contains ambiguity of the use of ‘NA’. ‘NA’ entries refer to a feature being unknown in some cases and not applicable in other cases (i.e. the house has no fireplace). The latter case was easily handled by replacing missing items with a new categories that signaled if the feature does not exist in the house. The former was treated on a case by case basis using mode imputation or simple regressions to impute. Lastly we crossreferenced  features that related to one another (i.e. Basement Finish Basement Height Basement Condition) to insure that there was consistency. We found inconsistencies across feature groups and imputed accordingly.Our target variable  sale price  has a positive skew that often formed nonlinear relationships with our most predictive features. Additionally the Kaggle competition evaluates models based on the root mean squared logarithmic error. Therefore predicting an $800000 home to be $600000 invokes the same penalty as predicting a $30000 home to be $40000. Transforming our target from sale price to the natural log of the sale price remedied these problemsand our model results improved materially.OutliersIn examining the relationships between the features and the target (sale price) we found a few outliers stood out.  For instance two homes with very high living areas had very low sale prices. After examining these records further we decided to omit them.  This ended up improving model accuracy and generalizability.Feature EngineeringNext we performed feature engineering producing additional predictors.  Some of these especially the feature interactions proved quite helpful in our modeling.  For example multiplying the various quality ratings (kitchen quality garage quality etc.) together got us an aggregate quality for the property and further distinguished high quality from middling quality.Given the linear relationships in the data we hypothesized that an elastic net regression would be predictive of sale price. Regularization allowed us to include numerous features without worry of multicollinearity and feature selection. We used crossvalidation with 10 folds to tune our parameters and examined residuals closely to identify and address residual patterns. After noticing that the cross validated lambda tended to score very well in sample and poorly out of sample we manually increased our lambda parameter until these two metrics converged.Aside from the linear models we also tried two tree based models: random forests and gradient boosting models. Here are our observations:Finally to further improve our score we used the common Kaggle practice called stacking. We trained several stacking models. For these stacking models we generally chose :Note that the last 2 gradient boosting models are far from convergent: their learning rates and training rounds are so small that they cannot even fit the training set to a decent level. We chose these worse performing models deliberately by experimenting many different settings and it indeed aligned with the philosophy that we want more diverse models.
Here is another direction that we could go to improve further if we had the time: experiment with more tree based models to gain a sense of what ‘simple’ ‘moderate’ and ‘complicated’ mean and then stack with all different combinations of ‘simple’ ‘moderate’ and ‘complicated’ models to cross validate. One final note on boosting our score: we took an average of our best stacking models. This step was simple yet effective. We have successfully built some machine learning models which have good prediction power but our job is not done yet. We still need to further explore and understand the models. The graphs below show the feature importance of both linear regression and random forest. It seems that they are very different at the first sight but if you look more carefully you will discover many similarities. The features with red underlines (add_OverallGrade add_GrLivArea*OvQual YearsOld etc.) are mostly related to the overall quality or overall condition the features with blue underlines (TotalSF SF_score BsmtFinSF1 1stFlrSF etc.) are mostly related to sizes of the house. If you look at these features in this way both random forest and linear regression models put great emphasis on the overall quality and the sizes of the houses on the sale prices. How to understand this finding in real life context? Imagine the normal procedure when we are going to buy a house. We usually go to the open house to take a look. Unlike what we have done in this project most people will make a decision if they want to buy it and the price range they would like to pay in about half an hour. It is impossible to go through a checklist of all 100 features. What are the most important factors for them to consider? First impressions trump the checklist. If you really like the house you would likely consider the higher price to be justified.  What makes a good first impression? The overall quality just as the machine learning models showed. There are other concerns that carry a great deal of weight like considering if it meets their particular requirements. Questions people do ask include: How many people do we have in the family (now and in the future)? How many cars do I have? Do I need a basement? All these considerations are related to needs and the needs largely determine the size of the target house which is also revealed by the models. Therefore the result from feature importance analysis is consistent with our real life experience. In summary as what we discussed above we learned a lot of practical experience in machine learning workflow from this housing price prediction project. In our opinion the key to a successful machine learning project is to keep trying. That means employing different feature engineering strategies that produce various models with different parameters and so on. There must be many cases the algorithm fail but we can still learn from the failures. In these tests a welldesigned pipeline becomes extremely helpful to reduce the heavy repetitive work. One last note do not stop just after you can make accurate predictions. Try to make sense of your model and get some actionable insights: this is how machine learning helps us in  real life. See our  for more details.",NA,Key Learnings from Predicting Housing Prices
https://nycdatascience.com/blog/student-works/machine-learning/machine-learning-project-ames-housing-dataset/,3,The Ames Housing Dataset was introduced by Professor Dean De Cock in 2011 as an alternative to the Boston Housing Dataset (Harrison and Rubinfeld 1978). It contains 2919 observations of housing sales in Ames Iowa between 2006 and 2010. There are 23 nominal 23 ordinal 14 discrete and 20 continuous features describing each house’s size quality area age and other miscellaneous attributes. For this project our objective was to apply machine learning techniques to predict the sale price of houses based on their features.In order to get a better understanding of what we were working with we started off with some exploratory data analysis. We quickly found that overall material and quality (OverallQual discrete) and above ground square footage (GrLivArea continuous) had the strongest relationship with sale price. Using a widget (shown below) we then examined the remaining features to get a sense of which were significant and how we would be able to feed them into linear and treebased machine learning algorithms. 53 of the original features were kept in some fashion.  For categorical features the majority were handled using onehot encoding with minority classes below a certain number of observations being excluded. Certain discrete features were changed to binaries if we found their presence to be more impactful than their frequency (eg. Number of fireplaces → Is there a fireplace?). Given the range of some of the continuous features in the data we found it useful to apply log transformations where appropriate like each house’s lot size (LotArea). Lastly there were some special cases like the selfexplanatory YearBuilt feature (figure below).  We determined there to be no meaningful relationship with sale price for values prior to 1950 so we made that the minimum value. After this initial round of preprocessing and deciding to remove two outliers (with square footage >4000sqft. and sale price <$200k) we were well on our way to making our first set of models!This problem lends itself well to linear regression. In fact we can draw a simple regression line between above grade square feet and sale price that explains 54% of variance in sale price! This model produces a crossvalidation error of 0.273 in terms of Root Mean Squared Logarithmic Error (RMSLE).As a quick side note we choose to use RMSLE for model evaluation in order to match the scoring metric of the Kaggle competition. RMSLE ‘standardizes’ prediction errors between cheap and expensive houses so that we are not incentivized to build a model that predicts better (on a percentage basis) for expensive homes than cheaper homes. Practically it also makes our crossvalidation results a more accurate indicator for Kaggle scores.While we can obviously do better than a onevariable model the simplistic case highlights an issue that we will need to account for in linear regression. Inspecting the residual plot we can see a classic case of ‘fanning’ residuals. This violates one of the key assumptions of linear models  that the error term be identically distributed.The underlying issue is nonnormal distributions of both sale price and above grade square feet.Applying the boxcox transformation to both variables results in much more normal distributions and a regression on the transformed variables produces a much betterbehaved error plot. Surprisingly this improvement is not associated with a reduction of model error in the case of simple linear regression. However box cox transformation does result in a massive reduction in error when employed for multiple linear regression models.Of course we can start to improve our linear predictions by incorporating the influence of additional explanatory variables. There are strong (and obvious) relationships between some of the explanatory variables such as above grade square feet and above grade rooms and lot area and lot frontage. Regularization techniques will be critical for controlling for this multicollinearity.Indeed elasticnet regularization reduces crossvalidation RMSLE from 0.251 to 0.118. Elastic net ridge and lasso all performed equally well in crossvalidation.The variable coefficients from elastic net confirmed our insights from exploratory data analysis. House size and quality seem to be the most important variables for determining sale price.Going beyond linear regression we next tried fitting our data to treebased models. The simple decision tree below with a maximum depth of 3 gives an idea of the features on which our models could split and the breakdown of how our data might be divided:We initially feed our data to a straightforward decision tree regressor from the ScikitLearn Python package. The feature importance plot is shown below:While these last two are almost trivial examples they help us get a sense of the features that might be important for this class of models as well as visualize the importance of the hyperparameters we can tune for our treebased learners. So far we see that the features we hypothesized to be important from our exploratory data analysis and feature engineering are in fact significant. We do not spend much time on the untuned decision tree model even though it resulted in a 0.141 RMSLE training score and a crossvalidation RMSLE score of 0.181; these scores are better than we expected and they most likely are the result of our extensive feature engineering. Nevertheless we move on to a random forest model where we can tune hyperparameters for the number of trees the maximum tree depth the maximum number of features considered at a split the minimum number of samples required to make a split and the minimum number of samples required at a node. The feature importance plot for our random forest is shown below:The tuned random forest resulted in a training RMSLE of 0.122 and a crossvalidation RMSLE of 0.131. This score is much better than the single decision tree in large part because a random forest reduces the variance (overfitting) one sees when working with only one tree. However there is a higher bias with a random forest than with a decision tree. This is because only part of the training data is used to train the model (bootstrapping) so naturally higher bias occurs in each tree. Additionally the random forest algorithm limits the number of features on which the data can be split at each node which in turn means the number of variables with which the data can be explained is limited inducing higher bias. In an attempt to lower the bias we next look to a gradient boosted treebased model; the plots below show the difference between the tuned and untuned feature importance for this model.Even with the untuned model the crossvalidation RMSLE is 0.116 (training RMSLE 0.037) which is already better than the random forest. This is because the bias of the model is reduced because of boosting features. This model tries to reduce the error in predictions by for example focusing on poor predictions and trying to model them better in the next iteration and hence it reduces bias (underfitting). One can see the importance of tuning hyperparameters though when looking at the important features from the untuned model. There are some potentially collinear features such as LotArea vs. LotFrontage and GarageYrBuilt vs. YearBuilt that show up while some features shown to be important in our previous models such as OverallQual and OverallCond are absent. Yet by sequentially tuning the number of trees the max depth the min samples for a split the min samples for a leaf and then increasing the number of trees while simultaneously decreasing the learning rate we see less “redundant” features and more relevant features (OverallCond OverallQual Functional) with high importance for our model. The tuned model is the best individual model we trained with a training RMSLE of 0.082 and a crossvalidation RMSLE of 0.112. Another visualization of the difference between the tuned and untuned models is shown below.,NA,Machine Learning Project: Ames Housing Dataset
https://nycdatascience.com/blog/student-works/machine-learning/predicting-housing-prices-in-ames-iowa-housing-prices/,3,Predicting Housing Prices in Ames IowaBy Ariani Herrera Erin Dugan John Nie Won KangThis Kaggle Competition offered a comprehensive dataset of nearly 4000 home sales in Ames Iowa between 2006 and 2010 with the various features influencing the sale prices. Our motivation was to explore and analyze the housing dataset to find the key features that influenced the sales price and then develop a machine learning algorithm to predict the prices. Upon observing the data our team wanted to create a systematic method to understand the variables before building any sort of model. We sorted through a sample of the data set which included 79 explanatory variables and the corresponding sale price for each home. Our team explored the different data types within the data which were divided into 38 numerical features and 43 categorical features.To predict the housing prices our team explored the variables to evaluate the key features that affect housing prices in Ames Iowa. We then wanted to understand redundancies collinearity and the relationships between different variables.We began our Exploratory Data Analysis by examining correlation between the features. The heat map below shows the correlation between all the features in the data set. The darker or lighter the color (correlation is either close to 1 or 1) the stronger the relationship between variables. From this graph we are able to see which features had the greatest influence on the Sale Price and note the correlation amongst the features. If there is multicollinearity or a strong correlation between features in the model it can contribute to errors in the model. For example both GarageYrBlt and YearBuilt were highly correlated so only YearBuilt was used in the prediction model.Our team also wanted to remove information that did not seem relevant or useful. If a variable was missing more than 50% of its data we removed those columns from our dataset. The graph below is a visual representation of the missing data.For other variables having a significant portion of missing data we decided to make educated assumptions to impute the missing data such as with Lot Frontage. After some research our team felt it was fair to assume that we could impute the missing Lot Frontage value with the average frontage for that particular neighborhood as most in a given neighborhood do not deviate significantly. For other features with missing values we imputed the missing data with zero or None depending on the feature.Generally speaking outliers are observations that appear to deviate outside the overall pattern. Also extreme outliers could dramatically impact the results of a prediction model. We conservatively removed two outliers which were homes with sale prices over $300000 and had less than 4000 square feet of living area. After the outliers were removed the graphs depicted a more linear relationship between sale price and square footage.There were certain numeric variables that actually described categorical features. In the graph below we illustrate this was the case with the month sold. While sales varied by month there was no numerical pattern represented by the month of the sale.Some categorical variables showed ordinal features. Kitchen Quality shown below is an example of where an excellent kitchen increased  the value of a house. Our team ranked the different kitchen qualities for our model to prioritize the kitchen quality if it is excellent (Ex) over if the kitchen quality was good (Gd) typical/average (TA) or fair (Fa). Skewed distributions can decrease the accuracy of a model’s predictions because it could reduce the impact of low frequency values which could be equally significant when represented by normally distributed data. As shown in the graphs below we used a log transformation to normalize the Sales Price. Other skewed features such as Lot Area were also adjusted in the same manner.After cleaning and transforming our data we explored various machine learning algorithms to predict sales prices. To investigate benefits of dimension reduction and look for strong patterns in the dataset we also evaluated the models with and without Principal Component Analysis. The various machine learning algorithms were optimized with a parameter grid search and used KFold cross validation to prevent overfitting. The two paths used in developing our models are shown below.Our team built a comprehensive and robust model that explored Linear Regression Lasso Ridge Elastic Net Kernel Ridge and Random Forest regression models and used a grid search with Kfold cross validation to ensure we had optimal tuning parameters.We tested these models with and without Principal Component Analysis (PCA).  After cleaning our data performing EDA and creating dummy variables for the categorical features there were over 318 features in the model. 85 Principal Components accounted for 90% of the variance in our data. PCA allowed for us to reduce variation and illustrate strong patterns.The chart below shows our results after completing Principal Component Analysis for the  Kernel Ridge Lasso Random Forest and Elastic Net models. With PCA the Kernel Ridge Lasso and Elastic Net models performed the best in predicting housing prices with a RootMean Square Error of approximately 0.026  0.027.Unfortunately for our models PCA has its limitations since it relies on linear assumptions. Although PCA does a great job with data that is linearly correlated PCA might not capture the full picture if there are nonlinearities. Our team also ran the models without PCA and as depicted in the graph below. These results indicate models using  Ridge Lasso ElasticNet and Random Forest perform better without PCA best in predicting housing prices with a RootMean Square Error of approximately 0.024  0.029.Finally once we optimized our models based on the training data we ran the Kaggle test set through the algorithms to predict the sale price. Overall the ridge model provided the most accurate predictions.ConclusionFrom this project we gained several key insights and ideas for improving the model in the future. While implementing PCA in the model did not always provide improved predictions it may have improved results if we are able to identify any nonlinear relationships within our data set. We believe the grid search to optimize the input parameters of each model improved the accuracy and could be further improved with additional tuning among a broader range of parameters. Additional regression models could also be implemented into the algorithm to optimize results as well as model stacking or ensembling techniques to incorporate several models into the prediction. Other considerations may include investigating other data that could influence the prices such as how long the home was on the market and whether it has a desirable layout. It would also be interesting to see how these models perform based on home sales outside the dataset for years after 2010.,NA,"Predicting Housing Prices in Ames, Iowa Housing Prices"
https://nycdatascience.com/blog/student-works/web-scraping/rolex-watches/,3,Brands like Rolex employ a number of methods to maintain their position as a luxury watchmaker. They are selective about the brand ambassadors they hire the location and decoration of flagship stores the events they sponsor and most importantly the price tag of the watches. With such effort Rolex has become one of the most iconic brands in timepieces and is known to many as the most expensive watchmaker despite the fact that there are actually far more exclusive brands that are more expensive.With the coming of the internet buying a timepiece is no longer limited to the Fifth Avenue flagship store authorized retailers around the globe or the stores that are not certified by Rolex to sell Rolex (the socalled grey market). A few years ago when a customer typed in Rolex on Google the results included the online grey watch market that often ranked above the official Rolex website a phenomenon that is not unique to this luxury brand. Customers realized at that moment there is an alternate way to obtain some of the most coveted timepieces from Rolex possibly at a much cheaper price though they may not have been certain of the price difference. This problem was compounded by the fact that official Rolex website didn’t show the price for their timepieces at all. The only way to obtain that price transparency online was by finding the watch on grey watch market. And if the customer would go to the retail store to check out the price of the same watch that is being sold online the customer would find that the online price is much cheaper a revelation that likely would result in the customer making the purchase online rather than at a retail store authorized by Rolex. This situation raises two key questions: Why are there Rolex watches on the grey market? Are these watches actually made by Rolex?The slide here illustrates the business model of the online grey market and it is  in this case as it draws a significant share of the online grey market.AW (AuthenticWatches.com) buys a particular Rolex watch from both Rolex (manufacturer) and from AD (authorized dealer/retailer). AW is able to obtain Rolex and other watches at a much lower price because of its relationship with both Rolex and AD. If there is a watch in low demand that either Rolex or AD can’t sell AW will have the opportunity to buy this watch in bulk and thus sell this watch at a price lower than the retail price. The opposite scenario is where a watch is actually in demand and AD has no need to sell at discount in which case AW will find it harder to drive a good bargain. Due to the special relationship between AW and its goto AD AW will still be able to obtain the watch just with less of a discount. This can be reflected by the availability of a particular watch on the AW website. If it is available in stock it might indicate that this particular watch is in low demand and can be discounted more. On the other hand if the watch is available only after weeks or even months then the watch could be a timepiece in demand which usually translates into a lower discount. From my research Rolex as a manufacturer does not deal/trade/sell to AW. It also places strict restrictions on its ADs to deal with AW. Despite the brand’s efforts one can still find plenty of Rolex timepieces floating online at cheaper prices.This causes three problems: 1) AD (Authorized retailers/dealers) lose profit 2)  Customers prefer to compare watches from online grey market 3) The luxury positioning of Rolex is affectedThe goal of this project is to use webscrapeddata to quantitatively address the first and the third problem namely how much profit is lost and how much has Rolex’s luxury positioning being affected and to qualitatively seek a solution for the second problem. The project will also address these three problems from Rolex AW and AD’s perspectives. In order to answer these questions AuthenticWatches.com’s Rolex section was being scraped using Scrapy. In the end about 2500 Rolex watches were scraped along with their online prices retail prices availability warranty and product name. In order to make sure the retail prices listed on AW are correct I have checked some of them against s official website to make sure they align with each other (AW has no incentive to lie about retail prices since trust is the key piece to online grey market).Out of the almost 2500 watches scraped from AW only 80 do not have the retail price listed. The findings from the rest of the watches confirmed our hypothesis that Rolexes on AW are cheaper than retail prices. However AW’s Rolex watches are only about 10% cheaper (though they are also taxfree which makes them nearly 19% cheaper)  as compared to retail prices. If 10% seems too much to you comparing other brands on AW such as Breitling where some watches are almost 50% off online Rolexes are not too cheap. Watches online are perhaps toward the cheaper end where most of the watches are around $10000 range although this cannot be confirmed without Rolex’s sales data. And given the same discount expensive watches will result in more “savings” ($30000 with 10% off comparing to $10000 with 10% off). If a desired Rolex timepiece is only 10% off online a customer might have more incentive to shop at a retail store where he or she will receive good service and satisfy instant gratification.One of the hypotheses about online watches is that if a watch is available in stock this is an indication that the watch is not desired by the market and usually comes with a deeper discount and vice versa. This has been confirmed true with AW’s Rolex. Watches that can be purchased right away have a discount mean of 18% whereas watches available after weeks and months usually come with less than a 10% discount. Another observation is that watches only available after 3 months have less discount range comparing to other availability. This could indicate a weak negotiating position from AW because of the popularity or rareness of some timepieces. Watches that are instock are usually cheaper than watches available only after longer periods. This might reflect the riskiness of carrying expensive watches in its inventory for AW. However this could also reflect the online popularity of watches where the preference and appetite is usually toward the lower end of the price tag.After seeing the comparison of different stock availability option you may ask what are some of the popular watches? In fact one of the most advertised watch on Rolex official website is Oyster Perpetual Rolex Deepsea which is not even on AW’s website. This could indicate this particular timepiece is so popular that AW does not even have access to it or even if it has access AW is in no position to sell it at a discount.Since this project has no access to Rolex’s database the only approximation to the popularity of a particular line of timepiece is through counting the number of products being sold under that category on AW and if that category has less of a discount than other categories then one can make the assumption that this line of Rolex is a popular line. Through this assumption the category with most products is datejust 36. When comparing the top 10 most popular category discount with the other categories it is statistically significant that the popular lines are less discounted online. If a particular line of Rolex (e.g.: datejust 36) has many products online and is giving out less discount there is a reason to believe this line is a popular line and could indeed be the cash cow for Rolex.Since we have all the information about Rolex’s discount on one of the largest online grey market it is time to make assumptions and calculate how much profit margin AD lost for every Rolex sold on AW or online in general. By making the assumption that most of the Rolex watches  sold online are around $10000 to $20000 range with average discount around 10% assuming no one would buy watches that cost twice the mean price sold online (larger than $40000)  the AD loses $1500 for every watch being sold online.As mentioned earlier there are around 80 watches on AW that do not have retail prices listed. This is not due to mistake or due to AW’s lack of information. Instead the reason AW does not want to put retail price for these 80 watches is because they are more expensive than the retail price listed on Rolex’s official website. In such cases someone who buys a watch at a retail store can profit by reselling online. In fact these watches are usually iconic Rolex lines that have been heavily featured in various channels and thus one may expect sellout of such watches at a retaillevel. Therefore if a customer has to wait to buy a watch from AD the customer may opt to buy online albeit with a higher price tag. This finding contradicts to AW’s image as a cheap seller that only sells watch that are not in demand.This also brings up the idea that AW should not market itself merely as an online place of cheap watches but an online place to get timepieces that one can not obtain offline.In conclusion Rolex is holding its brand well comparing to brands like Breitling but there is more it can do. Knowing that Datejust 36 is the most popular line and a cash cow for Rolex Rolex should be careful not to overproduce the item and thus make these products susceptible to discount which would impact the luxury positioning. Rolex should also adjust its position for popular watches because it is better for AD to be able to pocket the profit margin with enough inventory than for it to lose customers to AW because of inventory issue. Rolex should also place further restriction for AD to be able to sell online as the presence of discount harms the positioning.As for AW AW should market itself as a boutique timepiece dealer to rebrand itself and to attract more customers for its platform. AW should also optimize its inventory so that consumers would purchase the watch for instant gratification thus increasing sales. AW should connect with more ADs or deepen the relationship with existing AD to increase the selection and availability of some products.AD should prioritize its relationship with Rolex since Rolex is holding its branding well as compared to other brands. This will help AD to obtain more popular pieces and thus keep the profit margin from losing to AW. AD should also cut relationships with brands that have been significantly diluted by online presence especially Breitling. Last but not least AD should work with Rolex to improve online search results and online shopping experiences together so that customers would prefer to shop directly with Rolex’s certified channel.,NA,Is Rolex Truly A Luxury Brand? (How Online Market Is Diluting A Brand)
https://nycdatascience.com/blog/student-works/machine-learning/predicting-hospital-readmission-rates/,3,"In this project we set out to train a machine learning model that would flag patients from a skilled nursing home if they are at risk of being readmitted to the hospital. Preventing patient readmission to hospital can improve the patient experience lower costs and hopefully lead to a shorter stay in the skilled nursing home. We were given two datasets from ~10 different nursing homes in NYC that specialize in patients with acute/chronic heart failure. We set out to merge the two datasets and combine various metrics (ie. lab results patient’s weight) to have one unique observation per patient with all of their vital statistics and their outcomes (Home or Readmission to Hospital). We used and compared a logistic regression model and a treebased XGBoost model to predict outcomes. We used Google API to create a pipeline that will pull all the data and update our predictions and evaluation of our model.The number of patients in the dataset was limited which was a challenge given that the more observations you have the better your predictive model is likely to be.  As our goal was to predict whether patients will be discharged to a hospital we treated each period from enrollment in the program to discharge for any patient as a unique observation within out data set. This means that multiple observations can be derived from the same patient if they have been through the program multiple times increasing the number of observations we could use to train our model.Unfortunately data for each observation was plagued with missing data extreme outliers and nonsensical values like negative ages for example. Fortunately some of the features had redundant entries within the spreadsheets. So if a value didn’t make sense we could use the value from another part of the spreadsheet. Sometimes values were entered  into the spreadsheet in different formats a mixture of percentages and decimals for example. In cases like that it was clear how to fix the values. Any nonsensical values that couldn’t be fixed were treated as missing values.After removing observations with too much missing and erroneous data we had fewer than 500 observations with which to train our model.Within the dataset there were 31 variables of interest. They consisted of the patient’s age gender blood pressure ejection fraction length of stay diagnoses weight medications and various lab results. Since a patient could have multiple diagnoses the diagnosis variable was parsed out and dummified into 8 unique diagnosis variables (cad/mi heart failure unspecified diastolic heart failure systolic chronic heart failure atrial fibrillation cardiomyopathy left ventricle assisted device chronic heart failure).Here we have a first look at all the variables of interest lined up in a correlation matrix. Lighter values indicate positive correlation and darker values indicate negative correlation. Based on this there does not seem to be any strong correlation between the predictors and our outcome variable. Predictors that show correlation amongst themselves are BUN & CR BUN & BNP CR & BNP and systolic & diastolic. The first three pairs are related to kidney function. and the last is related to blood pressure. 
One variable that we expected to have a greater impact on our outcome variable was EF. After graphing the distribution of EF by outcome (below) we see that the distributions are fairly similar. Regarding the distribution of weight change as a fraction by outcome (below) there is slightly higher positive weight change in patients with a negative outcome. Overall the distributions are similar and echo what is shown in the correlation matrix.For medications those with positive outcome were seen to have a slightly higher frequency of prescriptions (except inotropes) but overall the distributions are comparable.For the eight unique diagnoses we noticed ‘lvad’ and ‘chf’ have low values and as such may not be important variables to keep as predictors. Further domain knowledge would help us better understand whether or not to drop these variables. In any realworld data analysis an unavoidable hazard is missing values. Our dataset also had some features with many missing values.Machine learning algorithms often have difficulty dealing with missing values. Nevertheless it’s not ideal to remove all observations with missing values as it can severely reduce the size of data or the data on one particular feature.  Not all missingness is the same. There are different kinds that require different forms of handline. If the probability of missingness in a feature is the same for all events we call it missing completely at random. When the probability of a variable being missing depends only on known features the missingness is said to be random. If data are missing completely at random or at random it is acceptable to drop events with missingness as it will not distort our distribution.Some of the missing values in our dataset is due to error in data input. It is reasonable to assume these are missing completely at random and safe to drop without causing significant distortion to the distribution.However if the missingness is not random dropping events may distort the dataset. For example if in our dataset we observed many missing values for blood pressure. When we found that patients with low blood pressure did not have their blood pressure reported then that will be an example of missingness not at random. Dropping observations with missing blood pressure values in that case will bias our distribution and any model built on that data will likely be flawed. We assumed that missingness in our dataset  is completely at random or at random.In most cases instead of dropping observations with missing values we Imputed values for features using different techniques. For example missingness in weight change or blood potassium level was imputed by median of the known values for those variables whereas missingness in “acute or chronic” state of patients  was imputed by the duration of stay in the nursing home. Some of the patients were missing data in the gender column so we filled in the missing data using a function called Gender Guesser. This function takes the name of the patient from the name column and decides whether it is a male or female. This made it possible for us to retain many observations. It should however be noted that imputing missing values in this way may underestimate the standard error. We also decided to employ feature scaling to some of the numerical features in our dataset. We subtracted the mean of the variable from individual observation and scaled it to unit variance. Standardization prevents the cost function from being dominated by features with a large range. It also makes gradient descent  converge much faster to the minima of the cost function. For example the ejection fraction and bnp change variables before scaling have drastically different ranges.After scaling the two distributions have comparable range (along the xaxis).Some of the numerical features in the dataset had skewed distribution. Taking the logarithm of a skewed variable may improve the fit in the modeling stage by making the variable more ""normally"" distributed.After all the cleaning feature selection and engineering was done we had over forty predictors in our model. Trying to understand and visualize data with over forty dimensions is needless to say difficult. So we used principal component analysis to reduce the dimension of our data down to a much more manageable three dimensions. Here is the scatter plot of the observations in our data set. Good and bad outcomes are colored blue and red respectively. Unfortunately we can see that the outcomes are mostly mixed uniformly in the data set. This meant that most of the more classical prediction models such as logistic regression and discriminant analysis should perform poorly. Nevertheless we tried a logistic regression with gridsearch and 5fold cross validation. It performed fairly well with a crossvalidation score of 0.61 and a holdout score of 0.68. We decided to also try and compare a treebased model using XGBoost.In order to better understand why this model was appropriate for the project we first need to familiarize ourselves with two machine learning techniques: decision trees and boosting.Decision trees are frequently used as estimators in machine learning. A decision tree uses a set of rules to predict an outcome based on one or more predictors or features. Trees are popular because they are easy to implement and simple to interpret when represented graphically. To demonstrate this let’s inspect the decision tree graphic below.We have a collection of patients and we would like to predict whether they will have a positive or negative outcome when leaving the hospital. A positive outcome is represented by 1 and a negative outcome is represented by 0. Each patient has a set of measurements associated with them (e.g. cad.mi bun bnp duration resting_hr etc.). The decision tree makes predictions based on a set of rules it produces. These rules allow the tree to make decisions about how to organize patients into different groups and ultimately assign a prediction to each group of patients. In the tree above the first decision is made based on a patient’s “cad.mi” value. Patients with “cad.mi” less than 0.5 follow the branch down to the left whereas all other patients follow the branch down to the right. The patients continue to be split based on new rules. The point at which the tree stops splitting is called a terminal node.In the decision tree above the terminal nodes all have a value of either 0 or 1. The value at the terminal node is the predicted patient outcome. For example if we start at the top of the tree and continue left at every node split (i.e. the patient satisfies the rule at each split) we will end up with a negative predicted outcome. Although in the tree above we see a 0 or a 1 at each terminal node the actual value behind the scenes is a probability that the outcome will be positive. The probability can be useful if we want to set a threshold for considering a case positive (i.e. only groups with probability greater than 70% will be considered positive). The threshold is relevant when we want to limit the number of false positives by setting stricter criteria for predicting a positive outcome. This is particularly important in health care because false positives can turn into dangerous situations. If we raise the threshold the model will predict a positive outcome only when we are sufficiently confident about it.Now that we have an understanding of how decision trees work we can discuss a powerful technique in machine learning called boosting. Boosting is a method that starts with a weak decision tree and improves its accuracy by sequentially adding corrections. Consider this example: my friend Sophie is decent at guessing someone’s age. I notice that whenever Sophie guesses an age she is off by +2 years for men and 2 years for women. If we correct Sophie’s guess using the pattern of error I have observed we end up with a more accurate prediction. We can continue to correct Sophie’s guess by finding trends in her error until we have a much more powerful model than we started with. This is the main idea behind boosting.XGBoost is a very popular machine learning model that utilizes the concepts of decision trees and boosting. XGBoost stands for extreme gradient boosting and it does a great job of pumping the most predictive power out of a decision tree based model. It is a powerful and efficient algorithm for large datasets. We test this model in an attempt to pull the most predictive power out of our dataset. Not surprisingly it performs better than a simple logistic regression: the model had a crossvalidation score of 0.66 and a holdout score of 0.69. However with such a small dataset XGBoost’s optimization for speed is not as important to us as the explanatory power of a linear model like the logistic regression. In order to compare our models we plotted the ROC curve (Receiver Operating Characteristic Curve) which compares the True Positive Rate and the True Negative Rate at varying thresholds for classification. This is a way to visualize the performance of a model without narrowing down to accuracy precision or recall for example. An ideal ROC curve will be as close to a right angle as possible. With XGBoost the terminal nodes each have a probability of being in one or the other class (Home or Readmitted to Hospital). With logistic regression the output predictions all fall on the sigmoid function between 0 and 1; they are also probabilities. In order to compare these results with the actual outcome from our training set we can change the threshold to be in one class from the default 50% to something more strict or flexible. In this case we would want to maximize our True Negative Rate and minimize any False Positives which would be patients we classified as Home who are actually not doing well. See the figures below which show the ROC curves for both our models. The ROC curve allows us to evaluate our model regardless of the threshold we choose and to see the tradeoff between specificity and sensitivity.The first thing we can note is that the AUC score of the logistic regression does not change much from the training set to the test set while the XGBoost performs  well on the training set. The huge decrease from 100% to 79.6% on the test set is a sign of overfitting. Because we are dealing with a relatively small data set we want to be cautious of overfitting as this could lead to very inaccurate predictions on future data. If we change the holdout test set in this dataset the XGBoost might have a very different area under the curve while the logistic regression would stay relatively stable. Therefore despite the higher accuracy of the XGBoost model we prefer logistic regression for its stability.Since our collaborating partners used a front end AppSheets with Google Sheets as the backend we used Google API to be able to download the dataset clean make our predictions and upload our predictions to a specific column of the Google Sheets. This would allow certain patients to be flagged in the front end based on the value of that column in the back end. We also added a tab to the Google Spreadsheet that would have information about the model’s accuracy the last time it was run and a history of its accuracy over time. We would hope that with more observations our model would become more accurate.We were very excited to be able to work with a realworld healthcare dataset for this project. Our goal with this project was to start with the simplest model possible and if we had time expand from there. Within the scope of two weeks that means we chose the most recent measurements for each patient. For example it made no difference if a patient had been weighed once or twenty times during their time in the nursing home; we only evaluated the most recent weight. Since most of these measurements were taken twice a month this was overlooking a vast dataset behind the scenes albeit a complicated one. One way to simplify it would be to treat each week a patient is in the hospital as an observation with new measurements or not and have a multiclass outcome: Home Stay and Hospital. While complicating the model significantly this would lead to a much more powerful and useful model. Predictive analytics is vastly changing our healthcare system in lowering costs and improving patient experiences and we were excited to work in this area. Please feel free to reach out with any additional questions or comments on our work.Summary of Results",NA,Predicting Hospital Readmission Rates
https://nycdatascience.com/blog/student-works/choosing-your-ob-gyn-in-new-york-city/,4,Introduction:So what’s being done about this? Since 2006 the California health system has introduced new protocols to help prevent those 60% preventable deaths. The hospitals that have taken up this new protocol have been able to dramatically reduce their maternal mortality rate and the state’s maternal mortality rate has decreased by 55%. The rate in the U.S. overall however has since continued to increase.I decided to investigate n the question of progress on this front in  New York State: Have we reduced our maternal mortality rate as California has? In order to examine this issue I chose to look at medical malpractice from obstetriciangynecologists licensed in New York City (I limited myself to the city due to time constraints). I used Selenium  from the New York State Department of Health’s websiteto get information about all the doctors licensed to practice in the five boroughs: their names medical schools graduation year and any payments they made from either settlements or arbitrations of malpractice. The information about these lawsuits is only publicly available for 10 years so I looked at the total number of lawsuits per month since 2009 (see the plot below). As  you can see tThere has been no significant downward trend.Next I decided to compared doctors from different medical schools to see if that had an effect on number of malpractice lawsuits. The doctors in my dataset graduated from over 200 different medical schools so I first filtered out and looked at the international schools. The histogram below shows number of doctors who have or have not been sued (successfully) in the last ten years. There was nothing very interesting to be found by comparing the schools by state: they all have relatively similar proportions of malpractice except states where fewer than 3 doctors studied (West Virginia Arizona).I next divided out a group of elite medical schools to compare them with the rest. They made up a much smaller portion of the dataset. I wanted to see if these schools would have a smaller proportion of doctors who had been sued from the elite medical schools than the rest. Again find the plot with raw number of doctors on the left and normalized on the right:Once normalized one can see that a slightly smaller proportion appears in the first group. I conducted a ttest between the elite schools and the rest of the schools from the U.S. and found no significant result (Tstat: 1.37 p0.169). So if not education what other parameters might predict which obgyn physicians commit malpractice or not? Since this is a webscraping project I decided to look at doctor ratings on two different physician databases: WebMD.com and Healthgrades.com.I scraped the name and rating (out of five stars) for obgyns in the New York City area. As I was pressed for time it was not comprehensive but I was able to download about 400 doctors from each website (compared with ~1000 from the NYS Department of Health dataset). Merging these datasets by name of physician I was able to compare number of lawsuits with ratings on each website.As a lot of our webscraping presentations indicated online ratings tend to not follow a normal distribution. Most rating distributions tend towards five stars. I found the same phenomenon on both WebMD and Healthgrades. Most of the doctors are ranked between four and five stars including doctors that have made payments for malpractice (on the xaxis). In the Healthgrades plot there is a doctor with seven malpractice lawsuits in the last ten years and he has a four star rating. I did compute a Pearson correlation coefficient for both datasets and needless to say they were both close to 0 and insignificant.As a reality check I also plotted the two websites against each other to see if their ratings matched for the doctors (on the right). As expected most of the doctors are in a cluster around 4.55 stars.I would hypothesize that these online ratings are mostly influenced by a physician’s bedside manner rather than  by her/his competency. This would explain how a doctor who is very friendly could have a four or five star rating online despite having been charged for malpractice multiple times.I have found that the number of medical malpractice lawsuits against obstetriciangynecologists in NYC have not gone down since 2009. I then compared doctors from medical schools to determine if that could be a predictor of malpractice. I did not find a significant difference between doctors from elite medical schools and other U.S. schools. I might have needed more samples or maybe there is more to medical malpractice lawsuits that could be driving this negative result.Comparing doctor’s malpractice numbers with her/his rating on websites such as WebMD or Healthgrades reveals no correlation. It seems online ratings are not a predictor of malpractice. I hypothesize that they tend to  function as a predictor of friendly bedside manner by the doctor. There are many parameters that would be interesting to add to this model to determine if it is possible to predict malpractice: graduate medical education (residencies) board certifications any board sanctions years in practice patient load etc.As for the story that got me interested in this project in California where the maternal mortality rate dropped by 55% since 2006most of this change was from toolkits designed to handle those deaths that the CDC had determined preventable (due to hemorrhage).  This change was mostly implemented with hospitalwide procedures not individual obstetriciangynecologists. In hospitals in California that did not implement changes the mortality rate only dropped by 1%as compared with 21% lower mortality from hemorrhage. So while this project led me to some interesting questions about parameters that might predict malpractice or the accuracy (or meaning) of online ratings it did not address the need for changes to hospital protocol for handling complications of the mother during childbirth.Further reading:,NA,Choosing your OB-GYN in New York City
https://nycdatascience.com/blog/student-works/housing-prices-ml/,4,The general process can be visualized below:Fourteen of the factor variables (mainly the ones associated with quality and condition) had levels that needed to be assigned. After that we recombined all the groups as training and testing sets.After feature engineering we had 50 numeric variables and 36 categorical variables.We applied onehot encoding methods to the categorical featuresWe also checked skewness. For example the QQ plot below shows that sale prices are not normally distributed. We calculated that there is a right skew upward to 1.879. To correct for this we took the log of SalePrice. By consulting the QQ plot below we found that the transformed sales price is normally distributed. After correcting for the SalePrice variable we repeated this process for all remaining numeric variables. We performed log transformation on any variable with a skew larger than 0.8 (normalizing the data with the perProcess function).,NA,Predicting House Prices with Machine Learning Models and Algorithms
https://nycdatascience.com/blog/student-works/web-scraping/sentiment-analysis-on-stock-ideas-articles/,4,"I am interested in training a model that could determine overall sentiment of articles/sentences related to stocks. This model could be useful in analyzing enormous amount of information from news websites social media etc. to pick up trends. The first step in training such a model is to obtain a large amount of labeled data. To that end I chose to scrape a very popular website that features  articles on all aspects of stocks contributed by a large community. In particular I choose to scrape the articles about long ideas and short ideas. This would serve as labeled training set where long ideas correspond to positive sentiment and short ideas correspond to negative sentiment. The codes for scraping can be found .The website is notorious for its antiscraping measures. To overcome this problem I first tried to use TOR. However I still get blocked most of the time. The reason could be that the known TOR proxies are blacklisted. (  is a tutorial on how to set up TOR with scrapy.)The second method I tried was actually simpler but efficient. I obtained a list of most commonly used user agents and a list of recently verified proxies that support https websites. After tweaking the settings of scrapy a little bit I found this method worked quite well.I let the spiders crawl for several hours and I ended up with roughly 8500 articles on long ideas and 10000 articles on short ideas. Then I realized that this dataset is quite small! Although I didn't try I suspected that a model from scratch would not work very well. ( More specifically a good sentiment analysis in this situation would require understandings of sequential meaning of the texts and thus require a more complicated model like LSTM etc.)What could we do if we are short on resources to build a powerful model? Build a simpler model on top of someone else's finetuned model. This strategy is known as transfer learning and it worked very well in this case! More details on the model are contained in a later section.Before digging into the details of the model let's look at the word clouds first to get an overall impression on what words are more frequent (and thus potentially more influential on the sentiment analysis) :First the word cloud for all articles on long ideas:Notably we could see words like ""strong"" and ""growth""  that indicate positive speculation.Next the word cloud for all articles on short ideas:Notably we could see words like ""may"" and ""likely"" that express uncertainties and words like ""decline"" and ""issue"" that are associated  with negative speculation.However we could see that there are many neutral words that are common between the two word clouds. This indicates that a model that relies solely on the word counts may not perform well.As I said before my model is built on a pretrained model. In particular  is the source of the idea. The ""blackbox"" model is a word embedding model provided. Roughly speaking word embedding model maps words ( which are usually coded as onehot vectors in the first place ) to vectors in a Euclidean space of a certain dimension so that words with similar meanings or words with similar functionalities will be ""close"" to each other. Interested readers can learn more about it from .Once we have the tool to embed words we simply take the proper weighted sum of word vectors to serve as the embedding of the whole articles. Then we run a standard neural network to predict the outcome. The graph could be summarized as follows:For comparison I trained 4 models based on the same idea:And the results are summarized in the table below:Here nnlmendim128 is the name of the word embedding model that we use.Note that even with the fixed random embedding the training accuracy is still comparable to the model using a more meaningful embedding. However the test accuracy is much lower indicating that our neural network is overfitting the training set. Also note how fast the model is overfitting the training set if we train the neural network together with the embedding.Finally let's look at the confusion matrix of the first model:Each row is normalized. Note that our model performs better on negative examples (i.e. articles on short ideas). The possible reasons are: First there are more negative examples. Second there are neutral words that are used both in positive and negative examples and our model cannot distinguish sentiments based on contexts.",NA,Sentiment Analysis on Stock Ideas Articles
https://nycdatascience.com/blog/student-works/a-travellers-guide-to-broadway-musicals-from-travellers-perspective/,4,"Motivation: Broadway is one of the signatures of New York City. Statistics shows that 13.8 million people attended a Broadway show during the 2017 – 2018 season () a number which is ~1.6 times of the NYC population. Statistics also shows ~60% of the attendance was contributed by tourists. As tourists make up a significant percentage of the Broadway audience it would be interesting to find out what their take is on the shows.k. Are there any patterns and can we use this information to guide the future tourists? In order to explore possible answers to these questions I did some research on the reviews of some of the most popular Broadway musicals on Tripadvisor. Tripadvisor might not be the most comprehensive or professional website for Broadway reviews but that is an ideal place to study the traveler's real opinion on those shows as local people  do not normally post their reviews there. If you are traveling to NYC and considering taking into a Broadway show those reviews might be helpful.(1) Methodologies:I used the Scrapy package of Python to do the web scraping. I chose ~10 most popular musicals in Broadway and collected the reviews and some user information. For  shows with over 5000 reviews I only grabbed half of review items. I ended up scraping ~ 20000 review items in total. This is the starting point for all my analysis.(2) Analysisi. The reviewersI firstly plotted the distributions for reviewers' review counts and the votes (indicating the reviews are helpful) they received. Since most of the reviewers are ordinary travelers the voting is not affected by biases such as reputations. As we can see in the charts most of the people published less than ten reviews and received very few endorsements. Only a small portion of them are active writers of reviews. I defined a metric called review quality (number of helpful votes received/number of reviews written) to roughly quantify the impact of a particular reviewer. I further divided them into three groups (low medium and high quality) and set the means on the ratings they gave to these selected shows.  A clear trend came to light. There may be two reasons. Probably people tend to think the reviews containing some criticisms are more trustworthy or people who are actively comment tend to be on the picky side.ii. Seasonal fluctuations:Then I looked at the number of reviews vs. the month. Clear patterns can be observed in the bar graph. After the holiday season the number of reviews drop sharply in February. If we assume the number of reviews are correlated with the attendance it indicates that the tourist attendance at Broadway shows hit the bottom in February. It gradually picks up in the spring and finally reaches the peak in July as NYC is a popular destination for travelers during their summer vacations.  Attendance in the second half of the year remains good with some random fluctuations .When looking at this graph you may wonder if there is also some similar pattern in tourists' overall experience (eg. satisfaction or not). Is there an optimal time to experience  a Broadway show? I looked at the ratings in detail and found the rating distributions do not fluctuate too much across the year. This is a good news for tourists: you can go to Broadway any time of the year and enjoy the same experience from the shows.You may have noticed that the overall ratings are pretty high. It is indeed one problem of this data since it suffers strong ""survival bias."" These popular shows in Broadway are the best of the best. Competitions in Broadway stages are fierce. Only 20% of the new production each year can break even. Far fewer shows can achieve success and survive to the next season and beyond. Therefore the most popular shows must be outstanding in many ways so that people traveling in from across the country or across the world are willing to spend their money and their vacation time here  to see them.Another question I sought to answer was: do these reviews just contain words of praise without any insight?Not really. I will show you some analysis on the review simply using the word cloud.iii. Analysis on the reviewWe begin by running a word cloud for all the reviews.From this word cloud we can see a lot of key words such as “performance” ”song” ”story” “cast” and so on. But it is not easy to see any pattern from it. Sometimes too much information is not all that informative. So we need to take a different approach:. How about looking at individual shows?We can do so with the musical  a new production just landed on Broadway last March and it has the highest rating on TripAdvisor. Based on true stories happened in a small Canadian town far away from United States in the following week after 9/11 the musical makes us believe in humanity over hate in the darkest hours.From this word cloud we can see that ”story” is the biggest key word in the reviews indicating that what the audiences appreciated most was the story of the show. “Music” and “cast” received a lot of attention too. The storytelling is its best part. With no props beyond a few tables and chairs and no elaborate stage sets or costumes a dozen people vividly conveyed a warm story.Next (I am not following the order of the overall ratings) let’s look at  the longest running musical currently on Broadway ' . The word cloud shows that members of the audience are most fascinated by the music of the show. The shows’ songs drew countless people into the world of musicals including me. Besides music surprisingly people mentioned ”seat” a lot of times. I think it is probably because Majestic Theater is a bigger theater. Where do you sit really matters on the experience so that people tend to keep talking about it. In contrast to  ”story” is not among the key words anymore; “cast” received less attention too. For this show music outshines everything else.The next one word cloud comes from . This is where things start to get more interesting.Although the music of  is outstanding and  the story is wellknown to everyone those are not what people focus on. Costumes are the most commentedon component. I think this is indeed the key of its success because the music and story are nothing new to the audiences. But the spectacular costumes give the audiences especially the kids (another key words in the word cloud) sitting in the theater a totally different experience from watching a movie. Besides costume we notice that “'ticket” receives considerable attention probably because its tickets are normally quite pricey.Now let us move to  the biggest Broadway hit in recent years. What do people talk most about this show? If you think people talks about music history story or even rap about this show most often you are wrong. Actually “ticket” is the word which enjoys the most attentions.. In my opinion Hamilton is truly a work of genius but when people pay more attention on tickets than on the show itself it is not something good. Besides tickets of course audiences should like the music the story the cast and the performance and all of them are highlighted in the word cloud.We can easily see the four shows have different key words from the reviews. Thanks to the diversity of Broadway shows theaterlovers can always find what they like on the stages. Diverse as they are good music is the bottom line for good musicals. So you will find “music” is a significant element in all of them.Furthermore if the show is in a bigger theater people tend to mention “seat” more often since it is a critical factor. Similarly the more expensive the tickets are the more often people will talk about it. We see in the case of    which is playing  in a smaller theater with lower ticket prices that you do not see  “seat” and “ticket” in the word cloud. That indicates to me that for that musical people can focus more on the show itself in contrast to the in whichis of central concern and  in which “ticket” dominates everything else.We have covered some overall patterns of the reviews. How about criticisms? We do see some low ratings in the previous bar graphs. I looked into the negative reviews (have 1 and 2 in ratings) of some shows. Here is one example:We can see besides the complaints on the ticket price that “understudy”  was mentioned quite a few times. I do not think it is necessarily because the understudy did a bad job. It is natural that people got upset when they did not see their favorite actors/actresses showing up on stage. But when you add in the very high price they pay as a factor on their feelings the disappointment grows to the point of exaggeration. I checked out some bad reviews and found  people often emphasize that they paid a fortune for the show but ended up watching understudies. So if you really care do some homework one the cast schedules.In summary what is the take away from this little study?",NA,A Traveler’s Guide to Broadway Musicals
https://nycdatascience.com/blog/student-works/web-scraping/global-data-scientist-market-demand-analysis/,5,"As for the number of job openings New York is the city with the highest number of postings with over 1400 for the past month. NYC is followed by San Francisco and Seattle in the USA London in Europe Bengaluru in India and Singapore.Regarding the title demand almost 40 percent of the job openings are interested in bachelor's degrees almost 30 percent in master's degrees and about 25 percent in PhDs. USA is the only country that has a much higher demand orf bachelor's than for master's. India and Canada have high demand for bachelor's/master's. Interestingly European countries have higher demand for master's than bachelor's and PhDs seems to be most valued in USA UK Singapore and Canada.What we want to know next is whether the variability between countries and titles correlates to differences in salaries. For analyzing the salary data an important limitation was the low number of postings that reported salary (~10%). The salary data shown in these graphs corresponds to the countries of the top 25 cities (per number of postings).  This salary data is the result of averaging the upper limit of the posted salary range and the lower limit of the posted salary range per location and normalizing to annual pay and US $ currency. As expected USA has the highest paying jobs and the highest paying cities within it are: New York San Francisco and Seattle.PhDs earn about $10K more than people with a master's degree who in turn earn about $2K more than those with only a  bachelor's degree.However if we normalize for cost of living and rent UK Singapore and Canada catch up with the US. This trend is also reflected in the cities chart in which London Singapore and Toronto stand now almost at the same level as the highest paying cities in the USA. For normalizing for cost of living and rent the average salary was divided by the Cost of Living Plus Rent for Country Index 2018 ().
However if we divide the data between USA (Panel A) and nonUSA countries (Panel B) we can see that while in the US a master’s degree is related to a higher pay in the NonUSA countries it is not the case probably due to the fact that most nonUSA countries demand master's degrees and the number of postings that require a bachelor's is very reduced.Finally let’s have a look at the top hiring companies. Considering all the job postings Amazon Microsoft and Facebook stand at the top.If we look at the three top companies in the top 10 cities we find: JUUL Labs Autodesk and Uber for San Francisco and NYU Langone Health Weill Cornell Medicine and JP Morgan Chase for NY.Summing up considering that I acquired a solid understanding of Python R SQL and Machine Learning at NYC Data Science Academy bootcamp and these skills are required in all countries and that I have a PhD which is most valued in USA UK Singapore and Canada and within these countries the highest paying cities are: New York San Francisco Seattle Toronto London and Singapore you’ll probably find me in one of these cities hopefully not too many months from now!",NA,Global Data Scientist market demand analysis
https://nycdatascience.com/blog/student-works/scraping_zillow/,5,Data Collection:Cleaning:Quick EDA:First I wondered what is the difference between Zestimate and sale price whether that is the price a house was sold at or what the current price listing is. Below shows the Zestimate minus the sold price and as we can see houses  in the designated zip code generally sell for less than their listed Zestimate. Looking Forward:,NA,Scraping Zillow
https://nycdatascience.com/blog/student-works/web-scraping/scraping-nonograms-to-build-a-solver/,5,Nonograms are logic puzzles that create an image when solved. Originally invented in the 1980's by two different people simultaneously and became popular in the late 80s90s. Now they're known by almost 30 different names the most popular ones being nonograms hanjie puzzles picross puzzles and griddlers. The clues on the side and top denote which squares should be empty or filled in. The numbers denote the black squares and each group of black squares must be separated by at least one empty space. Here's a really simple puzzle being solved stepbystep.Each move has a reason and nothing is a guess. My dad after ordering a book of puzzles thought they might be a good machine learning project. So this web scraping project is the first step of building a nonogram solver using machine learning.From this website:  I scraped around 8K puzzles for the size clues solutions and difficulty of the puzzles. The biggest issue I came across was scraping the actual clues themselves. The actual table was hidden behind the javascript and I couldn't scrape it like everything else in the html. My solution to this was using scrapy_splash in my spider as a work around as was able to the clues as a list of empty spaces and numbers. Here is the data and its corresponding puzzle.,NA,Scraping Nonograms to build a solver using machine learning
https://nycdatascience.com/blog/student-works/who-gets-hired-an-outlook-of-the-u-s-data-scientist-job-market-in-2018/,5,"For those who are actively looking for data scientist jobs in the U.S. the best news this month is the   According to the report  there is a shortage of 151717 people with data science skills with particularly acute shortages in   and . To help job hunters (including me) to better understand the job market I scraped Indeed website and collected information of 7000 data scientist jobs around the U.S. on August 3rd. The information that I collected are: Company Name Position Name Location Job Description and Number of Reviews of the Company (Download the  from Kaggle). Excel Python and SQL turn out to be the most desired tools for data scientists where machine learning modeling and optimization are key skills that are mentioned the most in job descriptions. Counter intuitively excel turns out to be the most desired tool that employers want. It has been mentioned in 83% of the job descriptions. Quick answer is not really : ( Have you gone through interviews where you are ready to be asked about Logistic Regression and Random Forest but end up whiteboard coding an algorithm problem? You'll never know if they want a data scientist or a software engineer. Exhausting. To address this concern I separated all the positions into three groups: and. By comparing the job descriptions of these three groups I would like to know if they show distinct features in job functions degree requirement and major preference. This graph compares the tool frequency between these three groups. The number on each bar is the rank of the tool frequency within each group the length of each bar represents how often this tool has been mentioned in job description for each group. The longer the bar the more desired is this tool. For example for data scientists the top popular tools are python SQL R Excel and Scala where programming languages are more important for engineers. Data scientists and analysts don't show significant difference in terms of the tools they use.For skills engineer only show preference for artificial intelligence. Data scientists show preference for skills including machine learning modeling and statistical analysis where analyst show preference for data analysis data visualization and research. From my analysis job postings with the title ""data scientists"" and ""analysts"" have a high level overlap concerning desired tools skills degrees and majors. For job hunters it is very important for you to read the job description carefully and make judgments. Back to our question who gets hired? If you have a master's degree (or higher) in a quantitative field proficient in Python SQL Excel R and Scala mastered machine learning modeling optimization data visualization and artificial intelligence and most importantly you read job descriptions carefully you'll be hired!",NA,Who gets hired? An outlook of the U.S. Data Scientist Job Market in 2018.
https://nycdatascience.com/blog/student-works/analyzing-the-effects-of-game-review-and-price-on-player-ratings-on-gamespot-com/,5,"     The video game industry earned more than $100 billion in 2017 and is poised to surpass that amount for the next few years. For game developers such a huge market presents both opportunities and risks; the reward for a popular game is great but an unpopular title could also potentially mar a developer's reputation to gamers. On the flip side the explosion of game titles makes it increasingly difficult for gamers to choose the right games for their limited time and money. As such gaming websites such as Gamespot.com have served as an intermediary for both developers and players allowing developers to gain feedback from players regarding their games and gamers to better judge the quality of newly released titles. One interesting question that rises from such websites is how much effect do reviews on Gamespot actually have on player ratings? Furthermore do price of games play a role in how players rate them? Those are the principal motivations for my web scraping project. Eventually I want to see if there is a way to predict the player rating of a game based on the official review on Gamespot and the price. Such an algorithm would give game developers a tool to determine the best price for the game to maximize player evaluation.     The two websites I want to scrape are  as mentioned above and the . Steam is an online video game store that also allows players to review the games they have purchased. I am scraping that site chiefly to get the pricing information for games. The rating system on Steam also allows for comparison of user ratings from Gamespot and Steam. To narrow the field of search and amount of data I limited the genre of games to ""strategy"" and only PC games. The final results page for  and  are shown here.     Initially I tried to use scrapy for both websites. Soon I find that I am getting empty lists for all the Xpaths that I tried so I had to resort to Selenium for the Steam website. From Gamespot I want to scrape the following fields:From Steam the following fields are scraped:One note regarding the rating system on Gamespot and Steam: on Gamespot both the official review of the game and user ratings are scored on a scale of 0 to 10 but on Steam the user reviews only gives whether the game is recommended or not and the total score for a game is the percentage of users that recommended it. This scoring difference should be kept in mind when comparing ratings from the two websites. Once all the fields are scraped the plan is to merge the data frames on ""Title"" so analysis can be done.     On the whole the scraping process was pretty straightforward. The only issue I ran into was on the Steam website some of the games had Japanese or Chinese characters in their titles so when trying to write the names into CSV file it would cause an error due to different encoding. Another issue is that due to some of the special characters used on Steam website such as ™ or © the title of the same game would not match between the two websites. After several attempts I incorporated code to clean the titles of any nonalphanumeric characters before writing to CSV.      While merging the two data frames I realized that while the Steam page had over 10000 titles only around 250 of them matched those in the Gamespot data frame. Because of this I decided to merge the tables in two different ways left merge to preserve all the data in the Gamespot data frame and inner merge to get only the common titles in one data frame. Because of the low number of matching titles analysis involving price of games will be limited and not representative of overall picture for all games.     My initial inquiry into my scraped data involved analyzing some trends on the categorical data such as genre and developers. Within the overall genre of ""strategy"" games there are many subcategories. Looking into the average Gamespot rating of each category it appears that there is a healthy spread of scores from 9 to 4. However when we look at how many titles are actually in each subgenre only three genres (""RealTime"" ""TurnBased"" and ""Management"") have significant number of titles most of the rest of the genres have only 1 or 2 titles. The top 20 most prolific game developers and their average Gamespot rating is shown in figures below.      Next I examine the relation between the Gamespot official and user ratings. Unsurprisingly there is a large correlation between the two ratings especially for scores higher than 8. For games with Gamespot score of 5 or lower the user rating can span a wide range showing larger variations. The looser spread of user rating for lower scores is understandable as most people can agree when a game is great but for mediocre games users will more likely have disagreement over what makes it bad or even if it is bad in the first place. The lower quality games will also bring out disagreements between the critics at Gamespot.com and users as they may have different visions on what makes a game good or bad.     Similarly the same trend can be found in the Steam vs. User Rating plot. Here we again see the points are more clustered together at the higher ratings and more diverged in the lower ratings. In the Steam vs. User case we also see that the correlations is not as strong as in the previous plot. There are two reasons for this one is due to the different scoring system the two websites used as mentioned previously the other is that for some games there are no Steam rating available therefore a score of 0 is used.        The plots for rating vs. price show more interesting behavior. At first glance there seem to be no relation between the two variables. However after looking through the data more carefully I realized the reason for the apparent none relation. In my data set there are a lot games that were released before 2000 thus the price being sold on Steam is very low not reflecting the true price of the game when first released. The user reviews however are averaged over the entire time since the game's release. Therefore plotting the two variables together as is can be misleading. Because of this I decided to filter out all the games released before 2010 and see if we can see a clearer relation between price and game rating.     After the filtering we can distinctly see a clearer correlation pattern between user rating and price. The other two ratings Gamespot and Steam don't have as clear a correlation to price as user rating but still show improvement when the date filtering is applied. Calculating the correlation matrix between the variables before and after the filter further illustrates this point as the correlation factor between price and the three ratings went from around less than 0.1 on average to around 0.3 on average. The correlation between user rating and Steam rating also increased. Viewing the postfilter user rating vs. price plot in more detail one is tempted to perform a linear model to quantify the relationship between the two variables. In fact I did make such a model and calculated a slope and intercept for it. I even performed a Ttest to show that the slope I got was statistically significant. But upon further reflection it became apparent that such a model is illsuited to the present situation. The main reason being that it's unclear that the relationship between these two variables should be linear we just don't have enough sample to support such a claim.     From the extent of the analysis I performed for this project we can definitely say there is correlation between Gamespot official review and user reviews. The Steam user reviews due to a different scoring system and much larger number of reviews show much more diverse and unpredictable correlation behavior. We also found some correlation between price of games and user ratings on Gamespot. Such correlation is found to a much less extent in Gamespot and Steam reviews. Unfortunately due to the limited data collected it is difficult to draw more quantitative conclusions beyond what I have done so far.      In doing the project I realized how inadequate my data set is. In particular I was surprised at the low matching rate of the titles on Gamespot and Steam websites. Because of this I was only able to obtain pricing information for a minority of game titles making analysis difficult. There are several reasons for this one is that not all games are currently available on Steam another is the sorting algorithms for the two websites can be different thus even though I use ""strategy"" games as my filter on both sites they do not return the same set of game titles. To make more concrete conclusions regarding the relations between the ratings and price I will need to collect more pricing information on game titles. Since Steam has proven to be unreliable in this respect I will try different websites to obtain that information in the future. As for my overall goal of making an algorithm to predict user rating I will also need to include data from all kind of game genres not just strategy games. I hope to complete this step in the future.",NA,Analyzing the Effects of Game Review and Price on Player Ratings on Gamespot.com
https://nycdatascience.com/blog/r/visualizing-local-health-data-across-500-us-cities/,6,Within an American city the health of its citizens can vary from one block to the next. I used a dataset from the  to visualize this difference between neighborhoods across five hundred cities in the United States. This project analyzed data from the annual core questions from the  and used these in combination with census data to calculate  within neighborhoods of American cities.The project covers five hundred American cities which involved around 100 million citizens or about . The  within this dataset were split into three categories: unhealthy behaviors preventative measures and health outcomes. For each measure the dataset included crude prevalence with 95% confidence intervals. At the citywide level the ageadjusted prevalence was also given in which the prevalence that would have existed in a standard population’s distribution of old and young people was calculated. See the full list of measures in the table below:I decided to visualize this dataset at three scales: statewide citywide and within cities. On the statewide tab a user could compare two health measures across all the states within the U.S. and see the correlation between those two measures. This would allow a user to compare for example unhealthy behaviors with health outcomes. In the example below I chose “Sleeping less than 7 hours among adults aged >18 Years” and “Mental health not good for >14 days among adults aged >18 Years” which has a Pearson correlation coefficient of 0.83.,NA,How healthy is your neighborhood?
https://nycdatascience.com/blog/student-works/r-visualization/kickstarter-revenue/,6,"Kickstarter could generate more revenue by focusing on projects listed in the US in three categories  Film and Video Music and Technologythrough these three methods:Had these three points been satisfied from early 2011 to 2018 Kickstarter could have generated $4 million in additional revenue.Kickstarter is essentially an online market that connects creators and supporters. For an online market to grow one of the most important features that cannot be ignored is the ""Network effect"". This means that Kickstarter needs to grow both its number of creators and number of supporters. A creator is someone or some company that has an idea that needs funding to let it come to fruition. A supporter is an online user on the Kickstarter who decides to contribute his or her money for the project created by the creator. Because Kickstarter makes money when a project is successfully funded (5% of the money funded goes to the platform according to   it is very important for Kickstarter to make sure a creator's project meets the funding goal. Therefore the aim of this project is to help both the creator and the platform to improve its success rate and consequently increase revenue (More information here: )(Fig.1)The United States is home to over 77% of the project listings on Kickstarter (Fig.1). Given its position it makes sense to focus on the listings in the US first to see what Kickstarter can do to improve success rate and thus generate more revenue.(Fig.2)There are only four categories which have a success funding rate above 50% (number of funded projects/total number of projects). The highlighted dot (Fig.2) represents the Music category which ranks second for the largest number of projects listed and the highest number of projects funded with a success rate above 50%. Film and Video category apply to most projects but it does not have as many funded projects as the Music category. In fact the Music category has the fourth highest (Fig.3) success rate out of 15 major categories whereas Film and Video category has an almost average success rate of 38% (the horizontal line indicates average successful rate). (Fig.3)Both Music and Film and Video represent a tremendous revenue making opportunity for Kickstarter. Film and Video has 51992 projects listed and Music has 43238 projects listed. This represents 95230 projects listed (Fig.4) about 32.5% of all the US listings and 25% of all the listings on Kickstarter. Two things that can generate more revenue for Kickstarter : 1) increasing the success rate for Film and Video category 2) increasing the listings for Music category.(Fig.4)How much revenue can an improved success rate in Film and Video and increase listings in Music generate for Kickstarter? For Film and Video the median amount funded for all the successful listings are $5000 (Fig.5). If the success rate can be improved from its current 38% to 50% which means around 6959 more successful projects there will be an additional $34795000 in which translates into  $1739900 in terms of revenue just from this category.(Fig.5)For the Music category (Fig.6) if the project listings can be increased to as many as Film and Video about 8754 more listings an additional $30639000  in funding and $1531950 in revenue will be generated given the median funding for successful projects in this category.(Fig.6)(Fig.7)Focusing on improving these two categories is not enough however. The Technology category has the lowest success rate 22% (Fig.7) and yet has an average 796 backers the second highest for each project under its category. This represents a tremendous waste of supporters’ time and money not to mention the creator’s effort involved in posting the project on Kickstarter. On average a successful project only asks for 270 backers (Fig.8) not 796 backers.(Fig.8) In short many supporters funded projects that will fail and many creators create projects that will not be funded. Kickstarter not only lost revenue from the Technology listing but also gives an idea perhaps to firsttime supporters that projects on Kickstarter are meant to fail. Instead Kickstarter should look into why these projects fail and why these projects attract supporters in the first place. One reason for example could be the amount of funding demanded for supporters (Fig.9). The median funding asked is $10800 which is twice as much as Film and Video asked. However this is not the whole story. Projects in technology receive attention because of what they promise. One place to look is “.”This project in technology has been overfunded by more than 10000% that is 100 times the original goal. The project has a very detailed description and various images of the products. Supporters who funded this project will likely to come back and recommend this website to others because they are finding a project that interests them one that can potentially improve their sleeping quality. How much potential revenue is represented in this space? On average the projects that have been successfully funded have a median funding goal of $4000(Fig.10). Assuming now all the technology projects have adjusted their scope and funding demand to $4000 and improved their success rate from  22% to average which is 38% . Furthermore the resources that have been distributed to projects in technology will no longer be wasted. Less funding asked also means more money will go to other projects and category perhaps Music and Film and Video.(Fig.9)(Fig.10)From this brief discussion above for listings in the US Kickstarter could have generated almost $4 million in extra revenue from three categories discussed above. Therefore Kickstarter should aim to improve the success rate for Film and Video increase listings in Music and educate creators in Technology category to generate more revenue in the years to come.",NA,3 Strategies To Help Kickstarter Generate More Revenue
https://nycdatascience.com/blog/student-works/sentiment-analysis-and-data-exploration-of-yelp-reviews/,6,":  Restaurants are constantly getting feedback.  is one channel but many restaurants document verbal feedback gather feedback from questionnaires or even from other review sites. When faced with a huge amount of text restaurants need a way to objectively interpret their reviews. It is very difficult for business owners to go through list of reviews and distill  relevant information from it. Also it does not provide advanced analytics to business owners to grow their business and improve their services. My solution will focus on providing advance analytics from Yelp data to help existing business owners future business owners and users.The objective is to design a system that uses existing Yelp data to provide insightful analytics and help both existing and future  business owners make important decisions about launching or expanding their business. It provides opportunity to business owners to improve their services and users to choose the best business from the available options. By building a way to relate textbased feedback to a star rating we can help these restaurants understand how they would rate on Yelp. This post is about using the web scraped online reviews from Yelp to have a better understanding of the industry. These review analysis can help the customers what to expect from a particular restaurant and also business can look into the different opinions and work on the aspects that could possibly lead to a low rating.  :I divide this project into 2 segments based on different objectives.I started with recognizing the pattern of 50 pages and implemented the same in my algorithm to set those as the starting URLs in a list comprehension. Most of the time it wouldn’t scrape beyond the second page. The whole process of web scraping (Yelp website) was quite a challenge but rewarding.While scraping the Pricing and Rating information I ran into scenarios where the Pricing was not listed for a few restaurants which resulted in some missing values. These missing values are imputed by using the high frequency value for pricing.As the data got bigger the categories also increased for each variable. In order to visualize the values we had to restrict the data to limit to top 15 places in the city with the most restaurants.Most of the attributes were obtained in an easy manner. However Pricing posed some challenges as some restaurants were missing this information. Also the class information for Location was not consistent across the restaurants causing us to handle these as exceptions.I got started by looking at the places in NYC which tend to be expensive. Below is the barplot of these attributes. As we see West Village and Greenwich Village has the high volume of expensive restaurants.Here is a plot of pricing and Rating of these restaurants. Surprisingly restaurants which are more reasonably price do seem to be having the highest rating. Does that mean that more and more customers visit the places that are more affordable causing them to have good rating? That is one possible cause.Now let’s see where are these highly rated restaurant which are also affordable are  located.The Lower East Side is the place to be. This place has the top most rating (a rating of 5.0) followed by DUMBO. The Majority of the restaurants in the financial district seem to have a very poor rating.These visualizations will enable existing and future business owners to decide to what extent  they need to consider location in their plans. This would help them identify good locations in which tor open their first restaurant or branches of their existing business .Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text. Sentiment is often framed as a binary distinction (positive vs. negative) but it can also be a more finegrained like identifying the specific emotion an author is expressing (like fear joy or anger).In our analysis I applied  both forms of sentiment analysis i.e. the traditional and the NRC.There are many ways to do sentiment analysis though most approaches use the same general idea:For example ""sick"" is a word that can have positive or negative sentiment depending on what it's used to refer to. If you're discussing a pet store that sells a lot of sick animals the sentiment is probably negative. On the other hand if you're talking about a skateboarding instructor who taught you how to do a lot of sick flips the sentiment is probably very positive.Since we are only interested in 4 highly reviewed restaurant we treated these separately. The 4 restaurants are Eataly Morimoto Tao Uptown and  Ilili. Scraping them individually also posed some challenges due to the limits beyond certain pages. The reviews are sorted by low ratings and these pages were scraped to analyze the sentiment behind these.The reviews from these 4 restaurants are then imported into R and combined into a single file.The review contents is then cleaned to remove any punctuations junk character white spaces and new lines. The stop words like ‘I’’ me’ ‘we’ etc. were also removed as they wouldn’t add to our sentiment analysis.I generated a word cloud of the 100 most frequently appearing words across all restaurant review textto get an idea of the vocabulary people tend to use in their reviews. I noticed a lot of the bigger words seemed neutral but we do see words like “time dinner disappointing wait”etc. It was also interesting to see words such as “manager “crowded” and “price” appear large in this cloud as it gave us an initial indication of things that might matter a lot to reviewers.:Future entrepreneurs always to consider where and what kind of business one should open to gain maximum profit. Using the above proposed solution business owners will be able to determine what kind of business is more profitable sustainable and attract more users in a particular city or area. In addition they will also be able to determine what type of restaurants (e.g. Mexican Chinese etc.) price range and location are more favorable to succeed in particular city.",NA,Sentiment Analysis and Data Exploration of Yelp Reviews
https://nycdatascience.com/blog/r/interactive-restaurant-discovery-with-shiny/,7,Welcome to the world of restaurants that we have yet to discover. There is so much to venture off too that what we know so far about food has barely begun to scratch the surface. Thankfully due to Zomato. A online database that helps inform us of all restaurants around the globe. Its like yelp but international. They are not as popular in the states but to those who are interested get a sense of wonder of what has left to be discovered. With Zomato I was able to construct a shiny app displaying the various types restaurants out there.,NA,Interactive Restaurant Discovery with Shiny
https://nycdatascience.com/blog/student-works/r-visualization/visualizing-pm-2-5-levels-in-five-chinese-cities/,7,     Due to the rapid industrialization of the past 30 years and still lax environmental laws air and water pollution has become a major issue in China in recent years. Among the air pollutants microparticles (P.M) pose a serious threat to public health. PM 2.5 refers to particles with diameter equal to or less than 2.5 micrometers. Due to their small size they can penetrate deep inside people's lungs and even into their bloodstream making them especially hazardous to health. I was born in Guangzhou China and though have emigrated to the U.S. since I was 10 years old I still go back to visit family in China regularly. This issue is therefore something I want to investigate not just for academic but personal reasons.         The data set used is available from Kaggle: . It provides PM 2.5 measurements in Beijing Shenyang Shanghai Chengdu and Guangzhou from 2010 to 2015. In addition to PM 2.5 levels it also gives information on variables like temperature humidity air pressure precipitation etc. The wide scope of the data affords us ample ways to analyze PM 2.5 problem in China. The main questions that I want to answer through this project are:     As downloaded the data set is pretty clean. It is actually separated into five different .csv files for the five cities so the first preprocessing step is to merge the different files into one data table. Also for each city the PM 2.5 levels were measured in three to four different locations. To facilitate analysis and since we are making comparisons between the cities and not interested in the variation of PM 2.5 levels within each city I averaged the PM 2.5 levels across all locations in each city and used that as the PM 2.5 level for that city. After performing the above step and combining the .csv files I removed all rows which do not have a reading for PM 2.5 level.  values in other variables were still accepted. Note that data is not available for all cities in the entire time period from 2010 to 2015. Only Beijing has data in 20102011 while others start from 2011 to 2013. Final preprocessing step involved combining and converting the yearmonthday columns into date format which allows timeseries plots to be possible.     Since the PM 2.5 data includes geographical information the most natural way to visualize and compare PM 2.5 levels is through a map. The first visualization tool I used is a GoogleVis GeoChart which shows the mean PM 2.5 levels in the five Chinese cities over a userdefined time period. Each circle is drawn and colored according to the measured PM 2.5 level during that time period. From the map we can clearly see that over any time period the mean PM 2.5 levels in northern cities like Beijing and Shenyang is much higher than southern cities like Shanghai and Guangzhou. Chengdu in the interior lies somewhere in between. Occasionally it can be better than Shenyang but it is still worse than Guangzhou and Shanghai in general. The mean values for PM 2.5 range from 51 µg ⁄ m in Guangzhou to 95 µg ⁄ min Beijing. According to the EPA a 24 hour exposure to PM 2.5 levels higher than 35.4 µg ⁄ mis consider unhealthy. Thus all cities in the data set have mean values higher than healthy and some cities can have peak values in the very unhealthy to hazardous range.       To compare PM 2.5 level to the other variables in the data set I made a scatter plot. While I could have chosen more the three variables that I choose to study vs. PM 2.5 level were temperature dew point and air pressure. For each city and year the daily value of PM 2.5 and the selected variable were plotted. Viewing data this way it is difficult to find the pattern due to the sheer number of points. Therefore I included an option to aggregate the data into monthly averages. I also grouped the points by season so it is easier to see pattern if one exists. Looking at the monthly data points we can discern a pattern in the temperature vs. PM 2.5 plot. A qualitative examination shows that the PM 2.5 levels are negatively correlated with temperature. For the plots of PM 2.5 vs. dew point and air pressure no obvious correlation is observed.      To examine the time evolution of the PM 2.5 levels one of my ideas is to make a histogram plot. Basically I am looking at the daily PM 2.5 level of a city in the course of a year and counting the frequency of days that a range of PM 2.5 level occurs. What I want to observe is that as I advance the years the histogram would shift to the right meaning there would be more days with lower PM 2.5 level. In practice this proved to be difficult as the range of PM 2.5 levels fluctuate from year to year changing to bin size and making comparison between years difficult. I tried to fix the bin width for each plot but still could not completely solve the problem.     The time evolution of PM 2.5 levels were visualized through a time series plot. Here I plot the whole series of PM 2.5 data as function of time (date) allowing option for user to select city(cities) to displayed. One very noticeable feature of the graph is that the PM 2.5 level peaks every year around winter time. This is observed for all cities. One explanation for this especially for the northern cities like Beijing and Shenyang is that China still uses coal as main source of energy for home heating thus creating more pollutants in the winter time. For southern cities like Guangzhou though the explanation is harder. My conjecture is maybe more intensive industrial activities in the winter? To definitely see if there is improvement over the years I used a bar graph to plot the yearly average PM 2.5 levels for all five cities. Though not all cities are as obvious but there is definite decrease in PM 2.5 levels for all cities in the time period studied. This is good news as the data shows that though the PM 2.5 levels are very unhealthy in general for cities in China the situation is improving. If we can get the more recent data from 2016 to now we can see if the trend is just an aberration or real improvement is being made.     In conclusion in the project I have studied the PM 2.5 levels for five Chinese cities from 2010 to 2015. While the plots and graphs I have made can definitely helped me answer the questions that I started with the analyses I must say are all qualitative in nature. With the time restriction I had for this project I simply did not have time to conduct more detailed statistical analysis of the problem. That hopefully I can come back to in the future.              During the work on this project there were a few challenges I had to overcome to complete it. One is simply familiarize myself with how the inputs and outputs work in a Shiny app. I decided to use GoogleVis for all my visualizations and for each plot I had to manipulate the data into a form that is recognizable by GoogleVis to make the plot that I wanted. The most difficult is making the scatter plot. Since I wanted to group the data points by season I had to separate the values of PM 2.5 into four separate columns by season then assign the four columns as separate variables. However for cities with incomplete data in a year such as Guangzhou in 2011 the missing data for a season would generate an entire column of  values which would caused an error for the plot. Eventually I was able to find a solution. Instead of generating the four columns for seasons by hand using statements which would generate a column with all missing values if there is no data for that season I used the  function in the  package to unpack the data automatically which would only generate columns for seasons that have data. This avoids passing columns of all  values to GoogleVis plot function which caused the error. I think able to find the cause and able to overcome that problem in my app is one of the more valuable things that I learn from making this app.           As I mentioned before this project so far has been a very qualitative analysis of the PM 2.5 problem in China. One area that I can definitely work on is to use more quantitative methods. For example correlation relation between the several variables I studied with PM 2.5 can be examined with a Chi Square test. The yearly average PM 2.5 levels for each city can also be analyze with an ANOVA test to see if their mean is truly decreasing or just is the result of statistical variation. I can also look into how to incorporate data from other areas such as industry traffic and weather/climate to gain deeper insight into the PM 2.5 problem in China and offer possible viable solutions to this problem.  ,NA,Visualizing PM 2.5 Levels in Five Chinese Cities
https://nycdatascience.com/blog/student-works/r-shiny/us-obesity-trend/,7,The main objective of this study was to observe the demographics of obesity in the United States. Obesity remains an important issue because almost half of the world’s adult population could be overweight or obese by 2030 according to  published by McKinsey&Company. Obesity is often related to health conditions such as heart disease stroke type 2 diabetes certain types of cancer in addition to contributing to economic and productivity costs.My main goal was to identify distinct segments with high obesity within the US population. Market segmentation could allow for more effective targeting of products and marketing campaign such as promotion of gym memberships healthy eating programs or healthcare discounts in order to move Americans towards healthier weights.The datasets for analysis were obtained from Data.gov. The datasets include national and state specific data on youth and adults’ diet physical activity and weight status across years and throughout different demographics. Youth data were collected every two years from 2011 to 2015; and adult data were collected every year from 2011 to 2016. Please refer to my Shiny app () for links to the raw datasets.The below heat maps show the percent population on the selective input chosen by the user. The selective inputs include obesity diet and physical activity attributes for both youth and adult groups. Looking at the obesity data the southern states have the highest obese population whereas states that have a more active lifestyle such as Colorado and Hawaii have the lowest obesity rates.The user can further explore the patterns of different diet and physical activity across the states with the heat map. Another way to investigate the correlation between obesity and the behavior factors is through scatter plots.The below scatter plots show a general trend between obesity versus the different behavior attributes that are included in the heat map selection. The data points on each scatter plot represent a different state’s data over time.The graphs below show that states with higher percent population of fruit and vegetable consumption tend to have lower percent population of obesity. But interestingly physical activity does not have a significant correlation with youth obesity levels. This suggests a dietfocused strategy might be more beneficial in moderating youth obesity.Adults’ obesity trend for fruit and vegetable consumption is similar to the youth group. On the other hand physical activity also has a significant correlation with adult obesity. For each category of physical activity the states with higher percent population that performs physical activity have lower obesity rates. The inverse relationship is shown for no physical activity. Both diet and physical activity play an important role in weight management for the adult group.In addition to obesity data provided for each state the datasets also provided a breakdown of obesity across different demographics for each state as well as the whole nation. Below are a few interesting trends I observed from different demographics at the national level.For the age demographic obesity tends to stay at the same level for youth group (age 1417) but gradually increases in the young adult group (age 1835) and reaches the highest from mid 40s to mid 50s.For ethnicity demographic obesity trends are similar among youth and adult groups. African American and Hispanic have higher obesity rates followed by Caucasian and Asian. But again adults have a significantly higher obesity rate compared to the youth group.Two additional demographic categories were provided in the adult dataset. The adult population with lower education (less than high school) have higher obesity rates (close to 40%) compared to college graduates (low 30%). Lower income population also tends to have a higher obesity rate.Perhaps educational campaign on a healthy lifestyle should start at an earlier age. Weight loss products and health insurance plans could be personalized toward low education low income and even certain ethnic subgroups of the population.The previous demographic data showed a unifying trend where adults have higher obesity rates compared to youth across all years. I was curious to see how obesity changes across age and if the change can be explained by the behavioral factors captured in the dataset.I took a snap shot in time and picked the most uptodate year to construct the box plots below. The data points represent each state’s data in 2015 – the median obese population sharply increases in the young adult group from age 1835.Fruit consumption has no obvious trends across age and interestingly vegetable consumption rises with age. This might seem to be a contradiction to the earlier trend observed for vegetable consumption but it is not entirely because the earlier trend was constructed within individual groups of adult data and youth data respectively whereas the comparison below is made across groups. The box plots below simply indicate that the diet parameters from the dataset cannot explain the sharp increase in obesity for the young adult group which I was interested in.As for physical activity different categories of physical activity are either similar or higher for the young adult group. This again indicates that physical activity may not be the main contributor to the significant increase in young adult obesity. Other factors such as high carbohydrate diet sleeping patterns stress levels and changes in metabolism should be investigated.,NA,US Obesity Trend
https://nycdatascience.com/blog/student-works/r-shiny/which-commodities-are-competitive-in-trade-with-europe/,7,■ Background■ Before the Main Subject The dataset used in this analysis is the trade data from 2000~2018 opened by  and includes the import and export amount by product . HS (Harmonized System) code is commodity classification number established by WCO (World Customs Organization) and used worldwide for trade activities. At present about 5000 commodities are classified but in this analysis we analyzed only 15 sections and 100 chapters in order to find out the general tendency of each commodity.■ Data Explore The most major importing country was Germany in 2017. With Germany some other major countries' the most import commodities were in Transportation and Machinery/Electrical Sections. ■ Looking Closer In Transportation Section Import of Ship and Floating Structures was greater than Vehicles in Greece UK and Norway. These status are seen in countries with lots of merchant ships. Recently  for shipping and took effect the new regulations since 2017. So Greece (with the most merchant ships in the world) and a few other countries needs to prepare for satisfying the regulations and increase import of environmental equipment for their ships.In Machinery/Electrical Section The Netherlands and Western European countries have a high share of Electrical Equipment while Russia and Eastern European countries have a high share of Mechanical Equipment. In 2018 All of EU countries adopted to achieve environmental object until 2020. As one of those efforts some countries are considering adoption of renewable energy technologies. Especially Spain already completed to bid for the renewable energy project (including wind power) and start to construct from 2018. Additionally The  has plan for installation of Wind Power Generation Facilities from 2019 to 2023. Secondly many global automobile and heavy industry companies invested Eastern European countries for their new manufacturing bases. In case of the Balkan countries Hungary attracted investment of Daimler Audi BYD and Bosch and they are building their factories. Czech and Slovakia already have many automobile and parts factories (Skoda PSA Jaguar and Land rover) and some makers expand their facilities for manufacturing high end and high technology cars.■ Conclusion Over the years global ship oversupply has continued and order has sharply dropped. Previously the competitiveness of shipbuilders was to build large vessels and high quality (ex. LNG) vessels. However in addition to new environmental regulations the company need to concentrate on developing environmental equipment such as Ballast Water Treatment Equipment and Emission Gas Reduction Equipment. In addition with the new power generation facilities that utilize renewable energy in some countries Utility Machinery/Electrical Equipment and Construction Materials are expected to increase. Finally regarding the establishment of manufacturing base of Eastern European countries it is necessary to secure the partnerships of initial facility supply and support parts supply and raw material supply and to secure the market of machinery and steel products.,NA,Which commodities are competitive in trade with European countries?
https://nycdatascience.com/blog/student-works/r-shiny/visualization-of-gun-violence-incidents-in-us/,7,US is famous for its second amendment: the right to bear arms.  As a side effect the amount of gun violence incidents are also very high. For my shiny project I chose to work with  and try to glimpse into some statistics behind these incidents. My shiny app can be found  and the source code is available on my github .The data was downloaded from the . It contains roughly 240k incidents ranging from 1/1/2013 to 3/31/2018. The record includes rich information about each incident such as date place information about participants information about guns involved incident characteristics and so on. I first cleaned the data with the following criterion in mind: I would like the information to be as complete and accurate as possible. So I filtered out those observations with missing or inconsistent information. I ended up with a dataset encompassing roughly 180k incidents.The dashboard contains three tabs on the sidebar: an interactive map to show the distribution of incidents on the map a collection of interactive charts to show aggregated statistics and a table to give further information about each incident. There are also two filters on the sidebar for the users to use to select the information they would like to display: either by number of victims or by characteristics of incidents.The interactive map displays cluster points of the incidents happened between the time frame of users' choice. And the popup contains brief information of the incidents alongside with a URL directed to the page from the gun violence archive's website where further information can be found. For example the map below shows the distribution of all incidents with drug involvement in the dataset and highlights a particular incident that happened in North Platte. ( The map is defaulted to focus on the mainland. though users can drag around to see more on Alaska and Hawaii.),NA,Visualization of Gun Violence Incidents in US
https://nycdatascience.com/blog/student-works/gun-violence-in-america/,7,Gun control is a politically divisive topic with people committed to strong opinions on the matter. But what does the data tell us?  I decided to take a look myself using the following Kaggle dataset:The dataset itself is a database of individual incidents complete with the date and location of the incident the number of people involved and the relationship between the suspects and victims.  A full analysis of all the information contained in the dataset would surely produce valuable insights though it would be a massive undertaking beyond the scope of the project. Consequently I decided to narrow my focus on only the time and space dimensions.  Furthermore because the data contains incidents for the years 2013 through 2018 but the data for 2013 is sparse and 2018 is incomplete I decided to omit those years from the analysis.To visualize gun violence over time I created calendar plots with heat map shading that indicates the severity of deaths per capita (as measured by the number of deaths per 1m residents) and the number of incidents.  The most striking result is the sheer ubiquity of gun violence in America; in the 4 years from 2014  2017 there were only 4  on which there were no incidents with the average number of incidents per day at around 100:Of course the violence is not spread evenly across the country; some states are plagued by more gun violence than others.  For example despite having half the population of California Florida has roughly the same number of incidents per year:Perhaps the city with the worst problem with gun violence is Chicago and the despite the fact that Illinois has a third of the population of California it has a substantially higher number of incidents:Chicago is home to Illinois's Congressional District 7 which is most affected by gun violence; disturbingly it has a similar number of daily incidents as the entire state of Pennsylvania:In terms of the number of deaths per capita the states with the worst problems with gun violence are Arizona Nevada Alabama and California:Because the dataset included the congressional district in which each incident occurred I wanted to see what if any relationship gun violence had with party affiliation.  Below are maps of the continental United States for each year where congressional districts are colored by party (Blue for Democrat Red for Republican and Green for DC) and the shade of the color is proportional to the severity of gun violence as measured by the number of deaths per capita:While the maps don't change very much from year to year (and it is thus difficult to glean insight from their progression) it is interesting to note that some of the congressional districts with the most severe gun violence are also heavily gerrymandered.  One possible explanation for this could be that gun violence is more common among lower socioeconomic classes with higher proportions of minorities which tend to vote Democratic and that the relevant congressional district borders were drawn to minimize representation of this demographic.This analysis was only a cursory first look at a dataset that is rich with information and could potentially provide valuable insight to help better understand the causes effects and potential solutions to a very real  and very serious  problem facing our country.,NA,Gun Violence In America
https://nycdatascience.com/blog/student-works/r-visualization/5-things-you-should-know-about-the-future-of-population/,8,About a decade ago I stumbled upon a TED Talk by a Swedish global health professor named Hans Rosling.  During the video I learned more about global macrotrends (infant mortality GDP etc.) opening my eyes to some misconceptions and piquing my interest.  Dr. Rosling’s moving bubble plots enraptured me and his broader audience. See here:I decided to analyze similar data for myself to see what I could confirm or uncover about the future of population.  I used R to transform data from the  and  (Dr. Rosling’s organization) into a suitable format for my analysis.  After leveraging dplyr to summarize my data sets I plotted the data using ggplot2 and googleVis.  Lastly I built an R Shiny app to showcase my findings and to enable the user to interact with the plots say by selecting the population category or by toggling by time period.  Africa made up 16% of the world’s population in 2015 yet it is projected to rise to 40% of the world’s population by 2100.  Therefore Africa and Asia together will make up roughly 80% of the world’s population likely moving the locus of the world’s economy further southeast.  In that same timespan the UN expects Asia's population to continue to grow until it plateaus and starts declining circa 2055. Potential implications for this growth include insufficient food and water supply breakdowns in security and increases in migration.While the explicit drivers of the UN model are unspecified two likely contributors include life expectancy and birth rate per woman.  First life expectancy is projected to continue to rise in Africa and in fact has already risen beyond the 2007 figures from Gapminder.  Second birth rates will continue to come down but have lagged significantly in many African nations contributing to the continent’s population boom.  By contrast birth rates in much of the rest of the world have dropped to around or below replacement rate (roughly 2.1). The UN data for 2020 show Niger’s birth rate at 6.8 (highest) versus Portugal’s at 1.3 (one of the lowest).  You can see the discrepant birth rates in the coloring of the below map.(Certainly I would be remiss if I did not acknowledge other interrelated features like women’s education access to contraceptives and improved economic opportunity.  The UN’s projections must account for a multitude of features while not doublecounting their shared impact but I do not know the precise modeling techniques employed).Nigeria will experience tremendous population growth and will overtake the U.S. as the third largest country in the world.  More babies will be born in a year (over the coming 25 years) in Nigeria than will be born in all of Europe.  Note that Europe is about 11x the size of Nigeria so Nigerian population density will be quite high.  Nigeria’s population growth is staggering and littleknown.Per 2020 UN projections over half of the world’s youth live in Asia and a quarter in Africa.  By 2100 this will shift to nearly half in Africa and a third in Asia leaving only about 10% on the remaining continents.  Significant potential implications include youth unemployment brain drain and insufficient capacity of education systems.The aging of the world’s population will become more pronounced not only in the usual suspects of Europe and North America but also in places like Latin America and Asia.  The below rose plot shows total population of each continent from 1990 to 2100 colored by age grouping. The increase of the darkblue illustrates the aging of the population as a percent of the total. Regardless of what sector you’re in implications abound.  In the private sector there will be approximately two billion new middle class consumers mostly out of Asia.  As it is this example speaks volumes: Alibaba’s Singles Day brings in $25 billion in revenue compared to Amazon Prime Day’s $1 billion.  In the public sector we will contend with how to pay out social security and pensions how to bend the healthcare cost curve given the graying of America and how to sustain economic growth in areas with population declines.  Finally in the social sector we will need to work to improve the prospects for water supply for tertiary education access in Africa and in job retraining in the US and Europe amongst many other implications.Explore the app yourself here:  (free account so may expire at some point)View the code: Browse the presentation pdf:,NA,5 Things You Should Know About The Future of Population
https://nycdatascience.com/blog/student-works/stack-overflow-who-are-the-people-behind-the-posts/,8,If you have been coding for a while you must have visited this website several times possibly even thousands of times. We might be amazed that this website can help us to get our code working over 90% of time. In many cases we may go so far as to say it is our only hope. After being saved again and again are you curious who are those people behind the posts?I am. so I chose Stack Overflow 2018 Developer Survey results for analysis. The data set itself is huge so I just use a small subset (~35000 observations ~25 attributes) for this visualization project.My analysis on this data has three main parts: general profile of the respondents work related information and life related information. I will show them one by one. (Link to Shiny app:),NA,Stack Overflow: Who are the people behind the posts?
https://nycdatascience.com/blog/student-works/r-visualization/examining-historical-nfl-gambling-data/,8,"This application is primarily interested in examining the disparity between the realized results of NFL games and the predictions made by the Sports Books in Las Vegas. The visualization tools attempt to isolate a small handful of variables to identify trends and predict error.Bet on the underdog to cover the spread! Don't only look towards the most recent Super Bowl Champion Philadelphia Eagles who were underdogs in all 3 of their playoff games. Take a look at the first chart on the spreads page showing that nearly 400 more games saw the underdog cover the spread than the favorite. The Error Variable (i.e. residuals) is expected to average 0 as we examine more games because we assume ""Vegas knows"" but instead a simple ttest shows that the average Error is 0.35 points and is statistically significant.Now let’s take a look at spreads from the 2014 season. If there ever was a year of the underdog that's it.This application uses a dataset of every regular season and postseason NFL game since the 1979 season through the Super Bowl in 2018 (the culmination of the 2017 season). In addition to descriptive data of the NFL games and their results the data includes what Las Vegas sports books offered as spreads (including the favorite team of course) as well as the over/under line.In both the spreads and O/U analysis there is an important variable called . Error is measured in points and can be positive or negative. In the case of the spreads positive Error means the favorite covered the spread while negative Error means the underdog covered the spread. In the case of the Over/Under positive means the teams combined to score more than the over/under line while negative means the teams scored less than the over/under line. Error is essentially another term for residuals. We would expect error to be 0 but the data proves that to not always be the case. We can also examine absolute error  measuring by how much Vegas missed the mark.The NFL is the highestgrossing sports league in the world. In 2017 the NFL generated approximately $14 billion USD. In July of 2018  that 29 of the NFL's 32 teams are included in their list of the 50 most valuable sports franchises in the world. The American Gaming Association a casino lobbying group projects that Americans bet $4.76 billion on Super Bowl LII in 2018 with more than 97% of that figure represented by illegally placed bets. Some of those types of bets may become legal in the near future though. In May of 2018 the U.S. Supreme Court lifted the federal ban on sports gambling. States will gradually legalize sports gambling over the next few years and the industry could very well evolve to become unrecognizable from what it is today.Two common aspects of the game to place bets on are the winner of the game and the final score of the game. Since not every matchup between two opponents is an even match this can be offset by the spread. The  is essentially a number of points handicapped to the perceived underdog so as to make the bet of who is going to win the game one that can be made with (nearly) even money. The  bet is simpler: it's a figure that represents the total score of the two teams combined.",NA,Examining Historical NFL Gambling Data
https://nycdatascience.com/blog/student-works/what-makes-a-successful-app/,8,For the last ten years Apple iOS store has now seen more than . By today there are more than 2.2 million apps included in Apple Store. While the most popular apps such as Facebook Netflix and Instagram see hundreds of thousands of downloads per day a huge number of apps don't get downloaded once in a month. As an app developer you might wonder what makes people download an app and what makes the most popular apps successful. With such concern I developed this shiny app as a reference tool for app developers to gain insights of the Apple Store market. In case you are not clear which genre your app falls in the genre comparison panel will be a good start. While Games has dominated the app market by taking over  50% Social Networking apps received the most reviews with an average number of 45496. Not surprisingly Facebook is the most popular app with a total reviews over 3 million. Before you publish your app you may want to  choose your competitors wisely. In this Genre Comparison panel you can compare genres by the number of reviews rating scores App Size App price etc.  The developers of Tinder must have thought this through and therefore categorized them as a lifestyle app which makes them the second most popular app in this category. What do the successful apps have in common? In other words what are the predictors of a popular/successful app? By analyzing the pattern in the popular apps we can find the clue. On the correlation panel of the Shiny App app developers can explore the predictors of success by looking at the correlations between variables. Take Social Networking as an example App size turns out to be the strongest predictor of the popularity. The larger the size the more reviews an app received. ; See my ),NA,What Makes a Successful App?
https://nycdatascience.com/blog/student-works/visualizing-wine-reviews/,8,The wines reviewed originated from 42 different countries and ranged in price from $4 to $3300. Reviews were written by at least 20 different professional wine tasters (some anonymous) and included a rating of the wine on a 100point scale. Only wines with a rating of 80 or higher are reviewed and included in the database. The rating scale used by Wine Enthusiast magazine is provided below. A “Classic” rating is extremely rare  in fact only 115 wines among the 110000 reviewed received a rating of 98 or higher.One of the first questions you may have when shopping for a wine and see its rating is to ask whether the wine will be good enough for the occasion or if it’s better to spend more money and get a better wine. To better visualize the relationship between price and rating I’ve plotted the rating against the wine price for 5000 wines sampled from the data set. It’s no surprise that yes higherpriced wines do tend to have higher ratings. However the extremely highpriced wines graphed on a linear scale make it difficult to see the relationship for the majority of the wines reviewed so I’ve also plotted the price on a logarithmic scale which shows a much more direct relationship between price and wine rating. The good news for (frugal) wine lovers is that the spread in the data for many of the wines in the $10 to $100 range reveals that there are still many wines with “Excellent” ratings of 90 and above within reach. Using the linear regression line in the Rating vs. log(Price) graph allows us to determine that wines plotted several points above the line are better values compared to others in its price range or rating category.Next I was interested in finding out if the ratings and prices for different varietals varied by country and if there was a significant difference in price and rating. These bar charts which compared the average rating and median price for several varietals from the 5 countries with the most wines reviewed showed some surprising insights as well. For example Bordeaux Redblends from Portugal on average were more highlyrated than the other four countries and had a much lower significant median selling price. On the other hand Spanish Rieslings were rated lower than the other four countries shown but sold for a similar price. Comparing favorite wine varietals in this manner enables consumers to find better deals on higherrated wines and encourage them be more adventurous in trying different wines.Finally I also put together a few tools to help explore the full database of wines reviewed and find the best values on wines specified by varietal and price range. The user can also choose a desired rating category and search for the wines Idetermined were the “Best Values” from my wine rating vs. price analysis. Another tool allows the user to see the most popular wine varietal from different countries around the world and each of the U.S. states that produce wines. This may be useful for travelers wishing to find the best type of wine to drink when visiting or to buy as a memorable souvenir.Cheers!Data source: Data analysis and interactive charts were developed with R and Shiny and can be found .,NA,Visualizing Wine Reviews
https://nycdatascience.com/blog/student-works/capstone/a-closer-look-at-tree-boosting-methods/,9,"In the book  the authors stated that “Boosting is one of the most powerful learning ideas introduced in the last twenty years.”  This statement was so captivating that we decided to do our capstone project on boosting and learn about the AdaBoost Gradient Boosting (GB) and XGBoost methods.  This capstone project is unlike others in the NYDSA Bootcamp because it is researchoriented in nature.  What we hoped to achieve from this was the ability to locate and understand relevant and reliable  literature and be able to understand them. This is important because there are so many sources of information nowadays ranging from blogs to online discussions with varying degrees of reliability.  Furthermore modern learning models such as neural networks are becoming increasingly complex. So it may not hurt to gain some technical exposure. Lastly we hoped to learn how to explain technical information to a nontechnical audience in our final presentation and in this blog.Boosting is a learning approach that was initially developed over twenty years ago for classification problems.  The publication years for the models we consider were 1996 for AdaBoost (Freund and Schapire) 2001 for GB (Friedman) and 2016 for XGBoost (Chen and Guestrin). Boosting turns out to be a general purpose method that can use .   But what is amazing as we shall see is the basic idea behind boosting with AdaBoost which was to combine a sequence of socalled “weak” learners to create a much better prediction model.  This was an innovative idea when it was first introduced in the scientific literature in 1996.  XGBoost appears to be the most popular choice as it is used in many .  We set out to find the answers to the following questions: Why is XGBoost so popular?  How is it different from AdaBoost and GB?  What is boosting?  Is boosting used exclusively for decision and classification trees? How do we use these boosting models? In this blog post we will provide answers to some of these questions.Our blog is organized as follows:  First we illustrate the fundamentals of boosting using AdaBoost.M1 (Freund and Schapire 1997) in a binary classification task.  Second we describe the basic ideas behind GB and XGBoost.  The background knowledge for these two models are somewhat steep so we will focus on an “in a nutshell” type of discussion. Third we discuss model parameters.  We will not provide a stepbystep recipe but outline a general strategy based on what we learned from our research.  Finally we present some results from an analysis of a  from Kaggle. Our primary aim in the data analysis is to compare the performance of the three boosting models. The main idea behind boosting in AdaBoost is to combine “weak” classifiers to achieve a much better classification performance.  A “weak” classifier is defined as a learning algorithm whose error rate is only slightly better than random guessing. In binary classification the “weak” classifier is better than tossing a fair coin.  A model example is a two terminal node decision tree called a “stump”. In AdaBoost a weak classifier is sequentially applied to a dataset.  In each iteration the data is modified according to whether observations are correctly classified or not.  Observations that are incorrectly classified are assigned higher weights than in the previous classification whereas those that are correctly classified have their weights reduced.  This allows previously misclassified observations to have a higher chance of being classified correctly in the next iteration. Initially the observations are assigned weights of 1/N where N is the number of observations in the training set.    Suppose that the binary outcomes are {1 1} and that there are M sequentially produced weak classifiers by AdaBoost.  A final prediction can then be obtained by applying the sign function (sign(x)  1 if x<0 0 if x 0 and 1 if x>0) on weighted predictions of the M weak classifiers.   These weights are determined by the boosting algorithm and are distinct from the weights that are used to modify the data. Higher weights are assigned to the more accurate classifiers which results in a weighted majority vote.  The AdaBoost algorithm process just described is illustrated in the figure below.  This illustration was adapted from a toy example in  by Freund and Schapire.  We suppose that there are M3 weak classifiers. After a classifier is applied to the data the misclassification error and classifier weights are calculated.  From these two items the weights for modifying the data can be calculated and consequently applied to the data. Note that in the figure the data that are misclassified and hence upweighted are slightly enlarged.  The correctly classified data remain the same size in the figure although these should really have reduced weights and sizes. At each iteration the weights sum up to one. The sign function applied to the weighted sum of the weak classifiers produces a classifier that is more complex and captures a good decision boundary for classifying the data.  We encourage you to determine how this final classifier was obtained by working out the arithmetic inside the sign function in the second figure for each data point.Surprisingly a combination of stumps produces a very good classifier.  ESL provides a simulated data example (10.2) which we reproduce below using codes from the .  Random samples from independent standard Gaussian variables are generated to produce 10 features. The classes are determined based on whether the sum of the squares of the 10 features is larger than 9.34 the median of the chisquare random variable with 10 degrees of freedom.  Recall that the sum of the squares of p independent standard normal random variables is chisquare with p degrees of freedom.  The simulated data has 2000 training observations with approximately 1000 cases in each class and 10000 test observations. We train and test a stump a ninenode decision tree and a boosted classifier of 400 decision tree stumps.   The figure below presents a visualization of the simulated data points using the first two principal components.  We can see roughly two sets of data points  one set inside (dark color) and another set outside (yellow color) the cloud of points. The figure below presents the classifier error rate versus the number of boosting iterations (or weak classifiers) of the three classifiers.  The green horizontal line on the top corresponds to an error rate of approximately 46% when using only a single stump as classifier. A decision tree with a depth of 9 has an error rate of approximately 31% (orange horizontal line).  When boosting stumps (blue lines) the error rates dramatically decrease with the number of iterations achieving approximately zero training error at around 250 iterations. Note that beyond this we expect our model to become more complex and hence overfit.  Note also that the test error appears to continue decreasing which suggests that there is no overfitting. This surprising result was empirically observed for boosting and it contradicts the conventional biasvariance tradeoff. But this is not always the case and there are examples of boosting leading to overfitting.AdaBoost has been actively studied for several years after it first appeared in the literature.  It was not until after a few years later that it was discovered to fit an exponential loss function under a general framework called forward stagewise additive modeling (FSAM).  We shall omit the discussion about FSAM and describe GB and XGBoost only in the general sense. A boosted tree model is a function that is formed by a sum of tree models.  Basically you initialize a tree and fit it to your data.  You then keep adding more trees to it until you obtain a more accurate model.  Each added tree is supposed to lead to an improvement of the previous treeaccumulating model that is a better fit in the sense of minimizing the loss function.  You may recognize this idea of accumulation that leads to an improvement from a numerical optimization procedure called gradient descent used in neural networks.  In gradient descent you initialize the model parameters and move in “steps” that should lead you to the minimum of the loss function. The accumulated “steps” will give you the parameters that yield losses that are closer to the minimum of the loss function.  However whereas in gradient descent we are searching within the parameter space in gradient boosting we are searching within a “function space.” Thus GB can be regarded as gradient descent in the function space (ESL; Nielsen 2016).   The “step” that we just described consists of a gradient which determines the direction of maximal descent.  In GB a tree is fit to the negative gradient using least squares. This will yield an optimal tree partition.  Then the weights or values assigned to the partitions are determined via an optimization procedure. Thus for GB optimization involves finding an optimal tree partition for the fitted tree and then finding the optimal weights for each partition separately.  XGBoost also approximates the negative gradient but the gradient is weighted by the hessian or second order partial derivative of the loss function and the fit is performed using weighted (by the hessian) least squares (Nielsen 2016). What this ultimately means for XGBoost is that the optimization for the tree partitions and the weights are done simultaneously instead of separately as in GB.  This may be a more efficient optimization procedure and could lead to better estimates. We also suspect that this leads to faster computation.Whew! That was plenty of technical stuff.  But we can actually derive a simple result from this.  For regression problems that use the squared error loss function the negative gradient is simply the ordinary residual.  Thus in GB the boosted tree model is obtained by fitting a tree on the residual from the tree model in the previous iteration.  In other words for squared error loss in regression GB combines trees that are fit sequentially on the residuals. GB is a general model because it can accomodate any loss function.  When using an exponential loss for binary classification GB will yield AdaBoost.  Note that in Python GB uses deviance as the default loss function. However these two loss functions are actually very similar (ESL); so we should expect AdaBoost and GB to yield very similar results.   We briefly discuss selected model parameters in this section.  We will not attempt to provide a specific prescription of how to tune these parameters.  Rather we summarize suggestions or recommendations that we found in the literature and recommend a “to start with” strategy.  The selected parameters are shown in the table below.  They are categorized into three. The first three are the main parameters and they are common to all three models.  The remaining parameters are grouped according to their purposes to induce randomness (in yellow) and provide regularization (green).Note that AdaBoost can be implemented in Python using the AdaBoostClassifier which includes the three main parameters.  However AdaBoost can also be implemented using the GradientBoostingClassifier with an exponential loss as the loss function (the default is deviance).  Using this implementation allows AdaBoost to also utilize the additional parameters subsample and max_features. Max_depth controls the depth of the tree.  The deeper the tree is the more complexity is introduced and this will not only increase the variance of the estimator but also decrease the computational speed of the algorithm.  Note that adding just one level doubles the complexity of the tree. A recommendation by ESL is to use tree depths of between 4 and 8 inclusive in practice. The learning_rate controls the contribution of each tree that is added to the ensemble.  It can be regarded as controlling the learning rate of the boosting procedure. N_estimators is the number of boosting iterations (or the number of trees in the boosted model) and it should be set depending on the value for the learning_rate.  Low learning rates typically need more iterations and viceversa. Note that more iterations translates to slower computational speed. ESL states that the best strategy might be to set learning_rate low (<0.1) and then choose n_estimators by early stopping.When tuning we found it reasonable to start first with these three main model parameters.  We also found it helpful to create a plot of the training and test error rates (for classification) versus the number of iterations (n_estimators).  The following plot presents an example showing the results from the simulated data we saw earlier with the boosted stumps for AdaBoost and learning_rate1.    The figure also includes the error rates in red when using a tree with max_depth3 instead of a stump.  For this more complex tree model the training error decreases much faster but the test error is worse than the stump’s.  It appears that the stump will yield the better estimator for the simulated data. This should come as no surprise because the data was created in an additive fashion with no interaction.  In general stumps capture additive main effects while deeper trees can capture more complex interactions. We found it useful to plot the learning curves above instead of doing a grid search where we select only a few values for n_estimators e.g. 100 200 400 and 1000.  A problem with this approach is that the learning curves are not always continually decreasing. In the figure above it seems that there is a minimum test error rate when n_estimators  50 which will be missed by our grid search example.  The parameters that induce randomness are subsample max_features (in GB) or colsample_bylevel (in XGBoost) and colsample_bytree.  When tuning these parameters only a portion of the data will be used for training as described below. This strategy allows the model to generalize better. To subsample is to use a portion of the observations or examples for training e.g. 30%.  This could produce more accurate models and can also reduce the computing time by roughly the same fraction as the subsample.  Note however that based on simulations a subsample can be harmful when learning_rate  1 (no shrinkage) (ESL).Max_features and colsample_bylevel is the same strategy used in random forests.  Tuning these parameters entails using only a portion of the features or predictors when constructing each tree level in an iteration.  Colsample_bytree means that you use only a portion of the features or predictors when constructing a tree in each iteration. Thus when using both colsample_bylevel and colsample_bytree the portion of features or predictors when constructing each tree level can be considerably reduced which may improve computational speed.The last two parameters reg_alpha and reg_lambda perform L1 and L2 regularization of the leaf weights respectively.  Recall that the weights are the function values assigned to the tree partitions. These parameters shrink the leaf weights in an effort to combat variance.  When tuning these other parameters you may want to first study the effects of each parameter on the learning curves.  For example you can study what happens to the learning curves when subsampling by 0.3 0.5 and 1.  Then you can study what happens when you vary colsample_bylevel.  By doing this you will hopefully gain some insights as to how the learning curve changes when tuning these parameters.  From there we hope that you can then formulate a grid search or other strategies to find the optimal model parameters.  We explore the performance of the three boosting methods using a  from Kaggle.  The task is to build a model that’s capable of classifying comments as toxic obscene insult severe toxic identity hate or threat.  The dataset contains 159571 comments from Wikipedia’s talk page edits.  The following figure presents some useful information about the data.The left side of the figure shows that this classification task is not a multiclass but a multilabel problem.  In a multiclass problem each example or observation is assigned to only one class or label. In a multilabel problem each example or observation can have more than one classes.  For example in the third row of the table above there are 3800 comments that can be classified as toxic insult and obscene at the same time. The right side of the figure presents the dataset as unbalanced in terms of the labels frequency.  Whereas toxic obscene and insult are at least 5% frequent the remaining labels are at most 1% frequent. The figure above presents the workflow we adopted.  For multilabel modeling we identified at least three approaches in the literature: one versus rest chain classifier and power set.  For our analysis we chose to implement one vs rest. This strategy treats each label as an unique target and converts the task into binary classification. As there are six labels the final model will be a collection of six binary classifiers. For natural language processing (NLP) we implemented stemming and TfIdf.  Stemming is a process that chops off the ends of the words but keeps its basic meaning.  TfIdf is a numerical statistic that is intended to reflect how important a word is to a text.We compare AdaBoost GB and XGBoost in terms of test accuracy and computation time.  We present only results for the toxic label in the following table. The performances of AdaBoost and XGBoost appear to get better with increasing model ""complexity."" By complexity here we mean deeper trees (larger values of max_depth) or more iterations (larger values of n_estimators).  The performance of GB appears to get worse with increasing ""complexity."" The table below summarizes the best test accuracies from the previous table and the computation time for all the calculations in the previous table.  The test accuracies are almost the same for all three models but they differ considerably in computational time. The computational time for XGBoost is approximately onethird that of either ADaBoost or GB.We also compared the three models on three other datasets: the simulated data from ESL spam email and  from Kaggle.  For the Evergreen dataset we excluded all observations with at least one missing feature for the time being which reduced the sample size by about a half.  Note that our aim is to compare the performances of the three models and not necessarily to obtain the best predictive performance. Note also that the best performance in Kaggle for this dataset was a test accuracy of around 89%.The results for the simulated and spam email data were very similar across all models.  We found differences only for the Evergreen data where there was increasing prediction accuracy from AdaBoost to GB and to XGBoost.  The improved performance in XGBoost resulted from tuning the regularization parameters reg_alpha and reg_lambda and using a tree of max_depth15.  In this capstone project we learned about the basic ideas that led to the development of the earliest boosting model AdaBoost and how well it performs.  We also attempted to describe the general ideas behind GB and XGBoost and learned that GB uses a first order numerical approximation while XGBoost uses a second order approximation.  We suspect that this optimization procedure by XGBoost could potentially lead to better results and may be responsible for its efficient computation. We also described the model parameters and learned that XGBoost has more tuning parameters than GB which makes it a more flexible model.  However this also makes tuning the parameters a more difficult process.  We outlined some strategies for parameter tuning in this blog post but this is an area where we need to do more research on. Although we have not seen clear predictive advantages of XGBoost compared to GB on the datasets that we considered we suspect that there may be other datasets where XGBoost could potentially beat GB.  For example there may be a dataset with many features in which the model parameter colsample_bytree can be used to sample the features and possibly lead to better predictive performance. We have already seen how tuning reg_alpha and reg_lambda can lead to better performance for the Evergreen dataset but we would like to see results on other datasets to confirm this advantage. For the toxic comments dataset we may consider exploring other multilabel approaches such as chain classifier and power set and also other NLP techniques that might help improve model performance such as lemmatization.",NA,A Closer Look at Tree Boosting Methods
https://nycdatascience.com/blog/student-works/credit-card-approval-analysis/,9,":  The decision of approving a credit card or loan is majorly dependent on the personal and financial background of  the applicant. Factors like age gender income employment statuscredit history and other attributes all carry weight in the approval decision. Credit analysis involves the measure to investigate the probability of a thirdparty to pay back the loan to the bank on time and predict its default characteristic. Analysis focus on recognizing assessing and reducing the financial or other risks that could lead to loss involved in the transaction. There are two basic risks: one is a business loss that results from not approving the good candidate and the other is the financial loss that results from by approving the candidate who is at bad risk. It is very important to manage credit risk and handle challenges efficiently for credit decision as it can have adverse effects on credit management. Therefore evaluation of credit approval is significant before jumping to any granting decision.: Algorithms that are used to decide the outcome of credit application vary from one provider to another and across sectors and geographies. However there are high degrees of similarities in the attributes used to generate those algorithms. In this project I have collected data from the Credit Approval dataset available in the archives of machine learning repository of University of California Irvine(UCI) ()The main objective of developing a Credit Card Approval Shiny App is to show the impact of different fields like Gender Age Income Number of years employed etc on the approval for a Credit Card. This app have some static graphs(which include histograms scatter plots box plots etc) and some interactive plots that will help user to select the fields of interest.The primary objective of this analysis is to implement the data mining techniques on a credit approval dataset. Risks can be identified while lendingdatabased conclusions can  about probability of repayment can be derived and recommendations can be put forward.The Credit Approval dataset consists of 690 rows  representing 690 individuals applying for a credit card and 16 variables in total. The first 15 variables represent various attributes of the individual like fender age marital status years employed etc. The 16th variable is the one of interest: credit approved(or just approved). It contains the outcome of the application either positive(represented by “+”) meaning approved or negative (represented by ““) meaning rejected. This dataset is a multi variate dataset having continuous nominal and categorical data along with missing values.Below is the structure of the dataset:str(Credit_Approval)Classes ‘tbl_df’ ‘tbl’ and 'data.frame':690 obs. of  16 variables: $ Male          : chr ""b"" ""a"" ""a"" ""b"" ... $ Age           : chr ""30.83"" ""58.67"" ""24.50"" ""27.83"" ... $ Debt          : num 0 4.46 0.5 1.54 5.62 ... $ Married       : chr ""u"" ""u"" ""u"" ""u"" ... $ BankCustomer  : chr ""g"" ""g"" ""g"" ""g"" ... $ EducationLevel: chr  ""w"" ""q"" ""q"" ""w"" ... $ Ethnicity     : chr ""v"" ""h"" ""h"" ""v"" ... $ YearsEmployed : num  1.25 3.04 1.5 3.75 1.71 ... $ PriorDefault  : chr ""t"" ""t"" ""t"" ""t"" ... $ Employed      : chr ""t"" ""t"" ""f"" ""t"" ... $ CreditScore   : num 1 6 0 5 0 0 0 0 0 0 ... $ DriversLicense: chr  ""f"" ""f"" ""f"" ""t"" ... $ Citizen       : chr ""g"" ""g"" ""g"" ""g"" ... $ ZipCode       : chr ""00202"" ""00043"" ""00280"" ""00100"" ... $ Income        : num 0 560 824 3 0 ... $ Approved      : chr ""+"" ""+"" ""+"" ""+"" ...And some stats for all these fieldsBelow is a quick overview of the missing values in the dataset:These initial plots showed that all variables have distributions that are skewed to the right indicating that the data is not welldistributed about the mean. In order to reduce the skewlog transformations were applied and then plotted again.Below are the plots of the discrete variables that appear to influence whether a credit application is approved.As expected Prior Default and employment status appear to have the most significant effect on the approval. Persons with prior default are rejected more than 90% of the time and those who not employed are rejected 70% of the time.Let’s see if the education level has any effect:From the graph  we see that people with education level ‘x’ have an 85% chance of approval ascompared to ‘ff’ who are rejected 85% of the time.As we see a high credit score resulted in approval 90% of the time and applicants with higher income have a higher than average approval rate.Finally I did a pairwise comparison of all the fields using a scatter plot you can see below:These plots do seem to have a scaling problem. One reason for this could be the presence of outliers. The range of the values is high causing the regression line to adjust for these outliers. For now we will not be working on handling these. But from the plot we can see that Years Employed has the highest linear correlation with the Approved field.From this initial analysis we are able to conclude  that the most significant factors in determining the outcome of a credit application are Employment Income Credit Score and Prior Default.Based on these insights we can work on building some predictive models. They can be used by analysts in financial sector and be incorporated to automate the credit approval process. These results can also serve as a source of information for the consumers.Modern credit analyses employ many additional variables like the criminal records of applicants their health information net balance between monthly income and expenses. A dataset with these variables could be acquired. It’s also possible to add complementary variables to the dataset. This will make the credit simulations more   similar to what is done by the banks before a credit is approved.The shiny application is available on this link: And the code is available on GitHub location below:",NA,Credit Card Approval Analysis
https://nycdatascience.com/blog/student-works/predicting-home-prices-in-ames-iowa-via-supervised-regression-machine-learning-techniques-in-python/,9,In this post we detail our process as we work through the data with Python constructing various regression models with the Sklearn package with the goal of optimizing our prediction score.To build a successful model one must first understand the data. We evaluated all features one by one to determine which would need to be manipulated in our data preparation step.   We first looked at our response variable Sale Price which appeared to have a rightskewed distribution. Given that the data is continuous and linear models work under the assumption of normality we applied a log + 1 transformation to normalize the distribution. Most commonly missingness represented a zero value the lack of the feature at a residence. This was true for Fireplace Quality Basement Quality Garage Condition Pool Quality etc. These were usually filled with a value such as “NoFireplace” “NoBasement” etc. In cases where there was a very low percentage of missingness we evaluated according to the rest of the data and outside context. For example for the “Electrical” feature there was only one value missing and the vast majority of houses were categorized under standard circuit breaker so the missing value was categorized under the majority as well. For Lot Frontage we used knearestneighbors (kNN) to fill in the missing values given that we could not assume that Lot Frontage was something that did not exist. The kNN method would also be taking into account an average of all the features. We conducted a grid search to find the best value for neighbors (k) by looping through and evaluating the negative mean squared error (MSE) at each level of k.  After evaluating we tuned the model to k  14. There were two types of categorical variables in the data ordinal and nominal categorical. Ordinal categorical variables are those that have an inherent order. The most obvious example of these variables are those with classes measuring quality such as “Poor” “Fair” “Average/Typical” “Good” “Excellent”. We assigned these ordinal categorical variables a numerical value based on their corresponding rank. Nominal categorical variables in contrast have no inherent order.  Most of these values were dummified so that a new variable was created for each class within a variable with “1” value if the class was present and “0” value if the class was not. While performing EDA we identified categorical variables to find those with a clear relationship to the output variable Sale Price and then used target encoding to change the category to ordinal.  One example of this type of variable was MSZoning. In the boxplot below showing MSZoning vs Sale Price it’s clear that there is an order from FV (Floating Village Residential) to C (Commercial). We therefore assigned numbers to the ranking found in the below boxplot and also combined RH (Residential High Density) and RM (Residential Medium Density) into one since these were similar.When looking at the relationship between GrLivArea (Above Ground Living Area) and SalePrice we observed four outliers that were distant and not in line with the other observations. All four outliers were greater than 4000 square feet in above ground living area. The two that we ended up removing also had an abnormally low sale price. These were removed as they did not follow the general linear trend of the data and removing these points helped to improve our prediction score.Given that normally distributed data is a key assumption underlying many machine learning algorithms we evaluated the skew of our our input variables alongside our output variables using the skew function in the Scipy package. We determined a threshold of 0.75 and applied a log +1 transformation to all features above the determined threshold to reduce skew. Ex. LotFrontageWe identified 17 pairs of features with a correlation greater than a determined threshold of 0.65 to each other and having a lower correlation to the response variable SalePrice.  We removed the feature of each pair that was less correlated to SalePrice. We also removed 4 variables with very low correlation to SalePrice regardless of other correlations.Overall we removed 21 features from the dataset.We created two additional features for our dataset based on contextual understanding. Several modelling methods quantitatively rank features an output which is very useful in providing a clear and interpretable understanding of which features in our dataset are the most important.We examined feature importance via two methods Lasso Regression and Random Forest while also trying out Principal Component Analysis (PCA) to reduce the dimensionality of all features used. Lasso Regression applies a regularization term to the modeling algorithm which reduces the size of the coefficients in a regression equation eventually reducing to 0 the features deemed least important to the algorithm. To compare we also ran the RandomForestRegressor within the scikit learn package to see if similar variables would pop as the most important. We used the feature importance measure from the RandomForestRegressor to obtain the ranking.In both the Lasso and RandomForest outputs we saw that OverallQual and TotalSF popped as the top two most important features. Lastly we tested applying Principal Component Analysis (PCA) to our dataset for dimensionality reduction but did not apply the results in the end given that the independent variables after applying were not interpretable. PCA works by rotating the axes so that features are reduced to the best combinations together. Below is a plot of our results:There is a clear elbow a flattening of the negative slope at approximately 10 components demonstrating that via PCA the majority of the variance of the dataset can be condensed into 10 features.We used 10 fold cross validation on the training set and tested several different models and combinations. Initially we tested both linear and nonlinear single models. For linear models we tested Lasso Ridge and Multiple Linear Regression while the nonlinear models tested were Support Vector Machines Random Forest and XGBoost. The hyperparameters for each model and their cross validation scores are outlined below. For single model selection we obtained the best results with the Support Vector Regressor using a radial kernel. To improve results we also tested ensembling and stacking methods. Ensembling entails averaging predicted y values from different models. Stacking uses machine learning models to perform regression on predicted y values from different models. The below chart outlines the top five stacking or ensembling submissions as well as hyperparameters that were used and the score we received from Kaggle.Our top model was a stacked model using Lasso SVR (rbf) XGBoost Random Forest Neural Network Perception with a Lasso MetaRegressor. The chart below visualizes our best model’s predicted price vs. actual on our training set since the test set did not contain the Sales Price data. While our best model fit relatively closely for most homes as sales price increased beyond $350000 the error increased as the model consistently underpredicted the sales price.,NA,"Predicting Home Prices in Ames, Iowa via Machine Learning in Python"
https://nycdatascience.com/blog/student-works/webscraping-zocdoc-in-new-york-and-analyzing-doctor-reviews/,9,Zocdoc is a company that was created aiming to make booking doctors easy based on a variety of factors that you can select such as your insurance what language you speak location etc. You can also make an informed choice by seeing reviews for doctors by their overall rating bedside manner and wait time rating. After leaving my employer to start bootcamp I was forced to face a reality where I was suddenly not on a partially subsidized employer sponsored health plan. Experiencing some of the pain points of being on a drastically different insurance plan sparked my curiosity to look into Zocdoc given that they are an online medical care booking service  that stores all information about doctors in various cities. Using Scrapy Splash I gathered information and reviews from all doctors in the New York area on Zocdoc. In scraping I ran into one challenge in terms of the site. I wanted to scrape all categories and I realized that no matter what category you land on you will never see beyond 10 pages of doctors all of which were dynamically rotating. To resolve this issue I had the scraper iterate through the maximum number of pages per doctor type. Once the scraper ran through I cleaned and reorganized the data. First I parsed out the reviews since overall rating bedside rating and wait rating were grouped together as one string. I also categorized each doctor’s office location into the different boroughs based on zip codes. Finally given that there were over 100 different doctor types I recategorized the doctors into broader categories to truncate the types to less than about 30 categories.After cleaning the data I began to do some exploratory analysis. I first looked at languages spoken overall in the NY area and Spanish Russian and Chinese (Mandarin and Cantonese) were the top languages spoken in doctors’ offices. The average number of languages spoken in each office was 1.9 so on average each office speaks at least one other language aside from English. Breaking this down by borough Staten Island had the lowest average  at 1.75 with a maximum number of languages spoken per office of 3. Meanwhile Brooklyn had the highest average at 2.24 with a maximum number of languages spoken of 5.Next I looked into doctor types by gender. There were many more men that were surgeons urgent care doctors and ENT (ear nose throat) doctors versus women who dominated as OBGYNs dermatologists and primary care doctors.Finally I did a deeper dive into the review data. Zocdoc asks you to rate three aspects once you have met with a doctor for an appointment which is shown in the screenshot below:In taking a closer look at some reviews from different doctors it’s clear to see that all interactions in the office are taken into account and not just the interaction with the doctor. Therefore a doctor's rating could be affected by poor outside office staff or conversely could be rated higher because of good staff even if patients are not particularly happy with the doctor's care itself. When looking at the distribution of ratings for each type of rating the distribution is similar and skewed pretty high on a scale of 1 to 5 for both genders. Where we see a slightly different distribution is for wait rating where males actually get down to a minimum of 2.5 which could signify male run doctor offices may not be as timely.I then created word clouds to compare words that popped in the best and worst ratings. Below is a word cloud for five star overall ratings on the left and five star bedside ratings on the right. Here words like “professional” “thorough” “pleasant” and “friendly” standout.In contrast below are word clouds for three star ratings to show words that popped for lower ratings. The left is a word cloud for wait ratings and the right is a cloud for bedside ratings. Unfortunately there was not enough data to look lower than three star ratings. For the three star ratings words such as “time” “wait” “long” popped for wait rating while words such as “worst” and “upsell” popped for bedside ratings. Even with a few more negative words the three star ratings still for the most part contained positive words.I used a TFidfVectorizer from scikitlearn in order to collect the most relevant bigrams from the lower rated reviews. Terms that are bolded are things that doctors should be aware of in order to improve their overall practice.Analyzing reviews for each doctor through tools like bigrams and wordclouds are some ways that Zocdoc would be able to provide value back to doctors on the platform so that they could adjust their respective practices if there are consistent painpoints.This analysis could be expanded through further Natural Language Processing using sentiment analysis. Zocdoc allows doctors to be sponsored on the site and pushed to the top of search results persistently. One thing I noticed however is that the review and doctor that is featured is not being vetted to make sure that a negative review is not being featured. For example in the screenshot below the sponsored doctor has a high rating but the review that is featured speaks negatively of the rest of the office.Using sentiment analysis this type of issue could definitely be avoided to make sure that doctors that pay money to be sponsored are featured with one of their positive reviews as well.,NA,Webscraping Zocdoc in New York and Analyzing Doctor Reviews
https://nycdatascience.com/blog/student-works/predicting-house-prices-using-machine-learning-2/,9,"Our machine learning project was a Kaggle competition for predicting home prices in Ames Iowa. The dataset had 1460 observations each with 79 features for homes sold in Ames between 2006 and 2010. We had to predict sales prices for the test dataset of 1459 houses in Ames which were also sold between 2006 and 2010.While achieving a high level of prediction accuracy was important to place well in the Kaggle rankings our team decided to also prioritize the interpretability of our results. As such instead of focusing on complex machine learning models that might result in the best fit we framed the problem statement as what models would give us both high prediction accuracy and also highlight the key factors home buyers consider when deciding on how much to pay for a home. We believe that framing the problem in such a way makes our models more generalizable and as such we could use them to derive insights in other instances with a similar context.Our team collaborated on the data exploration and data preparation stages of the project from which point we branched off and individually explored an arsenal of machine learning models best suited to meet our project goals. As a result every team member got handson experience developing machine learning models and we also had at our disposal a selection of models to choose from each with a different degree of accuracy and interpretability. Our chosen model was an ensemble of a Gradient Boosted (55%) and a Lasso model (45%) which resulted in a RMSE (root mean squared error) of 12.14% on the test data set and placed us in the top 20% of the Kaggle leaderboard (at the time of the submission).Our blog post is divided into four main parts: Exploratory Data Analysis & Feature Engineering Model Creation & Deployment Conclusions & Future Work and Relevant Links.The data contained significant missing data though most of it was intentional and simple to address. Several instances however posed trickier challenges. Aiming for greater accuracy we noticed a numeric column LotFrontage (denoting the footage of the lot that touched the road rather than another property) varied greatly depending on the shape of the house and the configuration of the house (located in a culdesac? Or perhaps at the intersection of two roads therefore having two sides face the street?). We took the median LotFrontage values for each shape and configuration combination and imputed the missing values accordingly.Regarding engineering we took a very proactive manual approach. We built heat maps to compare categorical data. We broke house sale prices into deciles and compared how many instances of each category fell into which deciles to attempt to determine trends. Consider the following example: as you can see below houses with no veneer are more likely to fall in the lower deciles for house prices while houses having stone or brick veneers are more likely to fall into higher deciles. Based on this we created a dummy variable asking if a house had no veneer and another asking if a house had stone or brick veneer. Because this is not a guarantee but simply a trend we determined it was safe to use and posed a comparably insignificant risk of data leakage.As a starting point we built a multiple linear regression model because of its interpretability power. This model will also be used to compare to other models' performance. Analysing the residuals plot of the MLR model we see that outliers were present in the dataset. According to the plot below we removed two outliers that had leverage and were influencing the model's result.After the MLR we implemented different regression models to reach better results. We tried Random Forest XGBoost Ridge ElasticNET however none beat the results given by the Lasso and Gradient Boosting Model. All models were trained considering the original dummified dataset the engineered dataset and a dataset with high power variables for some continuous features. For the Lasso model we had to standardize all variables eliminating the influence of the absolute value of a variable over the model. For tuning the hyperparameters we used the gridsearch with kfold crossvalidation method. The result for both models are summarized below:Since the two best models do feature selection we can look at the most important variables for each one. There is an overlap of variables that are key to both models even though the dataset used was different. We observe that the living area lot area year built overall condition and quality have a great influence on the models' results. Considering that some features are just used in one of the models we realized that ensembling both we could get a better performance. For that we used a weighted average to get our final result. The chosen weights were 55% for the GBM output and 45% for the Lasso.As a conclusion the topics are:  About  a manual Feature Engineering is good to have a better understanding of the dataset but the ""original features"" can provide valid information as well. For the next step exploring the original features and features created all together and let the model give the  can bring a new result in terms of reducing the error.The was strongly this involves starting on the most basic aspects of the problem such as reading features definitions followed by feature engineering and then train model/ evaluate error. The error evaluation often gave us ideas to modify our feature engineering even to revisit our understanding of the original features. So we were able to train the model again and have a new error evaluation this process became the dynamic to find a better model.To explore the categorical features we had the idea to create clusters using unsupervised techniques to do so our first try was using Kmeans. But there is a specific algorithm to create clusters using categorical features called  Kmodes. The Kmodes gave us clusters but they didn't perform well in feature selection.The Next Steps would involve ""hybrid approach"" to combine engineered features and original features stacking to combine 45 best performing models Bayesian optimization for tuning hyperparameters (mostly in  treebased models) and Investigate if external data sources can help increase explanatory power of the models (Eg: Interest rates employment data income etc.).As for next steps we can first try to feed the models all variables we have created as well as the original ones. This way we would let the model would select the features that are important for reducing the error. We could also try a different ensembling technique like stacking to see if the results would be improved. Since treebased models have many hyperparameters we could have to use Bayesian optimization instead of gridsearch to get a better tuning result.",NA,Predicting House Prices Using Machine Learning
https://nycdatascience.com/blog/student-works/visualizing-nycdoe-public-schools-demographics/,9,"Education policy leaders in New York City have finally noticed the misrepresentation of its talented and underserved students in the reputable specialized high schools.  Results in the highstakes assessment Specialized High School Admissions Test (SHSAT) are at the heart of receiving entrance into eight of the nine specialized high schools. In an effort to diversify these schools organizations have begun to stepin and offer services to students that deserve to be represented. is one such nonforprofit organization thatPresented as a competition on  PASSNYC seeks to improve their methodologies in identifying schools and students who would gain the most from their services.This  serves as an initial exploration into the many proxies that may help identify schools/populations that might benefit from services intended to boost SHSAT registration numbers.In the dataset provided by PASSNYC there were over onehundred columns that described each of the schools (ranging from grades 0K8) in proxies such as Percent Asian Percent White Percent Hispanic Average ELA Rating and Average Math Rating.  As many of these features paint a picture for the differences across NYC schools the App allows the user to not only see where these schools are located but to also choose aesthetics that may aid in spotting significant observations.In this view we get a very clear understanding of the distribution of economic need and school income estimates throughout New York City schools i.e. the larger circles have a high school income estimate darkergreen circles have a very low economic need index etc. To get a better understanding of race distributions we can change either the ""sizeby"" or ""colorby"" to a particular race.Here we can see that alarmingly high amount of schools have high economic need with a high percentage of hispanics.On the next tab I chose to investigate possible correlations between some features with the aid of a a correlation matrix.  Any intuitions can be further investigated by clicking a square on the matrix to view a scatterplot who axes are the variables in question.In this view the user has selected to visualize schools' Percent Black/Hispanic and average ELA proficiency.  With a correlation of 0.75 we can clearly see a cluster of schools with high percentages of Black/Hispanic population with extremely low average ELA proficiencies. * Note a 3 on the NYS ELA exam correlated to a student being at grade level while a 2 signifies under grade level expectations.Lastly I had another data set consisting of schools' reported SHSAT registration numbers and how many actually took the test.  The differences between number registered and number taken are shocking.  It stands to be further investigated as to why there is such a low turnout for test takers.  One school that stands out is Columbia Secondary School who saw 71% of registrants take the test.The insights seen are clearly ones that deserve attention.  Talented students in schools often go unnoticed and therefore miss their opportunity to shine.  Moving forward I expect that further insights and recommendations via machine learning algorithms can be added to this analysis.",NA,NYCDOE Public Schools' Demographics
https://nycdatascience.com/blog/student-works/capstone/local-used-items-analysis-with-python-and-tableau/,10,I wanted to answer what locations in my area had the most used items what the central tendencies of the item prices were by location and what the number of free items by location was. I also wanted to extract a price and summary of the description of each item as well as classify each item category using NLP.I used Scrapy to extract out the item information and this proved to be the easiest scrape of all three sites. is JavaScript heavy but a simple scraper that extracted out the JSON responses was all it took to get what I needed. is a robust JavaScript heavy website. Unfortunately I was unable to isolate the JSON requests from Facebook Marketplace to extract the data I needed.After some research and advice I determined the simplest approach was to use Selenium. The benefit of using Selenium is you can code any interaction that a user performs on a website. The downside is it scrapes much slower than Scrapy does.I imported the JSON objects into Pandas dataframes and the majority of time I spent of the project was cleaning the data.  The same primary categories were scraped but each site had its idiosyncrasies. A good deal of time and effort was necessary to clean the dataframes so they could be merged and produce insights during exploratory data analysis.If you'd like to view the entire .I had a couple unsuccessful attempts at applying unsupervised NLP with spaCy and pyLDAvis libraries inspired by this walkthrough  as well as creating a text summarizer with the Keras library inspired by this walkthrough. I decided to simplify the process and use the API to execute a text summarizer model as well as a price extractor model.  I also created a custom category classification model.I found this project engaging and challenging.  If I scrape items again I would also scrape the designated item type categories. This would make for a more interesting analysis of items by the designated types and I could use them as targets for my NLP classification model.  It appeared that some of the descriptions for the Facebook items weren’t scraped. I wasn’t able to determine why and I would pay more attention to that in the future. The free version of MonkeyLearn only allows 300 queries per month.  I would get my customized category classifier more accurate when my allowable query amount resets each month. I would also train it with many more items to see to see if that makes it more accurate.You can view the Tableau workbook here:  and my github repo here: .,NA,Local Used Items Analysis with Python and Tableau
https://nycdatascience.com/blog/student-works/web-scraping/scraping-used-items-on-craigslist-org-with-scrapy/,10,As the old adage goes ‘One man’s trash is another man’s treasure.’  This was my first introduction to Scrapy. My goal was to scrape used items from Craigslist.org and perform some basic EDA on the data. I expanded this project into more detail in my final project. You can check out if you’re interested .Using ScrapyUsing Pandas Matplotlib and SeabornUsing wordcloud,NA,Scraping Used Items on Craigslist.org with Scrapy
https://nycdatascience.com/blog/student-works/machine-learning/supervised-learning-with-kaggle-titanic-dataset/,10,Kaggle.com offers an introduction to supervised learning with the Titanic Dataset.  The purpose of the competition is to build an accurate classification model that predicts whether a passenger would survive the Titanic crash.  This is a helpful exercise to reinforce the fundamentals of Machine Learning. There are plenty of resources available to assist in filling in the page and deepen understanding of the fundamentals.  I had no experience in programming or advanced mathematics before starting the bootcamp so I wanted to stay focused on the basics. This competition was the most sensible for my needs.I created a Jupyter notebook that is split into two distinct parts.  The first is an overview of fundamental and important concepts of machine learning and the second is the application of those concepts on the Titanic dataset.  As I began the process I set out to answer the following questions: The data came from the Kaggle website. It was split into a training and testing csv files. The dataset was structured with the following features:I performed some exploratory data analysis to get a feel for which features appeared to have a significant effect on survival rate and the number of missing values in the dataset.I then filled in the missing values and engineered some new features to make the dataset more machinereadable.  This was in preparation for applying multiple algorithms within the SciKitLearn library.,NA,Supervised Learning With Kaggle Titanic Dataset
https://nycdatascience.com/blog/student-works/using-machine-learning-to-build-a-predictive-model-for-house-prices/,10,The data for this project was provided by  and was presplit into a training and testing set. Both data sets are near identical with 1460 observations in the training and 1459 observations in the test set and 79 varying housing features to predict the overall sale price. Our first thought in predicting the sale price was that the location would have a heavy impact. The sale price per neighborhood is plotted below. We found that the extremes on both ends were highly important variables.In moving forward with a linear model the first assumption to test is normality in the data. In plotting the distribution of the target variable sales price we discovered that it was positively skewed with a skewness of 1.88. After applying a log transformation to allow for higher predictive power the skewness dropped to 0.12 as shown in the picture below.Afterwards we investigated the skewness across all numeric features to identify which variables would also need transforming to allow for a better fitting linear model.  Any variable with a skewness above 0.75 was log transformed.The final aspect of our data cleanup and processing involved dealing with the missing values and the categorical features (e.g. which neighborhood the house belonged to or kitchen quality). The only numerical features that had significant amounts of missingness were the year that the garage was built and the lot frontage. Both were imputed with the median value for the respective variable. This was because the difference from a garage built in the year 1980 to the imputed year zero could cause significant issues with a linear model; the reasoning for lot frontage stemmed from the fact that if the house had a measurable square footage it did not make sense to impute the missing values with zero. All of the other missing values were imputed with zero as whatever was left we could gather was not a valid feature of the house such as missing values for the pool or fence quality/square footage. The remaining categorical features were ‘dummified’ or one hot encoded to obtain numerical values in order to (initially) give the model we were developing as much information as possible.Two families of predictive models were considered to solve the problem. The way we evaluated the models was by first splitting our training dataset into an 80/20% split where 80% of the data was used to train the model. After training the model the untouched 20% of the data was used to evaluate the predictions directly assessing the model’s performance. The RMSE was evaluated between the predictions and actual values of sale price from the 20%. The first family linear models achieved high performance with Lasso feature selection technique and minimal feature engineering that addressed the skewness of predictors and dependent variable. Other attempts to improve the performance using Ridge regression produced identical results however we observed a slightly higher RMSE for the Ridge regression model over the Lasso regression model. The treebased models such as Random Forest Extreme Gradient Boosting and XGboost produced inferior Rsquared and were discarded from further analysis. Treebased models would require subjective feature engineering and extensive hyperparametertuning to work well. With this dataset of the original 79 features 268 features were then generated after dummifying. This many features would make it extremely difficult to tune the number of features considered per split hyperparameter with treebased models. Also the number of training observations is relatively low (fewer than 1500) which further hinders treemodels’ performance. Finally the interpretabilityis lost for advanced treebased models which also makes the linear solution preferable. One of the main benefits gained by selecting a linear regression model is for its interpretability.  First let’s examine the home features that were most important to the model. Note that the larger the magnitude of the variable’s coefficient in the model the more influence it has on the sale price of the home.Not surprisingly total aboveground living area was the most influential valueadd variable in the data set.  Another feature which showed up numerous times (as both a valueadd and as a detractor to home value) is the neighborhood.  Tracking this back to the earlier boxplot (and recalling the mantra “location location location”) we see that our model again not surprisingly has indicated that location is a very important factor in determining sale price.  It is worth cautioning against directly extracting a dollar value for each of these potential changes in a home.Since many of these features are on a log scale (done earlier to reduce skew) and normalized (given mean zero and adjusted for the variance) the marginal value increases in a home’s price based on these coefficients requires a calculation but thankfully the model does exactly that.There are many next steps one could take to attempt to achieve better results.  A sensible starting point would be to go after improving the results on the subsection of homes the algorithm performed worst on.  Below we plot our models predicted price home vs actual price home (this is of course on the training set as we never knew the home prices for the test set).We can see that the model performed consistently worse for more expensive homes.  This implies that the effect of these variables on home price is not inherently linear and as such we need to augment the data (e.g. feature engineering) or model in order deal with these homes.  We could also augment the data set with a general “market” indicator (something like the S&P 500 Index as a rough proxy for the value of assets).,NA,Using Machine Learning to Build a Predictive Model for House Prices
https://nycdatascience.com/blog/student-works/maximizing-value-from-your-car-sale-in-india/,10,As a car fan I’ve been curious about the used car market in India. The economy and culture of this country have formed a special used car market with high volume and speed of trades. It would be interesting to get some insights of the used car market with web scraping.The website scraped is https://www.olx.in/cars/ . Olx Group is a commercial webbased service that supplies vehicle reports to individuals and businesses on used cars and light trucks for the Indian consumers. This project focuses on visualizing the used cars market in India. There data of more than 100000 used cars ads was scraped using scrapy. The scraped information contains body type color engine mileage model price drive type model year model name and make of each sample.For car fans car dealers used car holders and used car buyers this project provides answers to the following questions:1. What are the best brands to buy in terms of resale price mileage and km driven.2. How can a price of a car appreciate over the years.3. How can a special type of number plate add a significant value to a car.So this is how a used car ad on the website looks:I tried a few web scraping techniques and the best output came out with scrapy in R the data had more than 100000 used cars ads. Olx is a marketplace where buyers meet sellers and can upload their car data.So what are the features a car buyer looks for while purchasing a used car?For most used car buyers the mileage of a car and the year in respect to the price are few of the first basic features a buyer looks for therefore to have a successful sale the seller needs to provide the right data. I gathered these 3 main features and came out with a cluster analysis graph to visualize these features.After these aspects are fulfilled  a buyer sees what is the brand value of the used car?By just adding a special number plate to their car.These are two examples of cars from the year 19601990. The original price of these cars ranged from Rs.15005000 after 20 years the price of the car range from Rs.150000  250000. This shows an appreciation of  almost 10000% which is much expensive than some of the new cars available in the market.A common number plate in India consist of a 10 digit alphanumeric plate but these 5 alphanumeric plates make them extremely rare and precious. The common number plate in India as shown on the left is 'MH63CN3731' and the special number plate on the right is 'DEV2'.Both the cars are up for sale on olx and are from the same year and model but there is a difference of Rs.800000. The original on road price of the car is Rs. 1300000 and the used car with a special number plate on the right above is almost Rs. 700000/10000$ more from the price of a  new car. I think this example would explain how special number plates can add so much value to the car.,NA,Maximizing value from your car sale in India
https://nycdatascience.com/blog/student-works/us-health-exploring-risk-factors-policies-and-environment/,10,The motivation of this project is to explore and visualize the connection between specific health factors like obesity physical activity and nutrition alongside state level  health policies and  environments. According to the Center for Disease Control and Prevention (CDC) more than a third of US adults are obese being this condition one of the most important preventable factors in chronic diseases and healthcare costs. Healthcare spending is estimated between $147 and $210 billion per year and with projections to increase 5.5% per year – as per  the Center for Medicare and Medical Services (CMS) it will account for almost 20% of the total US GDP by 2026.In 2010 the US federal government announce the prevention and treatment of obesity as important part of its campaign to improve health of Americans. Since then several initiatives and programs have been implemented in order to revert the increasing trend in obesity that has doubled in the last 40 years. These initiatives and programs range from nutritional education food labeling through specific funding to improve the food system at local several policies and legislation at state and federal level. Nevertheless it is still questionable if any significant progress has being made.The Division of Nutrition Physical Activity and Obesity (DNPAO) under the Centers for Disease Control and Prevention (CDC) is one of the main federal entities at forefront of decreasing obesity among US population. The DNPAO has developed a comprehensive dataset of information from which the following datasets were used for this project:1)     Behavioral and Risk Factors Surveillance System: This database includes 9 variables with more than 50K records2)     Environmental and Policy Support: This database includes more than 30 variables in approximately 4K records.To simplify the visualization of different variables the project was divided in 3 sections:The first section includes high risk behavioral factors: obesity nophysical activity and potential not ideal nutrition (assessed by lack of vegetable portions in daily diet).The second section includes policies that have been implemented at state level. These policies have been recommended as positive influences in nutrition and health of communities. The Food Council policy aims to improve food systems by coordinating actions across different levels and stakeholders. The policy for Healthier Food Retailer seeks to make healthier foods more accessible among underserved populations. The Complete Street Policy works with local governments in designing streets/sidewalks that are friendlier and safer for pedestrians bicyclist motorist and transit riders of all ages and abilities.The third section compares relative accessibility to favorable environmental factors against obesity levels at state level. These environmental factors are: 1) the percentage of population living half mile of a park  2) the percentage of youth with parks or playground areas community centers and sidewalks or walking paths available in their neighborhood 3) the percentage of census tracts (neighborhoods) that have at least one healthier food retailer located within the tract or within half mile of tract boundaries,NA,"US Health: Exploring Risk Factors, Policies and Environment"
https://nycdatascience.com/blog/student-works/simple-interactive-visualization-of-16-17-nba-stats-with-shiny/,10,Before I set up my Shiny App I wanted to answer the following questions using dplyr:You can find the remainder of this answer in my GitHub repo.You can find the remainder of this answer in my GitHub repo.You can find the remainder of this answer in my GitHub repo.,NA,Simple Interactive Visualization of '16-17 NBA Stats With Shiny
https://nycdatascience.com/blog/student-works/machine-learning/an-interpretable-prediction-model-for-house-prices/,10,Whether it’s a newlywed couple looking to start a family or an investor looking to diversify their portfolio as a landlord purchasing a home is a major life event.  Often buyers need to balance items on their house ‘wishlist’ with a set budget. Similarly housing developers want to know which features will make a property stand out enticing buyers to choose their offeringst over their competitors’.  Our team explored these topics and more using the Ames Iowa housing dataset from . Though this Kaggle dataset project is a competition for best housing price predictive score our team wanted to focus on learning the application of fundamental statistical machine learning models.  With that emphasis the team was comfortable with using models whose mechanisms were easier to interpret such as a linear regression over more complex models. such as boosting.We applied basic city demographic analysis and employed some creativity of our own to create a dataset used to feed into our machine learning models. They are described below along with the results and findings of the linear regression model. That is followed by a projection to explore potential improvements using more complex models.The Kaggleprovided dataset is constructed from 79 features of which 51 are categorical and 28 are continuous. Reviewing each feature we identified problems such as sparse data (high percentage of zeros or missing values) numerous subcategories and unbalanced categories (in terms of number of values in subcategories). A few examples of our solutions to these problems are described below and in the appendix. One critical problem we encountered several times was sparse data. For example the dataset documented square footage of three different types of porches. This is problematic as over 90%  of houses do not have any form of porch. We collapsed the information from three continuous features into one binary feature indicating whether or not the house has an enclosed porch. This not only reduced dimensionality but also preserved information about a relatively uncommon feature Unbalanced categorical features is a common issue within this dataset. We found that features with multiple subcategories often included subcategories with less than 5% of all  observations. Our general approach is to use 5% as threshold to group these subcategories. The nature of these subcategories were evaluated individually and combined in a logical manner. For example the Building Type originally split townhomes into inside and end units. These were combined as a new ‘Townhouse’ subcategory. Similarly a two family conversion home was considered to be equivalent to a duplex and both types were classified as ‘Duplex’. Before multiple subcategories within the feature were less than 5%. After feature engineering all subcategories were greater than 5%. We also engineered a new feature based on univariate analysis. How many houses were sold in a given month? Combining two existing features (month sold and year sold) and treating this as a datetime object a clear seasonal pattern of number of houses sold is observed.   Every summer specifically May/June/July the number of houses sold reach peak. This agrees with our understanding of summer being the peak season for the real estate market. Consequently year sold and month sold were collapsed into one binary feature peak and offpeak season.Neighborhood was a challenging feature to engineer as it contains 25 classes. We developed a unique grouping according to their geographical location. Cquaw Creek naturally bisects Ames with Iowa state University located in the southwest corner and residential high density buildings south east of the river. Commercial areas and floating village (zoned undeveloped) are  west of Interstate 69.This suggests that a commercial high density area to the west of Grand Avenue as compared to more residential high density to the east of Grand Avenue. We used natural divisions and the zoning feature map to reduce 25 classes (neighborhoods) into three (regions) seen below.Now that we have features that are in tiptop shape we can proceed to modeling. A multiple linear regression (MLR) is a good starting model because it is easily interpretable when compared to more complex models.  However there are some important assumptions that must be verified to determine if MLR is an appropriate choice. This includes checking for constant variance and normally distributed residuals. We verify and discuss these findings . We  tested the adequacy of r an MLR model by checking some model assumptions. The initial residual plot revealed problems for our MLR approach including heteroscedasticity  and outliers. In the left plot the red line fit appears to follow the zero horizontal line very closely.  This means that the linearity assumption may be satisfied. However there appear to be some outlying points towards the upper right hand corner.  This may indicate nonconstant variance which can potentially be addressed by logtransforming the response. The residuals versus fitted values plot with logtransformed response is presented to the right of the first plot.  Note that the point cloud is now randomly scattered around the horizontal zero line. We can now claim that the constant variance assumption is satisfied.  Note that in both plots there are two outlying observations on the bottom right. Ultimately these two observations were removed from our model. We performed 5fold crossvalidation on the MLR model (Model I) to estimate the train and test mean squared errors (MSEs).  The results are presented in the table below. For Model I the train and test MSEs are almost the same. This means that the model may not suffer from high variance.  However as the train MSE is not zero the model may be biased. Ideally the best model would predict house prices very well and the train MSE will be zero or very close to zero.  Note that the house prices were logtransformed so that the MSEs appear to be very small. The table also presents the results from Model II to be described below.We considered two approaches to further improve our model: subset selection (by univariate selection) and regularization.  In subset selection we can build a model by adding features and selecting the best model.  Or we can start with Model I and exclude a subset of features until we arrive at a best model.  However with 69 dummified features subset selection will be computationally expensive because we would have to consider 2^69 models!  A compromise procedure is to select the features in a univariate fashion. This can be accomplished by the KBest procedure in python using an Ftest.    Below is a plot of the mean squared errors (MSEs) as a function of the number of features that have been selected by the KBest procedure.  The MSE’s were calculated from 5fold crossvalidation with the features selected from the training set.We can obtain some interesting insights from this plot:We therefore consider as our Model II the K28 best features model.  Note that the MSEs do not reach zero at 69 features (these are also the MSE’s for Model I). Both train and test MSEs were higher for Model II than Model I so we opted to prefer Model 1 on the basis of predictive accuracy.Regularization (also known as shrinkage) shrinks a model’s coefficient estimates.  This will cause the training error to increase and the test error to decrease allowing a more generalizable model.  In other words regularization reduces a model’s variance. The two most popular regularization approaches are ridge and lasso.  Both perform shrinkage of the model coefficient estimates but lasso may shrink the coefficients to zero and in effect also perform feature selection to produce a model with less features.  There is also another approach called elastic net which combines the effects of ridge and lasso. As we noted earlier Model I does not appear to suffer from high variance.  We implemented both lasso and ridge regularization and confirmed that this is indeed the case as subsequent  resulting train and test MSE’s were similar. Lasso regularization zeroed the coefficients of 14 features. However we discovered two seemingly undesirable effects with regularization and lasso:So will it be Model I or lasso?  If a model with fewer features is desired then lasso would be the model to choose.  Given comparable test MSE’s the predictive abilities of MLR and lasso models are comparable.  Therefore we would choose Model I: MLR because with comparable predictive abilities it has the very important advantage of having a nice model interpretation.  There are two immediate strategies to further reduce the bias problem in our model. The first strategy is to reevaluate our preprocessing and feature engineering steps. Predictive power may have been compromised with binning of some continuous features such as Total Basement square feet. However we anticipate that these adjustments would only reduce bias in small increments as most of the features in the model may have relatively small contributors to the predicted price of a home. Another strategy to reduce bias is to fit a more complex nonlinear model. Ensemble methods such as random forests and boosting may potentially increase accuracy but at the cost of model interpretability. Figure Source: http://blog.fastforwardlabs.com/Linear Regression provides model interpretability and can perform relatively well for accuracy metrics. Nonlinear models can be more accurate but at the cost of interpretability.Total living area and quality of exterior building materials were the top 2 features that determine house price from our model as determined by the KBest approach procedure as described above. These are features which intuitively make sense and it would be interesting to evaluate whether these features are generalizable over time and housing markets  which we could test with additional data from Ames or elsewhere. Our engineered features enable nice interpretation. One interesting feature we explored was whether homes sold during peak selling season of May June and July. According to our model selling a home during this time commanded an extra 1.46% in sale price. This raises an interesting question: does this feature provide some actionable information for realtor companies? Would it pay to try to optimize price by timing the sale? It is possible that  costs like advertisement space also rise during peak selling season Additional costbenefit analysis is warrantedto generate actionable nextsteps but our interpretable model could interact with internal realtor data to develop new business strategies.Nearly onethird of Ames IA is employed in the education sector. Is there a premium of buying property near Iowa State University  whether it serves as an investment rental property as a landlord or as housing for professors? Alternatively are housing prices suppressed because of rowdy college parties? Apparently not. Generalizing South Ames as neighborhoods south of Chawq creek our model suggests that these homes sold for 2.04% higher sale price. This suggests that there is small premium on sale price for campusadjacent housing.We spent a large portion of our work on feature engineering and fitting different models according to basic principles and standard approaches.  We finally arrived at a simple model that we consider to be reasonable and useful. This model has good interpretability but we learned that its predictive ability can be improved perhaps only slightly.  We could avoid collapsing subcategories as much as we did For example instead of collapsing five categories into only two we can try collapsing into three or four. We can also be less aggressive in converting continuous and ordinal features into categorical and further investigate the effect of high leverage data points. Finally we could investigate a better fitting treebased model like xgboost.  However we note that fitting more complex nonlinear models may improve predictive ability at the expense of model interpretability.,NA,An Interpretable Prediction Model for House Prices
https://nycdatascience.com/blog/student-works/nba-unicorns-shiny/,10,The modern NBA landscape is rapidly changing. Steph Curry has redefined the lead guard prototype with jawdropping shooting range coupled with unprecedented scoring efficiency for a guard. The likes of Marc Gasol Al Horford and Kristaps Porzingis are paving the way for a younger generation of modern big men as defensive rim protectors who can space the floor on offense as threepoint threats. Then there are the newwave facilitators  LeBron James Draymond Green Ben Simmons  enormous athletes who can guard any position on defense and push the ball down court in transition.For fans analysts and NBA front offices alike these are the prototypical players that make our mouths water. So what do they have in common? For one they are elite statistical outliers and this serves as the primary motivation for my exploratory analysis tool: The tool uses box score data from the 20172018 NBA season (source: Kaggle) and focuses on the following categories: Points rebounds assists turnovers steals blocks 3pointers made FG% and FT%. I also used Dean Oliver’s formula for estimating a players total possessions (outlined ).To assess all players on an equal scale I normalized the box score data for each player. For ease of interpretability I chose to use “per 36 minute” normalization which take a player’s perminute production and extrapolates it to 36 minutes of playing time. In this way the values displayed in the scatterplot represent each player’s production per 36 minutes of playing time.To ensure that the per36 minute calculations did not generate any outliers due to small statistical samples I removed all players with fewer than nine games in the season as well as players who averaged three minutes or less per game. intended to be used for exploratory analysis and player discovery. To most effectively understand and interpret the charts you can follow these steps:The correlation matrix uses the  as a reference to guide your use of the dynamic scatter plot. Each dot represents the leaguewide correlation between two statistical categories.The color scale indicates the direction of the correlation. That is blue dots represent negatively correlated statistics and red dots positively correlated statistics. The size of the dot indicates the magnitude of the correlation  that is how strong the relationship is between the two statistics across the entire league. Large dots represent high correlation between two statistics while small dots indicate that the two statistics do not have a linear relationship.We can get a flavor of these relationships as we move to the scatterplot. (.) For the purpose of identifying truly unique players let’s look at a pairing of negatively correlated statistics with high magnitude (i.e. a blue large dot): 3pointers made (“3PM”) vs. Field goal percentage (“FG%”). It makes sense intuitively why these are negatively correlated  a player making a lot of threes is also attempting a lot of longdistance lowpercentage shots. Given the value of floorspacing in today’s NBA a highvolume 3point shooter who is also an efficient scorer possesses unique abilities. So let’s select FG% for our xaxis and 3PM for our yaxis (using the dropdowns in the menu bar) and see what we find...The two dotted lines within the scatterplot represent the 50th percentile for each statistic. In the case of FG% vs. 3PM we turn to the upper right quadrant which represents the players who are above average in both FG% and 3pointers made. To focus our analysis we can zoom in on this quadrant for a close look. To zoom simply select and drag across the plotted space you want to zoom in to in this case the upper right quadrant. You can also filter by position by simply selecting specific positions in the legend.Scroll over a point to see who the player is as well as their per36 statistics. At the top of our plot no surprises here: Steph Curry. While his 4.7 threes per 36 minutes leads the league what truly separates him is his 50% efficiency from the field. But we already know that Steph is an exceptional anomaly so who what else can we find?While several superstars can also be found at the top of our plot  Kevin Durant Kyrie Irving and Klay Thompson stand out  we have quite a few role players up there as well: Kyle Korver J.J. Redick Kelly Olynyk and Joe Ingles. These are quality reserves who may not wow us with their overall statistical profiles but play a crucial highvalue role on teams by spacing the floor without sacrificing scoring efficiency.I recommend starting your exploration on the bluedots of the correlation matrix  blocks vs. threes rebounds vs. threes assists vs. blocks for example. These are where you can identify players with the most unique skill pairings across the league. (Note: When plotting turnovers be sure to focus  the median line as it is better to have low turnovers than high.)For fantasy basketball enthusiasts this is a great tool to identify players with specific statistical strengths to construct a wellbalanced team or complement your roster core.I really enjoyed building this tool and exploring its visualization of the NBA landscape. From an interpretability standpoint however it is not ideal that we can only focus on one player at time. To improve on this I plan include an additional table that provides a deeper look at players that fall above the median line for both X and Y statistics. In this way we can further analyze these players across a larger range of performance variables.,NA,Searching For Unicorns (And Other NBA Myths)
https://nycdatascience.com/blog/student-works/r-shiny/worldloppet-race-reccomender/,11,It can be less than ideal to travel across the globe and participate in a race that is above (or below) your skill level.After all…Matching racers with host countries can be a serendipitous and enjoyable international experience. So I designed and constructed a recommendation engine to match an individual's preferences to FIS Worldloppet races. Then  to help skiers find a race for the 2018/19 season. Read on to learn more!My first objective was to extract data on the Worldloppet races.  so I extracted the past 5 years of race results data using . This was done anonymously as racer names were not extracted  only the race time and skier nationality. In total over 600000 race results were gathered.Several features could be engineered from this data despite scraping only a few variables. For example I used regular expressions to identify the host country from the race name. This could be compared to the racer’s nationality to get an idea of how many skiers were tourists for a given race or how much a race has grown in popularity. Given the racer’s finish time and the race distance extracted from that page a race speed can be calculated. A limitation of this calculation is that it does not account for why these speeds differ. For example paces could be slower because the course is particularly challenging with major elevation gains. Conversely “faster” races could just be flatter. Nevertheless there appears to be a distribution of median race speed by host country:I envisioned the following scenario: given my own personal preferences for race distance race speed and how many racers I wanted to ski with which Worldloppet race would be a good fit for me? This problem can be solved using a KNN classification approach. I could compare my preferences to those which have already been recorded within the hundreds of thousands of observations (race distance race speed number of racers) from the scraping portion of this project. The next step requires finding consensus of which race (by host country) was a good fit by finding the most similar datapoints to my preferences. The number of neighbors I selected for the model was determined using a grid search and 10fold cross validation. The subset of classic skiers was extraordinarily accurate using this model with a minuscule misclassification rate of 0.001 to identify the host country. Classifying skate skiers represented a more challenging problem as the misclassification rate for the host country was (0.21). This less accurate match suggests that there are more skate skiing results with similar race distances/speeds than the classic races.Coming to this realization was rewarding but I wanted to share it with the broader XC skiing community. So I reimplemented the tuned KNN model in R and deployed it as a webapp using R shiny. . The app asks for your preferred XC style race distance preferred race speed and number of skiers you'd like to race with. Then using the KNN model described above a host country will be recommended. Popups on the host country markers have a link to that race's website. The Worldloppet series is a fantastic way to experience the best XC skiing that the world has to offer. I built a KNNbased recommendation engine for Worldloppet races and deployed it as a web app.I gathered data to build this model by scraping race results from the Worldloppet website and performed feature engineering. I hope that this app will make new suggestions for skiers and will enrich their XC skiing experience.,NA,Building a Worldloppet Recommender App
https://nycdatascience.com/blog/student-works/one-disney-to-rule-them-all/,11,Disney's acquisitions over the years reinvigorated the company’s force in the film industry. As you can see in the highlighted table below nine out of the 15 highest grossing movies are from Disney. The question is: are they really above average compared to other production companies or those films are just outliers? Do Pixar Marvel and Lucasfilm have a considerable impact on its outcome?Even though many production companies were scraped I selected the top six companies which produced 505 movies for analysis :Doing some basic analysis I discovered missing values on some of the features. The movies that didn't have any box office information were removed from the dataset leaving us with 452 movies. The ones that didn't have the worldwide gross could be implied using the USA gross since the correlation between those variables is around 0.93. Building a simple linear regression model derived the worldwide gross from the USA gross. This model was used to predict 63 missing values.I also created a new variable called net worldwide income to show the difference between the gross and budget amounts.Trying to understand how Disney managed to beat the record of total worldwide gross in 2016 I analyzed their movies over the years considering their subdivisions. From the bubble graphs below we see that many of the Pixar Marvel and Star Wars movies have greatly positively influenced Disney revenue. The size of the bubble shows the difference between the gross and budget to show which had the biggest net return. The link to the dashboard is at the end of the post and can shows interactively what each bubble represents and additional info.In the year 2016 alone we can see that every subdivision from Disney had released a major film. Under Marvel it released and. Pixar released . From Lucasfilm we had Disney Animation had released two additional movies: andMost of the production companies have divisions and subsidiaries. That could be a problem in how they are represented. For some movies IMDB didn't include the parent company in the list of producers. To make up for that Wikipedia can be scraped to gather the parent information of each subdivision for more accurate results.It’s also possible to apply analysis to the distribution of the films over the year and try to extract some insights from there. For example see how each production company makes its yearly planning.Disney has been leading the box office war against other major production companies and it will probably continue to. The indications for 2018 are good for Disney. Two Marvel movies are already in the top 10 box office list ( and ). The movie reached 1 billion dollars in record time (10th day).  (Lucasfilm) is anticipated to open to recordbreaking numbers over Memorial Day. Pixar is releasing the second  movie in summer. And  (Disney Animation) is due at at the end of the year. With such a lineup it’s possible that Disney will beat its own record this year.,NA,One Disney to rule them all?
https://nycdatascience.com/blog/student-works/web-scraping-industry-salaries-from-glassdoor/,11,My interest in scraping glassdoor is due to the fact that I recently graduated from college. I was interested in seeing industry salaries for popular cities along with their associated ratings.The questions I asked:Using Scrapy my plan was to look up every job posting on Glassdoor.  After viewing each posting I would be able to retrieve information such as the job title the company’s name the estimated salary for the posting rating and industry the job was in.One issue that occurred was my attempt to retrieve information about the company. Even though it was listed in the source code of the website the code that Scrapy looks at to gather information my program was unable to scrape it properly. When attempting to debug my code looking for a solution I found that even though the information on the company was in the source code the website uses JavaScript something that Scrapy does not recognize to get the information from another link.After learning this I searched through the network files that was being transmitted in hopes of finding the source of the JavaScript. I ended up finding a promising file titled “” which contained a request URL. That URL which was the same for every posting except for a string of numbers which was the company ID contained all the information needed for that company.Once I found this separate page which hosted the missing data I was able to scrape the company information successfully.Another problem I encountered was a strange error where my code would break whenever it reached the 31st page of Glassdoor. As I investigated the problem I learned that Glassdoor only provides the first 30 pages for its viewers. Even though it states “30 out of 5277 pages” there is no 31st page (or any page past it)In order to gather enough data I decided to look up 16 different cities across the US and find all the job postings located in those areas. I also discovered a way for my spider to go to the next website programmatically by decoding what each part of the URL meant.After overcoming these two obstacles I was able to scrape all the necessary information from Glassdoor.,NA,Web Scraping Industry Salaries from Glassdoor
https://nycdatascience.com/blog/student-works/demographics-of-party-misalignment-in-the-2016-election/,11,In this post I introduce my first interactive  investigating demographic differences in the 2016 general election results.Given concurrent Congressional and Presidential races in the 2016 election I can segment out districts into 4 groups based on the party of the winner in each race. I then study demographic features of each group to understand what is differentiating districts that voted along party lines versus those that were split between parties  districts that will be heavily contested in the 2018 midterms.Demonstrated R Skills:See my code on github .Political parties are amorphous and during a Presidential election varies ideologies compete in primaries to represent each party in the general election. While the parties then unify around the primary winner that candidate will not substantially represent a portion of their party's supporters. Sometimes that discrepancy between voters' ideal candidates and the party's choice of candidate is so large that they are persuaded to vote instead for the opposing party's chosen candidate and are thus disloyal to their identified party.The 2016 Presidential election delivered highly oppositional races within both the Republican and Democratic primaries and the general election. Virtually no one saw Donald Trump's hostile takeover of the Republican Party or believed Hilary Clinton would face a tough primary against Bernie Sanders. Therefore many voters who felt loyal to their local party's Congressional candidate felt '' to their party's Presidential candidate.Majority Vote winners in Congressional and Presidential elections in each Congressional District:This party misalignment occurred in 35 of the country's 435 districts and the party of the majority winning presidential candidate differed from the party of the elected Congressional representative.Of these 35 districts 23 majority voted in favor of both a Republican Congressperson and Hilary Clinton and 12 voted in favor of both a Democratic Congressperson and Donald Trump.Ideological beliefs are highly related to demographic traits.Thus the second part of this analysis utilizes the U.S. Census Bureau's to break down party misalignment  along 3 demographic variables.The ACS data displays the % of the population of each district at each level of the variables included.For each variable I provide a visualization of the mean and variance breakdown across party misidentification. To experience the of the graphics check out my.Data used for this analysis comes from three sources:Mean breakdown of household income brackets across party identification.Distribution of mean household income:InsightsMean breakdown of education level across all districts in voting group:Distribution for each education level :Mean breakdown of racial/ethnic identification for each factor of party alignment.Distribution for each racial/ethnic group  :Certain demographic differences stand out across district alignment.Clinton Democrat Trump Democrat The misaligned districts' demographics prefered Presidential candidates 'lost' their party's primary. Based on these results one can imagine a very different result had the Republican candidate been Marco Rubio and the Democrat been Bernie Sanders.While Trump's forceful ideology was not expected in 2016 he now holds significant sway over the Republican party looking toward to the upcoming 2018 Congressional midterm election. These  'misaligned' districts will be battleground targets for Congressional candidates of the opposing parties. Whether or not these candidates are successful will indicate just how successful the extent to which the national party schema has been 'Trumpified'.,NA,You Voted for Whom? My R Shiny App on the Demographics of Party Misalignment in the 2016 Election.
https://nycdatascience.com/blog/student-works/the-u-s-congress-campaign-finance/,11,Campaign finance practices have attracted a great amount of attention in the 21st century U.S. congressional elections. This blog’s primary objective is to present a databased insight regarding the involvement of money in elections mainly focusing on the (1) winning factor and the (2) incumbency factor in elections. The blog presents summaries of the money spent by winners in different states and how these are compared to their opponents. In this article one can find how the electorate have kindly treated incumbents across parties. An associate shiny web app for this blog can be found . The data was collected by web scraping the website  using Python and Beautifulsoup. The data is comprised of money (in USD) associated with all the candidates of all the house and senate races from the year 2000 to 2016. A note regarding the interpretation of data: for the 2000 and 20002 elections the website lists the “total amount spent” by candidates; whereas from 2004 and onwards the website lists the “total amount raised” by candidates. Therefore to produce an inclusive analysis the blog treats all amounts for all years as “the money associated with candidates” or “the money involved in races.” Furthermore no winners are listed for the year 2004 on the website; therefore the year 2004 is excluded from the relevant data analysis. The analysis focuses solely on the data obtained from the website  and does not take into account money spent by super PACs and other potential sources.  Figure 1 shows the staggering numbers for you to interpret the role of money in American elections. The total amount of money associated with congressional elections especially with winning candidates have typically increased consistently since 2000.Ratios of the total amounts associated with winners and their opponents were lowest in probably the most fiercely contested 2010 elections where these ratios were 2.13 and 1.43 for the House and the Senate respectively. Otherwise winners significantly outraise and probably outspend their opponents in all elections. Key Observation:Only 2 candidates have won a US Congressional district as a third party/independent candidate in the 21st century (1) Bernie Sanders from Vermont in 2000 2002 and (2) Virgil Goode from Virginia in 2000. Only 3 candidates have won a US Senate election as a third party/independent candidate in the 21st century (1) Joe Lieberman from Connecticut in 2006 (2) Lisa Murkowski from Alaska in 2010 and (3) Bernie Sanders from Vermont in 2012. However some of the above have switched to one of the two mainstream parties at various times in their career. There have been few nonincumbent one time winners from third parties. But the dearth of such candidates shows that the politics in the U.S. is heavily dominated by the two major parties: the Democratic Party and the Republican Party.We earlier noticed how the money has sided well with the winners. As it turns out the incumbency tends to help the candidate meaning once someone is part of the establishment the person stays in the establishment as seen in Figure 2 from the very low fractions of incumbents being defeated in subsequent elections.Observations:Figures 3 and 4 below show top 15 and bottom 5 states with winners associated with most money in the Senate and the House respectively. Figure 5 below shows money associated with winners in Senate races divided by the number of congressional districts of respective states. The number of congressional districts of states are typically proportional to the population of the state. You can see that Senate races in states with some of the lowest population numbers involve a lot more money per congressional district of the state. Most such states have only one statewide congressional seat. Tables below show the top10 candidates with most money associated with the Senate and House elections in the 21st century. :Tremendous sums of money are involved in U.S. elections. Winners are able to raise twice or thrice or even more sums of money compared to their competitors. Data supports the commonly held belief that people tend to like their own representative/senator. It would be interesting to contrast this with people’s approval for the entire Congress. Importantly the graphics shown here are intended to stimulate the viewer's curiosity about the subject of money in politics. Why are a significant majority of candidates funded so heavily especially when their probability of winning is already very high? Role of money in electioneering must be probed and communicated to the public. By reading this article I hope the reader can be an informed participant in our democracy and can engage in a broader conversation about money in politics.,NA,The U.S. Congress: Campaign Finance
https://nycdatascience.com/blog/student-works/observations-from-ted-talks/,11,TED is a notforprofit organization devoted to spreading ideas via short talks. Please see ted.com for more details. The dataset used in this blog was obtained from  The blog analyzes all the talks published from 2006 to 2017. An associate shiny web app for this blog can be found . This blog and the web app aim to (1) understand people's overall engagement with talks and (2) provide recommendations to potential viewers by providing popular talks and relevant ratings. Figures 1 2 and 3 provide insight into people’s level of engagement as demonstrated by the number of views comments and the ratio of comments per ‘1000views’. [Note: the ratios are typically less than 1% for a social media metrics like comments per views. For example a 0.5% comments to views ratio can be considered good for a youtube video (http://tubularinsights.com/3metricsyoutubesuccess/) this ratio translated into 5 comments per 1000 views. Therefore for the analyses in this blog the number of comments are divided by ‘1000 views’. ]  Viewers describe the talks as mostly inspiring informative fascinating persuasive and beautiful. Please see the full list of descriptions in Figure 4. For each talk every one of the 14 descriptions was selected by 0 or more viewers. The box plot shows the votes for all descriptions for all talks. For example: the median for the rating 'inspiring' for a talk was 220 meaning on average 220 people rated a talk as 'inspiring' and half of the talks were rated 'inspiring' by more than 220 people and half of the talks were rated “inspiring” by less than 220 people.   Ken Robinson’s “Do schools kill creativity” has the highest views 47 Million. 24924 people found the talk to be inspiring. The word cloud shows how viewers rated it.Summary:TED talks continue to be well received but the level of viewers' engagement as demonstrated by their comments has steeply gone down in recent years. Analyzing the youtube statistics for these talks can provide additional insight into the questions probed in this blog.,NA,Observations from TED Talks
https://nycdatascience.com/blog/student-works/scraping-the-partisan-divide-a-sentiment-analysis-of-an-online-political-discussion-forum/,11,In this post I apply data science techniques to investigate partisan division on scraped user post data from the online political discussion forum .Check out my Github gists for both my  and I chose to webscrape this online political discussion specifically because:I must additionally be careful to not fully extrapolate the results gleamed from these heavy forum users to the general public.I performed my scraping using the open source Python package  identifying and pulling text via html tags. >  > My Scrapy spider crawls through each thread chronology  identifies the total # of posts per thread and crawls through each post of page.Of the 253 users who posted on the forum 129 (51%) selfidentified politically who accounted for 62% of the total posts.I next consolidated these ideologies with n > 5 along the conventional liberal to conservative US ideological spectrum  discluding 'independents'.Interestingly enough despite equal numbers of liberal and conservative users on Liberalforum.net .Using the  package from sklearn.feature_extraction.text I created a 2 ngram corpus of phrases from the NLPprocessed userpost text data.I split the data between liberal and conservative users and sorted the output to gain the top 20 2word phrases for each group.:Using the Python package  I  ran a sentiment analysis on the text of each of the  250k+ scraped posts.I first applied standard NLP cleaning to the postText data removing stopwords and stemming the text. I then fed the simplified outputs into the TextBlob sentiment analyzer.The analyzer works via Naive Bayes classification built upon the Stanford NLTK program.TextBlob's sentiment analysis outputs two scores:I next look to identify partisan differences in sentiment regarding known polarized political keywords. .For each keyword I segmented out all containing posts along with the # of identifying users.Since the posts/user distribution is so skewed I chose as the the distribution of mean sentiment values of each user.This prevents the scores of a few high posting users from flooding out other users and thus keeps the analysis focused on the To what extent are political discussions on the forum crossing the ideological divide?To find out I implemented a network analysis via the Python package NetworkX.A network analysis is made up of  in this cases users linked by  in this case posting on another user's thread..I color my nodes via the partisan identification of the users (grey  not segmented):The network visually consists of:Next I used the best_partition() function of the  package to partition the nodes into the groupings which maximizes the  the ratio of the connection density  each groups relative to the connection density  groups using the Louvain heuristics. I also weighted the partition via the total number of posts by each user. In this case we are returned 5 segments.Now to  resolve my question I analyze the makeup of each of these 5 segments:With the caveat that partisan identity data is available for only about half of all users analyzing the breakdown among those users (the difference in count between '' and '' for each row) shows that the communities within the main cluster are   .Thus there is evidence that users are socializing across the partisan divide and the forum is serving as a space for political debate.Check out my code for this project via my  on Github.,NA,"Scraping the Partisan Divide: Sentiment, Text, & Network Analysis of an online political discussion forum using Python."
https://nycdatascience.com/blog/student-works/r-shiny/mapping-real-estate-sales-in-new-york-city/,12,"The most recent version of this app is accessible online  while its source code may be found .(It follows that its area will be  times the size of . Arguably this may foster a distorted perspective of the underlying price ratio and for that reason a later version of this app may well alter this. On the other hand the substantial difference in relative sizes makes it easy to pick out variations at a glance whereas smaller variations in size would be harder to see without zooming in on smaller areas of the map.)*All references to ""square footage"" here denote  meaning total floor space of a property.",NA,Mapping Real Estate Sales in New York City
https://nycdatascience.com/blog/featured/nyc-data-science-academy-introduces-remote-intensive-bootcamp/,12,has announced the launch of its  open to technology enthusiasts outside of the New York City area. The first cohort will begin on July 2nd 2018 with fulltime five days per week live daily instruction live communication realworld projects and personalized job support.NYCDSA courses are in highdemand. Due to this demand and the success of the remote bootcamp model with their corporate clients NYCDSA decided to launch an additional option for students who cannot relocate to NYC to attend inperson classes. The remote intensive bootcamp will instead offer realtime streamed lectures as well as access to prerecorded modules and over 1000 coding questions for additional practice. Students in the remote intensive bootcamp will participate in course discussions in realtime conversing with professors teaching assistants and other students. Students also have the opportunity to engage on a Slack channel with dedicated TAs available to answer questions and to converse with students in the NYC bootcamp as well as with the remote students located around the country.Ranked among the highest on both Course Report and SwitchUp the course covers the expanse of all the skills required in the data science industry including both R and Python as well as Machine Learning Theory Big Data and Deep Learning. Each student will complete four projects to practice various areas of skill sets that are deployable in the data science industry.Upon completion of the remote intensive bootcamp each student will receive personalized job support which includes full access to all NYCDSA resources (which includes over 1000 alumni) to help them find a data science position. The career services offered by NYCDSA are robust including resume and LinkedIn profile review and interview skills and elevator pitch workshops. Students will also undergo a mock technical interview and coding tasks oneonone postinterview review and feedback inclass industry expert series and a project presentation and hiring partner event series. Students are also equipped with vetted mentors throughout the entire program to help them learn how to navigate the data science industry.The first cohort runs from July 2nd through September 21st 2018. For more information on how to apply visit .,NA,NYC Data Science Academy Introduces Remote Intensive Bootcamp
https://nycdatascience.com/blog/student-works/nyc-restaurants-reviews-and-inspection-scores/,12,If you ever pass outside a restaurant in New York City you’ll notice a prominently displayed letter grade. Since July 2010 the Health Department has required restaurants to post letter grades showing sanitary inspection results. An A grade attests to top marks for health and safety so you can feel secure about eating there. But you don’t necessarily know that you will enjoy the food and experience courteous service. To find that out you’d refer to the restaurant reviews. For this project I looked at a simple data analysis and visualization of the NYC restaurants reviews and inspection scores data to find out if there is any correlation between the two. The data will also show which types of cuisines and which NYC locations tend to attract more ratings.Nowadays business reviews ratings and grades are the decision making for any business to measure for their quality popularity and future success. For restaurants business ratings hygienic and cleanliness are essential. The restaurant ratings and location information used in this project come from Yelp’s API. The inspection data was downloaded from NYC open data . I merge yelp restaurants review data and inspection data and remove NA rows which doesn’t haveThe data shows that an A is the most commonly assigned inspection grade for restaurants of all types in all locations. I plotted various bar plots to visualized the inspection scores and ratings based on borough and cuisine type.With respect to location this borough bar plot shows that MaAs for cuisine types the cuisines plots shows first 15 restaurants with highest number of counts for based on cuisine.  This indicates that the American cuisine has highest number of A grade compared to other. This indicate that american restaurants are focus more on hygienic and cleanliness compare to others type of restaurants. The review plot indicates that most  restaurants do achieve the top rating of 4 stars. Again Manhattan has the highest number of restaurants with ratings four stars while Staten Island has lowest numbers of restaurants with high ratings. It also shows that almost all borough have a low number of  2 star restaurants. Moreover cuisine reviews plot indicates that American cuisine tend to have the highest rating compared to other cuisines. The reasons could be more American restaurants under this category then others. The scatter plots shows theThe cluster map of NYC restaurants helps visualize locations and  to filter the restaurants based cuisine types. The color mark of the point indicates the ratings and includes  descriptions of the featured restaurants. The heat map show the density of the restaurants based on borough selection or cuisine selection. It indicate which area has a greater number of restaurants. This could be helpful for business people to make informed decisions about where to  open new restaurants based on the types of restaurants already in place.Finally this app can be useful for people to filter the data base on borough cuisine  ratings  and inspection grade.  The people want to go to eat with specific criteria can filters the restaurants and visit their favorite restaurants based on top marks for both ratings and inspection grades. The shiny app link is .,NA,NYC restaurants reviews and inspection scores
https://nycdatascience.com/blog/student-works/usa-x-china-who-is-winning-the-commodity-trade-war/,12,The United States has the title of world's biggest economy for a long time. However China's economy has been growing at a pace that is threatening that leadership. The Gross Domestic Product (GDP) in the US was worth 18.57 trillion dollars in 2016 while China's GDP was worth 11.2 trillion dollars in the same period.Recently the Trump Administration placed tariffs on Chinese products like flatscreen televisions medical devices and others. The Chinese counterattacked placing tariffs on products like soybeans and pork. The exportation and importation of these products can have a direct impact on the GDP.This project is designed to analyze and visualize the commodities exportation and importation around the world with a special focus on China and United States.The dataset is from the United Nations Statistics Division and covers import and export trade values in USD for 5000 commodities across most countries on Earth over the last 30 years. The size of the file is 1.25GB and a preprocessing step was necessary to reduce that size.My first attempt was to create an image of the database inside R that reduced the size of the file to less than 100MB. However the time to load the data from that file was unfeasible. The next logical attempt was to migrate to SQL database. However after creating the table the file size was still greater than 1GB and required additional manipulation. The following steps were made:The first step is straightforward since I had no need to store values that are not going to be used. The next step was necessary because there were two text columns (commodity and category) that had repeated values all over the dataset. Considering that text usually needs more memory space than an integer two new domain tables were built to store the text values and the unique identifier created was referenced in the main table. For the last step primary and foreign keys were created to improve the queries performance. The figure below shows the difference between the original and final database version:The Map section in opposite to the first tab will give you the opportunity to compare countries trade value for a specific commodity. The map graph below was generated by calculating the balance (exportimport) with all commodities taken into consideration. We can see that China and US are polar opposites in terms of trade values. China shows a higher balance trade value while the US shows a higher negative trade value.The last visualization option is a bubble chart. The graph enables a user to see the behavior of two commodities over time by selecting more than one country and the flow (Export or Import). Some insights can be extracted from the graphs below. We see that in ten years China has exceeded the US in exportation. Also in 2016 China was still leading the overall commodities exportation. One curious bit of information that is shown in this graph is that in 2009 all the countries had reduced the trade value amount exported. This was probably because of the 2009 global financial crisis that likely diminished the number of products exported or the value of each commodity.,NA,USA x China: Who is winning the commodity trade war?
https://nycdatascience.com/blog/r/the-changing-landscape-in-the-nba/,12,As an avid fan of the NBA and NBA technology one can argue that the landscape of the NBA has transformed in many areas of the sport. From broadcasting to players to the science of ankle injuries the NBA is moving into the era of data. Seeing the growth of the NBA I wondered how previous generations of NBA players stacked up against current players in terms of birthplaces. As technology brings different parts of the world closer together I would like to see how this trend would affect the way NBA scouts recruit players from places outside the U.S. My research began looking at how the NBA landscape altered itself from a U.S. oriented player to a larger international presence.From Kaggle I acquired two datasets. The first dataset included the names of every player that competed in the NBA along with their birthplace. For this dataset I removed all the NaN birthplaces to clean the data. The second dataset included player stats per season. I merged the two datasets where I left joined on the first dataset with the second dataset. In performing the join I was able to calculate the average career length of a player to be 5 seasons (5 years). I used this new information to group every 5 years of players as a generation. To see how the landscape of the NBA altered itself from U.S. dominate players to a larger diverse crowd I calculated the percent difference between U.S. born players and those born outside the U.S.In my research I used the percent difference of U.S. born players against international born players from 1942 to 1997. The players born in 1997 are either rookies or secondyear players in the NBA. In calculating the percent difference you can see in the graph below there’s a clear trend of overseas players increasing year over year. The graph shows the U.S. isn’t the only place to recruit toptier talent anymore. In an NBA that values players that can do a little of everything expanding an NBA scout’s overall candidate pool can give any team the edge they need.After visualizing the increasing presence of international players in the NBA I was interested in seeing where these players are coming from. In the graph below you can see the total number of players that come from each state outside of the U.S. The top country that produces NBA players outside the U.S. comes from Canada Serbia following closely.What’s interesting to see here is that first world countries are not at the top of the list. You would expect the powerhouse countries to produce the most NBA ready players but above shows this is not the case. Another interesting outcome of this graph it shows smaller countries produce NBA players unlike India or China. To view my analysis please click on my shiny app link:  There are a few routes I can take to expand my analysis but I think an interesting route would be looking at Nike and Adidas revenue sets or store locations in each country outside of the U.S. Nike and Adidas spend a fortune on their marketing budget. The research involved in their marketing strategy should include some sort of analysis as to where each company should open a new store. Maybe from scrapping store locations I will be able to see a trend or a correlation between the store location and the number of players that come from those countries. Taking a look at each country’s population relative to the number of stores can be an interesting insight as well.I could also look at each both companies in each country. For example it would be interesting to see Serbian citizens spending a significant amount of money on shoes and NBA products compared to other countries.My goal for this project is to come up with some predictive model that enables scouts to locate NBA ready players with the use of data and enable an easier and more costefficient way to find and recruit players.,NA,The Changing Landscape in the NBA
https://nycdatascience.com/blog/student-works/transactional-cost-on-high-freq-trading/,12,The orders in the file are generated by different Alpha Engines. Since the final executed prices are given in the data we can directly calculate the PnL for each order by combing corresponding midprices. After grouping order PnL for each engine we may get some insights into the  between AEs in  costs.,NA,Transactional Cost on High Frequency Trading
https://nycdatascience.com/blog/student-works/r-shiny/analysis-and-visualization-of-crime-in-chicago/,12,"Chicago has often been in national headlines for fluctuations in violent crime. Documentaries and TV shows like ""Chiraq"" from Vice and  “The Chi” on Showtime have also given a glimpse and feel of the overall nature of the south side of the city . The south side of Chicago is notoriously known for being riddled with high violent crime rates. In particular Chicago was in the nation's spotlight in 2016 for raising the nation’s crime rate overall because of the large uptick in crimes from 2015  2016. There is speculation that the significant jump in violent crime circled around the  and the subsequent release of the video that recorded the incident. My Shiny app will help to visualize and confirm the uptick in crime rate and where/when crime is happening in Chicago.The application can be found  and the corresponding code can be found  on my github.The DataI used the  spanning from 2012  2017. The data included information such as date/time when the crime happened block where the crime occurred type of crime location description whether there was an arrest and location coordinates. I removed data from 2017 as this data was not complete for the whole year so in any year over year analysis this would have made for an inaccurate comparison. Some additional columns I added were for the type of charge (felony or misdemeanor) the crime fell under in Chicago general location buckets based off the location description and also some parts of date and time parsed out in order to view the data by different aspects of time such as year month hour etc. Exploring the Data: R Shiny AppThe dashboard consists of three different tabs you can click on the sidebar:The heatmap displays a high level overview of crimes happening in various areas of Chicago. It allows the user to see the density in crimes by crime type location and year. Filtering by ""Felony"" and ""Residence"" below one can see in general a higher concentration of red in the southern half of the city.The cluster map depicts crimes by the numbers filtered by crime type location and year. The clusters move apart as you zoom into a particular location and allow you to see specifics of a crime when zoomed in far enough to its location. You can see below the details for crime type date of crime whether there was an arrest and location type that you see once zoomed into a particular area and point. Here we are zoomed into Englewood which is a neighborhood with one of the highest crime rates in Chicago. This particular cluster situated near a school shows a high volume of crime clustered in the same area. In the two time series below we can see trends in the number of crimes and arrests overall in Chicago. With arrests we see a general decrease from 20142016 aligning with the spike in crime rate following the Laquan Mcdonald case. In the crimes series there are a couple distinct spikes which seem to be occurring on New Years day of each year.We can then see crime types by year. The below shows the spike in homicide counts from 2015  2016. Further we see the crime rate by crime type normalized by Chicago’s population (2.7MM). Even with the data normalized there is still a large increase in homicide rate between 2015  2016. Chicago actually joined Memphis at this time as the only cities with a doubledigit increase in murder rate in 2016.Finally we can see crimes and locations by hour which allows users to see the hour at which a crime is more prevalent during the day. Here we see that homicides happen mostly from late afternoon 4 PM to 3 AM at night. In terms of location the below shows crimes happening at  schools/religious locations mostly during the day likely during typical school hours (7 AM  3 PM). Further ExplorationAs I continue to add to the application I would like to add additional datasets such as spending on social initiatives in Chicago and evaluating the impact. Also taking a look at the distribution in ages of people committing these crimes. If it has been a younger demographic perhaps this indicates a need for a more aggressive approach to social spending and initiatives. Overall the goal would be to see how the trends in crime can start reversing by honing in on the actual triggers and running regressions to see the effect of education spending income inequality and age  on crime rates by each neighborhood.",NA,Analysis and Visualization of Crime in Chicago
https://nycdatascience.com/blog/student-works/nyc-real-estate-market-visualization/,13,Perhaps you’re curious about the types when I set out visualize the NYC real estate marketcontains sales records dating back to 2003 that are updated a number of the neighborhood features including For frame of reference elevator apartments buildings are the majority of buildings types in Manhattanfor that are its proximity and many office buildingsthe New York City,NA,NYC Real Estate Market Visualization
https://nycdatascience.com/blog/student-works/ufc-analysis/,13,"The Ultimate Fighting Championship (UFC) idealised by the Gracie Family was inspired by countless exhibition matches termed the “Gracie Challenge” witch was an open invitation issued by some members of the Gracie Family to martial artists of other styles to fight them in a “Vale Tudo” (anything goes) match.The purpose of these challenges was to prove the effectiveness of the Gracie style of Brazilian Jiujitsu. The matches typically featured a smaller Gracie versus a larger and/or more athletic looking opponent. The Gracies defeated martial artists of many different styles such as boxing judo karate and wrestling while experiencing few losses.Looking at the initial UFC results we can see how well Brazilian Jiujitsu .  The data shows that Grapplers (Judo Wrestler and Brazilian Jiujitsu practitioners) were superiors. It means that Striking Athletes were not able to face a Grappler properly.But around 2010 the ""Win Ratio"" per style seems quite similar.Now most of fighters recognises that it is essential to learn how to be safe in different aspects of fighting. The consequence of this new understanding of ""how to fight"" is the similarity between Grapplers and Striking Athletes's ""Win Ratio"".We can see that most of them are Grapplers.Knowing that Grapplers tends to apply Submission techniques the table shows that something has changed in the Grappler's way to fight. They are not finishing fights using Submissions as they were before.These insights about Strikers and Grapplers point to the fact that both groups are training more than one style.The next questions that I'll try to answer will be: 1  Which styles combined has the best results? 2  Are there Athlete's characteristics that could be used to build a predictive model?",NA,UFC analysis
https://nycdatascience.com/blog/data-science-news-and-sharing/how-to-prepare-for-a-data-science-interview/,13,Founded in 2013 the NYC Data Science Academy offers the highest quality in data science and data engineering training. Their toprated and comprehensive curriculum has been developed by industry pioneers using experience from consulting corporate and individual training and is approved and licensed by the NYS Department of Education. The program delivers a combination of lectures and realworld data challenges to its students and is designed specifically around the skills employers are seeking including R Python Hadoop Spark and much more. By the end of the program students complete at least four realworld data science projects to showcase their knowledge to prospective employers. Students also participate in presentations and job interview training to ensure they are prepared for top data science positions in prestigious organizations. For more information visit .,NA,How to prepare for a data science interview
https://nycdatascience.com/blog/student-works/machine-learning/how-much-is-your-house-worth/,13,"Deep learning and machine learning are becoming more and more important for enterprises.For this project I have developped algorithms which use a broad spectrum of features to predict realty prices. The analysis relies on a rich dataset that includes housing data and macroeconomic patterns.Being a data scientist is like being a detective without being one. “There's the joke that 80 percent of data science is cleaning the data and 20 percent is complaining about cleaning the data” Kaggle founder and CEO Anthony Goldbloom told The Verge over email. “In reality it really varies. But data cleaning is a much higher proportion of data science than an outsider would expect. Actually training models is typically a relatively small proportion (less than 10 percent) of what a machine learner or data scientist does.”I've used data from  who compiled a detailed dataset of residential property sales in a North American city. It is used in one of the most popular practice  competitions. This dataset is characterised by a large number of predictor variables (including categorical ordinal discrete and continuous variables). See the  for a description of the original variables.The business objective is to predict the final price of each home based on explanatory variables describing (almost) every aspect of residential homes in Ames Iowa. There are many models available for forecasting. In general more advanced techniques such as stacking produces small gains with a lot of added complexity – not worth it for most businesses. But complex models are almost always fruitful so it’s almost always used in top Kaggle solutions. I've used Jupyter Notebook (Python) for this project.The general framework for my machine learning project is as following1. Loading Data 2. Adding new features 3. Missingness and Imputation 4. Exploratory Data Analysis and Data Transformation 5. Modeling and Hyperparameter Tuning 6. PredictionI have used Pandas software library to load the data into an easily readable DataFrame object. The index column ""Id"" has been dropped as it has no value in the modeling process.For our model to be more robust over time macroeconomic information combined into the same model is the ideal. To extract macroeconomic insights I have added the 5Years interest rate and 10Years interest rate as features to our data.The ten year treasury yield is regarded as the benchmark for the industry but it'll be interesting to compare it with the five year interest rate. Due to the fluctuations in supply and demand it’s during ""seasonal pattern"" that we find there isn't as much competition from the average homebuyer. With summer being the busiest moving time of year people buy more aggressively than in the winter limiting the number of available houses and raising market prices. In the winter though since nobody wants to deal with the inconvenience of moving during this time these lowdemand periods are perfect for those who are looking for a good deal. Because sellers aren’t necessarily getting a lot of interest or offers from others they’re more willing to negotiate and therefore results with a substantial discount on pricing. We can visualize this with our train data and see that the SalePrice totals are highest in June and July. Because of seasonality we can convert the MoSold feature as a categorical feature.I've looked at the responsiveness of house prices to US 5 and 10 Years interest rates as it was interesting to see the macroeconomics patterns. My graph titled Average SalesPrice vs Interest rates suggests that lags of interest rate changes would have to be included in an empirical model of house price determination. Using the correlation matrix we can determine the lagging period by shifting the interest rates as well as the best interest rate to use. I've selected the interest rate with the highest correlation to the SalePrice by testing over a 12 periods (12 months) which is the 10 Years interest rate with a 3 periods shift.The next step required most analysis. As mentioned above data cleaning takes most of the time in projects. Given that we have a lot of categorical features that will eventually be dummified I've analysed the categorical data and merged levels that had similar responsiveness to the SalePrice. Dimensionality can be a curse when our model has many variables and therefore reducing the dimensionality is ideal. For this step I have used boxplots to make decisions on which classes to merge together. If classes had similar boxes (quartiles) and median prices they were merged together.Next I have created a plot of  versus  to identify any outliers.  is a numerical predictor that is highly correlated with the response. Based on the plot there are 4 obvious outliers which have an above grade (ground) living area square feet greater than 4000.As the response variable  is continuous we'll utilize regression models. One assumption of linear regression models is that the error between the observed and expected values (i.e. the residuals) should be normally distributed. Violations of this assumption often stem from a skewed response variable. Because Sale Price has a right skew we'll log + Finally I have used pandas.get_dummies() function to convert categorical variables in both train and test datasets. Because there are different dummy variables in the test dataset than from the train dataset I have added the missing columns in test dataset.The following models were tested : lasso ElasticNet Kernel Rigdge GradientBoostingRegressor XGBRegressor LGBMRegressor averaged base models stacking approach and stacked average models stacking approach. I also used the GridSearchCV instance that fits on a dataset all the possible combinations of parameter values evaluated and the best combination is retained. Because LGBMRegressor had the best test score in my modeling I have used it for my prediction.Given all the hype on stacking and XGBoost I was surprised to see that the LGBMRegressor performed well with a Kaggle RMSLE result of 0.13207. It is a big improvement compared to a linear regression forecasting!It'll be interesting to spend more time hypertuning models and see if that will give better test results. We can also try different combinations of models when using the stacking/ensembling techniques. What I have learned over the course of this project is that machine learning is an art that can be perfected over time!From a business perspective I will still be inclined to use the linear regression even though it doesn't give the best predictions. It is a great model for it's high interpretability. It allows us to easily determine which features in our home should be changed to increase it's value.",NA,How much is your house worth?
https://nycdatascience.com/blog/student-works/r-visualization/manhattan-traffic-collision-insight/,14,I used the NYC Open Data Portal to obtain the NYPD Motor Vehicle Collision data. and analysis the of the 15205 crashes that occurred in Manhattan in 2016.I correlated that with weather data obtained from Kaggle to find the weather conditions when a crash happened. The geographic information system (GIS) data of VMT was obtained from NYSDOT.Vehicle Miles Traveled(VMT) data is defined as a measurement of miles traveled by vehicles within a specified region for a specified time period. It is a traditional index to represent traffic volume. In this project I want to visualize the crash data in Manhattan and help the people who are interested in the vehicle accidents gain insight into the space and time distribution. For every crash I record the type of the vehicle the cause of the crash and the weather condition when that crash occurred as the option combination. One can select any combination of options to take a look at one aspect of the data set.With over 15000 data points it is not practical or even necessary to visualize them all in the map.. I divided Manhattan area by 281 districts and use the red circle to represent the crash density in each district.I set three modules. The origin data” shows  the number of crashes happened in the district visualized by the radius of a red circle..The second module is called “normalized by number.”. For some option combination the total number of such kind of crashes are very small and the red circle would be invisible in the map. In that module the radius of the circle is the number of the crashes in the district given by the options selected divided by the total number of crashes with that options which represent the relative crash density.We may also consider the traffic volume. The high frequency of car crashes in some area may be due in part to high traffic volume. In the view of a driver one may be more concerned about the crash rate per vehicle. To show the crash density normalized by traffic flow I made the third option called ‘Normalized by VMT.’ In the third module we can find that the red circle in central park is very large. Although there are fewer crashes there the driver should be extremely careful because the crash rate per vehicle is high.A shortcoming of the hot map is that it doesn’t show the extract location of every crashes. In the heat map one can locate the accident at street level. The time distribution is also very important. I group the data by hours and analyze the time distribution of different crash causes weather condition and vehicle type. For most conditions the number of crashes start rising in the morning and reaches a peak around 16:00. There are some type crashes eg. caused by driving under the influence and the like which  have a different time distribution and reaches the peak at night.,NA,Manhattan Traffic Collision Insight
https://nycdatascience.com/blog/student-works/machine-learning-techniques-for-predicting-house-prices/,14,"By Weijie Deng Han Wang Chima Okwuoha and Shiva PanickerFor decades the American Dream has been defined by the pursuit of home ownership and in 2017 Zillow reported that the median value of U.S. homes hit a record high of $200000. Housing prices are determined by a slew of factors; some quantifiable and some not. Being able to isolate the best factors for predicting housing prices has a clear use for realestate investors as well as individuals looking to move into a new home. In this project we used compared and stacked multiple machine learning algorithms to predict housing prices in Ames Iowa. The data set included 80 variables each that have a varying impact on the final housing price. The variables included details about the lot/land location age basement roof garage kitchen room/bathroom utilities appearance and external features (e.g. pools porches etc.).The data frame provided was relatively clean which allowed us to focus on making the variables compatible with our models.After looking at the Sale Price feature we saw that it did not follow a normal distribution so we decided to transform it to better represent a normal distribution. The transformation we used was a logarithmic transformation of the form log(price + 1). After transforming the sale price we noticed several numeric features were not normally distributed. This was problematic as certain linear models are based on the assumption that predictors are normally distributed and violating these assumptions could negatively affect the linear model thus making predictions inaccurate. To fix this we decided to skew those with degrees of skewness larger than 0.75. We filtered the following features to be skewed which were '1stFlrSF' '2ndFlrSF' '3SsnPorch' 'BsmtFinSF1' 'BsmtFinSF2'  'BsmtHalfBath' 'BsmtUnfSF' 'EnclosedPorch' 'GrLivArea' ‘'KitchenAbvGr' 'LotArea' 'LotFrontage' 'LowQualFinSF' 'MasVnrArea'  'MiscVal' 'OpenPorchSF' 'PoolArea' 'ScreenPorch' 'TotRmsAbvGrd' 'TotalBsmtSF' 'WoodDeckSF'. We use the same logarithmic transformation log(x + 1) to make the features normally distributed.Now we had to figure out a way to deal with the ordinal features and categorical features.For the ordinal features we read in the descriptive text to extract those features so that we could turn them into numeric features. We fed them into a dictionary and applied them to columns.For categorical features we had tried both onehot encoding and label encoding. In linear models and random forest dummification had better performance than label encoding. So we use onehot encoding to handle all the categorical features. Onehot encoding will turn all the items in one predictor into a new column in which 1 means True and 0 means False and delete one item to make it obey the linear model assumption.We used cross validation to find the best parameters for a  gradient boosting regression. These parameters were learning_rate  0.04 max_features'sqrt'n_estimators500 max_depth4subsample0.7min_samples_split  10min_samples_leaf 2.The following graph is of the top 30 variable importance. The top two most important variables are ""Fence"" and ""Full Bathroom."" However there is a limitation that when using dummified data to fit tree models they will usually be misleading and have incorrect answers.Although the RMSE of gradient boosting is 0.118 the RMSE on Kaggle is 0.166. This model has a very serious overfitting problem. We considered the problem should result from the tree model.",NA,Machine Learning Techniques for Predicting House Prices
https://nycdatascience.com/blog/student-works/nyc-real-estate-market-data-analysis-and-visualization/,14,The more important question is how do you create business values from the dataset how do I differentiate from the tools that other data websites provide?The following is a simple comparison between several widely used data providers in the industry and the project to demonstrate where the business values are.When people think of New York they think of Manhattan Queens and Brooklyn. And that's where the brokerage firms mainly cover for their industry research reports. However the Bronx and Staten Island real estate markets are not covered. And those two borough's real estate markets have performed really well in the past year. ,NA,NYC Real Estate Market Data Analysis and Visualization
https://nycdatascience.com/blog/student-works/predicting-residential-home-prices-in-ames-ia/,14,The data for this project came from a presplit evenly into a training set and test set with 1460 observations and 79 features. The first model that we used was Ridge Regression.  Ridge Regression minimizes the sum of the RSS and the penalty which is the sum of squared coefficient estimates.  By finding the value for lambda that minimizes this sum the model aims to shrink the group of coefficients in our model which prevents the model from overfitting and allows for more robust predictions. After tuning the model we found that the optimal value for alpha(lambda) was 9.48 which resulted in an RMSE of 0.1207. Although Ridge Regression is able to shrink coefficients such that they asymptotically approach zero it cannot set coefficients exactly to zero like the next model.The second model that we used was Lasso Regression. Lasso Regression is similar to Ridge Regression because they both strive to minimize the sum of the RSS and a shrinkage penalty. The difference between the two models is that the Lasso Regression model uses a different shrinkage penalty.  Lasso Regression uses the  penalty whereas Ridge Regression uses the  penalty mentioned above.  After tuning our model we found our alpha(lambda) to be 0.00048413 and the model’s RMSE to be 0.1195.  A key assumption in Lasso and Ridge Regression is that the target data is linear. We chose to incorporate the next two models into our metamodels because they do not make this assumption.The third model that we used was Random Forest Regression. A Random Forest model builds different decision trees on bootstrapped data from the training set. The decision trees are different because each tree can only consider a random subset of the total number of predictors at each split.  Thus only predictors in that subset have the opportunity to be the splitting factor. This characteristic of the Random Forest model creates decision trees that are not correlated and reduces the variance of the model. After tuning our hyperparameters our model yielded an RMSE of 0.1239. Like the Random Forest model our last model is based on the premise of combining many weak learners into a single model that is a strong learner.The fourth model that we used was Gradient Boosting Regression.  Gradient Boosting Regression makes an initial prediction; then the algorithm improves on that prediction by adding an estimator term to the initial prediction.  Next it optimizes the model by fitting the estimator term to the residual using gradient descent. Our optimized Gradient Boosting model had an RMSE of 0.1301. After optimizing each of the individual models we used them to create two different metamodels.The first metamodel averaged the predictions of the four base models.  We weighted each of our predictions equally. If we were to continue to develop this metamodel we would perform a gridsearch using cross validation to find the weights that yielded the optimal result via a weighted average of the predictions. This metamodel yielded an RMSE of 0.12582 when submitted to Kaggle.The second metamodel stacked the base models.  We took the predictions of each of our individual models and created a fourdimensional matrix.  We then performed multiple linear regression to yield our final prediction.When we submitted this model to Kaggle we yielded an RMSE of 0.1267.In order to effectively predict residential home prices it's important to examine external factors as well. This includes supply and demand indicators such as construction pipeline net absorption days on market. Furthermore when predicting something like home prices it’s important to take into account the local economy. This would include investigating job growth income levels and familymix in the local area.,NA,"Predicting Residential Home Prices in Ames, IA"
https://nycdatascience.com/blog/student-works/social-network-analysis-with-scalable-user-behavior-scores-of-a-music-website/,14,"To build the desired social network we need to take into account various kinds of user behavior information including following and rating to artists saving spotlighting and listening of songs of an artist etc. We count and convert all the available user behavior data into different feature variables which will then be used to calculate an overall usertoartist fanscore. The fanscore accurately represents/defines the relationship from a certain user to an artist (or to another user in the case of one user following another user). Based on that a social network graph can be accordingly derived.To measure the activeness of an individual user we take into account the total number of user activities on the website along with some other specific user activities e.g. sharing and commenting etc. For each user behavior feature variable we intend to calculate a score of the same consistent range from 0 to 100. In order to do that we need to conduct some preprocessing on the various different feature variables. First all of these behavior variables are different kinds of activity count. We normalize all of them with their respective measuring period of time in terms of the number of months. As such each activity variable becomes a certain number of ""count per month"" which is very straightforward to interpret and control. the NetworkX Python package for SNA. We built a For the entire social network graph of all the users and artists of the music website we tested and conducted user influence analysis in many different ways (i.e. with ) and thus got an influence score for each user/artist. Specifically wWith all the above user scores and network works done finally we are able to give our answers to the targeted questions of the project. One future direction is to add unpopular artist handling. We found that f",NA,Social Network Analysis with Scalable User Behavior Scores of a Music Website
https://nycdatascience.com/blog/student-works/scrape-amazon-placeholder/,14,Amazon sells a vast selection of products among which many have very similar features yet different prices. Make It becomes difficult to make a informed purchase decision ,NA,Amazon Review Scrape and NLP Analysis: Creating a Cloud Parallel Scrape System
https://nycdatascience.com/blog/alumni/building-on-bootcamp-skills-to-bridge-data-science-and-business-teams/,15,Malavica Sridhar is currently a Product Manager at  a job that lets her use both her business skills and the skills she acquired at NYC Data Science Academy. She chose to work at this firm rather than at the bigger names that offered her a position  like Google McKinsey Dropbox Airbnb and Facebook  because she felt that she could have a greater impact at a smaller place. Malavica’s decision to attend NYCDSA was based on her general approach to decisions: she thinks about where she wants to end up and works backwards from there. She wanted to acquire a breadth of data science knowledge to qualify for a career in which she has a real impact on the direction of the business.As Malavica tells it her work experience before enrolling in bootcamp was “sort of all over the place.” Her most recent job before enrolling was as a Business analyst for McKinsey & Company where she did advanced analytics and strategic work for a variety of clients primarily in the banking space. Before that she worked in customer business development for Procter and Gamble following a stint at a startup called Custora that uses predictive analytics to provide customer insights to retail industry customers.  Malavica holds Bachelor’s degrees in both Business Administration (in Marketing and Strategy) and Statistics from the University of Michigan. Her experience was an important factor in the decision to attend NYC Data Science Academy’s bootcamp as she knew that she would need a program that was open to students with only an undergraduate degree. Next Malavica  “started working backwards”. She began thinking about what she wanted to do and what she’d have to learn to get there. For Malavica research included talking to people who had graduated from NYC Data Science Academy.  She found that they weren’t only data scientists but also people who followed somewhat different career paths. She wanted a program that would prepare her for any career direction she chose whether this be as a data scientist or as a product manager. One thing that made NYC Data Science Academy really stand out is that it would enable Malavica to learn data visualization and two languages  R and Python. In contrast other programs only taught a single language. The breadth of the course work and the acquisition of those two languages gave her not just the ability but the flexibility to embark on different career paths.She also found that she learned a lot both from the staff and the student body. In her bootcamp cohort students were evenly split between those who held a Master’s and those who had a Bachelor’s degree in addition to eight students with PhDs.  The benefit of having cohortmates from different levels and backgrounds is that they brought different things to the table. Those with coding experience from different languages were able to pass on “tricks of the trade” from what they knew that others may not have been exposed to Malavica observed. She also found it beneficial to connect with people who approach code the same we she does even if they had different programming training.  As some students had stronger coding backgrounds than others the school offered “prework” to give students the opportunity to get up to speed on technical skills that they would need to build on in the bootcamp. The material was helpfully tailored to the needs of the student.  In her case Malavica applied her “working backwards” approach to planning and set aside six weeks prepare for the bootcamp.She explained that there are a great number of topics that the program runs through rapidly. That leaves students with two options: either they can aim to cover it all in a short amount of time or they can think about what they want to specialize in and focus only on those areas. Her own approach and the one she recommends to others is to give a great deal of thought to where they “want to end up and then work backwards.” For example someone whose goal is to gain the expertise necessary to work in machine learning could focus on Python. In her case she wanted to pick up a broader perspective and gain a general “umbrella of knowledge of all of these topics.” This allowed Malavica to be comfortable discussing any topic in a job interview allowing her to keep her career options open. Though she received several attractive job offers from major companies upon completing the bootcamp Malavica opted to take on a bigger role in a smaller place.  At CircleUp her position is to define and scale their machine learning platform Helio. Malavica explains that though she doesn’t work as a data scientist she draws on her data science skills to serve as a “bridge” between technology and the business teams. As such she plans out what models need to be built and how the work should be arranged to drive effective execution.,NA,Building on Bootcamp Skills to Bridge Data Science and Business Teams
https://nycdatascience.com/blog/student-works/predicting-house-prices-using-machine-learning-algorithms/,15,Inspired by the accomplishments of the women in the movie “Hidden Figures” we named our team after the movie. We are an allgirls team of three who come from diverse parts of the world  Lebanon India and China. This blog post is about our machine learning project which was a past kaggle competition  The dataset contains “79 explanatory variables describing (almost) every aspect of residential homes in Ames Iowa…[and the goal is] to predict the final price of each home” ().We collaborated on certain parts of the project and completed other parts individually as if it were a research project. The goal of the project as aspiring data scientists was to utilize our arsenal of machine learning knowledge to predict housing prices. The following blog post is categorized into four main parts: Exploratory Data Analysis & Feature Engineering Creating Models Conclusion and Relevant Links.We started the project research by analyzing the data and visualizing it. Real estate is a new area for all three of us but we managed to gain some interesting insights. For example some variables are closely correlated with one and other. Some pairs are correlated by nature such as “Basement finished area” and “Basement unfinished Area” while other pairs were correlated by deduction such as “Overall condition” and “Year built.”Next we explored the data to see if there were trends in sales prices associated with seasons. We found out that there are more sales during summer. Research shows that high supply doesn’t necessary means high price since the price of housing normally peaked around summer. One theory is that when there are more options in the marketplace people are more likely to find their ideal house and put down the deposit.The neighborhood is an important factor when it comes to buying houses. Since the raw data does not contain school district information and crime rate neighborhood was an important factor implying above factors. After plotting out the graph down below we went back and checked the accuracy neighborhood with the higher price was equipped with highend facilities besides school districts and great locations. We can also see the variance of an expensive neighborhood is typically higher which explains the skewness of the sales price density as well.After trying the linear models approach it is good to see the data from a different angle so I decided to try the treebased models approach. These are the steps I followed:Why start with Random Forest? Random Forest Model is considered one of the best models for feature importance analysis. The reason is that the treebased strategies used by random forests naturally ranks by how well they improve the purity of the node. The below graph shows the 30 most important variables in our dataset:The accuracy score of Random Forest Model on the house price prediction is 0.88. Here comes the question: Is it the best accuracy score? The following step is a comparison between several treebased models to check which model has the best accuracy score in predicting House prices.After applying each of the following models on the data set different accuracy scores were achieved as shown in the following graph:The Gradient Boosting Model has the highest Score with 0.90 and with error 0.095. So in the following steps I relied on Gradient Boosting Model in my analysis.To have a better performance of the gradient boosting model on our data set I used the GridSearch function to tune the parameters of the model. After several trials of GridSearch the following parameters were chosen with specific ranges:Huber or LsRange( 0.0001 0.0010.010.11)Range(100 1000 10000 14800)Range(1 > 15 )Here are some of the parameters that gave me the lowest mean squared error:Since stacking or averaging several models together might give a better accuracy score in the prediction two gradient boosting models were used in the analysis depending on the first two rows of the above table as parameters.On the other hand linear models might help in improving the score as well. I used the following two models that gave me good accuracy scores as well.In Ridge model analysis after the parameter tuning step I chose alpha to be 10. The error of this model is 0.11. In Lasso model analysis the same steps of the previous models were applied and the error is 0.11.To sum up the final model was the average of the 4 best models together: Gradient Boosting Model 1 Due to the lack of observations the first step should be linear models. I started by implementing Lasso and ridge both yield 0.92 (pretty strong) CV scores. Since the data contains a lot of categorical variables I was curious how well tree based model fit the model. Random forest Extreme Random forest XGboosting all yield okay result at the best during crossvalidation. The best performer has to be Gradient boosting which Fatima mentioned in details. I also explored a little in SVM and finally combined my models using stacking.Each one of us collaborated on the initial exploratory data analysis and feature engineering part. Then we worked on creating predictive models individually. Thanks for reading this blog post. Feel free to leave any comments or questions and reach out to us on through LinkedIn.,NA,Predicting House Prices Using Machine Learning Algorithms
https://nycdatascience.com/blog/student-works/web-scraping/the-sentiment-under-movie-reviews-from-1999-to-2017/,15,Ten years ago when I was still a young boy I liked to read reviews of films on TV shows on the internet.  Back then people were generally more polite. Saying “great” did not necessarily mean that the person really liked the things they commented.  In fact it came to be applied just to anything not bad. As the trend to inflate the praise of things continued expressions like “That is the best thing I have ever heard!” started to appear to distinguish  what really is good from the merely not bad.I wanted to find out whether people did in fact change the sentiments expressed by particular phrase.  I set out to collect some data and apply data science to determine the answer.I collected the reviews of movie/TV shows on IMDb from 1999 to 2018. For each year I scripted the reviews of the top 150 movies/TV shows shown on that year.Graph 1: The yearly distribution of the number of reviews and the mean length of the reviews. The blue bar shows the number of the reviews in each year. IMDb was founded in 1999 and the reviews kept rapid growing from that year until 2005. The number of reviews is low in 2018 because this project is done in March 2018 so only three month is included. The dark blue line shows the average length of the review which shows a stable trend.I want to build the model to find the sentiment of the movie review. The sentiment of the review for each person is different. When some people says “the movie is good” it may just mean the movie is in average rank. Others may use “ the movie is good” to show that this movie is the best movie he has ever seen. For that reason  judging people’s intended meaning is difficult. However the platform of IMDb provides another way to show the attitude of people: the rating. The range of rating is from 1 to 10 the higher the better. If someone comments “the movie is good” but gives a rating of 5 we will not consider the word “good” a positive word in this comments. If someone use the word “good” and gives a rating of 10 we consider the word “good” has a strong relation with the positive attitude.Although every person has his/her own rating criteria  we can still analyze the trend along the year.I trained a machine learning model to predict whether the review is positive or not. When the model was trained and performed well I analyzeds the model to find which phrases prove  more important. For example if the word “good” is more important in my 1999 model but “best” is more important in 2017 model that indicates that people had tended to use the word “good” to show their positive opinion in the past but have generally shifted to use the word “best” in 2017. The details of the models are:1: I built a scrapy programming to get reviews from 1999 to 2018 and grouped it by years.For each year I trained a machine learning model. Notice that the reviews in 2018 may also contain the review of a movie that came out in 1999. 2: I gave each review a binary label. If the rating associated  with that review was higher than 7 it is labeled “positive.” If the rating is lower than 4 it will islabeled as “negative.” The other ratings are dropped to gain a better prediction accuracy.3: I used the tfidf method () to split the review in to phrases. The tfidf frequency of the phrase is used as features. The phrase here can contain at most two words. So the phrase “good” and “not good” will both be considered and a comment with “not good” will be  less likely to be mispredicted as a positive one.4: I trained a machine learning model to predict whether a review is positive or negative for each year. I used the random forest model. The feature importance of a random forest model shows how important a role a phrase takes and we can use the change of feature importance to analysis the sentiment change.Graph 2: The ROC curves of three machine learning model on 2017 movie reviews set. ROC curve is used for judging the performance of a binary classifier. The larger the area under the curve the better the classifier. If you predict randomly the curve is a straigth line from (00) to (11). All the curves here are above that straight line which means all the model are somehow workable. The random forest is chosen for later analysis because only a random forest model has feature importance among those three methods.The feature importance may not be robust when the dataset is changed a little. However random forest is an ensemble method and take the average result of several decision trees so the fluctuation of the feature importance is reduced a lot.The whole phrase set is very large.  To interpret my result I selected the top 20 phrases ( according to the appearance counts). The graph appears below.Graph 3: Feature Importance of the random forest classifier (Positive Words). In the Top 20 phrases there are 6 words  “believe” “good” “great” “love” “like” “best”  that have a positive meaning.From the graph I could conclude that people tended to use “believe”  “good” “great”  in early years to show a positive sentiment but started to use “best” more recently.Graph 4: Feature Importance of the random forest classifier (Negative words). There are 13 phrases in the Top 20 lists have a negative meaning: “bad” “awful” “boring” “poor” “terrible” “waste” “worst” “horrible” “worse” “crap” “stupid” “disappointment” “waste time”.From the graph the word “stupid” was more important (which means the model has a higher chance to predict a review as negative when the word “stupid” appears in that review) during 20062009 and the word “waste time” has took a more important role in recent years.One interesting thing is that the phrases with negative meanings occurs more in the Top 20 lists and the value of the feature importance is larger. It means the random forest model make its prediction more rely on the phrases with negative meaning. That may imply that when someone says some positive words about the movie he/she may not really mean that he likes it. But when he/she says some negative word he/she really dislikes it.  People tend to be generous with praise. There are some words do not contain emotion. I put the graph of the neutral words below for reference.Graph 6: The feature importance of the Random Forest Classifier (Neutral Words). Neutral words includes “really” “supposed” “thing” “time” “trying” “way” “contains” “film” “movie” “spoiler” “watch” “just” “plot” “spoilers” “money” “series”. Notice: The word “spoiler” and “spoilers” are the labels given by the IMDb platform when it believes the review is a spoiler. Both the positive and negative reviews could be labeled as “spoilers”.I also graphed the counts of those phrases in each year to help understand the sentiment of the reviews.Graph 7: The counts of Phrases in each year. The portation of each words remains stable from 1999 to 2018. It shows that the feature importance of the random forest model have little influence about the portation of the phrases. What we figure out in graphs 46 is the change of people’s sentiment under those phrases.After scraping and analysing almost 40000 reviews on IMDb I found that people do tend to use different phrases to reflect their sentiments about a film or show from 1999 to the present. Some word used in 1999 may have a different sentiment in the year 2017. Also people are more honest about their feelings  when using phrases with negative meaning.This project is just a start and not perfect. But when the wind of machine learning whisk the veil of the goddess of literature and reveal a small part of her beauty which has hidden for 19 years I can not help to stay and marvel at the magic power of data science.,NA,The sentiment under movie reviews from 1999 to 2017
https://nycdatascience.com/blog/student-works/r-shiny/mushrooms/,16,"When we hear the term ""mushroom"" this is the definition most of us think of. How much do we really know about mushrooms and the benefit of mushrooms. How many species of mushrooms are there? Which are edible and which are poisonous? Mushroom market was valued at $35.08 billion in 2015 and expected to reach above $59.48 billion in 2021.  With the food trend leaning towards vegan and natural foods mushrooms are the preference that a lot of vegans consumers tend to depend on due to the fact that they are great protein source and full of vitamins and nutritions. Most mushrooms you see in the grocery stores or super market today have been commercially grown by farmers in controlled environments.Mushrooms have a very short shelf life and cultivation is heavily labor intensive and requires high operation costs.  The rapid demand growth within the last ten years also means that mushrooms farmers will have to grow enough supply to fulfill the demand forcing them to use unwholesome cultivation methods which include excessive chemicals and fertilizers.  Such growing methods have consumers shifting away from conventional mushrooms and leaning towards organic or wild grown mushrooms.   This is one of the many reasons why mushroom hunting is still very popular in certain areas around the world where natural growing environments and climate allows. In the United States mushroom picking is popular in the Appalachian area and on the west coast from San Francisco Bay northward in Northern California Oregon and Washington and in many other regions.Mushrooms have been used in medicine as an ingredient in cooking in many cuisines and as a natural dye for natural fibers such as wool for clothing. Mushrooms for a long period of time now have been known as the ""meat"" of vegetable world.  There are about 100000 species of mushrooms and only 10000 are known to the North America region. So the question is should you eat wild mushrooms? The answer is yes and no. Yes because there are so many great tasting wild mushrooms out there.  No because there are more ""bad"" than ""good"" mushrooms grown wild. About 4% of the wild mushrooms are consumable and can be use in cooking. 25% are edible but not recommended and about 50% of all mushrooms are inedible with 1% of them being extremely poisonous.  Therefore only positive identified mushrooms should be eaten and do not combine different mushrooms type while mushroom hunting. ",NA,Mushrooms
https://nycdatascience.com/blog/student-works/how-many-data-science-jobs/,16,AbstractThe QuestionsThe Scrapping ProcessThe PlanningImages showing the relevant information that I would scrape (highlighted) from the job posting and the company description pages respectively.The Hiccups With ScrappingA picture of what was being “seen” by Scrapy due to the data being called by JavaScript. The company information is missing.The network file that Glassdoor uses to see which website to call to present company information.The URL and the page containing the company’s information.Despite Glassdoor stating that had 791 pages of listings…...I would get an error for any page past 30.The formula for the URL to search through all the listings for all of my cities.Extracting Description DataThe parts of the description that we still need to extractThe VisualizationsFurther Exploration and Next Steps,NA,How Many Data Science Jobs Are There? A Scrape of Glassdoor
https://nycdatascience.com/blog/student-works/is-your-broker-really-who-he-says-he-is/,16,My second industry standard data science project was another great challenge! Our primary task was to collect data from a web source by method of scraping. I wanted to explore the finance industry this time and decided to approach an Investor Relations Firm (IR firm) and offered to help them using this project!When looking for a great broker you want to do your due diligence and make sure to know whether or not you can trust your money with that person. FINRA's website  helps by telling you whether a person or firm is registered as required by law to sell securities (stocks bonds mutual funds and more) offer investment advice or both.The website is useful to this IR firm who looks at continually building a network of licensed professionals and maintaining longterm relationships within the financial community. The IR firm I have partnered with for this web scraping project organizes roadshows where only licensed financial advisors are permitted to attend and meet with the company's management. The firm maintains a database of over 40000 financial advisors that constantly needs to be updated. If not advisors retiring and leaving the industry would cause a permanent decline in number of contacts in the database. When roadshows are organized in cities where there are not enough advisors in the database the team puts an emphasis on finding licensed advisors in the areas that have the most need.To do so the employees would manually search the internet to find advisors or new firms that are not already in the database. However before a practicing advisor can be considered for the company database it must be checked that the advisor is series 7 licensed on .For this business problem I opted to use Selenium to scrape as the website is using Java scripts. It would have been difficult to parse using BeautifulSoup or Scrapy. I also used Katalon Automation Recorder to help me automate the refining of the results to only get brokers that are actively registered. Another challenge I have encountered was generating a message that would indicate if any of the examinations the broker has is Series 7  General Securities Representative Examination. I was able to write a code that can generate the results expected by the Investor Relations Firm in a csv file. It allows them to easily and efficiently access the information on the broker and their company.Once all the advisors in a city are checked and entered in the database my web scraping program can run again to find the movement in the population of licensed advisors using a comparison tool like Beyond Compare and easily identify new advisors that have joined the industry and those that have left or moved firms. This will save the IR firm an enormous amount of time searching the internet for new advisors and reduces the time wasted finding advisors online that are already in the database!Shall I have more time in the future I would like to have an additional use of R Shiny app to this project so they can select any city and have the shiny app call the web scraper. That would reduce the chance of creating an error in the code when changing the keys to send. I have to say it feels great to know that the knowledge we acquire over the course of this data science program gives us the ability to build strong tools that can help businesses in many industries! It will be interesting to measure the increase in the number of contacts added to the database as well as the increased time spent interacting with financial advisors in lieu of internet searching.,NA,Is your broker really who he says he is?
https://nycdatascience.com/blog/student-works/web-scraping-building-an-app-to-find-the-perfect-tennis-string/,16,Finding the ideal tennis racquet string is a challenge for many players because there are thousands of different types available varying in material construction shape/texture and thickness. To add confusion two strings with the same ‘specs’ can play very differently and the same string can play differently when used by different people or strung at a different tension. Because of this dizzying array of possibilities many players just ask their stringer to pick a string for them and do not put much thought into optimizing this important piece of equipment – the only part that actually touches the ball during play.For players who do try to find the best string for their game the only thing to do is test out a variety of strings until you find what works. As an avid tennis player who tinkers with different string combinations I have found  to be the best resource for finding strings to try because the site has so many reviews (17500+ reviews by 4400+ unique reviewers) and covers almost every string on the market (2350+ varieties). However even though it is the best resource currently available the website has very basic search filtering and ranking capabilities that limit its usefulness.This is especially tragic because the site gathers great data! For each review stringforum not only collects information about the the  (ratings across seven categories an overall satisfaction rating the adjectives that best describe the string and a text review) but also about the  (gender age bracket playing style ability level swing speed and how much spin they use in their strokes) and the  used in testing (manufacturer model frame size string pattern string tension level). In addition to data gathered from reviews the site also has general information about the strings (price thickness material construction and features).‘comfort’ ‘control’ ‘durability’ ‘feel’ ‘power’ ‘spin’ ‘tension stability’ and ‘overall rating’. The site's rankings are not very useful however because users can only sort by one characteristic at a time. This would be fine for a player interested in only maximizing control or only maximizing power but any player who has preferences for more than one characteristic is out of luck. The problem is every tennis player I know has some degree of preference for all the characteristics  the only question is how much. Instead of asking  the player prefers a better ranking system would list all the characteristics and ask the user  to put on each. One player may place a high emphasis on comfort and control low emphasis on durability and power and medium emphasis on the others. Another may assign entirely different weights. The point is that it's natural to take all the characteristics into account when deciding what makes for a good string and a ranking system should reflect this reality. It’s also a shame that users on stringforum aren’t able to rank strings based on adjectives. Each review includes a list of adjectives to describe the string being evaluated from a list of 22 possibilities (e.g. ‘soft’ ‘lively’ ‘explosive’ ‘spongy’ ‘springy’ ‘stiff’ ‘precise’ ‘dull’ ‘boring’). Since there are a finite number of options it would be easy to rank strings according to how often reviewers chose to describe them by an adjective. Just like for string characteristics users should be given a list of all 22 adjectives asked to provide weights for each and get an individualized ranking based on those preferences. In this case however the user should be able to provide negative weights in case he/she wants to penalize strings for certain adjectives.Review Criteria String Rankings and String Profiles and they For characteristics users are allowed to assign weights from 0  10 with a default of 5. This means that each characteristic counts a medium amount in the overall rankings by default and the user can decide if it should instead count for nothing a small amount or a large amount. It makes sense that the default is medium because all eight are important components of good strings. A user who uses the default ranking would still get a perfectly acceptable (although bland) ranking with all characteristics weighted equally.,NA,Web Scraping: Building an App to Find the Perfect Tennis String
https://nycdatascience.com/blog/student-works/web-scraping/what-are-the-most-under-rated-hiking-destinations/,17,Using data collected from the web apply exploratory data analysis to find hiking destinations that are underrated.,NA,What are the most under-rated hiking destinations?
https://nycdatascience.com/blog/student-works/flight-booking-scraping-project/,17,the first onethe second oneThe data from these visualizations is  filtered. Now the analysis is applied on flights going to Dubai from London and mainly in the mentioned days.,NA,Dubai International Airport Flights Analysis - Kayak.com Scraping project
https://nycdatascience.com/blog/student-works/some-simple-observations-of-used-cars-on-the-u-s-market/,17,This project aims to survey and observe some simple but interesting patterns based on the data collected for used cars on the U.S. market. As a car enthusiast and an owner of several cars (not simultaneous ownership; I am not rich) I feel I can give my insight from the consumer side to talk about what those patterns mean.,NA,Some Simple Observations of Used Cars on U.S. Market
https://nycdatascience.com/blog/student-works/from-visualizing-nyc-2017-house-sales-data-to-postulating-its-thermodynamics-theory/,18,"This project aims to (1) visualize the patterns of NYC 2017 house sales in each of the five boroughs (2) put forth some preliminary postulates to explain the patterns. I focused on the patterns of transaction volumes (i.e. how many houses sold) and house prices with respect to location and time. I chose this topic because the scope is focused and the data set is relatively clean. Also there was a personal reason: I bought a house last year (2017) in Staten Island. After going through the whole process (looking for a house in the right location putting in bids applying for the mortgage and finally closing the deal) I became interested in learning NYC's real estate patterns since then. I feel maybe I can talk about something specific and interesting to house sellers/buyers.The raw data set is NYC Department of Finance's 2017 property sales records (locations/building years/squarefootages/sales dates/prices/etc.). It consists of five files corresponding to the five boroughs of NYC: Manhattan Bronx Brooklyn Queens and Staten Island. For raw data seeTo simplify analysis we concentrated on two quantities: transaction volumes (henceforth shortened as ""volume"") and prices with all other information (such as buildings' ages squarefootages etc.) not considered. Also for practical consideration all transaction records with dollar amount less than $100000 were filtered out for this project. Finally analysis for each borough is based on the study of all zipcode areas in each one.Some summary results appear in the following graph. Left: transaction volumes by borough; Right: median house prices by borough.We now move to the next level to find more interesting or fundamental patterns. But before that we perform such data transformation: Boroughwise Normalization. It is a term coined by the author of this project and is no different than the usual normalization in statistics (i.e. calculating zscores). We spell out its context (i.e. boroughwise) to remind how the normalization should be done. Say we want to normalize a particular zipcode area's  house sales volume and say the zip code is 10305 ∈  we normalize it with respect to the average level μ and standard deviation σ of house sales volume in Staten Island in January. Note that we normalize with appropriate boroughwise values. Similarly we normalize house prices boroughwise. Further data visualization and analysis are based on the   volumes and prices. For more algorithmlike description (so that interested readers can implement them with programs) see the following linked notes which I wrote for this project:Why do we want such normalizations? The reason is both practical and philosophical. Practically it makes the pattern sharper (hence easier to identify a pattern). Philosophically as everything is relative we need to put a quantity in a most appropriate context. When you claim that a certain zipcode area (e.g. 10305 ∈ Staten Island) appears active in terms of house sales volume you claim so not because you compared it with other boroughs but with the relevant boroughwise average (Staten Island in this case). When you claim a zipcode area (e.g. 11234 ∈ Brooklyn) appears to be competitive in terms of house prices you didn't compare it with Manhattan's price level but with the relevant boroughwise average (Brooklyn in this case).Below are two visualization examples that use heat maps to show the transaction activity patterns; the heat intensity is set to be proportional to  house sales volumes. The heat dots are plotted per zipcode in a borough. The more intense a red dot the more transactions occurred in that zipcode. Please note that a zero heatintensity does NOT mean no house sold in that zipcode; it just means the volume sold there was at the borough’s average level (recall that we are plotting the normalized volumes not the volumes themselves). Likewise a green dot indicates a zipcode area has belowaverage house sales volume.In this example some of the most eyecatching patterns include: (1) In both May and June 2017 there is a dominant red dot (highly active in transactions) in the neighborhood of Flushing Queens where MTA 7Trains end and the largest Asian populations in NYC reside. (2) In May a constellation of red dots led from Flushing to Elmhurst Queens each of which corresponds to a major 7Train stop; however those red dots became green (belowaverage) in June. (3) The opposite process occurred in the neighborhood to the west of Forest Hills and to the east of Meadow/Willow Lakes: both changed from green to red from May to June.The above observations may or may not be surprising depending on an observer’s familiarity with Queens’ housing market. You can explore the Shiny App I developed to discover “surprising” findings by playing with Month and Borough choices. The Shiny App can be accessed by the following link:The potential business point is that identifying this activity pattern may be useful for buyers or sellers or real estate developers. For example assuming this kind of pattern will be the same in 2018 if I anticipate buying a house in between Flushing and Elmhurst and hope to get the business done around May I could expect more transactions in May and less in June. Sound business decision may be further made. For example buyers may think about should they avoid high activity time so that they will not have to compete with other bidding buyers and pay more than a house's actual value. As a resident of Staten Island I find the patterns in this example make a lot sense. (1) Notice the neighborhood right near the Verrazano bridge (the Arrochar neighborhood east end of Island and at the last exit of crossIsland highway I278) which appears to have at or belowaverage transaction volumes. That's where my house is so I can confirm that nearly no one in this neighborhood was selling properties. To use this information to make better business decisions you would want to first answer the question: is property investment in a lowtransaction neighborhood wise or not? As for most things that depends. If such a neighborhood experiences low sales volume due to say high crime rate it is wiser not to invest there. But a low transaction volume could also be a sign that not many residents want to sell because they're happy in the neighborhood. In such cases whenever you have the opportunity to buy in at a reasonable price you probably should do so. (2) You can clearly see the neighborhood in the Island’s belly (the New Dorp neighborhood) changed from redhot to greencool from midyear to yearend. The reason is this: One of the best high schools in Staten Island is in that neighborhood. At the end of spring semesters (from May to July) families move in and out which is typical at the end of an academic year; but such activity is muted at the end of fall semesters (winter time). Say if you want to invest in properties in New Dorp and you are in no hurry it is more advisable to buy in during winter than spring since you may compete with more bidding buyers in the spring but less so in the winter. On the other hand if you are house sellers it may be more profitable to sell properties in spring. Now that I am an Islander so I know local things. But for an outside investors (from Brooklyn Long Island or even further) who may not be familiar with Island affairs and who may not have time/energy to travel here to survey this transaction activity pattern can provide valuable intelligence to help make sound business decisions.  It is a little surprising to find there seems to be a periodic oscillation in transaction volumes for each borough and the oscillations occur with the same frequency. See the following plot:To go beyond visual inspection we can look at the Fourier transform of the data (or equivalently the timeautocorrelation). A preliminary result looks like the following plot which suggests some periodicity exists.But I have to say the conclusion is not robust yet. First I have not carefully conducted the discrete Fourier transform. For example the discrete Fourier transform was done with Fast Fourier Transform (FFT). But how many points in the FFT should I use given 12 data points in time to be certain so that I am not oversampling or undersampling? Second a more rigorous examination of periodicity should take into account more data in time say data merged from 2016 2015 etc.So far we have not discussed about the house prices yet. Now let us examine the prices. Below are density plots of the distributions of the  prices in five boroughs aggregated over 2017.Actually the more indepth thought of this project stems from observing the above distributions. Let me save a longwinded description but directly point out to the following observation. The five distributions seem to come from the same family of a particular kind such that it grows at the beginning like a polynomial (linearly quadratically etc.) but then after certain point decays at least exponentially. In thermodynamics we see this kind of behaviors from the MaxwellBoltzmann distribution for velocities of ideal gases or the Planck’s blackbody radiation law relating radiation power density (per wavelength) to wavelength (essentially 1/momentum of photons) of “gases” of photons. The real power is that even though those models only take one underlying parameter that is temperature (no one hundred features!) they can make predictions about macroscopic properties associated with seemingly complicated random motions very well. We always desire models that work well and are simple enough but not simpler.The following plot illustrates the analogy between distributions of normalized house prices at different locations (different boroughs) and distributions of radiation power densities at different temperatures. For clarity we only plot Manhattan’s price distribution to compare with the radiation power density distributions. It is clear that the role played by location in determining price distributions parallels the role played by temperature in determining radiation power density distributions. This confirms an old saying about the three things that matter most in real estate business are “location location location.” In this preliminary examination we had not fitted the price distribution curves with the functional form taken by radiation power density distributions. But that is not difficult to do should we choose to do it since we now know exactly what functional form we want to fit.We further postulate the following correspondence relations between real estate business and thermodynamics. Since not everyone is familiar with thermodynamics we will save the technical definitions.Please note that correspondence is by no means equality! The correspondence relations just suggest what analogies can be drawn. (In fact I suspect that price is technically analogous to 1/momentum instead of momentum.)If we can quantify temperature and entropy in real estate business we may further quantify quantities such as free energy (maximum useful work a system can do under certain constraints) chemical potentials (“energy” introduced per transaction which may tell how transaction volumes influence price distribution) etc. Those may help understand the mechanism of real estate business better with a theoretical framework from thermodynamics to guide our thinking. One more point: if the price distributions behave closely in the style of blackbody radiation or MaxwellBoltzmann distribution we should be happy since it indicates that prices are determined by a free market. Why? If we go back to the original derivation of Planck's blackbody radiation law or MaxwellBoltzmann distribution be it gases of photons or ideal gases the assumption is that the only interaction between particles is elastic collision and there is no other kind of interaction or external forcing. If there  some other kind of interaction or external forcing the radiation will not be blackbodylike nor will the velocities be distributed MaxwellBoltzmann like. Therefore analogously if the house price “radiation” is Plancklike or MaxwellBoltzmann like it implies the underlying microscopic bidding/buying activities are elastic “collisions.” That means that we'd see no loss of overall momenta\um after “collision” (bidding/buying) and no other kinds of interaction or outside forcing (largescale organized speculations or price controlling say). It provides some metric to monitor the healthiness of house market.Why would Planck blackbody radiation be my first thought? Two years ago when I was a math PhD student and had nothing better to do beside research I took a course in introductory astrophysics. For the course project I was assigned to verify the current estimate of the minimum and maximum radius of the star Alpha Persei (the brightest star in constellation Perseus). Here we study the house price distributions from house sales data and hope to find the mechanism behind pricing. There and then we tried to find the radiation power density distribution (with respect to wavelength) from actual field observation. We know the mechanism (Planck’s radiation law at least approximately) and hope to establish a link between a star’s radius with its observed radiation power density distribution via such mechanism. (The difference is that to study stars we did actual field work instead of “dplyr”ing data sets and the field work was to use a large telescope (see the picture to the left below) with a set of wavelength filters to capture and count photons of different wavelengths from a star.) The end conclusion was not surprising that we showed the current estimate of Alpha Persei’s radius was convincing (see the graph to the right below) but it left me a deep impression that how such a simple model (Planck’s radiation law) could help us unlock ""secret"" messages sent from a star galaxiesfaraway!This project is merely a preliminary examination of NYC real estate patterns. There are a number of possible improvements to apply in the future. They include the following: (1) The Shiny App can be made more sophisticated. (2) More historical data (i.e. not just 2017 Sales Records) can be taken into account in exploring the patterns. (3) The study can be refined to separate houses into finer categories: commercial use residential apartments residential onefamily houses residential twofamily houses etc. The lists for improvement can be endless but the goal is to discover patterns that are simple to understand and make use of.",NA,From Visualizing NYC 2017 House Sales Data To Postulating Its Thermodynamics Theory
https://nycdatascience.com/blog/community/hiring-partner-event-at-nyc-data-science-academy-january-16th-2018/,18,"January 16 2018 marked the hiring partner event for NYC Data Science Academy Cohort 11.Over fifty hiring partners attended the event including partners from Morgan Stanley JPMorgan Chase Samsung Bloomberg Spotify Impact Radius M Science National Grid Goldman Sachs Jefferies AIG SITO Mobile and more.
Out of the hiring partners twelve were alumni seeking new grads to bring on to their team. These alumni include Samuel O’Mullance (National Grid) Chao Shi (National Grid) Yabin Fan (Samsung) David Letzler (Impact Radius) Xinyuan Wu (M Science) Yannick Kimmel (Bloomberg) and Dani Ismail (Unified).NYC Data Science Academy graduates had opportunities within the bootcamp to work on realworld data science projects for companies including local startups to larger corporations. A majority of graduates hold higher education degrees in the various STEM fields.If you want to join our April 2018 hiring partner party or have openings now reach us at .  There are NO FEES to place our candidates! We are looking forward to supporting your data science talent pipeline and matching the best candidates with the right positions.We host the hiring partner/student party every quarter around the middle of Jan April July and Oct.Hope to see you at the next party!",NA,"Hiring Partner Event at NYC Data Science Academy, January 16th 2018"
https://nycdatascience.com/blog/student-works/where-to-find-a-clean-restaurant-in-san-francisco/,18,The Department of Public Health in San Francisco conducts unannounced inspections of restaurants at least once a year. It checks food handling food temperature personal hygiene and vermin control and gives restaurants inspection scores. Unlike New York City where the higher the letter the worse the score in San Francisco the higher inspection score means indicates  more sanitary conditions. My purpose of developing this shiny app is to help the users find clean restaurants in San Francisco.The inspection data used was obtained from Kaggle.com. The dataset contains the name address zip code phone number and inspection score of each inspected restaurant in San Francisco. On the first map I categorized the restaurants by their inspection score and added them to the map. To show the restaurant score at a glance I opted for emojis. My first idea was to plot restaurants using different colors to represent different categories of inspection score. However I changed my mind after noticing that users may have their own preconceptions about what colors means. I saw a map online  that  used blue and red to represent clean and unclean. When I first looked at that map I thought blue markers represented the clean restaurants and red markers represented unclean restaurants. When I realized that the author intended it the other way around I thought that in order to avoid possible confusion I would use graphic icons to represent restaurant rankings rather than colors. The restaurant with an inspection score between 100 and 90 is thumbsup meaning that that is a very clean restaurant. When you zoom in on the map you will see the name address score and phone number in case you plan to make a reservation. I used a smiley face to represent restaurants with score between 89 and 80 which means that this place is clean enough to eat in. For restaurants with score 79 to 70 I used fearful face meaning that you should probably not go there. For restaurants with score below 70 I used vomiting face which is pretty intuitive and selfexplanatory representation of why you should never go to those restaurants.On the second map I grouped the restaurants by their zip code. Knowing what restaurants are available to you is one consideration when moving to a new neighborhood. When you use my app simply select the zip code; it will show you how many restaurants have thumbsup smiley face fearful face and vomiting face. In fact you can get the exact number of restaurants in each category from the info boxes.On the third map I plotted the top five most sanitary foodie streets and the five least sanitary foodie streets in San Francisco. To do this I grouped the restaurants by their street name and took the median of restaurants’ inspection scores for each street. Using this map it is very easy to find the most sanitary foodie streets and the least sanitary foodie streets. If you were to hang out with your friends in San Francisco on a Friday night you wouldn’t want to go to a street that is full of unclean restaurants right?,NA,Where to Find a Clean Restaurant in San Francisco
https://nycdatascience.com/blog/student-works/sneakers-reselling-solutions/,18,There used to be a long line waiting outside the Nike stores to buy sneakers not for themselves but to make money. People resold their sneakers on eBay and publicized their business on Twitter and Facebook. At the prime time of sneaker reselling the market was worth one billion dollars per year. Back in the day I took part in that market as a seller. After a while I noticed that sneaker reselling business was declining. The resell price of sneakers on eBay was even lower than its original retail price meaning you would lose money by reselling it. So I stopped buying sneakers with the intent of selling them.  Based on my personal observation I decided to find out if my experience was a true representation of a change happening in the sneaker resell market. I designed my project to determine whether the sneaker reselling business was actually on a decline. First  I gathered data and analyzed the changes in reselling price. I mainly concentrated on Air Jordan sneakers as they are the most popular sneakers to resell. Second if the sneaker reselling business was declining I want to find out the reasons. Third I wanted to find out if  a sneaker reseller could still make a profit in the current market.To determine whether the sneaker reselling business was declining I looked at the numbers on  a website for buying and reselling sneakers. I scraped all the resale transactions including sneaker name resell date size price from 2011 to the beginning of 2018 and created a graph. Before we look at the graph it is important to understand the term “price premium.” Price premium is a calculation utilized to figure out whether there is a profit or loss when an item is resold. If price premium is positive it means you will make a profit. If it is negative it means you will lose money.The graph shows that the price premium of Air Jordan sneakers was still profitable from 2011 to 2015. However from 2015 the premium started to decrease which confirmed that Air Jordan reselling business was in decline.Competition was another cause of the Air Jordan price premium decline. When Kanye West left Nike to join Adidas he also brought his sneaker brand Yeezy with him. After Adidas released Yeezy in 2015 the premium of Air Jordan began to drop.In the winter of 2015 Adidas released another style of sneakers called NMD which affected the price premium of Air Jordan. Interestingly NMD's price premium also started to drop in 2016. When Adidas initially released them in 2015 NMD sneakers were limited editions. People needed to download an app and sign in to get a pair of NMD. However starting from 2016 Adidas decided to stop limiting the number of NMD sneakers that people could buy. That decision had a major impact on sneaker resellers.Here's the question how can one continue to make money in the current sneaker reselling market? Most importantly don't put all your eggs in one basket. In addition to Air Jordan sneaker resellers should consider stocking sneakers from other brands such as Adidas.It also comes down to style. Let's take Air Jordan for an example. There are a variety of styles of Air Jordan such as Air Jordan one Air Jordan two so on and so forth. On the graph we can see that Air Jordan one sold the most for the past seven years. However it  did not make the most profit. When you look more closely at the data the price premium of Air Jordan one was negative. Actually the more Air Jordan ones the resellers sold the more money they lost. When I multiplied number of sales and their price premium of all the Air Jordan styles it is clear to see that reselling Air Jordan eleven and twelve can make the most profit.,NA,Sneaker Reselling Solutions
https://nycdatascience.com/blog/student-works/global-terrorism/,18,"The world has been conflicted for decades on how to deal with the rising threat of terrorism across the globe. From September 11 2001 to the more recent migrant crisis terrorism has been a major talking point in every American Election cycle and many others throughout the world. Easily accessible information related to these concerns is becoming more necessary not just for Intelligence agencies but for the news reporter and everyday voter. This app provides easily accessible and high quality analytical data to anyone who is interested in understanding how terror affects each region of the world within the last decade.This app focuses on two primary questions:How has terrorist activity changed over the last three decades?Where are terrorist most active?The Data:The Global Terrorism Database (GTD) is an opensource database maintained by researchers at the National Consortium for the Study of Terrorism and Responses to Terrorism (START). The GTD is supported by the US department of Homeland Security and The US Department of State.[1]Sources: Unclassified media articles (Note: Please interpret changes over time with caution. Global patterns are driven by diverse trends in particular regions and data collection is influenced by fluctuations in access to media coverage over both time and place.). See the  for important details on data collection methodology definitions and coding schema.[1]To clarify terms we use this definition of terrorism:""The threatened or actual use of illegal force and violence by a nonstate actor to attain a political economic religious or social goal through fear coercion or intimidation.""[1]Variables:Time Period: 2006 to 2016Geography: Worldwide/Regions /Countries/Provinces/cities/ longitude and latitude coordinatesTerrorist OrganizationDeath tollNationality of PerpetratorsTargets of PerpetratorWeapons usedThe first tab is intended to allow the user to explore every region of the world and get a general sense of terrorist activity in that region across the last decade.Findings/ConclusionOrganizing the data demonstrated one common theme throughout almost every region or country. They all have a large number  of unknown factors. There are some countries were information was nonexistent altogether. This is said to be a result of the record keeping in those countries or cases being unsolved. This app displays all the information that was made available but cannot be taken as a complete data set for that reason.Although this App was designed to just show available data there are some insights that maybe presented in future continuations of this project. It could be reasonable to predict missing information based on patterns that we see throughout the data we do have. For example looking at the second tab we see that southeast India has suffered many attacks from the We also see a lot of incidents in the same region involving unidentified perpetrators. It would be reasonable to predict the likelihood of one of those incidents being related to the  by factoring in frequency of this terrorist groups in this time and year as well as looking at proximity to other incidents made by the same group. We can also look at types of weapons and attacks carried out by certain groups to see if patterns and group habits emerge. Then we can use this to suggest a criminal suspect list.Another interesting future inquiry would be determining predictive factors might lead to more terrorist activity. Correlations to consider include poverty corruption indexes and other national internal indicators. In particular it could be interesting to see how a countries corruption index correlates with the quantity of missing information in the data set.References[1]. Global Terrorism Database http://start.umd.edu/gtd/",NA,Global Terrorism
https://nycdatascience.com/blog/data-science-news-and-sharing/progress-in-world-records-in-select-track-and-field-events-and-detection-of-doping/,18,"For those just wishing to play with the app click .The modern Olympiad began in 1896 in order to promote harmony and friendship between participating nations. Despite crass commercialism and episodes of corruption the Olympic games still bring a period of peace and goodwill every two years in which obscure sports suddenly come to the forefront of our lives.  Whether the Jamaican Bobsledders or Irish Curling Team charms us or we are suddenly taken by a Ukrainian gymnast we all stop and watch the spectacle.  The Olympic motto is ""Citius Altius Fortius"" or  ""Faster Higher Stronger""  and the competitors serve as exemplars of our best ideals.  For better or worse they are the gods who live among us.  Their behavior and actions can influence the values of millions.  Thus it is in our collective interest to ensure that competition is fair.  However performance enhancing drugs serve to imperil this very notion and are a threat to the Games as a whole.  As the heart of the Summer Olympiad Track and Field (or simply ""Athletics"" in Europe) is the sport with the highest profile events. The winner of the 100m race is traditionally known as the world's fastest man (or woman).  The winners in the high jump competition are the world's best leapers.  If you doubt this consider for one second what it would be like to leap in the air and clear the door frame in a typical room by one foot.  Track events have been besieged by scandal at a distressing rate since 1988 when Ben Johnson had his 100m medal stripped from him after testing positive for anabolic steroids.  Soviet and EasternBloc countries had systematic and statesponsored doping programs beginning in the early 1950's but steroids were not formally banned at the Olympics until 1976.  However there was considerable debate about the efficacy of anabolics until one saw them in action in the person of Mr. Johnson.  His devastation of the best 100m field ever assembled served notice that steroids not only worked they could make you superhuman.  In the intervening years the world has seen scandals in the US China Russia and other countries.  Most recently Russia was banned from the Rio Olympics in 2016 for running a (yet another!) statesponsored doping program.  Despite our circumstantial knowledge that many athletes are doping positive drug tests are a rarity. As we will see the most troubling aspect of these scandals is that most highprofile cases hinge on the concept of the ""nonanalytical"" positive.  In the  ledgers were found that detailed doping regimens linked with athlete's names and cancelled checks were found that accompanied shipment details.  So a nonanalytical positive is said to occur when there is sufficient circumstantial evidence to conclude that an athlete is doping even though they have not failed a drug test.   Some may well remember Lance Armstrong's frequent proclamations that he was one of the world's most tested athletes and had  failed a drug test.  Putting aside the failed test for corticosteroids that was expunged on the basis of a backdated prescription he is correct.  Yet his US Postal Team ran one of the most sophisticated and widespread doping operations in athletic history.  This suggests that our current testing protocols are fairly ineffective and leads one to wonder whether they can be fixed.At this point it would be reasonable to ask  if doping is inevitable then why fight it?  We seem to waste lots of energy testing athletes who are one step (or many!) ahead of antidoping controls.  Why not just accept the fact that doping will occur and set some reasonable limits?  First steroids and other drugs are highly correlated with excess morbidity.  Young and frankly stupid athletes will abuse their bodies for fleeting fame and glory.  Others are forced to dope without their consent and/or knowledge and pay the price . If we watch we participate. If we don't the Olympics and their ideals will perish.  In a time when we are increasingly connected digitally but socially isolated this would be tragic.For now we will only consider performances by male athletes due to the availability of a much larger data set over a longer period of time.  Women only began to complete at races longer than 3000m in the Olympics after 1984 and there was probably never a period when 'clean' competition prevailed given the proclivity of both China and the Soviets/EasternBloc regimes to dope their athletes.  Two of the events we analyzed are ""bookends"" in track and field  the 100m and 10000m footraces.  They represent opposite ends of the respiration spectrum.  The 100m race is purely anaerobic and the 10000m race is estimated to be about 95% aerobic.  Sprinters have impressive physiques with large muscles.  Distance runners are lithe and elfen and when highly fit are typically thin to the point of appearing fragile.In addition we consider the 1500m race which is sometimes referred to as the 'metric mile' in the US although it is 109m short of the imperial distance. This race requires a highly developed aerobic engine with a concomitant powerful finishing 'kick' that can see the last 400m covered in 49s.  Experts estimate that the balance of this race is 8084% aerobic and 1620% anaerobic.In general terms the  are the following:Anabolic steroids promote muscle growth and allow an athlete to recover more quickly.  This allows them to simultaneously increase the intensity and frequency of workouts and leads to a higher degree of fitness. Many people think that steroid use is like the cartoon ""Popeye"" where the hero consumes spinach and spontaneously develops very large and strong muscles. It fact steroid supplementation without hard workouts is ineffective and pointless.Corticosteroids reduced inflammation.  Consumption of insulin leads to higher glycogen storage in muscles and reduces tissue damage.  This is extremely dangerous as it can cause blood sugar levels to drop to lethal levels.  Human growth hormone (hGH) allows one to recover more quickly and to build muscle.Blood doping can occur through both autologous and homologous transfusion.  In the former one stores blood only to reinfuse it right before competition.  Before storage the blood is centrifuged to increase the red cell density which allows one to more efficiently carry oxygen to cells.  This is manifested in a higher hematocrit level (HC) which represents the percentage of red cells by volume in the athlete's blood.  Homologous transfusions occur when an athlete is injected with someone else's blood (of the same or compatible blood type).  These are the only transfusions that can be detected through testing.Another form of blood doping utilizes the hormone erythropoietin (EPO) and it's variants.  Here the ingestion of EPO stimulates the body to produce more red blood cells.  Unmonitored usage of the drug can lead to death due to heart attacks as the blood becomes to viscous to push through vessels.  There have been  in groups of cyclists who were using the drug.  They were otherwise healthy and some of the most fit people on our planet.  These cluster deaths started shortly after the drug was introduced in Europe in 1987.  Officials were completely at a loss as to how to deal with this development and had to resort to banning all athletes who had a hemocrit level in excess of 50% until their blood levels were ""normal.""  This approach was not without detractors as it is an arbritary standard and does not seem to be guided by strong science. One has to wonder how many athletes came in at the 49.9 mark during these years.The first test for EPO was not developed until 2000 and it was ineffective.  Refinements were made and the first truly effective test was introduced at the Athens Olympics. The fact that it was a urine rather than blood test made storage and collection much more efficient and effective.The primary weapon in the antidoping agency's arsenal is serological and urine tests for prohibited substances.  As the reader may have gleaned they have been woefully inadequate in detecting doping except for athletes from developing nations and those who have been unfortunate enough to consume tainted supplements.  Competitors from wealthy nations or those with a statesponsored doping apparatus can be tested with impunity but will not yield an analytical positive.  In addition there are many athletes from developing nations who have no out of competition testing from their national associations.Athletes from wealthy nations can resort to using ""designer"" steroids that cannot be detected or they can simply take anabolics or blood dopers under the guidance of physicians or trainers.  There have been a number of infamous collaborations between physicians trainers and team management   is perhaps most notorious. Investigations that produce nonanalytical positives are expensive and time consuming.  They use questionable methods that may impinge upon athlete's human and civil rights.  For example the US antidoping authority (USADA) pursued Lance Armstrong in a manner that was reminiscent of the way in which US Attorneys have pursued mafiosa in the past.  Do we really want to run roughshod over people to catch a few dopers?  More recently the World AntiDoping Authority (WADA) has promulgated the concept of the "".""  This consists of digital records that mark key indicators in blood values over a period of time.  If an athlete's values are deemed to be sufficiently anomalous they are banned and considered to be a nonanalytical positive.  However this approach is not universally agreed upon and is generally considered quite controversial especially with respect to how the 'suspicious' levels are determined.Targeted testing and followup investigation can work and more importantly be fair when the decisions are datadriven.    What if performances could be analyzed to determine suspicious patterns to guide followup testing and analysis?  Would this allow us to increase the efficacy of testing?The data used in this study was obtained from a variety of sources but most principally.  Each event has approximately 50007000 performances in a ranked list.  For the purposes of classification we consider alltime 'AT' and world record 'WR' performances separately.  The world record data is relatively sparse and consists of no more than a few hundred points given the relative rarity of this occurrence.  Each race presents unique opportunities and challenges for and to analysis.  Typically sprinters dope with anabolics and distance runners with blood dopers although there have been notable cases that shed light on the complicated cocktail that many athletes have consumed.  For example Marion Jones was found to be on EPO Insulin hGH and designer anabolics.  As she was a sprinter this caused surprise and consternation among those who follow track and field  was everybody on everything? There is no widespread agreement as to why she would be on such a regimen.  Perhaps the increased O in the blood aides recovery.  Perhaps the athlete demanded to be on everything much the way patients with viral infections hector physicians for antibiotics. Whatever the reason it has quickly become clear that athletes and their enablers are becoming ever more sophisticated in their approach to performance enhancing drugs and evading antidoping controls.First we will consider the 100m and 10000m events.  The 100m has a very useful subset of annulled performances in which athletes were known to be on PEDs while competing.  There are at least 120 performances on this list and they range from world records to 'median'.  This will form the basis for comparisons between known dopers and the larger pool of performances that may or may not include dopers.The 10000m event is the most grueling running event that is held on the track.  It is exactly 100 times longer than the 100m race and it typically takes >160 times longer to complete.  It is a race which depends on very different energy systems than the sprints do and therefore has different training and performance requirements.  People who excel in this distance running are ectomorphs and do not have the need or desire to build muscle like the mesomorphs who dominate the sprints.  Thus anabolics are not considered to be a primary mode of doping.   However blood doping will enhance the O carrying capacity in blood and lead to greater efficiency while running.  If we draw our attention to the plot which displays the 10000m world record over time we  see that there is a sudden and unremitting assault on the world record beginning in the late 1980's.  Curiously enough this coincides with the release of EPO in Europe and the US as a therapy for cancer patients who were suffering from anemia.  Thus the period of time marked by the red rectangle represents a period of little or no doping control of EPO and its variants  a kind of 'wild west' period in distance running the likes of which has not been seen since.  In fact further examination of elite performances i.e. sub27 minutes shows that while twice as many performances have been recorded under this mark in the post than in the EPOera no one has come within 25s of the world record since 2011.  This plot can be accessed under the 10000m analysis tab of the Shiny App. A cursory look at the ""alltime"" density plots for each event shows that the data are significantly skewed and there is room to question whether it is permissible to employ approaches suited to normally distributed quantities.  Traditional approaches such as transforming the data on a log scale were unsatisfactory and might lead to more questions.  The safest approach seems to be to begin with  for the equality of variances.  Applying this to the 100m dash we typically get a very large value if we compare a sample of 120 performances from the alltime list with the 120 that are annulled due to doping offenses. This test is sensitive to heteroskedasticity in the data as can be seen from the plots below. The large value suggests that the data is homoskedastic. Further if we employ a  to test the equality of the  of the distributions it can be seen that the large value is suggestive that we should retain the null hypothesis i.e. that the distributions are the same. Coupled with the fact that both distributions have identical medians we are left with a startling choice  either doping is ineffective or we cannot distinguish between a group of known dopers and a random selection of the same size from the alltime list.  The inescapable and unpalatable conclusion is that most of the world 's top 100m runners are doped.  One can see more detail by clicking ""include annulled performances"" in the shiny app.Analyzing the 10000m races is somewhat more difficult.  It is hard not to notice that the world record took an unprecedented assault over a 68 year period where EPO is readily available and there was limited to no doping control.  If we group the times by ""era"" as in before EPO(<1990) during the period where it exists and testing is lax or nonexistent (19902005) and after testing becomes at least marginally effective ( 2006 ) we can see that the distributions are markedly different as displayed in a density plot. A  test is a kind of nonparametric one way anova that can be used to good effect when the data does not meet normality requirements.  The value obtained is 0 and it tells us that these distributions are all very different.While the record time drops slightly in the period between 20002005 it has remained unapproachable since.  Moreover of the 113 sub27 minute performances recorded in history 70 occurred after 2006.  Yet when we examine density plot shown in the '10000m Analysis' tab it is clear that there are twice as many performances below 26:45 during the EPO era than after it.  So while the median time continues to drop for the group as a whole the top performers are slower.  It is astonishing that the world record has not been approached within 25s (or 1s per lap) since 2011 yet the median time has significantly dropped as one can see from the plots below.If we perform both a Levine and Wilcoxon test on the data shown in the sub27 minute plot values of close to 0 are obtained.  These distributions are very different and while the median times are similar the skew and kurtosis are markedly different.  It does beg the question of what might cause such a change in the distribution of times.  While EPO use does seem to have been curtailed somewhat by the more recent testing protocols evidence suggests that the drug continues to be abused.  In the summer of 2016 Spanish police burst into the hotel rooms of one of the world's most foremost middle distance groups and found EPO.  There have been other and less consequential busts before and after.  It appears that many groups may be utilizing a protocol known as ""microdosing"" in which athletes take repeated subtherapeutic doses that clear the system within hours. While the efficacy of this dosing regimen is  and even the subject of some controversy it appears that many athletes are following it.  Thus it might be expected that athletes would still gain a training advantage from microdosing but not to the degree that they would from the full dose.  If the smaller form of doping was more widespread it could explain why more athletes are running faster today yet currently the fastest athletes are slower then they were twenty years ago.  The only comparable period of stagnation occurred in between 19241937 when running was in its infancy and there were no consistent training practices.Moving on to the 1500m race we see that there is no similar trend in the World Record as that for the 10000m.   The periods before and after EPO testing becomes effective are markedly similar.  There are several possible explanations for this ranging from a lack of widespread doping in the event to microdosing being as effective as a full dose of EPO in this particular race.  It seems more likely that a combination of anabolics and blood doping agents would be the approach that most athletes would take given what has been gleaned from nonanalytical positives.  It may be interesting to examine the combination of 1500 and 5000m times that top athletes in both events post.  Beginning with Said Aouita in the late 1980's a surprisingly large number of athletes have become fast at both events  running in the low 3:30's or below in the 1500m and sub13 minutes in the 5000m race.  The most high profile drug bust in this group is Dieter Baumann who famously blamed his failed test on ""spiked toothpaste.""  In addition Daniel Komen ran otherworldly times of 3:29 and 12:39 and failed a test for caffeine but was later cleared on a technicality.An analysis of alltime performance lists in three events along with worldrecord data shows that there is at least a strong case to be made that doping has been an operative mechanism in track and field and that the dance between antidoping authorities and doping athletes has left an identifiable signature in the progress of median times and world records in these events.  Further analyses could be made of other events and finally women's events as well.",NA,Progress of World Records in Select Track and Field Events and Detection of Doping
https://nycdatascience.com/blog/student-works/the-facebook-effect/,18,Yet this does not mean more cannot be done to better leverage this relationship. Although manipulating the treatment trending on Facebook is not feasible another method would be to find a comparison group that is similar to the treatment group in every way except the treatment. ,NA,The Facebook Effect
https://nycdatascience.com/blog/student-works/new-york-city-weather-and-vehicle-collision-data-analysis/,18,"“” website followed its “robots.txt” rule. The website is well organized and I mThe data set includes: To get the originally downloaded weather data set ready for analysis I first did the following data preparation tasks.To get collision data set ready for analysis I did some similar work on data cleaning and some different tasks as follows.To check correlations I tried/tested w/ several different plotting tools.While ""Clear"" and ""Overcast"" have the two largest number of collisions they are also occurred most of the time. Therefore for a more fair check of each weather condition's impact on collisions I divided total number of collisions by total number of occurring hours i.e. the collisions per hour graph. Herein heuristically set minimum hours threshold  10 (for now).The more steep slopes/variations we see in a frequency graph the higher impact that weather factor may have on collisions. Herein roughly we can see that: snow and humidity has more significant impact than that of temperature and visibility.Note that herein in order to calculate total occurring hours of a continuous variable we have to properly binning them so as to treat them the same way as for categorical variables. Also the result of pressure and wind speed is not shown here as the simple binning between min and max is not working well w/ them and I need better binning approach to handle many outliers of them. As for the result of dew point and wind direction there is no much steep variation slope and hence no much potential impact observed which is also omitted herein.  The frequency graphs w/ the consolidated severity index quantity show a slight promising difference with those w/ the simple total number of collisions graphs where we can see a little more steep variation herein than that w/ the number of collisions. However whether or not or how much this much difference can help on modeling/prediction performance is a problem that definitely needs a careful check/comparison in actual modeling practice.   Overall it is unfortunate that: from the graphs we cannot see any significant correlation coefficients between different weather factors and collisions. Herein the actual question is: how we should interpret this result properly. Esp e.g. for snow factor for which we observed significant impact in frequency graphs. To collect more insights either on collisions data itself or on its correlation with the weather factors I also checked the correlation plots involving top collision causes and/or top involved vehicles. ticeable significant correlation spots are:Thank you! 🙂 ",NA,New York City Weather and Vehicle Collision Data Analysis
https://nycdatascience.com/blog/student-works/nyc-real-estate-analytics-manhattan-2017/,19,When it comes to real estate business choosing the is the hardest step!'Note: The application still needs some enhancements and updates. Why there are two different months for the best investment?! and which month is better?,NA,NYC Real Estate Analytics - Manhattan 2017
https://nycdatascience.com/blog/student-works/nycs-seven-major-felonies-what-where-and-when/,19, My MotivationEven though people in New York City are streetsmart crime is always a possibility and certain areas are more dangerous than others. But where are these unsafe areas? What crimes are taking place and when are they happening? Using NYPD’s Historic Complaint dataset I decided to look into these questions. The application can be found on my . The code used to create and run the application can be found on .The QuestionsAs I was envisioning the app I thought of 6 questions that I wanted my app to answer:The DatasetI used the  which includes information on all felonies misdemeanors and violations that have been reported to the NYPD from the start of 2006 to the end of 2016. The data set included information about the crime such as the date time a description of the crime committed and the location of the incident (including latitude and longitude). I focused on seven major felonies that occur in NYC: Murder Rape Felony Assault Grand Larceny Grand Larceny of Motor Vehicle Robbery and Burglary. I also knew when looking at the crime rates for each borough I would have to control for population. In order to do this I found the  for 2010 and its estimate for 2016. In order to estimate the population for the missing years I used the logistic growth formula. When I first downloaded the dataset there were around 5.6 million observations. I initially filtered the rows to include just the seven offenses I was focusing on. After inspecting the data I noticed that a number of the offenses had missing labels or were labeled inaccurately (e.g.: the row’s three digit code did not match the listed offense description). I went through the data and made sure what I was collecting was accurate and inclusive. After making the appropriate edits to include and correct for these entries I created helper columns to provide additional filtering for the mapping and statistics of these crimes. I also consolidated information together from certain columns to add information to the maps and dropped the unnecessary variables that were no longer needed. Once my data was cleaned having just over 1.2 million observations I added in my estimated population to the cleaned file. Once I had my data I began coding the shiny app.Visualizations and Statistics:The icons are color coordinated based on crime type making it easier to see what types of crimes occur near them when they do not have a specific crime filter set. If you click on a crime pinpoint information appears stating the type of crime that was committed where the incident occurred and when it happened.In order to help a person find crime in a specific area I add an address lookup feature. This allows users to quickly focus their search to specific areas rather than have to zoom in and constantly having to orient themselves to find their area of interest. In order to provide a more highlevel overview on where crimes occur in the borough I created a heat map. Although you are not able to see the specific details about each individual crime it lets the user know generally where crimes occur and how the location changes as time passes.  The map can be filtered with the same specifications as the cluster map.Crime Trends Based on Crime Type and Borough When I started the assignment I wanted to accomplish two tasks in terms of statistics. I wanted to be able compare trends of specific crimes within each borough as well as see if a specific crime differed between borough. In order to accomplish this I created two statistics tabs focusing on each question specifically. Both tabs provide information on the overall trend of crimes per 10000 residents as well as the frequency that specific crimes occur within various months day of the weeks or time of day.In this tab you can pick a borough to focus on and  see how the crime rates compare within each borough. The graphs measure the total amount of crimes that occurred within that time factor (year month day of the week or time) and displays how the percentages changes. So for the example above we can see that the number of  felony assaults in Staten Island have been increasing per 10000 residents while the number of grand larcenies have been staying the same despite a dip for the years 2009  2011. Grand larceny also appears to increase as the year goes on while assaults are more frequent in the spring and early summer. We can also see that assaults are  more likely to occur on the weekends while grand larceny is more frequent during the week days. The final notable takeaway from this example is that felony assaults are more frequent in the early morning and late evening while grand larceny generally occurs in the late morning to early evening.In this tab we can compare specific crimes and whether or not there are significant differences between the borough.  So for this example we can see the number of robberies have been going down for almost all the boroughs with the exception being Staten Island. However we can also see that the likelihood that robberies take place controlling for months days of the week and time of day don't appear to differ very much between boroughs. Viewing the DataOn the final tab you can view the underlying data filtering based on the types of crimes the day of the week the time of day as well as for specific boroughs. Similar to cluster map it provides you with the date the crime was committed the time of day the type of crime and where it occurs. You can filter the data based on these categories as well.Further ExplorationAs I continue to add onto this application I would like to include information about the income distribution for each borough. I would also like to look into whether educational spending has an impact on crimes rates within each borough. I also would like to add weather as a factor that I could potentially control for seeing its impact on crime rates. This app is only for trends and analysis but I add more data to the application I would like to run regressions measuring the effects of income inequality education spending and gentrification on crime rates for each borough.,NA,"NYC's Seven Major Felonies - What, Where, and When"
https://nycdatascience.com/blog/student-works/citi-bike-riders-in-different-ages-the-potential-of-target-advertising/,19,As more and more people enjoy healthy and efficient lifestyles riding bikes to work is recognized as a better commuting choice than driving or walking. The Citi Bike program launched in 2013 with 332 stations and 6000 bikes. It now owns 706 stations and 12000 bikes making it be the largest sharing bike program in the U.S. In 2016 Citi Bike riders took an average 38491 rides per day a number that nearly doubled in 2017. While Citi Bike has a large number of customers and subscribers  it can win over more by advertising to car drivers bus riders or pedestrians. With a view of the potential market for CitiBike services I would like to explore the data and try to seek a way to better know the crowd.As we known different age group would have different needs and desires so I am going to use age as the main factor to analyze when and where to place advertisements to different people. To that end  I built an app based on R Shiny and use ggplot2 plotly and leaflet to visualize my finding. This is the link to and my Shiny app is also available.I downloaded the 2016 January to December Trip Data in Jersey City from.The original data structure was as follows:I used dyplr tidyr and data table to clear up and manipulate the data set. Based on the data I divided people into six age groups: 1525 2535 3545 4555 5565 and 6575.In this analysis we try to figure out: let's start to analyze the relationship between each age group changes and different times of day.In the bar chart we can see 25  35 and 35 45 take up a large portion of the bikes' use. However these two groups have different behaviours.  Leaving at 20:00 23:00 and returning between 1:00  4:00 applies to both the younger age groups:  1525  and 2535 yearolds.For 35  45 yearold the riding time is relatively stable likely because they have less nightlife. They would ride bikes to work or exercise more in the daytime. we are going to analyze how different age groups behave differently in each day.From the 2D Histogram we can see that 3 different age groups ranging from  25 35 35 45 and 45 55 make up the major users. Compared to other groups that 25 35 yearolds would be more likely to go out on Saturday. Accordingly it makes sense for ads to target young people on Saturday. Also we could purposely market on the days when demand is down among a certain group to try to increase it for example stores and eateries put out promotions on slower days of the week rather than on days when they are already busy. I use a scatter plot to illustrate how each age group tends to ride Citi Bike in each month.In the scatter plot we can see that15 25 5565 and 6575 remain stable throughout the year while 25 35 and 35 45 have a huge deviation from month to month. The curve for 25  35 and 35  45 goes up from February to October and goes down from October to February. This variance might be caused by the weather. When the temperature gets cold fewer people will ride a bike. So for these two group July to November is an optimal period for targeted ads. I create an interactive map to show which location is popular for each age group. In this map the area of the yellow circle represents the number of people starting their journey. On the right side of the screen R shiny app users can choose age group user type gender and date to filter out the data.In the map I summarize the following chart to highlight the most popular bike station for each age group.From the chart we can see that though Grove St. Path shows up as a favourite for most of the age groups there is quite a bit of deviation for the second rank among them. That may indicate good choices of locations to target for particular ages.I will combine both the time and location data together to further see how they react to each other and check whether there is some huge deviation from the results we have so far. I will also provide more detailed analysis for specific date and location and expand the scope to every city in the U.S. that runs sharing bike program.https://en.wikipedia.org/wiki/Citi_Bikehttps://www.citibikenyc.com/systemdatahttps://archpaper.com/2014/10/bikerstancitibikesystemopensinmiaminextmonth/,NA,Citi Bike Riders in Different Ages - the Potential of Target Advertising
https://nycdatascience.com/blog/student-works/crime-and-demographics-in-new-york-city/,19,Note: Staten Island data were incomplete from the dataset between 2000 and 2012 and an executive decision was made to disqualify these years,NA,Crime and Demographics in New York City
https://nycdatascience.com/blog/student-works/visualizing-clinical-trial-operations-in-breast-cancer-and-prostate-cancer/,19,I got the data from clinicaltrials.gov. The dataset contains:The map below shows the concentrate of  total clinical trials around the world. From the legend we can see that the darker the color on the map the more trials are recruiting or already completed. The map shows that the United States Europe and China have the most clinical trials in breast and prostate cancer compared to other countries.These two maps below show the clinical trials in both breast and prostate cancer that are recruiting now around the world. When you room in it you can see the trials at specific cities and locations.  I created the histograms that show the number of recruitments(active but not ready to recruiting recruiting completed and etc) phases study types and top 10 sponsor collaborators. We can see the different counts between the two cancers clearly by looking at the histograms. From the two histograms below we can see that the three biggest cancer centers sponsor the most trials both in breast and prostate cancer. The  Phase I assesses the safety of a drug or device.Phase II tests the efficacy of a drug or device.Phase III involves randomized and blind testing in hundreds to thousands patients.Phase IV is often called Post Marketing Surveillance Trials that are conducted after a drug or device has been approved for consumer sale. You can see that there is a large number of NA in the two histograms above that is because some companies or research centers didn't report the integrated report to the clinicaltrials.gov. The boxplot above shows how fast the different sponsor collaborators recruit people for the two cancers. ROR means the rate of recruitment. ROR  total amount of volunteers/ total number of months. From the boxplot above we can see that the rate of recruitment of majority The boxplot above shows how fast the people get recruited in different phases. Breast cancer is the fastest in Phase2&3 and prostate cancer is the fastest in Phase 4. Clinical trialshas become the hottest topic in the healthcare industry. It uses the big data to analyze and recruit people in different types of ,NA,Visualizing Clinical Trial Operations in Breast Cancer and Prostate Cancer
https://nycdatascience.com/blog/student-works/food-desert/,20,This is my shiny project examining the development of food deserts in American neighborhoods. Minorities today disproportionately suffer from poor health outcomes. African Americans are twice as likely to have diabetes compared to whites and nearly 40 percent of Latinos are overweight or obese .Food deserts are commonly characterized has places that have limited access to affordable and nutritious foods and have a surplus of restaurants fast food chains bars and convenience stores (instead of grocery stores). And it make sensestands that such access to food sources plays a role may contribute to thesein persistent health differences across groups.Therefore for in this project I aim to:On top of that the NAICS has over 10000 categories. Consider what that amounts to with over 40000 zip codes in American and nearly 15 years of dates used for this study. The data required A LOT of cleaning.Caption: From left to right and top to bottom choropleth map of fast foods unfresh grocery stores fresh food and alcohol in the San Francisco Bay areaCaption: Map comparing number of fast food restaurants in San Francisco (L) and New York (R).Caption: Changes in the number of fast food restaurant in New York (Left to right; top to bottom  2000 2003 2006 2009 2012 2014).,NA,American Food Deserts: Analyzing the development of
https://nycdatascience.com/blog/student-works/r-visualization/department-of-consumer-affairs-charges/,20,"""NYC Admin Code § 20822(a)  SALE OF EXPIRED MEDS:  BUSINESS OFFERED FOR SALE OVERTHE COUNTER MEDICATION LATER THAN EXPIRATION DATE ON THE LABEL.""                                 ""NYC Admin Code § 20708  STORE DID NOT CONSPICUOUSLY DISPLAY THE TOTAL SELLING PRICE AT POINT OF DISPLAY FOR ITEM.""                                                              ""6 RCNY § 570(a)  NO PRICE LIST FOR SERVICES DISPLAYED""                                            ""6 RCNY § 570(a)  PRICE LIST NOT DISPLAYED CONSPICUOUSLY""                                           ",NA,Department of Consumer Affairs Charges
https://nycdatascience.com/blog/student-works/predicting-clicks-in-mobile-advertising-an-experiment/,21,"Advertising is a multibillion dollar industry that acts as a bridge between companies and their customers. While most people are conscious of the ads around them they likely underestimate the power of those ads and the influence of advertising in general. Research suggests that simply making someone aware of products events and brands increases the odds of that person actually buying those products attending those events or supporting those brands.Mobile advertising is a form of advertising that takes place on mobile devices such as smartphones and tablets. Mobile ads are served via Real Time Bidding (RTB) an auction process that happens in mere milliseconds. A mobile device user together with the available ad space on his or her device comprise a . Advertising companies bid to serve ads to these bid requests and the winning bid results in that company's client's ad appearing on the mobile device. Such an ad referred to as an  also provides the user with the option of obtaining more information about the ad by directing them to a website when the ad is touched; this is called a . Mobile advertising companies use the ratio of clicks to impressions known as click through rate (CTR) to gauge the success of their clients' ads. The process is well described by the following graphic from insight.venturebeat.com:To illustrate these points consider the following hypothetical scenarios.Yellow  0.08 Orange  0.15 Red  0.2. The above assumption led us to our second one which is that Thus we endeavored to determine if campaignspecific models those that were trained only on data relating to a specific campaign were more accurate in predicting click probabilities than a general model that was trained using all available data.The general methodology we employed to investigate this question was to conduct an experiment in which we compared the performance of two campaignspecific models with the performance of a general model on campaignspecific data. This allowed us to make a direct campaigntogeneral model comparison. The figure below describes our workflow; each section of the diagram will be discussed in more detail below.Ads Anonymous provided us with three data sets: Unfortunately Ads Anonymous was unable to obtain data for the bid requests on which they bid but did not win. For this reason we chose to exclude the bid requests data set and used only the impressions and clicks sets to train our models. Because the clicks data set was a subset of the impressions data set we were able to merge the two together to determine which impressions did or did not lead to a click effectively leaving us with a single data set. Finally at Ads Anonymous' suggestion we filtered the data for only those impressions that occurred in the U.S. and those that belonged to an ad campaign with a nonzero CTR. This reduced ""general"" data set of 73 million impressions with a CTR of 0.63% is what we used to train our general model. We created our two campaignspecific data sets from this general data set but only after cleaning and preparing the general set for modeling.A caveat to our approach is that because we were unable to analyze lost bid request information our models were inherently biased towards bid requests that Ads Anonymous bid on and won. Therefore we made a (nontrivial) assumption that the impressions on which we trained our model were representative of all possible impressions.Our now single general data set included several types of information for each impression including variables relating to the user the user's device the ad the app on which the ad was served and the bid request from which the impression came. Detailed below are examples of how we treated different variables.Useless InformationOur data set included some variables that possessed little to no variance and thus provided limited predictive power. For example all our data came from impressions that occurred in September 2016 thus the variables Month and Year were uniform for all impressions. .Incorrect TypeThe variables Ad and Campaign were labeled with unique numeric identifiers but these numbers do not have an inherent ordinality. That is Campaign 234 does not have an ordered relationship with Campaign 235. Thus the Ad and Campaign variables are actually categorical rather than numeric with each unique identifier representing a different class or level. Machine learning algorithms however would treat these numbers as having an ordinal relationship. Buried InformationWhile the variables themselves contained information useful for prediction often additional information exists ""hidden"" in the values after manipulating them in some way. To access this hidden informationAn example of a feature we extracted was derived from the variable Day which included values ranging from 1 to 22 corresponding to the days in September from which the data were collected. We hypothesized that people may have different click behavior depending on different days of the week. Thus we created the new variable Weekday which described the day of the week (e.g. Monday Tuesday etc.) the ad was served.Other examples of extractions are:An example of a feature interaction we made was the combination of Weekday and Hour. Similar to our rationale regarding the creation of the Weekday variable we hypothesized that click behavior might be different both on different days and at different times of different days. People may be less active on their mobile devices during weekdays than on weekends but this may differ depending on the time of day. That is Friday evenings might be more similar to Saturday evening click behavior than during that same time on Sunday.Other examples of interactions are:Messy LevelsA large majority of the variables in the dataset were categorical and several of them contained a large number of levels. Many of these levels however contained the same core information and thus would be more appropriately treated as the same level. An example of a variable with messy levels that we reduced was BestVenueName which describes the app or site on which the ad was served. Seen below on the left are values for BestVenueName for three different impressions. Each observation contains the core information ""Meetme"" but if left untreated a machine learning algorithm would have treated them as separate levels. We cleaned these impressions such that each value was replaced with only the value on the right ""meetme.""Infrequent LevelsSome categorical variables still contained a large number of levels even after cleaning the messy ones. In several cases a majority of the impressions only included a small set of these levels whereas other levels were seen only a few times. An example of such releveling was the variable Carrier which describes the platform from which the device is receiving Internet service. This variable contained more than 350 levels most of which corresponding to only a small number of impressions (see below top). We releveled Carrier by grouping all levels that constituted less than 0.5% of all impressions into an ""other"" group (see below bottom).Minor MissingnessThe Location variable comprised of the latitude and longitude of the user at the time the bid request occurred was roughly 4% missing. This geospatial data is import in understanding the demography of the user and thus is useful in targeting the appropriate audience for a particular ad. Machine learning algorithms do not handle missing data well so Major MissingnessWhen preparing datasets for modeling variables with high amounts of missingness are often either dropped losing any information in the variable or the missing values are imputed in a systematic way artificially populating the variable. While both strategies have advantages and disadvantages the one employed is often based on the perceived importance of the variable in question.For our data almost half of the impressions were missing the variable Gender. suggests that gender is an important factor in a determining a person's response to advertising. In a metaanalysis of more than 30 years of research a scientist found that women will purchase products marketed towards both genders whereas men will only purchase products marketed towards men. Based on this information we considered gender to be an important factor in predicting clicks therefore we opted to impute the missing values with machine learning.To execute our imputation we used a random forest to predict the missing values. We only included variables that were descriptive of the user and their device and excluded information pertaining to the ad or click. After a coarse crossvalidation process our best random forest model included 40 trees a max depth of 20 and included four variables at each split. The model resulted in an 85% accuracy in predicting gender. Once our data had been cleaned and all feature engineering was complete we created our two campaignspecific data sets. For the general model we ended up using ~73 million impressions (with clicked/unclicked labels) and from this we sectioned off all impressions related to company Hair Care and all impressions related to company Sports Bar. Hair Care included 3.2 million impressions and a 1.45% CTR and the other campaign Sports Bar was comprised of 1.9 million impressions and a 0.55% CTR. These campaigns were chosen for their manageable size their duration and for clickthrough rates that straddled the general data set's CTR (i.e. 0.63%).For each of the campaignspecific datasets we had to further split it into training validation and test sets. To approximate a production setting in which we would be using past data to predict on future data we opted to do our train/validation/test splitting by time. Our data spanned from 9/1 to 9/22 in 2016 and we used impressions from 9/1  9/17 as our training set and impressions from 9/17  9/22 as our test set; this meant that our training set constituted the beginning 80% of the campaignspecific data and the test set constituted the last 20%. To obtain a validation set we further split the above training set. For this we used a  such that our new smaller training set constituted roughly 60% of the overall campaignspecific data and the validation set constituted roughly 20%. In hindsight it would have been more consistent to also use time for this split. However our approach was motivated by wishing to replace the crossvalidation step on the 80% training set with a single validation step on the 20% validation set and we believed that using a random subset of that 80% set was still justified.Predicting clickthrough rate is a binary classification problem which we decided to approach using logistic regression with stochastic gradient descent. The advantages of using logistic regression on this particular data set are many:Information surrounding Logistic Regression and its tolerance to imbalanced classes is both limited and mixed so we decided to run a subexperiment. One common way of addressing class imbalance is through undersampling the majority class. That is by throwing out some percentage of the majority class observations in order to diminish the degree of class imbalance in the model training set. The risk of undersampling too much is that a model can be left with too little data to train on which can lead to underfitting. To see how undersampling affected our models we trained models with different degrees of undersampling. We trained each model three times: once on the entire training set (no undersampling) once on a training set where we undersampled the majority class to 10% and once on a training set where we undersampled the majority class to 1% (reaching near parity between clicked and not clicked). The results of these different undersamplings on our models are given in below in Prediction and Evaluation.  The other side of an underfitting problem is an overfitting problem which happens if model complexity is too high and the model fits to noise in the underlying data. To prevent this Spark's logistic regression has builtin regularization in the form of ElasticNet; it has an alpha parameter that controls the mixture of Ridge and Lasso (L2 vs L1 penalty to the cost function) and a lambda parameter that controls the size of the penalty term.Since we were working with big data we had to be deliberate in any choice that could result in long running times. As such we chose to reduce the number of parameters we tuned from two to one. We used pure Lasso regularization (alpha  1) and only tuned the lambda penalty parameter. For our grid search on lambda we also had to make some practical sacrifices. Rather than run a kfold cross validation we used one validation set and we kept our grid search fairly coarse. We evaluated the fit of our models using loglikelihood. As a guide for an appropriate starting lambda we used MLLib's objective history for the unregularized logistic regression model because it captures the training log likelihoods. With an idea for a starting lambda value we then searched 5 values at different orders of magnitude around that.0.5",NA,Predicting clicks in mobile advertising: An experiment
https://nycdatascience.com/blog/student-works/an-analysis-of-311-complaint-data/,21," :Beginning in 2010  NYC launched an initiative to expose government data via  in an effort to ""improve the accessibility transparency and accountability of City government this catalog offers access to a repository of governmentproduced machinereadable data sets. ""For my Data Visualization project I took inspiration from  on storytelling through data and sought to create an interactive dashboard to unearth any notable insights on 311 complaints for the year of 2015.The dashboard has also been published to shinyapps.io where you can interact with the data yourself The initial dataset of all 2015 311 complaint data contained over 2 million rows coming from the various government departmentsand within those rows were over 250 different types of complaints recorded. I sought for a way to consolidate these complaint types to make the analysis more interpretable and quickly found that each department had its own labels for complaints. For example for the ""tree"" category I grouped the complaints through regular expressions on any type that contained the words ""tree"" or ""branch"" which returned the below list:After grouping the bulk of the remaining categories this way I then ordered them by frequency to arrive at the below top ten categories:After this consolidation I then sought to uncover more insight into seasonality and timing of incident reporting. With that I also generated additional features from the timestamps to be able to filter by month week of the year day of the week day of the year and hour. Lastly I wanted to identify whether any particular areas in New York had a disproportionate amount of incident calls within a given timeframe so I retained the latitude/longitude data to be able to generate a geographical visualization.The app itself contains two interactive visualizations a chart and a heatmap. I opted to use the R implementation of Plot.ly for the line chart given its interactivity and clear interface. For the geographic data I used Leaflet with additional addons for custom tiles and layers.To show some highlevel details of the dataset one can view apply chart filters to view the entire dataset's incident volume by month:As you can see in the lefthand navigation pane one can filter by borough complaint type and time scale. Additionally some basic measures of mean median maximum minimum and total are shown dynamically beneath the graph.A somewhat straightforward interpretation of the above graph's 1266555 incidents shows that complaints tend to rise in the winter months which may help a department forecast their staffing needs.Moreover the timescale and complaint types can be filtered on a more granular level to uncover further trends:From this chart filtering by weekday one can see that complaint volume is considerably lower on the weekends for environmental incidents such as vermin sightings fallen trees or sidewalk repairs. One could then investigate further as to whether these reports are more common in commercial or residential areas and understand if any of these factors could be affecting business.For the second visualization I developed a dynamic heatmap that can highlight areas of incident concentration from borough to borough. For a uniform user experience the filtering options are again present on the left hand side. Along the top are displays of the total number of incidents as well as the count of incidents which are associated with the top 50 addresses given the specified filter criteria. This proportion was added to highlight whether a few locations were repeat offenders of a certain complaint compared to the average.In addition to the heatmap one can also zoom in and toggle the ""clusters"" and ""top_50"" displays to view a more granular distribution of the incidents upon clicking on the house icon additional data will appear in a popup:Here I've zoomed in on a particular address in Northern Manhattan that appeared in the top 50 given my filtering criteria. One can additionally doubleclick on the cluster itself to see a full distribution of the complaints at a given address from the below image one can see that while the top complaint at this address was ""heating"" a fair number of ""unsanitary"" and ""noise"" complaints were also filed when hovering over a pin:On average one can observe that certain neighborhoods in Manhattan such as Inwood and the Lower East Side have a particularly dense concentration of complaints given most measures. With that this visualization tool could be used to further investigate a given address when considering a relocation or event in an area and determine whether it has historically received a large number of complaints.I found developing this Shiny App to be a deeply rewarding experience since I was not only working with a realworld messy dataset but could also produce tangible results in a dashboard that can be easily extended to other types of analyses.  I would love to further extend this app by bringing in external data sources such as pricing or crime data to unearth any correlations. Furthermore I would want to add functionality that integrates directly with the NYC Open Data API to retrieve up to date data that would allow this dashboard to provide more timely insights. Thanks for browsing my work and don't hesitate to reach out if you have any additional questions or feedback on my approach and techniques used within this project Feel free to access my code repository on github ",NA,An Analysis of 311 Complaint Data
https://nycdatascience.com/blog/student-works/mental-illness-classifier-platform-wecare-mind-com/,21,Depression is a condition that reportedly affects one in ten Americans at one point or another and the incidence of depression is higher in some states than others. According to the research conducted by  44% of college students report having symptoms of depression and 75% of them do not seek help for their mental health problems. Suicide is the third leading cause of death among children and young adults aged 10 to 24. As for people over 30 about 50% of all adults experiencing symptoms of depression will not talk to a doctor or seek help for depression. These statistical findings merely cover depression not to mention including other forms of mental illnesses. Some depressed people avoid seeking help from external resources because they fear social judgment. Others may be more concerned by the high cost of treatment  even with insurance. As Ayush Chetan Suprith Shi and I share the same values and goals we teamed up at the HackUMass event creating the WeCaremind.com to solve the problems stated above. is a mental illness classifier platform for individuals and families affected by mental illness to give them a better understanding of their mental health condition. WeCaremind.com uses machine learning to analyze text entered by users and predicts the kind of disorder that could be affecting them. There is still a lot of stigmas associated with mental illness and that prevents people from reaching out to others for this reason we created this website to enable people to be heard and to find possible solutions easily.Ayush and I chose the mental health forums because people diagnosed by trained practitioners mostly post there about their day to day life. That gave us a good basis for assuming labeled data. For example if a user posts an article in PTSD group we put it in the PTSD category. We also scraped through Reddit original posts. We thought they would be a good fit because of the popularity of the post and tags of mental diseases. In the end we gathered 520 reviews across mental diseases like depression PTSD (Post Traumatic Stress Disorder) ADHD (AttentionDeficit/Hyperactivity Disorder) and PPD (Paranoid Personality Disorder) and labeled them accordingly. Codes are available on .After gaining our labeled reviews Ayush and I preprocessed the data by removing stop words tokenizing and stemming. Subsequently we trained the multinomial Naive Bayes classification model with fivefold grid search crossvalidation to tune the best hyperparameters. The accuracy score of the test set is about 70 percent.Suprith Shi and Chetan collaboratively built a web app that is supported by Django.The Frontend of our project was created using Node JS and the back end was created using Django. The Django project was put into production on a Digital Ocean droplet with minimum processing and space requirements. Gunicorn was used to spawn threads for the Django server to serve realtime user requests and Nginx framework was used to create an interface between Network and the Django threads. An object of the trained classifier was created and stored using pickle. For serving realtime diagnosis requests each time the classifier object is unpickled and used to classify the user request string. Based on the classification a custom page is created and returned which consists of disorderspecific coping mechanisms i.e. a diagnosis of ADHD would return a list of curated resources for ADHD and likewise for every disorder that our model predicts. In the future we hope to add information about healthcare practitioners within a 20mile radius who specialize in the predicted disorder.Ayush Sharma: CICS UMass AMherst (M.S. in Computer Science)Chetan Manjesh: CICS UMass AMherst (M.S. in Computer Science)Suprith Aireddy: Illinois State University (B.S. in Computer Science)Shi Zhang: New York University (B.S. in Computer Science)YuHan Chen: New York University (M.S. in Management and Systems),NA,Mental Illness Classifier Platform: WeCare-mind.com
https://nycdatascience.com/blog/student-works/team-machine-learning-project/,21,"Kaggle is a sort of data science candyland in which one can pick and choose from almost any data set imaginable. Kaggle is also a website that allows for Data scientists to compete against each other to see which models are most effective and predicting a certain feature. Though the competition was closed our team’s goal was to predict the housing prices of Ames Iowa by submitting our models to the public leaderboard.  This was an exciting opportunity because such information can be quite useful. Real Estate agents bankers home buyers and home sellers could all make use of knowing the most powerful predictive features identified by the most successful models.The data was composed of 80 variables. Seventynine of those features were predictor features. These features ranged from categorical variables such as the neighborhood that the house was sold in to primarily quantitative variables such as the square footage of the first floor.  The target feature was the quantitative variables sale price in U.S. dollars.  The data was presplit into a training set and a test with each set being given in the form of a csv file. This totaled to 1460 rows on the training set and 1459 rows for the testing set ( minus the sale prices).It’s always great to start with a correlation plot. You can see here that the features most strongly related sale price are not surprisingly the overall quality and GrLivArea (which is). The features most related to each other are the year it was built / year the garage was built and the first floors square feet and the total basement square feetThe graph below represents the features that seem to have strong linear relationships of the data in greater detail. The scatterplot visualization allows you to see the linearity of the data and the degree of variance each feature has. Linear features follow the line of best fit such is in the case of case of overall quality predicting sale price. Although linear this corrplot appears to vary more than total square footage and predicting basement square footage since the data points don’t appear to cluster as much around the mean. Perhaps total square footage and total basement square footage can be collapsed into one variable. Overall this is a very nice visualization that gives a more detailed explanation of the relationship between the features.  Scatter PlotThe two graphs below represent missing data. The first graph represents the data that has the most missing values. The second graph is merely an extension of the first graph. Don’t be deceived by the size of the bars in the graphs. The highest bar representing pool QC represents over 2500 missing values. The highest bar in the second graph MSC zoning represents only 4 missing values.  Some of these missing values are actually not missing at all. They represent they lack some value such as the case of fences. It's not that there are no records of missing fences. It is the fact that there are not any fences surrounding certain homes. This fact will play a major role in how the data should be cleaned in the project. Another interesting thing to note regarding missing data is the case of missing garages.Features with a lot of missing values. Features with only a few missing values.The first task was to identify which features where quantitative. Categorical features were imputed in python by changing NA values into the most common value. Ex: data.loc[all_data.Alley.isnull() 'Alley']  'NoAlley'.'BsmtUnfSF']  house_all.BsmtUnfSF.median()'BsmtFinSF1']  house_all. BsmtFinSF1.median()Only one is null and it has type Detchd?Only one is null guess it is the most common option WD?Many columns have missing values and not all can be treated the same way. Some are missing because they are just not applicable to the house in question (e.g. columns giving info about fireplaces/pools/garages for houses which do not have those things). The rest of the numerical values were imputed with the mean or median.  Imputing allows for feature transformations. Feature transformation is critical for meeting the assumptions of several models including ridge and lasso regressions. One assumption of these models is that features are normally (bell curve) distributed with data points being clustered around the mean.  The Y variable must first be transformed.It was found that the original sale price was skewed to the right but that the log transformation (log + 1 of sales price) did an excellent job of centering the data around the mean. Lastly some of the categorical variables needed to be dummified to incorporate them into the regression model. The process of dummification involved converted categories into 1 or 0 based on whether a condition was true or not. If the house had a garage then it would be assigned a 1 value. If it did not have that value then it would be assigned a zero. This allowed of the data to be incorporated into regression models since all the data was now numeric.SKEWNESS_CUTOFF  0.75'SaleCondition']  'Normal'The data of course must be split back from the train to the test set. Once the data had been thoroughly cleaned we could attempt feature engineering to see if there was a way to improve a model’s prediction in any way. “YrSold” and “MoSold” although numbers were categorical variables and were changed accordingly. As mentioned in the data visualization section it appeared that a lot of the square footage variables were related to each other. This would make common sense because you would expect the floors of the house to have a similar amount of square feet. It would be unlikely to have a significantly larger second floor relative to the first floor.  Living Area Square Footage (Total_Liv_AreaSF) was the combined features of 1stFlrSF 2ndFlrSF LowQualFinSF and GrLivArea.  Our hypothesis is that reducing these similar features into one would reduce the complexity of the model(s) and thereby improve overall predictions.We officially chose one model 4 models to test although one of team members dabbled with XGboost. Each of the models was broken into 5 folds with the 5th fold reserved for testing and the rest for training. Our initial hypothesis was that the treebased models (gradient boosting and random forest) would be most successful at predicting sales prices but we chose to use the ridge and lasso regression since because they would be easiest to interpret and because we are still learning.  The ridge and lasso regression are generally better than the simple linear model because they introduce a regularization parameter. This allows for the model to avoid overfitting due to an increased complexity (number of features in this case). Overfitting will result in higher variance. Our team must reduce that variance by introducing such parameters since many features can certainly an issue. The main reason behind our group using Ridge Regression was because of the model’s ability to alleviate multiple collinearity or the independence of each variable. Too much multicollinearity can result in an overwhelming amount of variance and destructive overfitting.  Our best prediction from the Ridge regression was having the root mean squared error of .12623 and the alpha (regularization parameter) being of .10. We found that for the most part that the residuals were centered around the mean. This meets the assumption of having relatively independent errors. Lasso model is useful because of its tendency to prefer solutions with fewer parameters. This is because the Lasso eliminate features that don’t have any value regarding the prediction. Certain variables such as RoofMatl_Metal and Street_Pave were totally eliminated from this particular model.Treebased models are advantageous because they do not assume linearity. You don’t need to meet as many assumptions as for the linear models when attempting to predict a variable. Decision trees tend to rapidly overfit so we chose to use the more complex models in Random Forests and in Gradient Boosting. Random Forest is effective at dealing with a large number of features since it automatically selects which features are the most significant features at predicting the target value. Random forest works by randomly sampling from each feature or bagging.  In our case our RMSE was .14198 with min samples per leave being 1 and the minimal split per tree being at 4 The maximum number of features is 67 and the random state remained at zero. You can see the number of features below. Gradient boosting “learns” by improving on each decision tree by minimizing the number of residuals. We chose to use Gradient boosting because it frequently outperforms random forest models. Post crossvalidation our optimal parameters were having a learning rate of “0.05"" a max depth of 5 minimum samples split of 4 and the number of estimators being at 1000.  Our tuned model contained an RMSE of 0 .12916. Our original goal was to find the best model to predict housing prices in Ames Iowa. Our initial hypothesis was incorrect. The linear models turned out to be the most effective at predicting house prices. In particular the Lasso model proved to be the most effective at prediction with a RMSE of 0.12290. This is likely the most effective model because it was able to drop the unnecessary variables and because linear models tend to work fine with fewer rows/columns of data. Future paths of research would be to limit the number of features to reduce complexity in tree based modelsAlso potentially forge Principal Components to reduce complexity by reducing the number of dimension.Also finding more ways to effectively feature engineer through gaining a better grasp on Real Estate in this area",NA,Team Machine Learning Project
https://nycdatascience.com/blog/student-works/27008/,21,"For our Capstone Project at NYC Data Science Academy we chose to work on a big data supervised learning problem provided by an adtech company that specializes in user behavior and location analysis. We chose to focus on the problem of CTR (ClickThru Rate) estimation with the hope of improving current company metrics. Our client is an active player in Real Time Bidding (RTB) markets and gave us access to 2 weeks of data from 3 exchanges. The data was split in 3 categories:The following graph shows the overall workflow with some companies as illustration. Additional introductory information about the functioning of RTB markets is available . Our strategy was to explore which machine learning algorithms were used by the industry implement them and compare the results. We downloaded the data to Spark in order to take advantage of parallel execution and collaborated using Databricks notebooks.In our data only about 0.5% of impressions were clicked.This tells us that statistically speaking the clicks (‘1’) are more valuable than the notclicked ads (‘0’) in order to learn the boundary between the 2 classes. As long as said boundary exist we should be able to subsample the elements of the majority class with only a small impact on our final results.The graph below shows that the coefficient variances obtained by fitting a model to the training data tapers off as the class ratio increases. For our modeling we used the workflow described below. We split into training and test according to the date so that it would correspond to a normal production setting in which the object is to predict the click through rate for the next day or next few days. All the 1s were kept in the training set.  We ended up with about 11 times more 0s than 1s. In the test set we kept the original inbalance to minimize bias in the prediction. We just applied a unique downsampling ratio to the whole dataset.All the features in our dataset were categorical with the exception of age; we also made that categorical through binning including the extreme version which considers each value as a group. Since most of the features were identifiers with relatively high cardinality (e.g. ad id campaign id) the traditional treatment of these with one hot encoding impacted performance significantly. This was addressed in the FollowTheRegularizedLeader model using the “hashing trick” and forcing sparsity through regularization.A few papers on CTR prediction report good results with the use interaction features. It makes logical sense that an ad for Gucci would have a higher rate of success (as measured by the probability of a click event) on the Vogue website versus motortrend.com. We didn’t observe improvements justifying the additional performance drain in our dataset. One explanation is that we were dealing exclusively with ads displayed on apps (e.g. ‘Words with Friends’) and not on websites. The algorithm couldn’t find as good a content match between those apps and the creatives (ads).Predicting CTR in the ad industry has been studied extensively in recent years. Logistic Regression is a common algorithm to handle the volume of data efficiently. We therefore started to implement the standard Logistic Regression available in the PySpark ML library. It gave decent results (see graph below) and performed better than the Decision Tree and Random Forest which were also available. In order to improve performance and predictive accuracy we proceeded to  research industry publications for nonstandard algorithms. We tried the FollowTheRegularizedLeader Proximal (FTRL) implementations presented by Google. It uses a single weight layer to allow the training of large amounts of data. Because we implemented the algorithm from scratch it didn’t take full advantage of parallelism in Spark. Still we were able to train both the subsampled and the full dataset (42 million rows in the training set) even with a single thread. Fitting the 42 million rows took approximately 10 hours.FTRL attempts to produce both sparsity and high predictive accuracy. Sparsity is measured by the proportion of nonzero factors in the weight matrix (here a single column). Having lots of zeros enables low memory usage and fast vector operations.Let’s dive into some of the details of the implementation.Stochastic Gradient Descent performs the update:Where t is the round of update (one round for each training observation) η is the learning rate in round t and w is the weight layer in round t.FTRL instead tries to solve for the weight satisfying :where:
and using the notation: For memory efficiency we store:and using:we solve for w in closed form and get the equations detailed in the algorithm summary below:Notes: Our main performance metric to compare algorithms was Area Under the Receiver Operating Characteristic Curve (AUROC). Area Under PrecisionRecall and LogLoss are 2 other common metrics but we couldn’t interpret them as easily (note that for LogLoss probability estimates are affected by the downsampling and need to be transformed back).A summary of the results is presented below:FTRL performed best which is consistent with industry research It was also able to squeeze additional information when given the full dataset (AUROC was still in the same range though as expected from our subsampling analysis). Simple logistic was the next best performing model.We plotted the decrease in performance when predicting CTR on impressions further into the future. Although we only had a total of 22 days of data we were able to observe a decrease in predictive accuracy. We would expect this trend to be more significant if more days were available meaning than the model would need to be refreshed at regular intervals in a production environment.Analysing real world production data was an exciting challenge. The team experienced ‘classic’ big data struggles including inconsistencies/ noise in a massive dataset and cluster reboots during model runs. We also noticed that Spark is a relatively new technology with some outstanding bugs and missing functionality compared to ScikitLearn. In the future we would love to tweak our algorithm on more data and maybe try to bring an improvement even if it is just a small one  to the FTRL implementation.",NA,CTR Prediction in RTB Display Advertising
https://nycdatascience.com/blog/student-works/development-of-game-ai-for-two-sigma-halite-ii-challenge/,21,"Halite is an open source artificial intelligence programming challenge created by Two Sigma where players build bots using the coding language of their choice to battle on a twodimensional virtual board. Each game starts off with either two or four players in which they  compete in a match to either occupy the most planets or destroy the most enemy ships. Each player starts off with the 3 ships and you must dock on planets to gain control and produce more ships. The purpose of our project was to utilize machine learning techniques to develop a bot which would learn how to play the game. In order to do that we fed batches of games from the best players into our deep learning models with the hope that our bot could mimic highlevel strategies from the best players’ bots. Below is an example of a 4player game:Our first challenge was to understand the features outputs and coding pipeline that was included with the game. These aspects were important for us to build accurate models. Once we understood the framework of the game we needed to learn how to navigate through python’s tensorflow package and create our deep learning models. Once this was completed our execution challenges included learning how to deal with the computational complexity of the model and train it in an efficient way. Finally once the bot was trained we needed to translate our predictions into game commands and making sure our bot was fast enough in order to avoid it timing out.We used an adaptive agile approach that was inspired by Bernard Ong data scientist and guest speaker for NYCDSA fall 2017 cohort. The fundamental basis of Agile Process is to maximize the productivity of teams through implementing a divide and conquer strategy in a quick and parallel fashion. By incorporating iterative and parallel tracks the team can quickly disregard approaches which don’t work and either stick with the current approach or explore other approaches. The idea is to fail fast and move quickly.The Agile Process is a “standard industrywide software development and engineering life cycle where strategies and solutions evolve through collaboration between selforganizing crossfunctional teams”.  Unlike the traditional Agile Process our modified agile process is designed for machine learning purposes and AI development. Our agile process contains different components but follows a similar parallel architecture to Bernard’s proposed machine learning agile process.The components for this project included game framework understanding cloud computing navigation pipeline navigation and engineering data preprocessing feature and prediction engineering  algorithm selection hyperparameter tuning model fitting model evaluation model reengineering compilation and submission. In traditional Machine Learning pipeline each process is executed sequentially; model fitting could only be achieved after feature engineering is decided and the process of feature engineering could only start after data preprocessing was completed. However with our agile process approach we were able to leverage multiple individuals in the team to run data preprocessing feature/prediction engineering and model fitting in a staggered parallel fashion. By the end of the first week of our timeline we submitted our deep fully connected bot which placed us in the top 10%. Below are some of the approaches that we explored in parallel among the team: Feature Engineering was a fundamental component in all of our approaches to improve bot performance in the game. The bot’s performance lies not only in the complexity of the model but what features we feed into the model and what predictions come out of the model. These predictions could then be translated into game commands to the bot. Therefore we had to constantly update our features and predictions to best suit our approach and improve bot performance.Some of thefeatures that we engineered to suit our approaches:One of the crucial steps before implementing a convolutional neural network is to transform the dimensions of our features into a 3D array (width * height * num_channels) which then could be fed in batches into the CNN. We came up with 3 different channels: health ownership production. The health channel is a 2D map frame array which contains all ships and planet health regardless of ownership. These values are placed in the array corresponding to their respective coordinates in the actual map. Any coordinates which do not contain ships and planets in the map will have a 0 value in the map frame array. Likewise the ownership channel is a 2D map frame array which has 1 1 or 0 values. These values correspond to our bot’s territory enemy’s territory or uncharted territory in the map frame. The production channel follows the same structure as the health channel.After building the structure of the 2D map frame array we use minmax normalization to normalize the health and production values to (0 1) and standardize the size of our map frame arrays. In each game the size of the map frame changes but maintains the same 3:2 aspect ratio.  By standardizing the size of our input 2D image array into a set dimension (100 x 100)  we can maintain consistency in the output dimension size for each batch of 2D map frame arrays after the convolution and pooling operations in the CNN. We also proposed the idea of mirroring and rotating the input channel frames by 90 degrees to increase the effective data size by 8x.  To make our predictions independent of the game state and applicable to any amount of ships that we have we utilized the kmeans approach to analyze ship movement. Our idea was to try separate player fleet into up to 10 clusters so it is easier to analyze and predict the issued commands. After the model is trained it would predict the coordinates of the clusters and then the ships are distributed to move to those coordinates.The idea was implemented in our CNN network but we were not satisfied with its performance and the decision was made to move forward with predicting ships coordinates directly.We also explored the different ways of representing ships positions on the map. The most straightforward way is to use the x and y position provided by the game code however this is not a robust solution because the map size is randomized and the prediction will be only relevant for the game with a particular map size.The better approach is to normalize the map coordinates so that they are ranging from zero to one. Furthermore we could reshape the map to be the square size this way we can rotate and mirror it this way we expand our dataset 8 times.Another approach is to switch from Cartesian coordinates to polar coordinates. This way we don’t have to worry about the exact positions of the objects; instead we can just provide angles and distances between them. To address the complexity of the network and stay within size requirements and time constraints we decided to transform our input features into sparse tensors and sparse matrices. The reasoning behind it was justified by having large areas of empty space in our map representations. This would also help with data preprocessing times when we were extracting data from JSONs.One of our first approaches was to develop a baseline neural network model and establish a baseline rank for our game bot. We leveraged the template provided by Halite to expedite our approach.The template constructed an architectural code pipeline to develop the game bot as well as provided default features and prediction labels to train the neural network.  Using the template we built a shallow fully connected neural network on Tensorflow with 2 fully connected layers (12 and 6 neurons in respective layers) 11 default features 28 prediction outputs and softmax activation for the output layer. The default features are as follows:The prediction outputs were the allocation distribution of ships to send to each planet on the map (max of 28 planets). The total number of observations is variable depending on the number of games we download and the number of frames in each game. This could be formulated as follows:Total number of observations  number of games * number of frames* number of planetsIn our case we trained 430 games a total of 56014 frames. Thus there were around 1.6 million observations. Above is the cross validation and training loss for our baseline neural network. We defined our loss function to be the crossentropy loss because of the nature of our prediction outputs as probability distributions. Each step corresponds to a feedforward and back propagation process and we used 1000 steps to reduce the errors of our weights and biases. Our final cross validation loss was 3.025 and once we compiled the model and submitted the bot we established a baseline rank of top 70%.In conjunction to establishing a baseline neural network model we developed a deep fully connected neural network. The deep fully connected neural network had the same inputs and outputs as our baseline neural network but with different number of layers and neurons per layer. This approach decreased our cross validation loss to ~2.50 and increased our bot rank from top 70% to top 10%.Finding the optimal hyperparameters (number of layers number of neurons per layer) was a challenge for this approach and subsequent neural network approaches. Machine learning algorithms such as Random Forest and Gradient Boosting can use grid search and bayesian optimization for hyperparameter tuning within a reasonable time. However  implementing grid search and bayesian optimization for neural networks have significant challenges particularly being time and computationally expensive. These are some challenges for Bayesian Optimization using Gaussian Process with Expected Improvements (GP EL):Because of these drawbacks we decided to hand tune our hyperparameters (number of layer number of neurons per layer). We experimented with 579 layers and 20 50 100 neurons per layer and compared cross validation loss for each layer neuron combination setting for the model. We found that the optimal layerneuron combination was 7 layers and 50 neurons each layer.There were different design strategies experimented to structure our CNN including networkinnetwork skip connections batch normalization and very deep networks. Although these methods increased bot performance the computational cost outweighed their benefits.The most successful architecture proved to be a very simple convolutional network with 2 convolutional layers with leaky relu activations (slope  0.3) 2 max pooling layers (2x2 and stride of 2)  and 5 dense layers with softmax activation for the output layer. The number of filters per convolutional layer was determined by incrementally increasing it up to a point before timing out became too excessive. At the end of the network  the default prediction: 28 probabilities of ships to send to planets will be returned. Although the cross validation scores for our CNN model was ~2.3 our CNN bot only ranked in the top 30%. In the early phase of a game our bot would get stuck in position; going back and forth. We hypothesized that this was due to the lack of early game frame data as well as the nature of our prediction output. In each game turn our model will predict allocation of ships to send to a planet. These probabilities are turned into game commands which will move the ships in the direction of a certain planet. It happens to be that in early games there are a lot of different starting combinations that ships can go. With only a few hundred games to train there is not enough information to capture all the early game shipplanet combination resulting in abnormal behavior from the bot. Furthermore if differing game commands are issued in each turn the bot will exhibit the erratic back and forth behavior.We had a few things in mind to improve the performance of our model. First we would eliminate the need for pooling layers in our CNN. Pooling layers help to reduce dimensionality of the array and provides rotational and translation invariance. They are perfect for use in classification situations because they are insensitive to rotations of the image array and the location of objects in the image array. In our case however we are not classifying objects in our map frame but we are concerned with the specific location of our individual ships in each turn. Therefore we do not want to throw away information in that regard. Our prediction outputs would need to be updated as specific coordinates for individual ships to move to instead of allocation of ships to send to each planets. Finally we plan on rotating and mirroring the image arrays to increase our data size. In traditional neural networks we assume that all inputs and outputs are independent of each other however for many tasks this can be a major shortcoming.There are multiple such cases wherein the sequence of information determines the event itself. For these types of cases we need a network which has access to some prior knowledge about the data to completely understand it. We applied various types of recurrent neural networks as they are especially useful with sequential data because each neuron or unit can use its internal memory to maintain information about the previous input and can handle arbitrary input and output length. In our case we feed the network a sequence of turns. One of the early challenges we faced was that the number of frames (turns) varies from game to game. To combat this we used tensorflow’s dynamic unrolling feature which allowed a dynamic variable in terms of the number of time steps. Internally it uses a tf.While loop to dynamically construct the graph when it is executed. 
The basic RNN design struggles with longer sequences but a special variant—long shortterm memory networks (LSTM) — can work with these. LSTMs don’t have a fundamentally different architecture from RNNs but they use a different function to compute the hidden state. The memory in LSTMs are called cells and you can think of them as black boxes that take as input the previous state and current input . LSTMs resulted in a slight improvement of score compared to basic RNN network as they were able to include information from the early phase of games. To combat overfitting we used a common regularization technique dropout. Additionally we used multiple combinations of learning rate training steps neurons number of layers activation functions (relu tanh leaky relu). In an LSTM model there are 3 gates an input output and forget gate. This gating mechanism is used to help with longer sequences. Similar to LSTM GRU models are used to avoid the issue of vanishing gradient. The main difference between the GRU and LSTM model is that GRU only has 2 gates and contains less parameters which make it more efficient and faster for training. Although the performance for the GRU was very similar to the LSTM this technique allowed us to experiment with different parameters because of its faster speed. The cross validation for our RNN model was ~2.6 ranking in the top 25%. Similar to the CNN bot the RNN bot struggled during the early phase of the game. We hypothesized that this was due to the lack of early game frames and small sample size (game replays).  However the bot adjusted in the middle and late stages of the game and on numerous occasions it was able to overcome the poor early game play. In order to improve the performance of the RNN model we would like to incorporate a Bidirectional RNN. This model will value past inputs and future inputs in order to predict the current state. We believe that this can help with the early game poor performance. Neural networks are considered to be black boxes: given input returns output. Internal information about the model such as the architecture optimisation procedure or training data can be hard to visualize and explain to an audience. Tensorflow recently introduced Tensorboard which makes it easier to understand debug and optimize TensorFlow programs. Tensorboard acts like a flashlight on the black box of neural networks by visually breaking down the mechanics under the hood.We added a lot of details to the TensorBoard so that we can observe while the model trains. Our main intention using TensorBoard was to assist in debugging as well as visualizing the Tensorflow graph to track the various transformations (reshape transpose etc.). In the future we plan to use it to plot quantitative metrics and show additional data like images that pass through it such as the input for CNNs. It can also be used to see what the model is learning especially when the training time increases.GPU accelerating computing is the use of graphics processing units (GPU) together with a CPU to accelerate machine learning deep learning and engineering applications. They help power and accelerate platforms ranging from artificial intelligence to selfdriving cars and drones. GPUaccelerated computing segment computationalintensive portions of the application code to the GPU and the remainder portions of the application code to the CPU. Although GPU cores are slower than CPU cores they make it up with their large number of cores and faster memory for parallelization of operations. That is why they are suitable for handling expensive computations for deep learning. Meanwhile CPU by itself is still faster than GPU for sequation code processing.We were able to maximize efficiency and maintain our agile process through the use of Atom and associated packages Teletype and Remote FTP. Atom is a free open source code editor which contain numerous capabilities and packages. One of those packages that we used was Teletype which allows users to share their workspace and collaborate on code in real time. Another package Remote FTP can edit files directly on a server without having to create a local project. Therefore we don't have to download the files of the complete project from  AWS virtual machine but simply connect and edit our remote files from Atom. When saving the files they are automatically updated on the server.Our next steps would be to utilize reinforcement learning to create a bot which has direct control over issuing game commands. For this we are planning to use DeepQ network where we use a combination of reinforcement learning techniques and neural networks to predict expected rewards for each possible action.To reduce the preprocessing times we could offload the JSON conversion and initial matrix operations needed for feature engineering to big data systems. Distributed computations will help speed up the process and the convenience of PySpark would make it a relatively simple migration.",NA,A.I. Development for Two Sigma Halite II Challenge
https://nycdatascience.com/blog/student-works/machine-learning/kaggle-competition-house-pricing-in-ames-iowa/,22,. The dataset which consists of 2919 homes (1460 in the training set) in Ames Iowa evaluated across 80 features provided excellent learning material on which to perform exploratory data analysis imputation feature engineering and machine learning (linearbased models treebased models and ensembling). Our main objectives for the project were 1) to gain facility in the endtoend process of a data science project in a collaborative environment and 2) to better understand the implementation and evaluation of various supervised machine learning techniques. In order for machine learning algorithms to provide meaningful insights we needed to ensure that the data was relatively clean. For our dataset we had to change some feature types and also handle missing values.In many cases we want the model to treat observations with missing values as a separate category. For example we know from the data description that a missing value for ‘PoolQC’ means that the house does not have a pool. It is important to let the algorithm know that some houses do not have pools because this may affect their value so we flag the missing values as ‘none’. The only exception is 'Functional': we still want to flag the missing values for this feature but we assign the value ‘typ’ instead of 'none' because the data description says that missing values here mean ‘typical functionality’.For numeric features when the house does not have attribute being measured it usually works to impute zero. It makes sense for example that the area of a missing garage is zero square feet and that a missing basement has zero bathrooms.  It is usually the case that quality features produce better models and one way to improve the quality and variety of features is to strategically create new ones by combining existing ones. However just adding more features isn’t necessarily helpful because one might encounter such issues as multicollinearity the ‘curse of dimensionality’ increased processing time and overfitting. Since there is a cost to adding features we had to exercise judgment in which ones to add. Maybe townhouses are predictably cheap in one neighborhood but predictably expensive in another. And conversely maybe a particular neighborhood tends to have expensive townhouses but cheap singlefamily houses.,NA,"Kaggle Competition : Predicting House Prices in Ames, Iowa"
https://nycdatascience.com/blog/student-works/kaggles-advanced-regression-competition-predicting-housing-prices-in-ames-iowa/,22,Response VariableMissing DataOrdinal CategoriesNominal CategoriesOutliersSkewnessNear Zero Predictors,NA,"Kaggle's Competition: Predicting Housing Prices in Ames, Iowa"
https://nycdatascience.com/blog/student-works/webscraping-running-shoes-portal-runrepeat-com/,22,What are popular shoe brands?What are the popular shoes for specific needs?What features may have critical influences on customers satisfaction?For my web scraping project I decided to scrape  a running shoes discovery and review platform. It has over 134867 expert reviews and over 1000 shoes for users to choose from.In order to narrow down my research scope I focused on the top women's running shoes in all categories. I was able to scrape 400+ shoes with top scores in terms of popularity and top reviews. For product datasets I scraped brand name shoe name overall product rating run score rank summary and reviews. Plus the web scraping review dataset includes shoe details like terrain use release dates score reviews review summary etc. My web scraping codes are available on    ,NA,Webscraping running shoes portal runrepeat.com
https://nycdatascience.com/blog/alumni/alumni-spotlight-katie-critelli-data-scientist-at-deutsche-bank/,22,My advice would be to first of all take it seriously. If a TA or instructor says something they’ve been through it before and really know what they’re talking about. Take advantage of that and try to pick up all the pieces of information you can. The other thing I’d say is to have fun with it. You could have a creative idea or come from somewhere different from everyone else so even if you aren’t the most advanced person in the room you shouldn’t underestimate what you can do.,NA,"Alumni Spotlight: Katie Critelli, Data Scientist at Deutsche Bank"
https://nycdatascience.com/blog/student-works/impressions-to-clicks/,22,122 impId/dimensions combinations has more than 5 clicks per impression (0.025% of all combinations in Clicks) We took a closer look at them...Of 27 ads with multiple clicks per impression 14 had multiple clicks on more than 1 occasion.Ad sizes 320x480 & 300x250 were most likely to had multiple clicks on many occasions  despite their lower share in Clicks data (share  proportion of rows for a given Ad size).Of 15 campaigns with multiple clicks per impression 9 had multiple clicks on more than 1 occasion  not in line with their share.3rd party videos were much more likely to have multiple clicks per impression despite their low share.Toyota.com for example had experienced much larger number of multiple clicks per impressions than its overall share would dictate.Now our new column in Impressions could be used as a target variable for predictions (1  click 0  no click)The following columns had no missing values and were used as is: Ad Size (7 levels) Ad Type (3 levels) Device Type (4 levels) Exchange (4 levels) Venue Type (4 levels) and Target Group (1200+ levels).Looks like Males are more frequent ‘clickers’.Clicks seem to be somewhat less frequent for ‘US Cellular” and some minor carriers (fall under “unknown”)Larger ads are clicked on relatively more frequently.3rd Party Video ads are clicked on relatively more frequently.PC users seem to be less generous with their clicks.Ads purchased on Rubicon seem to attract fewer clicks.Ads shown on apps seem to attract more clicks than those shown on websites.Relative frequency of clicks is different from state to state.Relative frequency of clicks is different from url to url.Looks like people are less likely to click on ads in the morning.Looks like on Friday people have better things to do than clicking on ads!Our target variable (clicks) had imbalanced classes: only ~0.6% of all impressions were clicked on. We tried to address this issue by::15 IAB dummies were fed into the model as is.All predictors were then standardized by default by LogisticRegression procedure (imported from pyspark.ml.classification)All categorical variables were label encoded using Spark’s StringIndexer. The encoded categoricals were then fed to the random forest classifier along with the unaltered numeric variables.Top Positive Regression Coefficients for Logistic Regression:Top Negative Regression Coefficients for Logistic Regression:Predictor importance results from Random Forest were partially in line with logistic regression’s findings: (with 1200+ levels) was the most important predictor.The top predictors were:,NA,Big Data Analytics: from Impressions to Clicks
https://nycdatascience.com/blog/student-works/web-scraping-analysis-wines-vivino-com/,23,"As someone with a taste for  good red wine and coffee but with the limited funds of a student I decided to webscrape my favorite app  by using Python and Selenium where I scraped information about 16690 bottles of wines. Vivino is the “goto” app when you want to discover good red wines especially on a student budget. Thus the prices range from $10  $6000 a bottle so it should be possible finding a wine matching almost everyone preference and budget. After couple of hunts for good affordable wines I started wondering: . Those are the questions I explore  throughout this blog post.Vivino is a Danish founded company and is today the most downloaded wine app used by more than 26 million users around the world. With millions of wines featured the database makes up the most extensive wine library in the world. Vivino has established a community and developed an app which especially is great for us who love red wine but aren’t sommeliersintraining. The Vivino app enables consumers to snap a photo of a bottle’s label and the app will immediately pull up information about the wine its rating score reviews and much more.I scraped the following variables on Vivino.com:The box plot reveals that the median rating for red wine tends to be slightly higher compared to white wine and sparkling wine. The majority of the red wines obtain a rating approximately between 3.5  4.0 (the interquartile range box). Further it should be noted that the rating for  red wine white wine and sparkling wine seems to be normally distributed though there are a few outliers. An interesting observation is that a slightly wider rating range for red wine is observed compared to what we see for the white wine and sparkling wine categories.I expected to see this correlation. The overall average rating score per bottle and price seems to “some extent” to correlate particularly for red and sparkling wine. The correlation coefficient reveals a “somewhat” strong relationship between rating and price for red wine (0.68) and sparkling wine (0.69) while the correlation for white wine is significantly smaller(0.57). However that correlation peters out for red wine priced above $100 a bottle; the rating  only increases slightly in tandem with the price at that point.As observed at the initial boxplot a slightly wider rating range was observed for red wines which is why I decided to look into the “spread of the data points” around the average rating (variance) for red and white wine. I wanted to investigate whether ratings for red wine have a higher variance than white wine.H0: Variance of average(rating red wine)  Variance of average(rating white wine) H1: Variance of average(rating red wine) ≥ Variance of average(rating white wine)The variance test revealed an extremely low pvalue (2.2e16) why the null hypothesis is rejected in favor of the alternative the rating for red wines has a higher variance compared to white wine. The higher variance for red wine rating can be caused by several factors.  Some part of the explanation might originate from the fact that red wine and white wine are served at two different temperature. A common rule of thumb is that red wine is best served between 50F and 65F while white wines are best served between 45F and 50F. As too much cooling causes a  loss of flavor the odds of flavor loss are higher for white wine than for red. Lastly red wines are typically aged in oak barrels while white is aged in stainless steels vats. These two entirely different processes result in two distinct taste experiences. The variety of flavor experience tasting a red wine versus white wine are bigger and could therefore be a part of the explanation why the data points for the rating of red wine are more dispersed.  Nothing really changes here compared to the previous scatterplot. However it’s again worth pointing out that it’s possible to buy a red wine with top rating for around $75  $100. The jump in the year is because no bottles were present in the missing years in the scraped data. The scatter plot reveals a correlation at 0.377 which could indicate that an increase in the year will decrease the rating. Therefore we need to test whether it’s significant or not. An interesting insight here is that older wines tend to have a more narrow rating range an increase in years also increases the range of rating. This could indicate that old wines are quality wines wines which people have saved. While the wider rating range for newer wine indicates that from a consumer perspective that both quality and lower quality wines are produced. In sum the plot makes intuitively good sense when considering year and rating within red wines.The boxplot reveals from my perspective a quite surprisingly insight! Wines in the United States have on average a low price while the median rating is the highest compared to the remaining countries. That would indicate that wine lovers who want a decent quality for a reasonable price should look at wines from the US!I have often heard that French wines are the best. During my internship at adidas I had the privilege to live with a French mate for a few months and we often discussed why people tend to claim that French wines should be a better choice (my mate tried several times to convince me). With the scraped data from Vivion.com I can check if the ratings support the claim of french superiority.H0 : Average rating France  Average rating remaining countries H1 : Average rating France ≥ Average rating remaining countries My test revealed that the scraped data did contain any evidence that the average rating for France wines is higher than the remaining countries.:
Next I wanted to figure out which regions and bottles to look for within a reasonable budget. Therefore I specified the following requirements:Playing around with scraped data revealed some interesting and surprising insights which pointed me in the direction of the following question. Can the predictors price and year explain the obtained rating score for red wines at Vivino? Therefore I decided to test the following model:Rating Score  β0 + β1 √price + β2 #Number of reviews per bottle + εThe analysis was carried out in Rstudio where I obtain the following output of my regression analysis.The adjusted Rsquared for this model was 0.5306 which is decent given the few number of predictors; and  The model reveals that the baseline rating score for red wines at Vivino.com is 3.6. Further every unit increase in √price would increase the rating by 0.053 while each additional reviews increase the rating score by 0.000007265. This means that it takes 137646 reviews to bump the rating by 1 point. It requires a significant amount of reviews to move the rating.",NA,Web Scraping and Analysis of Wines on Vivino.com
https://nycdatascience.com/blog/student-works/web-scraping/chewy/,23,Sales of pet food in the US has increased by 40 percent for the first quarter of 2017 compared with  last year .  The number of  food options available has also grown.As a pet owner I am always struggling on choosing a “better” food for my fluffy friend who refuses to touch some that I’ve bought.. I decided to work on this web scraping project to get insights of the pet food. The findings would not only benefit pet owners but also new vendors who try to break into this market.Pet food makes up the largest part of  $15.92 billion pet market in the United States in 2016 .  That is a highly concentrated; just give vendors accounting for about 70% of retails sales (Nestlé Mars Big Heart Colgate and Blue Buffalo).  All vendors have to appeal to what pet owners seeks. According to survey results their main criteria are  “high quality” or “real meat” products.What is considered a “high quality” pet food product? The assumption for the research is e that the quality of pet food product depends on its  ingredients. The guaranteed analysis would be the first place to start the analysis for pet food ingredient. It tells you how much of the total percent of the food comes from protein fat fiber and moisture.  There are ~4500 products available in the market and product information is scraped from the US leading online retails Chewy.com for both cat and dog. Below is a sample of the data from scraping. The data is preprocessed through Python Pandas and the ingredient is separated as individual features with complicated multistep data processing.  Base on the hypothesis  the research would includes below three steps: Methods used in experiment:There are ~4500 products data preprocessed. Fiveclusters of products are optimized by unsupervised learning methods based on approximately 70 different ingredients.  And products are clustered into 5 segments using Kmeans clustering based on portion of different ingredients. The data is PCAreduced to 2 dimensions. The charts displays the 5 clusters of product based the distance to the mean of each ingredient attribute. The result is based on assumption that  the number of reviews and star ratings of the product reflect its perceived  quality by customers.  In order to differentiate different product segments by quality the ANOVA test is conducted. Cluster 3 and 4 is observed to have higher rating and more customer reviews than cluster 1 and 2. That indicates that in fact  perceived  product quality  does depend on the ingredients.  the stars of ratings among 5 clusters of product  the numbers of customer reviews among 5 clusters of product  Cluster 0 is represents outliers with missing reviews.Following cluster Cluster 3 and 4 are analysed as ‘Good’ quality products ( more reviews and higher ratings)while Cluster 1 and 2 contain ‘Bad’ quality products ( fewer reviews and lower ratings). Below is a scatter words for product ingredients.a frequency comparing between ‘good’ and ‘bad’ quality productsCertain ingredients appears only frequently in ‘good’ products:Certain ingredients appears only frequently in ‘bad’ product:The portion of major ingredients is also compared between ‘good’ and ‘bad’ products. Good products have a higher portion of Glucosamine and Chondroitin (supplements) than bad products. Bad products have higher portion of moisture than good products. That the portion of moisture squeeze the portion of other nutrients such as proteins. This explains that the wet food is less popular than dry food.   Ingredient DistributionFunctionality and Nutrition Level are the main contributors to quality differences. Joint/vision support higher level of protein/fiber and better tastes are highlighted by good feedbacks from customers. While digestion support with ingredients other than fiber and higher level of moisture/ash are observed with bad feedbacks from customers.  The ingredients differentiate the qualities considered by customers. The findings of ‘good’ and ‘bad’ ingredients will be helpful for manufacturers to produce ‘good quality’ pet food and better adapt to the fast growing pet food market.Due to the limited time of the project the research’s focus had a limited scope. For further study it would be helpful to observe other differences to better understand the pet food market. That would entail looking at attributes like   and other available information about the product.,NA,How to Recommend Pet Food Product from Unsupervised Learning
https://nycdatascience.com/blog/student-works/web-scraping-bike-index-uncover-analyze-stolen-bike-data/,23,During my junior and senior years of high school on many occasions I would hop on the downtown 6 as soon as the school day ended and head to Theatre 80 St. Marks.  Theatre 80 was a cozy revival cinema house that often featured doublebills of film classics a great bargain for a high school student with little more than pocket change.  I was fortunate to be a patron of this cinema house before it stopped showing films in the summer of 1994 for  it was here that I was introduced to one of my favorite films the Italian neorealist classic “Ladri di Biciclette” (“Bicycle Thieves”) by Vittorio De Sica. Although the story music and acting all affected me deeply when I first saw it the scene in which the thief steals the main character’s bicycle and sets the entire movie in motion did not fully resonate with me until more than 20 years later when I myself had my bicycle stolen also while working.  Similar to Antonio Ricci  the main character I witnessed the moment when the thief took the bicycle and even futilely gave chase on foot until I lost him in the shadows of the Williamsburg Bridge.  It appeared at the time that even the quickly fading autumn light was conspiring against me. It was after this incident two years ago that I looked into resources that were available to people who had their bikes stolen. One of these resources I came across was an online bicycle registry called “Bike Index” that was founded in 2013. The  has close to 150000 bikes registered from around the world though predominantly in the United States and approximately a third of these bicycles are marked as stolen.  In addition to allowing individuals to search the database Bike Index partners with local businesses and organizations law enforcement agencies and other apps to alert the community when a bicycle is stolen. It was nice to discover an organization out there dedicated to helping people recover their stolen bicycles.Given my experience as a bicycle theft victim I decided it would be an interesting and worthwhile undertaking to web scrape data on the stolen bicycles registered on Bike Index and see whether I could detect any themes in the larger community affected by this crime. These include geographical distribution of the data features of the bicycles the circumstances under which they were stolen and demographic analyses of certain areas with a high incidence of bicycle thefts. Some potential uses of the analysis include pinpointing locations where Bike Index may be deficient in registering stolen bicycles or conversely areas that have a high incidence of bicycle thefts that may merit further attention from local law enforcement agencies and the wider public. For the web scraping portion of the project I used Python’s Scrapy a web crawling application that allows for extraction of data from HTML documents.  Bike Index’s database essentially has two levels of information.  The first level is a thumbnail list of  with ten bicycles per page and some distinguishing attributes for each bicycle.  After clicking on the image or header one arrives at the second level which provides more detailed information on the bicycle and the circumstances of the theft. Here is an .For this project I intended to scrape all bikes stolen in the United States in the past five years from October 2012 to October 2017 or approximately 3700 pages of search results.  As a practice run I first scraped only the information on the first level with random download request times of one to three seconds. I was able to download  little more than half of the bicycles as it appeared that my requests were met with HTTP 403 errors from the server.  Therefore when I web scraped the second level I added a constant download delay of 3 seconds. The process ended up being too slow so I aborted the attempt early and iteratively lowered the delay time based on a small subset of pages until I reached 0.25 second without receiving any HTTP 403 errors.  It is unclear why this constant delay of 0.25 second was more successful than the first attempt using a random download delay request.  In any case I was able to download information on at least 80 percent of the bicycles. But when I examined the information I realized that the field that contained the color description of the bicycle was missing because I did not have “color” in upper case in my code.  It felt incomplete without this information so I web scraped the website once again and ended up finally with the following fields in no particular order:From Bike Index I was able to scrape information on approximately 30000 bicycles. After cleaning the data by excluding those observations that were outside the United States or did not contain zip code information I ended up with approximately 25000 bicycles. In addition to the data from Bike Index I also used  sourced from the American Community Survey (“ACS”)  a part of The United States Census Bureau’s Population Estimate Program. Demographic information for select counties was used in the analysis as will be discussed later in this blog post.The first analysis I performed was to map all the zip codes where at least one bicycle was stolen in the last five years to get an idea of how the data is geographically distributed.   The map is from Leaflet’s OpenStreetMap and the following image is a screenshot from the Shiny app I used to present my analysis and findings.Hovering over each zip code one can ascertain information on the city the zip code and the number of bicycles stolen in the last five years.  It is not surprising that we see a lot of red dots around the major metropolitan areas on the coasts especially on the West Coast and the Northeast. However I was a bit surprised that a large portion of the country between the coasts was so sparsely marked with red dots.  The banner across the top of the image is a list of the top three cities by a number of bikes stolen all on the West Coast.  Anecdotally I understand that San Francisco Seattle and Portland have a thriving bicycle scene but nevertheless I was surprised that New York City was not among the top three for any of the years selected.  In fact it did not even make top ten.  This inspired me to do a state by state comparison which more clearly revealed to me that we might not be dealing with a representative sample of stolen bicycles. The following is a map of the country with each state shaded according to the number of bicycles stolen.  The map clearly demonstrates that the dataset is heavily skewed towards the West Coast among the states of California Oregon and Washington in particular all three of which accounted for more than 60 percent of the bicycles in the dataset.  According to the data the State of New York had only 895 stolen bicycles over this time period which clearly did not make sense if one were to view the data as a geographical representation of the total population of stolen bicycles in this country.  This was further confirmed when I performed additional research on the founders of Bike Index both of whom are from or are living in the West Coast. Quite possibly the website first gained traction among communities along the West Coast and remained a popular service for bicycle owners there relative to those from other parts of the country.I then proceeded to perform some descriptive statistical analyses of some of the attributes I scraped from Bike Index.  First was the distribution of bikes stolen across the various manufacturers. The top three bicycle manufacturers (Trek Specialized and Giant) account for approximately a third of the bikes stolen registered on Bike Index. As an interesting factoid over a half of the bicycles stolen in the past five years were either black or multicolored.  And the data clearly shows a downward trend in the number of bikes stolen as the weather gets colder.Approximately a third of the bicycles that were stolen was locked using a cable lock.  The second most frequent case is the one in which the bicycle was not locked at all accounting for about 20 percent of the bicycles stolen. This was surprising to me initially as I couldn’t understand how anyone would not lock up their bicycle. After reflecting on my own personal history however  I can asseverate not locking up one’s personal property is an issue when it comes to bicycle thefts: I did not lock my bicycle two out of the three times it was stolen.In terms of how the bicycle was stolen more than 50 percent of the time the lock was cut.  It should be noted that in the graph below the “Other” category is a catchall category for those who wanted to provide more detail of the incident in another field where they were not limited to predefined choices. Therefore the “Other” category may overlap with some of the other bars in the chart below.As mentioned previously registered users of the website can provide further details of the incident.  Curious to know what some of the more common words that appeared in this field was I performed a word cloud analysis.  The following are the top 100 most frequent words with at least 50 occurrences.Understandably it appears that many people offered rewards in this field for the return of their bicycle. It is also interesting to note how many words were associated with the home: “home” “house” “storage” “patio” “apartment” “building” “basement”.Going back to the analysis in which I examined the number of bicycles stolen by season of the year I was curious to know whether one could conclude from the data that seasonal differences were statistically significant. In particular can one conclude from the data that the average number of bicycles stolen per year over the past five years was different for at least one of the seasons? The hypothesis testing was framed as follows:: The average number of bicycles stolen per year in the population is the same for all four seasons.: The average number of bicycles stolen per year in the population is different for at least one of the four seasons.Before I could perform a oneway analysis of variance (“ANOVA”) I had to reach some level of comfort that the assumptions of the test were satisfied. There was nothing in the data to suggest that number of bicycles stolen in each season would be dependent on another.  Also based on the following qqplot of the observations I got comfortable that the number of bicycles stolen in each season was approximately normally distributed as they all fell close to a straight line.Although technically a qqplot of each season was required based on my understanding that the oneway ANOVA was robust with respect to normality and that a qqplot of only five points for each season may seem sparse I decided that a qqplot of the entire set of observations was sufficient for the purpose of the test.  Lastly I applied both the Bartlett’s Test and Levene’s Test to see whether we could reject the null hypothesis that the variances were the same across all four seasons. Because of the high pvalues we couldn’t reject the null at the 5 percent significance level.After performing these preliminary analyses I then proceeded to the oneway ANOVA. I arrived at a pvalue of .0566 just above the 5% threshold. Therefore I could not reject the null hypothesis that average number of bicycles stolen per year in the population is different for at least one of the four seasons.  Despite the vast difference in average number of bicycles stolen per year between summer (1621) and winter (942) it appears that there were wide variations within each season across the fiveyear period driving down the Fstatistic.  The website received funding through a Kickstarter campaign in late 2013 and this may have accelerated the growth in registered users over the past several years increasing the variance in each season.  The next set of questions I wanted to research was whether one could detect any demographic differences between bicycle theft “hotspots” versus other areas.  To focus my research I looked at the top ten counties by number of bicycles stolen in the past ten years. Seven out of the top ten counties were on the West Coast.  I then divided each county between the top quartile of zip codes and the bottom three quartiles of  zip codes ranked by number of bicycles stolen.  Finally I joined the Bike Index data set with the demographic data set from the ACS.   The first demographic field I compared was  the percent of males in the population. As the following chart shows there does not seem to be much difference between the top quartile and bottom three quartiles in each county.Next I examined the distribution of the population by age.  Across all ten counties a higher percentage of the population  in the top quartile fell within the 2534 and the 3544 age groups compared to the bottom three quartiles.  The difference appeared to be the largest for the 2534 age group.  This is consistent with my impression that a larger portion of the young adult population ride bicycles compared to the other age groups and therefore the opportunity to have their bicycles stolen would be greater. For illustrative purposes the following is the chart for Cook County which had the highest differential in the 2534 age group.Lastly I examined the distribution of the population by race. Across all ten counties the top quartile had a higher percentage of people classified as “White” versus the bottom three quartiles.  The biggest difference was in Orleans Parish which is shown in the chart below:Sadly in this country income and wealth are highly correlated with race and I wonder if the differences between the top quartile and bottom three quartiles in terms of race might also be related to the differences in the income and wealth distribution of the various zip codes. The web scraping project provided a wonderful opportunity to further examine an issue that is personally relevant using data that in all likelihood would not have been available otherwise. Furthermore if I had not scraped the data and combined it with visualization I would not have discovered how skewed the data was geographically. Either the West Coast is really a hotbed of bicycle thefts compared to the rest of the country or in the more likely scenario the users of the website over the past five years have been disproportionately represented by people living on the West Coast. One way to test this would have been to web scrape the rest of the website by including those bicycles that were not marked as stolen. If this subset were also skewed towards the West Coast it would have provided further evidence that the latter scenario was the case.  This information would also indicate that Bike Index has an opportunity to deepen their footprint in the rest of the country.  With a more robust dataset of bicycle thefts  the demographics and variables analysis also hinted at future work that might yield useful insights.  One could test for example the statistical significance of the differences in age and race distributions mentioned earlier across a larger portion of the country. Another possibility is to use variables such as the demographic profile of a geographic location to predict the number of bike thefts over a certain period. This information could then be used to concentrate recovery and enforcement efforts at certain “hotspots”. Bike thefts will not be entirely eliminated but a strategic effort to analyze the issue using data science could hopefully help turn the tide against this perennial bane of bicycle owners.,NA,Web Scraping Bike Index to Uncover and Analyze Stolen Bike Data
https://nycdatascience.com/blog/student-works/scraping-riaa-drastically-music-industry-changed/,23,Over the course of the past 20 years the music industry has experienced an incredibly drastic change and in particular it has to do with the way in which we consume music. Music services like Spotify and Apple Music are wonderfully intimate forms of which we listen to music. In the digital age we live in  the user have full autonomy over what we get to listen to. We can press skip we press pause and we can compile songs from albums and singles and put them together into playlist. How has this changed consumer behavior though? Given the digital platform that exists now there has grown an extreme saturation of artists. Anyone can viably learn and instrument record a song on their phone and upload it to Spotify. So with such an abundance of music in the world  how does one go about garnering attention and stand out? Click  if you want learn more details one how the RIAA defines a unit and their certification process.The data was obtained from scraping the RIAA’s website listing all awards ever given out since its inception which can be found on the following link: . Each entry is of the most recent award that a particular album or single received. These for each album or single to be awarded we have listed the: artist title the date it is was most recently awarded the name of the record label the format (single or album) the release date the type (digital or physical copy) the group type (band solo or duo) and the genre.,NA,Webscraping Every Platinum Record: What Happened to the Album?
https://nycdatascience.com/blog/student-works/lending-club-closer-look-pre-post-scandal/,24,Lending Club is an online platform that connects borrowers and lenders. It enables borrowers to create unsecured personal loans of up to $40000 on either a 3year or 5year term. Investors can then browse loan listings on the website and select loans they want to invest in based on a myriad of variables such as amount of loan purpose of loan loan grade fico score etc. Like most secondary marketplaces lending club profits by charging a “middleman” fee for screening borrowers facilitating transactions and servicing loans. Since its establishment in 2007 lending club has revolutionized personal loans in the US and is the world’s largest peer to peer (P2P) lending platform. The company raised $1 billion in what became the largest technology IPO of 2014 in the US. P2P is a new business model and like most business models it has to be tested it has to go through some issues and come out well on the other side. In early 2016 lending club was involved in a scandal over some of the firm’s loans and concerns by the board over CEO Renaud Lepanche’s disclosure leading to a large drop in its share price and the CEO’s resignation.This led me to explore how loans have changed through lending club’s growth and their outlook postscandal.The data used for this analysis includes all of from 2007 up to 2017 Q2  approximately 1.5 millions rows and 140 columns (of which 110 columns were removed for processing purposes).To explore this issue we must first understand the basics and how Lending Club works. Lending club assigns grades to loans as an indicator of risk. Grade A is considered to be the safest or least risk while Grade G loans have the most risk. As witnessed below interest rates increase proportionally over subgrades and there isn’t too much overlap between their interest rate density distributions. Both of these charts show that Lending Club seems to have a consistent model across loan grades.Next we look into their model further on how different variables impact interest rate. Unsurprisingly Fico score seems to have the one of the strongest relationships with interest rates  as fico score decreases interest rates decrease. Credit Age annual income and number of accounts opened in the past year are some of the other variables correspond with interest rates.Now that we have an understanding of how lending club’s model works we can look at changes in their business over time. Geographically California consistently has the highest loaned amount from 2008 onward. Coincidentally Lending Club is headquartered in California. In recent years Hawaii has had some of the highest interest rates however as a whole interest rates seem to be volatile from year to year . Overall both loan amounts and interest rates seem to be converging toward uniformity  a good sign for any business.Next we look at how interest rates have changed over time across loan various loan grades. The safer loans  Grades ABC seem to be relatively consistent. However as you can see interest rates have been rising for the riskier grades as high as 30%!We also look at how loan amounts have changed over the years a good indicator of lending club’s growth. They have consistent and stable growth from 2007 to 2014. While they do see incredible growth PostIPO there is also great deal of volatility.We have to dig deeper into this volatility to identify the scandal. Looking at the monthly distribution across years we see that prior to 2016 July and October have seen consistent jumps in loan amount likely due to holiday seasons. However 2016 saw an abnormal spike in 1st quarter specifically in March  the month of the scandal. In fact the difference between March and the second highest month in terms of loan amounts (Oct 2015) is nearly the amount of loan amount involved in the scandal  $145 million.Finally to get an idea of where lending club is heading we take a look at trends in default rates. Default rates are final once loans are completed. Lending Club offers 2 types of loans: 3year term and 5year term. Default rates for 3year term after 2014 are not finalized as these loans have not completed their term. Similarly for 5year terms we have final default rates up to 2012. For example if we invested in 1000 loans with a 3year term initiated in 2015 payments will be made until 2018. Therefore the current default rates on those loans will only go higher as loans could potentially default in late 2017 and 2018.The first thing that stands out is loans with a 5year term have a higher default rate than 3year term. Also as expected default rates are higher as grades get riskier for both terms. The safer loans (grades ABC) seem to have consistent default rates but the riskier grades (EFG) are trending toward higher default rates. This is a sign of concern and I believe lending club has also noticed this trend as they are increasing interest rates for these grades.Another sign of concern is that default rates are already high for years that we do not have finalized default rates for such as 2015  2017. For example in 2015  Grade F and G for 3year term have the highest default rate more than any prior year and this number will only go up as these loans have not completed their term. Lending Club’s credit rating model seems to be consistent over various factors and allows investors to choose their risk level accurately. After the 2016 scandal and the CEO’s resignation we noticed a dip in investments but their investor base remained confident and investments have bounced back.While Lending club is offering interest rates higher than ever before one should be cautious over investing in loans on Lending Club especially for riskier investments (Grades D through G). Default rates have been increasing. Perhaps lending club is accepting riskier borrowers increasing default rates but also offering higher interest rates than ever before to account for these default rates and to attract more investors.  Now more so than ever before the investor must look at the variables linked to a loan to differentiate the good investments from the bad. Due to high interest rates on riskier loans (grades DEFG) there is a lot of value and opportunity if one is able to identify the correct loans. The next step in this analysis could be to take a deeper dive into these riskier grades to identify patterns or trends that an investor can take advantage of to increase their bottom line numbers. Check out the shiny app . The details of the code can be found .,NA,A closer look on the effects of the Lending Club Scandal
https://nycdatascience.com/blog/student-works/analyzing-predicting-european-soccer-match-outcomes/,24,Soccer in my opinion  is not only  the most popular but  the  best sport in the world. I always wakeup early on Saturday and Sunday mornings to watch the  matches on television. I love the emotion the skills the drama and everything about it. That is why for my Capstone project I wanted to find out if I could create something of value from the numerous hours I have devoted to watching my favorite sport. I decided to create a shiny app in order to visualize the data and use the numerous machine learning algorithms I had learned in an attempt to correctly predict the outcome of soccer matches. Below I describe where I obtained my data the data cleansing feature selection interactive plots of the data and the algorithms used to predict the outcome of soccer matches.      I was able to identify a comprehensive data source of football matches on Kaggle. The data source was a .sqlite file which contained 7 tables:Using the RSQLite library I was able to transfer all the tables to a R file and into a data table. When given a new data set the first check performed is the number of missing inputs in the raw data. As we will see in the figures below missingness was a big issue especially with the match team attributes and player attributes table. Either the data had incomprehensible information or the data was missing.The figure above contains a description of the missingness in the player attributes table. The histogram on the left is a percentage of data that is missing per feature. The plot on the right is a grid that shows the combination of features most prevalent in the data with red indicating features that are missing and blue signifying available data. So we can see that for a big portion of the data ~98% there is no missingness (all the features area available). Although a small percentage of some features are missing in this table it is still something we will need to handle in order to not blindly throw away observations.As we can see from the figure above only one feature from the team attributes table is missing a significant amount of data. There are more observations with that data missing than not and since each team is different it does not make sense to replace the missing observations within the feature with a mean value or a randomly imputed value. Finally in the match table we can see that a huge percentage of some features are missing. Like the team attributes table combinations of features with missing data are more prevalent than combinations of features with no missing data. The reason why three features appear to be missing altogether is because most of the features indicated above deal with betting data for winning losing and drawing a game from different betting companies. For each company it appears that if one of the observation is missing (odds of winning losing or drawing a game) then there is a good chance that the remaining 2 features will be missing. In that case we can say that the feature (odds of winning losing and drawing a match) is missing at random (MAR) since the probability of one of the odds feature missing depends heavily on the availability of the remaining odds.Since the remaining combination of missing data appears to be random we can conclude that  the remaining missing features are missing completely at random (MCAR) since the probability of a value missing does not depend on another feature value (MAR). Also we can definitely rule out missing not at random (MNAR) since the feature value itself has no bearing on whether or not the value will be missing. As we will discuss in the upcoming section a lot of the betting features from different companies are highly correlated with one another so we can drop certain features without losing significant information. This allows us to keep more observations and prevent any bias that might have been introduced from dropping observations with missing data. Since the match table will need to be merged with the player attributes table and team attributes table it is vital to select the right features from the three tables in order to decrease the number of observations with missing data and to develop custom functions to properly handle missing values rather than using mean imputation random imputation or some form of regression imputation.  To perform preliminary feature selection that accounts for missing data I decided to use a correlation plot to find the correlation between all the features in their respective tables. If two or more features are highly correlated then there is a good chance they carry the same information. Consequently I would only need to pick only one of those features. From the figure above we can see that although there is some correlation between the attacking and defensive attributes in the team attributes table none of the features were highly correlated with one another. I decided to merge all the features from this table with the match table. As I still had to deal with the missingness with some features I decided to write a custom function that performed in the following manner:In the correlation plot in the player attributes table we can see that attacking features are highly correlated with each other and defense features as well the same result we observed in the team attributes table. Due to a shortage of time I decided to use only the overall player rating feature from the player attributes table since it was a good representation of all the features. Another reason I decided to use only the overall player ratings feature was to avoid piling on too many features.Each player per year had a corresponding value and as each team has 11 players selecting only one feature from this table would translate into adding 22 features to the match table (home and away team per match). So if each player had two features from the player attributes table it would double to 44 features added to match table. As different players would need different features (attackers to attacking features defenders to defenders features etc.) it made sense to use overall player ratings for now and based on model results see if adding more features would lead to improved results. For the player attributes table I performed a slightly different custom function in terms of identifying and replacing missing data:The match attributes table was interesting; missing data corresponded to features related to betting odds (Odds of home team winning away team winning and draw). As we can see from the figure above there are a  number of marked correlations. All the odds related to home teams from different companies are highly correlated with each other.All the odds related to the away team from different companies are highly correlated with each other. Even all the odds related to the match ending a draw from different companies are highly correlated with each other. Due to this I decided to use only the odds from the betting company B365 because it  was the one with the least missing data. Some features also contained garbage information (incorrectly scraped from respective websites) so I dropped those features from match table. After merging the match table player attributes table and team attributes table I was left with the overall ratings per player all the team attributes the betting numbers for home team win away team win and draw odds and the goals scored by each team in a game. Without some form of imputation only ~7% of the data had complete cases.  But with the custom functions and after analyzing the missing data I was able to retain ~ 68% of the data (complete cases). For the data visualization section I decided to create a shiny app that showed trends of wins/losses/draws for each team home and away from 20082016 trends of the team attributes from from 2008  2016 and a box plots highlighting the overall ratings of the 11 players on each team from 2008  2016. Rather than highlighting only one league the user will be able to look at the English French Belgian Spanish German Italian Netherlands Scottish and Portuguese leagues to see which teams had the better most wins per year the ratings of their players and of the overall teams. I decided to create models that predict the outcome of the home team winning/losing/drawing a game. This is a multiclass classification problem since there are three outcomes win (W) loss (L) and draw (D). Below is the distribution of classes.The win category has almost twice as many outcomes as the other classes so this is something we will need to be wary of especially when splitting for a traintest. We want the train and test set results to be similarly distributed. To do this I used the createDataPartition function in the caret class. We should be wary of this distribution of classes since predicting a win always provides an accuracy of 46%. Consequently any model that we build needs to be better than this accuracy. For analyzing the results I will be using the following metrics:Although having all parameters above as high as possible is the best case scenario I will be tuning for overall accuracy because it does whether the result is a win draw or loss. All that matters is correctly predicting the outcome of the soccer matches. For my first model I chose xgboost because  it is quick works well with classification and does not force the model to assume a certain shape like regressions of all types. For xgboost I used a 10 fold cross validation with the following parameters:After 10 fold cross validation I ended up with the following parameters for xgboost:One thing I noticed using xgboost was that removing the individual player ratings as features had negligible effects on the results of the model (those features essentially had little variable importance values). I reran the model with the grid provided above and arrived at the same optimal parameters mentioned above. After training and testing the model I obtained the following results:Right away we can see that the overall accuracy is better than the null case. It also appears that since the win category almost doubles the other categories the model is predicting a lot of wins when the results are draws or losses (high sensitivity and low specificity). Next I decided to use neural networks from the nnet library. Although I am able to use neural networks which is fast and has a history of being very accurate the downside of the nnet library is that it allows only 1 deep layer. Again I tried this algorithm with and without the individual player ratings and I got extremely similar results. I trained the model with 90% of the data and with no player individual player ratings as attributes I obtained the following results:The results obtained from this algorithm perform better than predicting all matches as wins but it is not an improvement on the results of xgboost. There is a slight improvement in the specificity for Win/Draw category but it is still very poor. This again is due to the fact that the distribution of the results is heavily weighted towards the win category (high sensitivity low specificity). Rather than using only one algorithm I decided to use a stacking method that used the following optimized algorithms to create meta features and use those meta features along with the initial features presented as inputs to an xgboost model:Once the meta features were created I used xgboost to predict the results of the matches and got the following results:Although this model performed better than predicting all wins for its complexity it is not an improvement on the results obtained from using only xgboost and neural network algorithms. For this project I collected data from Kaggle cleaned it up to deal with null cases merged certain tables and performing feature selection in order to visualize the data and perform some machine learning algorithms in an attempt to correctly predict the outcome of the soccer games. Although many simple and complicated models were created to accurately predict the outcomes of soccer games and we were able to predict better than the null case it appears that the features need to be revisited in order to obtain better model results. We could consolidate certain features such as player attributes or drop certain features to simplify model since model complexity. Another issue could be that more data needs to be collected to better reflect a more even distribution of the win/loss/draw classes. This could potentially assist in correctly predicting the outcome of the soccer matches.  ,NA,Analyzing and Predicting European Soccer Match Outcomes
https://nycdatascience.com/blog/student-works/new-york-city-psychotherapists/,24,There are many mental health professionals to choose from but it is  difficult to find the information on them Of course not all patients are able to pay the full amount out of their own pockets.  Most therapists (to be in a more convenient location for more perspective patients,NA,New York City Psychotherapists: Who Are They?
https://nycdatascience.com/blog/student-works/ds_for_corporate_under_nda/,24,Would an angry person prefer sour flavors?  When beverage makers use corn sweetener instead of sugar do people like the drink better or worse?  Do they even have a reaction to this change?  The questions you start with are often different than the answers you find but when you have real data to work with even the unexpected can prove highly useful.  Our team recently presented our findings to a major beverage company for whom we signed an NDA.  Names of beverages and flavors in this write up as well as technologies used have been intentionally left out to protect the innocent and the guilty.  Just kidding.  Names and other details of our research have had to be excluded to honor our NDA agreement.The “sexy” side of Data Science is machine learning and model building but there is another aspect that easily comprised half of this project.  In the real world data is not always clean properly organized and ready to go.  For this project we were presented with two data tables:The data model includes a unique user id for each person identified in the image capture table a unique recipe name and id for each combination of drink dispensed and date/times which comprised the best candidates for joining the two tables.  “Image Capture” data was really numbers reflecting machine intelligence evaluations of the images captured to determine: human emotion gender and age. Due to privacy concerns photos were kept just long enough to create the data and then the images were destroyed.  The two systems though deployed like a single machine were completely separate.  Date/Time stamps were not properly synced up to the central server and were not synced to each other at all.  A preliminary look at the data revealed that the time stream of the Image Capture data started and ended about one day earlier than the Beverage Dispense data.To get insights and value out of the data we identified three main challenges. Even when implementing the best possible resolution to each challenge each one of them  adds uncertainty to the overall interpretation:The problem of linking and syncing the data was explored from many different angles and even ended in a final brainstorming session with the whole team.  To sync the two different timelines of the beverage and image capture data a delta or shift value is needed. When the shift value is added to one table the timelines lie in the same zone (in the other). The question was how to find the optimal shift value? We played around with likelihood and gradient descent methods but finally chose an error based approach where the shift with the lowest error is searched via brute force. As an error function we defined the sum of seconds of all beverage events to the nearest image capture event.After syncing times we moved forward to link the beverage events with the most likely image capture event.   In more detail we sought to rank image capture events for how likely they are to observe the person which is responsible for the beverage event. The strongest indicator is if the two events are in the same time window but which time window should we use? A fun side note to all of this:  We located a machine for our team to visit that was like the one used to create the source data.  All members of our team visited this machine many times as we were thinking about the data.  Each of us ran our own tests that probably created outliers in its data for whoever is collecting it.  One member of our team actually did a stakeout of the machine for two hours and recorded fortyfive observations of how long it took for people to dispense a beverage.  The minimum was 7.5 seconds and the average was about 44 seconds.  But there was a lot of variance in the data and this observation set was rather small.  To compensate for this data was randomly sampled 10000 times thus creating a larger mixed distribution from which the standard deviation (sigma) could then be used for linking.We assumed the time needed to get a beverage is normally distributed and used a 4 sigma time window to do a first filtering on the image capture events for linking them to a beverage event. Within the time window of a beverage event we used a formula to rank the image capture events which determines which persons are most likely to be the customer.  Better results and insights could have been obtained with more research but we were limited in both the source data we had to work with and time was a factor.  Consequently we simplified the join formula down to the following:For age and gender: the system could record a person as male in 8 frames and female in 2.  It could have contradictory ages (some records saying one age / some saying another) and for each age recorded an “age_c” confidence interval of +/ 5 years or in the extreme +/ 12 years was provided.  To clean the gender field it made sense to either take the value that came up more or when this was impossible to determine (or missing completely) rely on domain knowledge for a default.  For this data male was more likely than female.  To clean up the age data the process gets a bit more complicated.  A missingness analysis of the data proved useful not just for imputation but for coming up with an overall strategy for the emotion data.Charts like these . In this case if you focus on blue (present) versus red (missing) you will quickly see that the most complete data of the 4 emotions is for Happy / Angry.  Drilling into the data itself to investigate we saw that surprise was barely there often only appearing for a single frame.  If we wanted to use sad or surprise data the sparseness would be a real challenge.  Note too that current research holds there are 6 main emotions that psychologists now combine into .  This system simplified emotion to just the 4 basic ones shown:  Happy Angry Sad and Surprised. They were presented as numbers on a scale of 1 to 100 (almost like “percent happy”) but each record was independent of each other. The machine could believe a person was 80% angry and 80% happy based on two image frames captured within seconds or fractions of a second of each other.  Taking into account the missingness and uncertainty of the emotions data our team decided to employ  as a strategy to simplify the data into three categories: “Positive” emotions “Negative” emotions and “Unknown.” This made working with the data overall much easier and helped facilitate the analysis results which follow:The x/y axes show the dominant emotions driving the Positive / Negative colors in the clusters but really each data point in a given cluster becomes a hybrid of the different values melding into the cluster (Happy Angry Sad Surprise in this case).One of our more fun observations:  The oldest males in the demographic and the youngest females seemed to prefer the same flavor in their mixes.  This was presented to the beverage company in  like the one shown here:In this case the red box in the top plot represents the same flavor as the purple box in the bottom box plot.  There was a median age in this data set that hovered around the middle 40’s and the range of ages present was mostly between the midthirties to midsixties.  Data was skewed so that about 75% was male while only 25% was female.When creating and presenting visualizations  the code included a label for how many records from each source table underlied the observation.  Only a small number of records from our already somewhat limited sample were at the core of this particular finding.  So while the male/female flavor correlation is interesting it cannot be generalized to larger groups of people.  There was also an overall trend in the data that older beverage consumers seemed to like flavors that were less sweet than younger beverage consumers.  Experimentation with mixing of flavored syrups with a core beverage tended to happen more when the offerings were carbonated than when the offerings were not carbonated.Only 4 flavors were offered to mix with 14 base beverages. Beverage consumers (in this data sample) opted to try 74 distinct recipes or mixes.  Over 90% of this mixing occurred within the top 10 choices and as you progress down the list of mixes the numbers drop off sharply rather quickly. The xaxis of this chart had beverage names that have been omitted out of respect for our NDA:When we revealed the top choice of beverage and/or beverage and flavor mix our research seemed to agree with research already performed by the data science and marketing teams of the company we presented this to.  Diet was preferred by older drinkers over younger drinkers and seemed to be preferred by women over men in our sample.  Women in general seemed to like sweeter flavors but an association to positive emotions did occur for a sour flavor in the youngest age group for both genders.  Women also had a stronger preference for water than men; ordinary water was the number 5 choice out of all recipe / mixes selected.As an experiment data was enhanced with an ingredients table downloaded from a website relating this data to the same or similar beverages as those in our sample. There is known bias in the data and the R square values confirm that the linear models used hold no predictive value.  But the goal of this research was more descriptive in nature.  Multilinear models were used to identify what ingredients might represent significant factors for consideration and this significance is backed up by the pvalues produced by the models. In this context there were some interesting findings to share with the beverage company:In presenting these final findings we remind the reader that we had the most data on Happy/Angry and that the way this data was collected we really have no way of knowing the causal relationships behind observations: It should also be stated that data came from a single machine and was collected over a 4 day period.  A limited sample such as this may not be representative of larger populations (the American marketplace for example).  In statistics we say the data has bias but in this case the bias is a simple reality of the opportunity for data collection behind this particular experiment.  The opportunity for “nonbiased” data is not always available.Given all of this findings should be treated as descriptive rather than predictive.  They provide useful insight into the data collected and can still be used in that context.  The process and code behind these findings can also be used as a proof of concept should a larger more representative data sample become available.When conducting data science research you can’t help but construct a wish list of what would make this research better and more useful if it were possible to do this in the future.  For this particular area of research:We acknowledge however that these ideas may be harder to do in the real world then they may seem.  Limitations in equipment and budgets can create challenges and complexity not readily apparent to an outside observer.We wish to end by thanking The NYC Data Science Academy and the beverage company who provided this data for this intriguing opportunity to problem solve a real world data scenario and ultimately explore this data.  We all learned a lot from the experience and are grateful to have had this opportunity.,NA,I’d Tell You But Then I’d Have to Kill You: Secret Beverage Data Science Research
https://nycdatascience.com/blog/student-works/scraping-forbes-ratemyprofessors-create-college-recommendation-app/,24,I created a web application using data collected from  and  as a tool for prospective college students and their families. The application conveniently synthesizes general statistics with student sentiment at elite American colleges and universities.The user is able to input their SAT score budget desired student body size and/or preferred state in order to find schools that would be an appropriate match. From those schools the user can select two and see how the RateMyProfessors ratings are trending over time in any given rating category as well as a sidebyside comparison of the schools with statistics from Forbes and average ratings from RateMyProfessors.The application was created using R Shiny and is available .                           The data was scraped using the Selenium and Beautiful Soup Python packages.When a school's review page is visited on RateMyProfessors 20 reviews at most are visible in the browser or the HTML code. To load 20 more reviews the user must click the “Load More” button located at the bottom of the page. In order to load all the reviews the user must repeat this process until the “Load More” button disappears.While Selenium in general is a slower scraping method it is necessary in this case.  The Selenium WebDriver Object interacts with the webpage by locating the “Load More” button and continuing to click that button until all school reviews are visible in the HTML code.Once fully loaded the entire HTML code is saved as an Object which Beautiful Soup can then parse. It is possible to do this parsing in Selenium but Beautiful Soup is much faster.Two techniques were integral in the scraping of RateMyProfessors and can be useful in scraping numerous other websites. I have described them below. RateMyProfessors has numerous advertisements that significantly increase the page load time and in some instances inhibit the browser’s ability to scroll on the page without exiting out of the ads. Even if you have an adblocker installed on your normal browser the automated Selenium browser will not have it installed automatically. The following code snippet adds AdBlockPlus to an automated Chrome browser.The Selenium click command does not execute unless the load button is scrolled into the browser’s current view. The normal Selenium commands for scrolling an element into view were not responding; however Selenium commands did yield the correct location of the button. I used the following code to execute the JavaScript command that loads the desired location into view and subsequently uses Selenium to click the button.,NA,Scraping Forbes and RateMyProfessors to Create a College Recommendation App
https://nycdatascience.com/blog/student-works/nyc-leading-causes-death/,24,"Being a yoga teacher I am a firm believer that health is determined by a person’s individual behaviors. However factors like ethnicity and sex are cannot be chosen and impact the probability of death from a certain cause.The goal of the project was to determine what trends exist in the leading causes of death when looked through the lens of ethnicity and sex. I decided to focus on the four major ethnicities (White Hispanic Black and Asian) that reside in my hometown New York City. To conduct my research I used data  whichUsing dplyr I deleted any information pertaining to ethnicities that were not clearly defined by race (""Other Race/ Ethnicity"" and ""Not Stated/Unknown""). I then renamed the factors under Ethnicity and Sex to more simpler terms. Afterward the number of deaths and death rates were transformed from character strings to numeric values.In the sevenyear span 418760 people died. Of the total 49.3% of them were White 26.5% were Black 17.9% were Hispanic and 6.29% were Asian. In regards to sex 51.2% of deaths were women. Interestingly more White women died than all Asian Black and Hispanic women combined.",NA,NYC Leading Causes of Death
https://nycdatascience.com/blog/student-works/delivery-reviews/,24,Another significant change in the last years was the market for food delivery that keeps growing with the creation of several websites and apps delivering meals from restaurants that sometimes haven’t traditionally offered the option food togo. For restaurant owners the extra business is often welcomed but introducing a third party can create a large number of problems.,NA,Food delivery: a new revenue source but also more complexity to manage
https://nycdatascience.com/blog/student-works/san-francisco-restaurant-inspection-analysis-visualization/,25,Moreoverthe health department would be able to anticipate when a problem arise. For example  in a scenario where total high risk violations is greater than low moderate or no risk violations for a certain period of time the health department can plan beforehand and act accordingly to prevent a possible food related crisis.,NA,San Francisco Restaurant Inspection Analysis and Visualization
https://nycdatascience.com/blog/student-works/runners-data-insights/,25,DataTo help with my research I used data from Mani’s running group. The data was collected by a group of competitive runners for over a period of nine years. The group trained to run the JPMorgan Chase corporate challenge and gradually moved to other challenges. Training runs with group or ‘speed runs’ as they call it are to challenge and motivate. The group’s coach would then collect information from each runner and enter it manually into excel to yield calculations like pace speed and to provide comparative analysis. The group also uses running apps like runkeeper or mapmyrun.Research QuestionsFor my analysis I categorized the runs as ‘Speed Runs’ ‘Corporate challenge’ and ‘other races’ based on the race dates.  I selected two runners one who trained regularly and one that did not but still ran other races. With that I attempted to answer the following questions:InsightsFirst to get a sense of what the pace looks over the years I selected one runner who trains with group regularly and has been running corporate challenge for a long time and eventually ran other races.The below graph uses plotly to trace the average pace over the years grouped by the race type.Note:Runner 1: trains regularly.,NA,Runner Data Insights
https://nycdatascience.com/blog/student-works/citibike-business-opportunity-advertising/,25,                                                               1)West St & Chambers St        2) Broadway & E 22nd St        3) Broadway & E 14th St         6PM         Monday     ,NA,Citibike Business Opportunity: Advertising
https://nycdatascience.com/blog/student-works/the-growth-of-startups/,25,The Startup scene has seen dramatic change over the last decade. Barriers to entry have become increasingly lower as Investors are constantly looking for the next big opportunity. With billions of dollars being invested every year small startups have turned into major players in today's global economy. It is important for both the investor and the entrepreneur to visualize the changes occurring in this market so that way they can make the right decisions when it comes to investments and capitalize on key trends and performers.The data collected was from Crunchbase.com which is a online resource for learning about companies and investments. The file I used contained information from Q1 of 2004 to Q1 of 2014 on over 630000 investments. I was provided the name and location of both the company and the investor along with the funding type and amount of money in USD. From this information I hoped to gather the following details:As expected US has had a firm dominance over the rest of the countries in terms of volume and size of investments. Silicon Valley rules the startup scene with some other major States including NY Texas and Massachusetts. However through my research I realized investors soon started finding opportunities elsewhere. During the later half of these 10 years there was a lot of new startups coming up in emerging markets such as Latin America Eastern Europe and Africa. Even within the US States with secondary cities such as Colorado and North Carolina started receiving major investments. The map below shows how spread out investments are located in 2013. With money spreading into new areas new economies are booming which leads to more opportunities for both investors and ventures.Fueling this global change are Angel Investors Venture Capitalists and many other types of institutions. Usually angel investments are capital provided at the beginning stages of a company's growth these are then followed by a rounds of series investments(ABC+) and then finally once a company has scaled large enough and has shown potential for high returns Venture Capital and Private Equity groups will invest with usually very high amounts of capital fundingAngel Investments have consistently increased throughout the decade with other forms of investment still trending upward but showing more signs of volatility. Since VC and PE firms are much larger the amount of risk they take is more dependent on economic conditions and fiscal policies For example during the financial crash we saw slight decreases in overall spending.When observing specific industries I started off with over 40 different industries. In order to visualize high performers I narrowed down the categories to investments that have surpassed $5 Billion USD per year. In my finding there is no surprise that software has outperformed many of the other categories. But surprisingly Biotech has been a consistent high performer almost every year. I believe that this is due to the advancements in genetic research and changes in the Healthcare Industry (ex. ObamaCare).We slowly start to see new industries come into the market and remain as valuable opportunities to invest. For example ECommerce and Enterprise came up in 2011 and stayed at high investment opportunities. I think that moving forward we can start to see uptrends in other categories such as Education and Analytics (shown below) and I believe those will break the $5 Billion barrier in the years to follow.  Overall we have seen consistent growth in the industry and I believe that more opportunities will continue to show up as the market expands into new countries and cities. Silicon valley will continue to be the center point of ideas but as emerging markets start to see more companies grow investments will continue to increase in activity as investors will find huge potential in these ares. Finally Software and Biotech will continue to be your safe bet for investment. However other industries such as education media sports/games are starting to create new technology services which will soon raise their value. Right now it is possible to find undervalued situations with lots of upside if the industry moves in the right direction. Crunchbase: DataWorld: ShinyApp:,NA,The Growth of Startups
https://nycdatascience.com/blog/student-works/machine-learning/zillow-prize-competing-improve-zestimate/,26,Zillow is a popular online real estate and rental marketplace dedicated to providing customers with data to make the best possible housing decision. In the “Zillow Prize” Kaggle competition Zillow released data (3 million observations) with the hope that competitors could use it to help improve the accuracy of the Zestimate their house pricing algorithm. Our objective was more complex and indirect than simply predicting housing prices from given data though: we were instructed to predict the log errors in the Zestimate defined as log(Zestimate)log(Sale Price). Although the Zestimate already predicts house prices within 5% accuracy machine learning models that closely predict the log errors of the Zestimate would allow Zillow to improve their algorithm further in the situations where it over and underestimates. The first challenge we faced in the Zillow competition was the cleaning and management of the data. We were provided data on 58 features associated with each property. Many of the variables were completely or partially redundant and over 50% of them had some degree of missingness sometimes over 97%. Our most fundamental decisions involved which variables to drop and how to interpret existing missingness.In order to understand the fundamental relationships within the data we created tableplots in which we could visually inspect relationships of all variables with log error before removing any missingness. The tableplots suggested variables related to property location and tax would be especially important in our models. We also created correlation plots in order to understand the preexisting relationships between variables. Since some machine learning models do not handle multicollinearity well we eliminated highly redundant variables and used techniques such as PCA and regularization in multiple linear regression to further minimize redundant information. While modeling we used an iterative process in which we ran various treebased models to obtain outputs on variable importance. These results helped us determine which variables could be dropped and which should be kept and imputed in a more sophisticated way. The strategy for handling missing data was one of the key challenges of this project.  Our approach to this problem was to systematically assess each variable in order to better understand the reason for missingness.  These variables can be separated into two broad categories: first those in which the reason for the NA is implied by the data and second those for which the reason for the NA is unknown.  For several of the variables with high missingness we believe that missing data points simply implied that the property does not contain the item in question.  These variables contain a distribution of ones signifying that the property contains the item intermixed with NAs.  These categories include topics such as tax delinquency lot size fireplace pool garage basement and hot tub; it makes intuitive sense that many of the properties in southern California do not have some or all of these features.  Thus we imputed NAs in these categories with zero to signify that the feature is not present for those observations.  For other variables the reason for missing data is less clear.  For features such as unit count bedroom count bathroom count number of stories or year built missingness was relatively high and it does not make sense to consider an NA as a feature that is not present for a given observation.  Imputation is more problematic in these cases as KNN can not be realistically calculated on the full Zillow data set within the time allotted for this project and mean/median imputation can change the distribution of the variable which could induce further error into the predictions. We also hypothesized that missingness in these variables could help to predict log error in sale price vs. Zillow’s estimate as it is possible that Zillow has more difficulty predicting value for an observation that is missing more variables.  Thus for these variables we imputed with 1 to preserve any possible hidden reasons for missingness in these observations.  We used mean imputation for the calculated finished square feet variable as this variable is nearly complete so imputing with the mean value has a negligible effect on the overall distribution of the variable.  We dropped the other floor space variables as they are either entirely redundant or highly missing.  For the location variables missingness was also very low so we imputed by randomly sampling from the distribution of each variable.  Given the importance of the tax assessment features we took care to make informed imputations for missing values. For properties with property taxes paid values we divided these values by the median tax rate (these were fairly consistent) across all properties to get an estimated assessment value. Over 99.9% of these cases had zero bathrooms and zero bedrooms so we imputed the estimated assessment value as the land assessment and imputed zero for the building assessment. For properties with no taxes paid values we imputed the average building and land assessment values grouped by zip code number of bedrooms and number of bathrooms  assuming that properties with the same number of beds/baths in the same zip code were likely to have similar land and building assessment values.Since some machine learning models require normallydistributed scaled data we transformed certain variables for these cases. Before performing linear regressions with regularization we were careful to check the distribution of each variable transform it if necessary and then scale it. One effective way to do this was to employ a skewness function on each variable: In this function c is a constant added to the data before log transformation. Our goal was to find the value of c which gave a skewness value as close to zero as possible. Then we plotted the data visually to inspect the results of each transformation.The multiple linear regression process was conducted iteratively starting with selected features that intuitively made sense and subsequently trimming the number of features used. The first linear model tried included variables related to amounts of land and structure taxes paid whether there had been a tax delinquency in the past and ones pertaining to the overall structure of the houses (numbers of bathrooms bedrooms garages and types of hot tub or pool if any). The result was poor as the Rsquared of the model was below 1% and several features had high VIFs suggesting multiple instances of multicollinearity. In addition the QQplot and residual plots showed that assumptions underlying linear models were violated including heteroscedasticity and lack of normality of the residuals.As several features tried in the first model also had large pvalues the second attempt removed those that had both high VIFs and large pvalues. However the result didn’t yield any improvement as the Rsquared remained under 1%. Although VIFs across the features that remained were within more normal ranges the model as a whole didn’t provide adequate explanatory power.Further trimming the model continued to demonstrate promising signs of improvement which led us to add regularization to the model. Shrinking the coefficients did produce better results as both ridge and lasso regression models generated MSEs that were both reasonable. The final model picked for presentation included only six features: finished square footage of the property square footage of lot tax amounts number of bedrooms and number of units. However since the linear assumptions were still violated meaning fitting a linear model – however modified – to the data was simply not the best method.We used grid search and cross validation to optimize our Random Forest model on three key parameters: number of variables considered at each split (mtry) number of trees to grow for each model (ntree) and minimal size of terminal nodes (nodesize).  We left the maximum number of terminal nodes (maxnodes) to grow to the level determined by the nodesize parameter.  We also considered variable importance by testing versions after eliminating some of the less important variablesDue to the high computational complexity of the random forest model on large data sets we initially sampled 1025% of the Zillow data to more efficiently narrow down the search for optimal tuning parameters.  We then used a 75%/25% training/test split on the full data set for precise model tuning.  The optimal Random Forest model featured mtry  2 ntree  1000 nodesize  12 observations and included all variables generating an MAE of 0.0658.  Despite the time invested in cross validating and tuning to arrive at the optimal parameters this result was outperformed  by several other models (both simple and more complex).  While the result from the standalone Random Forest model was disappointing this model was useful in determining variable importance to assist in variable selection for other models and was useful as an input for our final ensemble model.  Gradient boosted models are ensembles of decision trees the same fundamental structure used in random forests. Instead of averaging the predictions of hundreds of independent trees boosting takes an iterative approach  build a tree calculate the output of your objective function (incorporating training loss and regularization) and use this output as an input for the subsequent tree’s objective function. Boosting adjusts the weights associated with each split in a tree based on the error from the prior tree  slowly descending along the error gradient (gradient descent) until the algorithm can settle on a local minimum that optimizes your objective function. A “learning rate” hyperparameter lets you decide the rate of descent. Some other important hyperparameters are the number of trees maximum tree depth and minimum number of observations per leaf. Gradient boosting performs well for most machine learning problems. One drawback relative to random forests is boosted trees are necessarily dependent so your model may suffer from overfitting.This is an oversimplified description of the gradient boosting process. For more info the creators of XGBoost have an excellent description .Analytics Vidhya also has a great tutorial on hyperparameter tuning in gradient boosted models . Our team used sklearn’s gradient boosting regressor on a number of numeric and categorical variables resulting in our most successful model with a mean absolute test error of .0516. We used 5fold grid search cross validation to choose hyperparameters. We also used XGBoost a package that 1) has computational advantages over sklearn’s gradient boosting (i.e. it runs faster) and 2) includes regularization at each step of the process to control for overfitting. This  has a great description of XGBoost. Analytics Vidhya’s hyperparameter tuning tutorial .Our best XGBoost model tuned using 5fold grid search cross validation and using the same features as the sklearn gradient boosted model (GBM) performed slightly worse than the GBM with a mean absolute test error of .0529. As we mentioned before XGBoost regularizes at each step. Ridge regularization performed better than Lasso or Elasticnet.While GBM and XGBoost gave us our best performing models we noticed that both were conservative in their error predictions. The below graph compares actual logerror distribution to the predicted logerror distributions using our best GBM and XGBoost models.kNN is another nonparametric regression method we decided to explore. kNN calculates the “distance” (typically either  or ) between each observation based on their features. Observations with smaller distances between them are closer “neighbors”. kNN takes the k closest neighbors and averages the logerror (or whatever outcome variable) among them as the prediction. kNN can also compute weighted averages based on distance (i.e. closer neighbors count more). We thought this algorithm made the most sense given our problem. When people buy and sell houses they typically look at houses that are most similar (“comps”) to inform pricing. kNN identifies the k most similar properties and uses only those properties to inform the logerror prediction. We used 10fold cross validation to choose the k (the number of neighbors) and landed on 10 right around the elbow of the below graph.Our initial kNN model had a fairly low mean absolute test error of .0567 and the distribution of predicted logerror values was more consistent with the actual distribution of logerror values  meaning it didn’t underestimate as badly as the GBM and XGBoost models (see below graph). However kNN scales poorly with more observations and features. When we used this model to predict the values for the entire properties dataset (over 3 million rows) it took over 10 hours and our computers crashed. As a next step we hope to use dimensionality reduction techniques and a more powerful computer to put this kNN model into production for the Kaggle competition.With all of these regression models in hand we ensembled the five highest performing ones  averaging the predicted logerrors of each model to inform the final logerror to submit to Kaggle. The theory behind ensembling is that incorporating the predictions of multiple models results in less biased predictions. You are seeing the problem from different viewpoints and can leverage the strengths of each perspective. Our ensembled predictions performed markedly better on Kaggle than our best single model the GBM  consistent with our expectations.,NA,Zillow Prize: Competing to Improve the Zestimate
https://nycdatascience.com/blog/student-works/citi-bike-visual-exploration/,26,"Citi Bike is New York City's bike share program that started in May 2013 and has expanded rapidly ever since.  Operated by Motivate the nearly ubiquitous blue bikes have become a highly visible component of daily traffic flow in many parts of the city. Citi Bikes are now available in over 600 stations spread throughout most of Manhattan and parts of Brooklyn and Queens. As of August 2016 Citi Bike has even expanded across the other side of the Hudson River to Jersey City.One of the major challenges of operating such an extensive network of bicycle stations is making sure that bicycles are available to check out when users need them and that docks are available when users arrive at their destination otherwise known as the ""rebalancing problem"".  One could easily imagine that the rebalancing problem is exacerbated in the summer months during the height of tourist season when in addition to the typical Citi Bike commuter there are also many outoftown visitors using the bike share system to explore the city.Given the complexity of the rebalancing problem my goal for this Shiny app project is modest.  Citi Bike operates on a simple pricing system in which one can sign up for an annual membership of $163 per year (""subscriber"") or one can pay $12 for a oneday access or $24 for a threeday access (""nonsubscriber"").   Using data visualization I attempt to gain some insight to the ridership behavior of the latter group of users who tend to be visitors to the city and who contribute to the increase in volume of Citi Bike trips especially during the summer weekends.  In particular I attempt to gain a better understanding of which areas nonsubscribers concentrate in how this varies over the day and the flow of bike usage at particular stations in select areas. It is hoped that the analysis will provide some level of assistance to those interested in improving the experience for all Citi Bike users.The datasets used in the analysis are from the January 2017 and July 2017  Citi Bike Trip Histories downloadable files located in the System Data section of Citi Bike’s website ().  From these datasets I excluded those trips that were over 60 minutes in duration as well as those observations in which the value for the “usertype” field is missing. I also excluded those observations that appear to be related to Citi Bike’s transference of bikes to and from their operations facilities in Brooklyn and Manhattan.To get a sense of how nonsubscriber ridership volume changes from winter to summer relative to subscribers I compared their share of total average daily trips in January and July of 2017.  As can be seen in the following graph their share of total average daily trips increases more than twofold during the weekday and the weekend. It is also interesting to note that with the exception of the first week in January nonsubscribers’ share of weekend trips is consistently higher than its share of weekday trips for both January and July. However in July the weekend spike in volume appears to be more dramatic.To arrive at a more detailed understanding of where this overall increase in summer ridership is occurring I added markers to the following Leaflet map showing the top twenty Citi Bike stations in terms of percentage increase in number of trips that started or ended at the station. It is not wholly surprising that almost all the markers are along the waterfront (Hudson River Greenway Brooklyn Bridge Park and Red Hook) or along the parks (Central Park and Prospect Park West).   It is worth noting that even if we expand the list to the top fifty Citi Bike stations the vast majority of these stations are in the same aforementioned areas.Given that weekends are especially popular for nonsubscribers a breakdown by neighborhood of where these Citi Bike users start their trips may uncover some interesting trends. For this analysis I overlaid neighborhood boundaries defined by a  GeoJSON file managed by Ontonodia on top of the Leaflet map  shown previously.  I then tabulated the number of trips started in each neighborhood for each time period of the day and shaded the neighborhoods based on the number of trips.  From  9 a.m. to 9 p.m. Central Park is the most popular neighborhood among nonsubscribers followed by the Upper West Side and Midtown. This is in marked contrast with the ridership behavior of subscribers who consistently concentrate in Chelsea and East Village during the same time period. For illustrative purposes the following two images compare the neighborhood concentrations of subscribers and nonsubscribers during the 3 p.m. to 6 p.m. time period.  The darkest region on the left image is Chelsea and the darkest region on the right image is Central Park.The results of the previous analysis served as a motivation to examine the flow of Citi Bike usage for the stations around Central Park on the weekends.  For each time period of the day I calculated the net inflow or outflow of Citi Bike trips at each station represented by a circle on the map. A green circle means that there is a net inflow (more Citi Bike trips end at the station than start at the station) whereas a red circle means that there is a net outflow (more Citi Bike trips start at the station than end at the station). The size of each circle corresponds to the relative magnitude of the net inflow or net outflow. The purpose of the analysis was to gain a sense of where bike availability may or may not be an issue.  The analysis suggests that there may be a surplus of available bikes in the weekend mornings whereas beginning at 3 p.m. there may be an issue with bike availability at many stations on both the eastern and western perimeters of the park.  For illustrative purposes please refer to the following two images. The first image represents the time period from 9 a.m. to 12 p.m. and the second image represents the time period from 3 p.m. to 6 p.m.Central Park is not the only area in which there is a marked contrast between nonsubscriber and subscriber ridership behavior. During the weekday in July nonsubscribers are responsible for approximately only 13 percent of interborough Citi Bike trips (trips that begin in one borough but end in another). This figure jumps to approximately 32 percent in the weekend. This is most likely driven by the decrease in Citi Bike users commuting to work during the weekday.  However  it is also driven by the fact that average weekend interborough bike trips is more than double the weekday interborough bike trips among nonsubscribers. Using a similar analysis employed for the Citi Bike stations in Central Park described previously I examined the flow of trips among the top ten Citi Bike stations used by nonsubscribers during the weekend and represented the results visually using the same types of markers as those used in the Central Park analysis. Based on the analysis there is a net inflow of Citi Bike trips in the morning and early afternoon whereas there is a net outflow at most of these stations during the evening starting at 6 p.m. Again for illustrative purposes I have included the following two images to contrast the flow of Citi Bike trips among these interborough stations. The first image is for the time period  from 9 a.m. to 12 p.m. and the second image is for the time period from 6 p.m. to 9 p.m.With the aid of the Shiny package certain intuitions and hunches about Citi Bike ridership behavior were corroborated by data visualization tools and techniques. A simple mapping of Citi Bike stations that experienced the greatest increase in activity since January clearly showed a preference among all Citi Bike riders for the waterfront areas and the parks. Furthermore choropleth maps allowed easier comparisons between nonsubscriber and subscriber activity on the weekends across the various neighborhoods with nonsubscribers clearly preferring Central Park throughout most of the day whereas subscribers concentrate further to the south in Chelsea and East  Village.  Detailed analysis at the Citi Bike station level in Central Park also revealed a net inflow of Citi Bike trips at most stations in the morning whereas beginning in the midafternoon most of the Citi Bike stations in this area experienced a net outflow.  And finally although nonsubscribers do not play a significant factor in interborough trips during the weekday they comprise almost a third of all interborough trips during the weekend. The dynamics of net inflow and outflow of Citi Bike trips from the most popular interborough Citi Bike stations are similar to the dynamics found at Central Park with a net inflow of trips in the morning but a net outflow towards the late afternoon and early evening.  My experience on this project demonstrated for me the power of interactive data visualizations to corroborate or disconfirm certain beliefs as well as to serve as catalysts for other avenues of research and analysis.  The sensitivity of Citi Bike nonsubscribers to season day of the week time of day and location served as a great opportunity to deploy a data visualization tool such as Shiny to study their behavior.  As Citi Bike continues to expand to other areas of the city the rebalancing problem is certainly expected to remain a challenge to providing the best experience for all Citi Bike users.  However  I am optimistic that  problems that once seemed intractable will become less so as data science continues to advance and tools are developed to address these issues.",NA,Citi Bike Visual Exploration
https://nycdatascience.com/blog/student-works/identifying-fake-news-nlp/,26,What is fake news? We’ve all heard of it but it is not always easy to identify. Fake news is a type of yellow journalism or propaganda that consists of purposeful misinformation. It has traditionally been spread through print and broadcast mediums but with the rise of social media it can now be disseminated virally. As a result large technology companies have begun to take steps to address this trend. For example Google has adjusted its news rankings to prioritize wellknown sites and has banned sites with a history of spreading fake news. Facebook has integrated fact checking organizations into its platform.How significant is this issue? Buzzfeed  the 20 mostshared fake and real news articles leading up and relating to the 2016 presidential election. They found that the top fake stories had more engagement on Facebook than the top real stories.Our goal for this project was to find a way to utilize Natural Language Processing (NLP) to identify and classify fake articles. We gathered our data preprocessed the text and converted our articles into features for use in both supervised and unsupervised models.We knew from the start that categorizing an article as “fake news” could be somewhat of a gray area. For that reason we utilized an existing  that had already collected and classified fake news. The articles were derived using the B.S. Detector a browser extension that searches all links on a page for references to unreliable sources and checks them against a thirdparty list of domains. Since these fake articles were gathered during November 2016 from  a news aggregation site we collected our real news data from that same site and timeframe. To ensure we did not include articles from questionable sources in that dataset we manually identified and filtered on a list of reliable organizations (i.e. The New York Times Washington Post Forbes). In the end our final dataset included over 23000 real articles and 11000 fake articles.The performance of a text classification model is highly dependent on the words in a corpus and the features created from those words. Common words (otherwise known as stopwords) and other “noisy” elements increase feature dimensionality but do not usually help to differentiate between documents. We used the  and  packages in Python to tokenize our text and perform the following preprocessing steps:These steps helped reduce the size of our corpus and add context prior to feature conversion. In particular lemmatization converts each word to its root form turning different words into a single representation. Ngrams combine nearby words into single features which helps give context to words that may have little meaning on their own. For our project we tested both bigrams and trigrams.To analyze and model text after it has been preprocessed it must first be converted into features. Techniques may include TFIDF or Word2Vec.TFIDF is a statistic that aims to reflect how important a word is to a document in a corpus. It increases proportionally with the number of times a word appears in a document but is offset by its frequency in the overall corpus. While TFIDF  is a good basic metric for extracting descriptive terms it does not take into consideration a word’s position or context.Using TFIDF we found the relative importance of words in both our fake news and real news datasets. There was significant overlap between the two  “trump” was the most important word in both types of articles and words like “clinton” “fbi” and “email” also ranked highly.The Word2Vec technique converts text to features while maintaining the original relationships between words in a corpus. Word2Vec is not a single algorithm but a combination of two techniques – CBOW (Continuous bag of words) and the skipgram model. Both are shallow neural networks which map word(s) to the target variable which is also a word(s). Both techniques learn weights which act as word vector representations.The quality of word vectors increases significantly with the amount of data used to train them so we used  trained on the Google News dataset (about 100 billion words). The model contains 300dimensional vectors for 3 million words and phrases. We averaged the word vectors within each article to get a single vector representation for every document.Since our data was not split evenly across both classes we chose metrics that would not overstate our results when evaluating our models. A confusion matrix is useful for gauging outcomes in classification problems. Since our goal was to recognize fake news articles the ones we correctly classified as fake are our True Positives and the fake articles we incorrectly classified as real are our False Negatives (Type II error). Real articles that we correctly classified are our True Negatives and incorrectly classified real articles are our False Positives (Type I error).To build an effective model our goal was to minimize both the False Negatives and False Positives. The F1 score helps strike a balance between precision (fake articles classified correctly over the total number of articles predicted as fake) and sensitivity/recall (the proportion of fake articles classified correctly). For that reason we used the F1 metric as our optimization parameter when using crossvalidation to tune our hyperparameters.Finally we used the ROC AUC score to visualize our model results. The ROC graphs the True Positive rate (sensitivity/recall) on the yaxis and the False Positive rate (real news articles that we classified incorrectly) on the xaxis.We utilized crossvalidation and a grid search to find the best parameters for the TFIDF algorithm and each individual model. The Logistic Regression and Support Vector Machine models produced the best results using TFIDF to convert our text to features. However the Logistic Regression model was much faster to train which is important from a timecomplexity standpoint when evaluating model performance. Seen in the ROC graph below the Logistic Regression model had a high sensitivity (it predicted fake news articles quite well) and a low False Positive rate (it did not predict a large portion of real news as fake).We also trained each model using Word2Vec to convert our text to features but the results were worse across all model types.Since our articles covered a wide range of topics we utilized unsupervised learning to better understand our data. Topic modeling allows us to describe and summarize the documents in a corpus without having to read each individual article. It works by finding patterns in the cooccurrence of words using the frequency of words in each document.Latent Dirichlet Allocation (LDA) is one of the most popular models used in NLP to describe documents. LDA assumes that documents are produced from a mixture of topics and topics are generated from a mixture of words associated with that topic. Additionally LDA assumes that these mixtures follow a Dirichlet probability distribution. This means that for each document we can assume there should only be a handful of topics covered and that for each topic only a handful of words are associated with that topic. For example in an article about sports we would not expect to find many different topics covered.The graphic above illustrates this process. For each document the model will select a topic from a distribution of topics and then a word from a distribution based on the topic. The model will initialize randomly and update topics and words as it iterates through every document to find a certain number of topics and associated words. The hyperparameters alpha and beta can be adjusted to control the topic distribution per document and word distribution per topic respectively. A high alpha means that every document is likely to contain a mixture of most topics (documents will appear more similar to one another) and a high beta means that each topic is likely to contain a mixture of most words (topics will appear more similar to one another).LDA is completely unsupervised but the user must provide the model with a specific number of topics to describe the entire set of documents. For our dataset we chose 20 topics. As shown below topics are not named but we can get a better understanding of each topic by looking at the words associated with each.Based on the words associated with Topic 2 it seems to be related to the election. The distance between topics directly relates to how similar topics are to one another. As we can see Topic 15 is far from Topic 2 and likely relates to the arts.For our final model we generated a stacked model using the predictions from our original seven models. Stacked models often outperform individual models because they can discern where each performs well and where each performs poorly. We also added our topic modeling results as new features as well as the length of each article and whether it had an author.These features in combination with logistic regression gave us quite good results  only 34 of our fake articles were misclassified from a test set of 2208 and our AUC score was .9876.The rise of fake news has become a global problem that even major tech companies like Facebook and Google are struggling to solve. It can be difficult to determine whether a text is factual without additional context and human judgement. Although our stacked model performed well on our test data it would likely not perform as well on new data from a different time period and topic distribution. The chart below displays the “most fake” words in our dataset determined by looking at words that were proportionally used much more often in fake news than real news.Words like “hillary” “clinton” and “email” were used much more frequently in fake news with a ratio of almost 2 to 1. Therefore our model might have trouble classifying new real articles about those subjects correctly because they are so prevalent in fake news.Writing style is also crucial to separating real news from fake. With more time we would revisit our text preprocessing strategy to maintain some of the style elements of our articles (i.e. capitalization punctuation) and improve performance.Link to our .,NA,Identifying
https://nycdatascience.com/blog/student-works/real-time-yelp-reviews-analysis-response-solutions-restaurant-owners/,26,Before trying a new restaurant we frequently consult with review platforms such as Yelp Zomato or Google where we can read comments from previous diners. Reading those reviews helps to make more informed decisions and can lead to a better dining experience with friends and family. It is clear that reviews are influential with diners but how powerful can a single review be? If a business receives one more star in overall rating it can generate a 5 to 9% increase in revenue according to ’s research “Reviews Reputation and Revenue: The Case of Yelp.com”. On the flip side a single negative review may cost a business 30 future customers which implies that an unfavorable comment can significantly damage reputation profitability and trustworthiness of a business.Given that a single bad review can harm a business how can the business owner mitigate the negative impact? One simple solution is to improve the business’s customer relationship management system by raising review complaints to the business’s attention and developing an automated realtime complaint response engine. Our code is available on  and we have also created a to demonstrate the responses in action.  After selecting the problem and possible business resolution we chose the  published on September 1 2017 as our dataset. After inspecting and cleaning the data we conducted sentiment analysis to separate the positive reviews from the complaints. We then used the  package to preprocess the complaint review text and we used the  package to train ngram phrase models and Latent Dirichelet Allocation topic models. Finally we built an automated response chatbot to demonstrate the response to different types of complaints.Determining the sentiment of a review is a deceptively complex problem. As our dataset was unlabeled it was necessary for us to use an unsupervised method to identify the complaint reviews. We considered several options to achieve this goal.  First we considered using the number of review stars as a pseudolabel assuming all one and two star reviews were complaints. However upon further inspection it became clear that different reviewers have different standards; a threestar review from one reviewer may describe  a negative experience while a twostar review from a different reviewer may describe a balanced experience.  Second we considered normalizing each review against the reviewer’s own standards to attempt to control for individual preferences and biases. To do this we calculated the difference between the number of stars given for each review and that reviewer’s average star rating across all reviews. For example if a reviewer’s average star rating is 3.5 a 2 star rating for an individual review would generate a  “stars_dif” rating of 1.5.  However this approach raised problems as well. The mean average star rating across all reviewers is 3.75 which means that the distribution of stars_dif will be skewed negative. We also found a large grouping of data points at stars_dif  0 due to reviewers with a single review or reviewers giving the same star rating across all reviews.  Thus we believe that stars_dif may not be a reliable indicator of sentiment.FInally we decided to look into the actual text of the reviews to determine sentiment. We created a sentiment score rating by tokenizing and processing the text of the reviews using NLTK and calculating the ratio of positive words vs. negative words in the text using dictionaries of sentiment words. We used the following formula for each review:  Each review received a score bounded by 1 and 1; a more negative is closer to 1 while a more positive review is closer to 1.  After we separated the complaints from the positive reviews we built topic models to categorize the complaint reviews based on the subjects discussed in the text. We trained bigram and trigram models using the Gensim package creating phrases by linking individual words based on a ratio of the frequency of their appearance together versus the frequency apart.  After completing ngram transformations we trained a Latent Direchlet Allocation model which is designed to uncover some of the latent topic structure within a corpus of documents. The user selects the desired number of topics as a hyperparameter and the model will then assign to each topic an assortment of the individual tokens from the ‘vocabulary’ of the corpus. The LDA model assumes that the combination of topics within each document and the combination of tokens within each topic follows a Dirichlet probability distribution which means that each document will contain just a few topics and each topic will contain a small number of tokens.In practice the LDA model returns a group of the most frequent words for each topic. Each review in the corpus can be rated with a percentage for each topic and the LDA model can also be used to classify new reviews by the same topic categories.  After testing a variety of inputs we determined that three topics generated the most interpretable results with the greatest separation between topics. We also adjusted the models to eliminate some words which appeared high in the output for all topics in order to improve separation. While the meaning of each topic necessarily involves an element of human interpretation we believe the three topics most closely correspond to complaints about price service quality and food quality.  In order to classify and respond to new Yelp reviews we wrote code to apply these models to a new input. The code uses the models to assign a primary complaint category and generate an appropriate response to the reviewer based on the complaint category. We then returned these responses to the reviewer via chatbot for demonstration purposes (see below). In this map we plot all of the restaurants with negative sentiment score reviews. Users can select the complaint type and slider inputs can adjust the sentiment score star rating and the number of reviews. Notably the clusters are not spread broadly across the country because Yelp has only included reviews from 12 states in the dataset. Detailed restaurant information including restaurant name category average star rating and the number of reviews is available by clicking on the location tag.We divided negative reviews into three categories. Users can click a circle in the left panel to select a topic and the bar chart in the right panel will display the 30 most relevant terms for the selected topic where we define the  of a term to a topic given a weight parameter 0 ≤ λ ≤ 1 as λ log(p(term | topic)) + (1  λ) log(p(term | topic)/p(term)). To further explain the influence of changing the value of λ it adjusts the term rankings  small values of λ (near 0) highlight potentially rare but exclusive terms for the selected topic and large values of λ (near 1) highlight frequent but not necessarily exclusive terms for the selected topic. As the chatbot is built based on LDA results we classify complaints into three categories  price food quality and service quality. Once the chatbot receives a message it runs the text through the NLP preprocessing and classification algorithms ranks the percentage of possible complaint groups and then replies to the review immediately. For the demo video we selected threestar reviews from NYY Steak (Yankee Stadium) which are new information to the chatbot and are viewed as a neutral comment without applying sentiment analysis. Our chatbot Complainteller successfully replies to those negative reviews with appropriate responses.If you would like to talk to Complainteller please feel free to register an account on  and then you can start your conversation with @YelpReviews_bot.  Telegram is a free nonprofit cloudbased instant messaging service; bots are represented as Telegram accounts operated by an automated program. They can respond to messages be invited into groups and be integrated into other programs.For NLP and LDA modeling we could run more tests on removing different common words to improve the topic groups.  We could test more advanced sentiment score algorithms and models to improve the separation of complaint reviews.The chatbot currently responds with a single reply to the most prominent topic. However sometimes customer complaints are mixed problems. We can further improve the chatbot to reply with an appropriate response incorporating multiple solutions. Additionally the accuracy of the result depends on the volume of the text. Therefore creating a common word database and tagging those words into different categories may improve the performance.,NA,Real time Yelp reviews analysis and response solutions for restaurant owners
https://nycdatascience.com/blog/student-works/r-visualization/crypto-currencies-overview/,26,Note: The 'market' column means market capitalization and is the sum of the values of all coins in circulation.Nevertheless the pace of new ICOs globally remains strong.As a few coins clearly dominate trading we will look at their relative share of the overall crypto currency pie as measured by market cap. This graph shows a key trend in the past year: the emergence of other coins such as Ethereum and RippleNote: when we say 'emerge' we are talking about market cap not when these companies started.Finally I plotted the value of the final portfolio as a function of the volume threshold at which I would buy each coin. The gains get exponentially higher as the threshold diminishes. That means that if I buy each coin “right away” say when the daily volume is only above $1000 then some of the coins will experience a meteoric rise and the portfolio will grow more during the holding period. The same conclusion applies here: we would need to check with exchange data if these coins were truly available at such small volumes. At the very least we can conclude that this trading strategy would only be available to small players.,NA,Crypto Currencies as an investment vehicle
https://nycdatascience.com/blog/student-works/washington-dc-crime-shiny-app/,27,2012 2017 Washington DC Crime ReportThe capital city of the United States of America Washington DC does not have a very good reputation for public safety in the history. This shiny app is focused on giving the user information about Washington DC’s crime from 20122017(July 13th).As an International Student I know my personal safety is always the priority concern for family and friends at my home country. This shiny app can also provide information to newly arrived International students in search of a safe area in which to live.This Bar Chart shows total crime recorded from different shifts: morning night and midnight. The user can choose different years to see the change of the quantities of crime that happened on different shifts. Although the specific time range about morning night and midnight are not provided from the data source. We still can observe a relatively low quantity of crime during midnight time range. The Bar Chart gives information about different types of crime that usually occur in the Washington DC area. Assault is a defined as an assault with dangerous weapon or caused serious injury. Theft F/Auto include stealing motor vehicle parts and anything inside of the motor vehicle.  It is very important to find out which area is relatively dangerous and which area is a good place to stay. The Bar Chart provides some information about each district’s total crime quantities.The cluster map is clearly giving information about the number of recorded crimes on every street. It intuitively shows valuable information for the user.The density Map provides another vision of the location of crimes. The user can select different crime types to see which is most common in a particular area.Crimes are said to escalate during holidays. The definition of holidays for this plot is New Year Independence Day Thanksgiving and Christmas. The result actually shows the total number of crimes occurring during holidays is a relatively minuscule percentage of the total number of yearly crimes.The growth rate shows the total quantities of crime is slightly declining since 2014.This shiny app can give the newly arrived international student a very simple and clear information about Washington DC’s public safety situation. It is a very helpful tool for them when choosing a place to live.Link:https://xiaoweicheng666.shinyapps.io/XiaoweiChengShiny/Code: https://github.com/nash13cxw/1stshiny,NA,Washington DC Crime Shiny App
https://nycdatascience.com/blog/student-works/discover-billboard-music-charts/,27,Billboard is one of the most important and famous music charts.  It provides lots of information for both listener and producer. The Billboard music chart used a straightforward cumulative total of yearlong sales and airplay points to calculate each piece’s  ranking position.  I wondered how are the Billboard ranking directly related to the artist's’ production. So I performed some simple analysis to test.      I created eight different Python spiders to scrape different kinds of charts from Billboard. I scraped different genres such as country music electric dancing music rap music rock music and r&b music. I also collect information about top hottest artists every year and the top ranking artist all time(since Billboard’s inception). I used the python seaborn package to visualize my result. I calculate the total quantities of each artist's’’ work that is ranking on the chart for the 20062016 period. I picked the top 20 artists from each chart to visualize more detail. Rihanna has a domain advantage for this decade. As we can discover from the graph she has a total of 30 songs on the Billboard Top 100 songs from this time period.The second graph shows us information about how many songs from the artists are ranked in a particular year so that we can discover how those artists’ work performed each year.This word cloud shows the most popular words from all the top ranking song titles. That indicates which topics are popular in this decade.  The gender chart shows if gender is a factor in appearing on the top 100 chart. It appears about evenly divided between male and female artists. The group category stands for music group or bands.This chart is showing how popular are these artists every year in this decade. We can see that Rihanna and Kenny Chesney are on the chart every year.This swarm plot shows that top 20 popular artist ranking is very stable this decade.I made a cross check with top artists (20062016) with all time top artist. We can see how many times the artist is ranked in the 0616 period. The ranking column shows how the artist ranks in this all time chart. The count column shows how many time they were ranking from the 0616 chart.It shows that the artist's production has no direct linear relationship with the Billboard chart ranking.For the fully detailed code and the result please check with my GitHub: https://github.com/nash13cxw/BillboardAnalysis.,NA,Discover the Billboard Music Charts
https://nycdatascience.com/blog/student-works/carrecommender/,27,Buying a new car is a big and exciting step especially when it is your first car. Research (a new study to be published in the ) has shown that young people tend to be biased in favor of the car brand that their parents own. This tendency might be caused by brand loyalty or the positive experience people had with the car brand; however it could also be caused by the fact that they choose the familiar because they simply do not know how to decide among brands. Even those who actively pursue the option to buy a different brand and model would find most car purchasing websites not helpful as they require that a search be based on a particular brand and model. In order to remedy the problem of making a truly informed decision about buying one’s first car the following research question was proposed: In order to answer this research question and assist people is choosing their dream car there is a need for data that identifies the car brand and model that meet an individual’s specifications. As such data is not publicly available it can only be extracted from existing car sales websites that represent the cars currently on the market. Consequently 13000 cars containing 20 brands and 37 car characteristics were scraped from a popular car sales website using Beautifulsoup in Python. As the cars on this website can be considered to be new cars the recommendations the platform will present are limited to these cars and cannot recommend cars outside this sample. To ensure that the data that was collected could be used for further analysis the data was manipulated within R and missing values were imputed using the K nearest neighbor machine learning algorithm. Most of the missing values were found for fuel consumption (8.6% missing) and for the acceleration of the car (6.9% missing). As mentioned earlier these missing values were imputed using KNN based on the car price car brand and car type using the Euclidean distance and a K equal the square root of n resulting in a dataset without missing values. As KNN is an unsupervised machine learning algorithm it is not possible to quantify its performance or accuracy especially in a multidimensional solution space. Analysis of the data indicates interesting relationships. For example there’s a strong relationship between the car price and the engine size; cars with a higher price tend to have larger engines. Additionally the data indicates that  expensive cars tend to have worse fuel mileage. Overall the data seems to be strongly correlated over many variables allowing the use of various machine learning algorithms.In order to predict features that might be of importance to potential car buyers and to make the recommendation platform independent of external data sources specific car features will be predicted using machine learning algorithms. This holds most true for the car price which is dependent on multiple variables as indicated in the previous section. Since the data was collected in a specific manner (cars with the same make and model were located in the same section of the dataset as they were scraped by order of brand) serial correlation was introduced into the dataset. In order to eliminate the serial correlation in the dataset the sequencing of the dataset was randomized before the data was used within machine learning algorithms. Additionally for validation purposes the data was split in a training and testing data set using a 4  1 ratio.First a multiple linear regression model (first model) was developed utilizing the features that exhibited high correlation with the car price found in the EDA stage and presented in the previous section. An adjusted R² (calculated using the test data) of 0.899 was achieved. After performing the BreuschGodfrey test on the estimated model  serial correlation was detected due to the fact that the observation in the dataset were listed by car make and model. In order to resolve this issue the dataset was manipulated by the method mentioned in the introduction of this section. Next to serial correlation the residuals were found to be heteroskedastic implying that the level of variance in the residuals is unequal for varying car prices. Although  this is a violation of one of the Gauss Markov assumptions for best linear unbiased estimators the review of the residual plot of the model did not identify high levels of heteroscedastic residuals and the model is assumed to be appropriate.Second a multiple linear regression model with forward stepwise selection was developed. This algorithm compared every possible model starting from the mean (a model with no independent variables) to a model that contained all the features and determined the best combination of features that returned the lowest Bayesian Information Criteria (BIC). An adjusted R² (calculated using test data) of 0.914 was achieved.In order to further improve linear models the Boxcox transformation was performed on the variable that represents the car price using a predetermined lambda. This transformation mutates the data in order to make it approach a normal distribution. This model returned an adjusted R² of 0.9 which is not an improvement in comparison to the first manually fitted multiple linear regression model. Third a shrinkage/regularization method was deployed to improve upon the results from the previous presented models. Traditional shrinkage methods (Lasso and Ridge regression) have drawbacks with regards to datasets that contain multiple variables that are highly correlated with the dependent variable. If Lasso regression would be deployed the majority of the coefficients would be reduced to zero while they could have high explanatory power. The Elastic Net model can overcome the aforementioned issue by introducing an additional hyperparameter which balances between ridge and lasso regression. Additionally a second hyperparameter that determines the balance between the reduction of the MSE and complexity of the model was used. In order to determine the best hyperparameters to minimize the MSE while retaining a simplistic model  10fold cross validation was performed. The  adjusted R² obtained from this model is 0.9218.Fourth and last a gradient boosting model (GBM) was utilized to further improve the prediction accuracy. The model was constructed with 7000 decision trees an  interaction depth that equals 4 and a shrinkage factor of 0.1 which were all determined by means of cross validation of the hyperparameters. With an astonishing adjusted R² of 0.9569 the GBM model appeared have the highest prediction power comparing to all previous models mentioned.  Overall it is possible to conclude that even though the GBM model outperforms all other models there is always a preference for simple models. Therefore since the simple multiple linear regression model performs very well the simple linear regression model will be deployed in the car recommendation platform. Being able to predict the car price and other interesting features about the car creates the opportunity to present more information to the user when recommendations are made. However at this point we have not yet explored how car models are recommended the topic of this section.Recommendations in general are based on the assumption that users with same preferences will rate items similarly. This suggests that if someone likes all the features of a specific car he or she most likely will like the car as a whole. In order to find a car that is in line with the desires of a particular user one has to find users with the same desires and aggregate the types of cars these users have. The comparison between these users and the user for which the recommendation is made is based on the k nearest users where the distance is determined using the Pearson correlation coefficient or the Cosine similarity. Once the users have been identified their ratings are aggregated and the recommendation is made for the new user. This technique is referred to as User Based Collaborative Filtering. User Based Collaborative Filtering is a semisupervised learning technique that uses specific entries in a training rating matrix to determine unspecified entries in a testing rating matrix. Consequently it is possible to determine the accuracy of the recommendation. In order to evaluate the recommendations 10 folds crossvalidation with 7 given items was performed where the performance can be investigated by means of the ROC and Precision/Recall graphs. The  curve indicates the relation between the true positive predictions (y axis) and the false positive predictions (x axis). The results from the ROC curve indicate that the majority of the recommendations are correct while only having a small number of false positives (wrong predictions). Overall the ROC curve indicates that the model predicts accurately as the ROC curve approaches the top left corner where perfect models fit exactly in the top left corner.Thecurve indicates the relation between the precision (how useful the search results are) and the recall (how complete the results are). It is possible to infer from the graph that for small number of recommendations the precision is very high; however with increasing recall the precision tends to decline towards zero. Overall it can be concluded that the model performs well as the curve tend towards the right top corner of the graph which represents a perfect model. Furthermore the drop in precision is prevented by limiting the recall to a maximum of 10 recommendations.Based on this model and the desired car specifications 10 car brand including the car model are recommended to the user. By combining the recommendation model with the machine learning models it is possible to build a user interface that presents car recommendations based on user inputs as for example the desired engine horsepower or car type. These user inputs can be controlled manually through drop down menus and checkboxes for specific car options. Otherwise it is possible to describe your dream car (limited to certain characteristics) in one sentence and allow the recommendation model to determine 10 possible cars. The recommended cars are presented one by one with a car price emission level fuel consumption indication and image for easy comparison. When a user likes a car he or she can use the like button which will redirect the user to a website where the car can be purchased. Additionally the information that was used for the recommendation and the “liked” car are stored externally of the application and used for future recommendations. The application can be found here: In conclusion the recommendation application employs user based collaborative filtering and regression techniques in order to precisely recommend the car brand and model to users based on specific car characteristics. Performance verification has proven that both the recommendation algorithm and regression models have good performance resulting in a trustworthy recommendation platform.  ,NA,Recommending your car brand
https://nycdatascience.com/blog/student-works/zillow-zestimate-kaggle-competition/,27,Zillow is an online real estate database with data on homes across the United States. One of Zillow’s most popular features is a proprietary property value prediction algorithm: the Zestimate.  As one might imagine Zillow is constantly trying to improve its Zestimate.  To help advance its accuracy even further  it launched a Kaggle competition with a $1.2 million prize.  (Feel free to check our codes at .)The objective of the Kaggle competition is to predict the logerror between the predicted log error and the actual log error. The log error is defined as:In plain English the goal of the competition is to predict the difference between the Zestimate and the actual sales price of homes. This may help Zillow identify where their algorithm falls short.Kaggle provided real estate data from three counties in and around Los Angeles CA.  Each observation had 56 features; no additional features from outside data sources were allowed in the analysis.  The data was split into two files: a training set with the actual logerror and feature information for 90725 properties and a prediction set with only the feature information for 2985217 properties.  Submissions were scored based on the mean absolute error across all predictions.Since we had only two weeks to complete the project we implemented a workflow with four major components: exploratory data analysis (EDA) handling missingness feature engineering and modeling.  As shown below we repeated the process to improve our model until the project deadline.As illustrated above the process was iterated tuned and recycled until the project deadline when final submissions were due.We began by exploring the training and test data to gain more insight into the overall task. As we were predicting log error and training our models on a subset of data we first wanted to ensure that the data was normally distributed. This is an important step for maintaining the consistent distribution and performance. If we randomly split our training data into a training and test set the model can perform differently on our test data if we do not have the same type of distribution.There is a Gaussian (normal) distribution to the log error; this means that the random sample we use to test our model will have the same distribution as our overall data and we can guard against overfitting.In order to begin to understand which of the 56 features were useful for our models we looked at the absolute log error in relation to some categorical variables. For example we can see that the absolute log error distribution is closer to 0 for higher quality buildings.We also visualized the data using latitude and longitude to identify any additional features that might help us predict the error more accurately.In the visualization above pockets of newer homes are highlighted. Looking at the log error (transformed for the purposes of this graph) we can see that Zillow does a better job of predicting the actual sale price for newer homes.One of the major pitfalls of building a predictive model is overfitting. A model that has been overfitted will perform well on training data but fail to accurately predict new observations. In order to address this we compared the training data to the prediction data.Overall the distributions of the training and test data (full properties set) were similar. There were some slight differences which may have impacted the accuracy of our models.Given the total number of features we looked at the correlation of numerical features with both the absolute log error and each other to identify variables that might be particularly helpful in prediction.While we did not find any numerical variables strongly correlated to the absolute log error we did identify groups of correlated features. We also used Principal Component Analysis (PCA) to find groups of related variables. The PCA biplot (shown above) groups correlated variables. The closer the features are to one another the more positively correlated they are. Features across from each other are negatively correlated. As expected PCA grouped the tax variables and variables indicating home size (e.g. bedroom count bathroom count) together.  These analyses helped us understand the relationships between features in our data set.Both the training and test data had a large number of missing values that required or attention. In the training data alone 17 variables had over 90% missingness.In the base case we utilized three imputation techniques. First we identified several binary variables and imputed with 0 and 1. Second  we imputed continuous numerical variables using the mean and discrete numerical variables using the mode. Next  for variables with a larger percentage of missingness we imputed randomly using a weighted distribution of observed values in an attempt to maintain the original distribution.Following imputation we dropped variables based on three principles: extreme missingness duplication and zero variance. Variables with over 90% missingness and no feasible way to determine the correct value were dropped. If variables captured the same information such as  FIPS (Federal Information Processing Standard code) and Zip Code we only kept one. Finally variables with the same value across all observations were  dropped as they would have had no impact on our model.We also created new variables based on our findings from PCA including Total Room Count (bedrooms + bathrooms) Age of Home (2017  year built) and Value Ratio (parcel tax ÷ property tax).We began by fitting a multiple linear regression model to our data to better understand how our features related to each other and impacted log error. As with any line the regression line is described by an equation that relates the predictor and response variables by assigning coefficients to the predictor variables.  The coefficient is the “influence” of the predictor on the response holding all other predictor variables constant. The equation for the regression line is the predictive “model.”  In the equation below the 𝜷 are the coefficients the “xi” are the predictor variables and the “yi” is the prediction.Fitting a linear model requires a number of assumptions: homoscedasticity of residuals independence of errors linearity normally distributed residuals and independence of predictor variables. Our model violated most of these assumptions but we proceeded for the sake of exploration. We tried some standard transformations (e.g. Box Cox) to address these violations but they did not yield better results. We used all 25 features to train the initial linear model which yielded poor results (an Adjusted R2 value of 0.002422.  The Adjusted R2 value measures the effectiveness of the model with respect to variance in the residuals. Next we reduced the model complexity using several different methods: removal of features with high (> 5) variance inflation factors (VIFs) a stepwise feature reduction using both AIC and BIC values a Ridge analysis and a Lasso analysis.  See Ridge analysis results in the figure below:As anticipated model effectiveness with regards to Adjusted R^2 did not improve by much but it was interesting to see the significant predictors across the different methods.  The other models we used for this project fell into the regression treebased category. Treebased modeling is based on a structure in which the data is partitioned repeatedly at each “decision node” to minimize the residual sum of squares (RSS) or the distance of each point to the average of that group in the partition. Treebased algorithms implement a recursive binary splitting approach to “grow” the tree that is both “topdown” and “greedy.”  “Topdown” means that the data set is split into binary components successively starting with one initial node.  “Greedy” means that splits at each node are based on the option with the best possible outcome at that node not based upon a future segmentation that may lead to a better final result.  One downfall of this approach is that it may lead to overfitting. In the extreme case each observation would have its own partition (aka terminal node).  In this case the RSS would equal 0 but the model would perform poorly if required to predict a new observation that is not in the data set.  ootstrapregationBootstrap Aggregation (aka “Bagging”) is a method to mitigate against overfitting by simulating a larger training data set to reduce overall variance. Bootstrapping takes a sample of observations with replacement from the training set to create a new data set with an equal number of observations. To estimate the testing error of a bagged model the model predicts the response for “out of bag” observations that were not used to grow the tree.  The results are then averaged to calculate the out of bag error.Random forest is an ensembling treebased modeling technique that uses bootstrapped training sets to build a “forest” of decision trees. Random forest helps decrease correlation and reduce model variance by randomly selecting a subset of predictors to analyze at each node of the decision tree.  Overfitting is unlikely due to the random decorrelated nature of the tree building process.GBM builds trees sequentially using the residuals from the previous trees to build the next tree.  Given a decision tree model we fit a new tree to the residuals of the current tree. The new decision tree (based on the residuals) is then added to the current decision tree and the residuals are updated.  To combat overfitting the tree depth (number of splits away from the initial node) is limited and a learning rate (shrinkage rate) is introduced.For all models Kfold crossvalidation was performed in order to prevent overfitting and search for the optimal hyperparameter settings over a tuning grid.  Kfold crossvalidation involves splitting the data into K folds setting one fold aside training a model with the other folds and testing with the fold that was set aside. The results are then averaged together for a single estimation with reduced variance. Due to time and computing power limitations we limited our analyses to 5 and 10fold crossvalidation. We also partitioned 25% of our training data and held it out for testing before we trained a full model with all training data.  This helped us quickly assess model performance without predicting log error for 2985217 observations and submitting our prediction to Kaggle for feedback.  Hyperparameters are the “higherlevel” model properties that have an effect on the model training process (e.g. learning/shrinkage rate tree depth).  Our team used a grid search to obtain optimal hyperparameter settings for all models considered. Conceptually a grid search iterates a model through a hyperparameter grid to find the combination of hyperparameters that minimizes the loss function (or maximizes the objective function i.e. ROC). We also used a random search for optimal hyperparameters but the results were similar and  the random search took much longer than the standard grid search.  With more time we would have attempted a Bayesian Optimization hyperparameter strategy since research suggests that this technique yields better results while expending less computational power and taking less time. Bayesian Optimization retains information from previously sampled points when searching for the global minimum/maximum of the objective function.  GBM yielded the best results from our initial round of modeling. See the table below for the hyperparameters selected and results for the 3 model types:We treated the results from our first round of modeling as a baseline and returned to the beginning of our workflow to try to improve our models. We implemented several strategies independently to see how they would affect our results.Ensemble learning is used in machine learning because the generalization ability (predictive power) of an ensemble is greater than a single “learner” (model). For the best results correlation between learners should be minimized  where one model is deficient another might excel.  Ensemble learning did not improve our results likely due to high model correlation. Our Random Forest model produced the lowest logerror across all models we tried. Surprisingly a grid search for the optimal mtry (the number of randomly selected features that are analyzed at each decision node) returned a value of 1. Since several of our features were correlated selecting only one feature at each node might produce the least correlated trees. The final model run results are captured in the table below:Given the amount of missing data and relatively large number of features in this project imputation strategy and feature engineering were very important with respect to reducing prediction error. Using a simple imputation strategy was the most significant factor to improving our results.With more time we would continue to iterate on our workflow and introduce new strategies to improve our model results.  There were several modeling techniques that the team did not have a chance to explore due to time constraints including Neural Networks and Support Vector Regression. Given the importance of choosing the right imputation method we would attempt methods that require more time and computational power such as KNN.,NA,The Zillow Zestimate Kaggle Competition
https://nycdatascience.com/blog/student-works/web-scraping-influenster-find-popular-hair-care-product/,27,Are you a person who likes to try new products? Are you curious about which hair products are popular and trendy? If you're excited about getting your hair glossy and eager to find a suitable shampoo conditioner or hair oil merchandise using  could help you find what you seek in less time. My codes are available on .What are popular hair care brands?What is the user behavior on Influenter.com?What kind of factors may have critical influences on customers satisfaction?Is it possible to create a search engine which takes charge of phrases and returns related products?To obtain the most uptodate hair care information I decided to web scrape  a product discovery and review platform. It has over 14 million reviews and over 2 millions products for users to choose from.  In order to narrow down my research scope I focused on 3 categories: shampoo hair conditioner and hair oil. I garnered 54 top choices for each one. For product datasets I scraped brand name product name overall product rating rank and reviews. Plus the web scraping review dataset includes author name author location content rating score and hair profile.Firstly the “other” category represents the brands which have one or two popular products. Thus judging from the popular brands' pie chart we can see that most of the popular products belong to huge brands.As to checking users’ behaviors on Influenster in the United States I decided to make two maps to see whether there are any interesting results linked to location. Since I scraped top 54 products for each category the overall rating score is high across the country. As a result it is difficult to see regional differences.However if we take a look at the number of hair care product reviews on Influenster.com across the nation we know that there are 4740 3898 3787 2818 reviews in California Florida Texas and New York respectively. There is a negative relationship between rating and number of reviews. As you can see Pureolog receives the highest score 4.77out of 5 but it only has 514 reviews. On the other hand OGX is scored 4.4 out of 5 though it gains over 5167 reviews.As we may be interested in what factors customers care about most and what contributes to their satisfaction with a product I decided to inspect the most frequently mentioned words in those 77 thousand reviews. For the first try I created word clouds for each category and the overall reviews. However there is no significant difference among the four graphs. Therefore I created a comparison cloud to collate the most common words popping up in reviews.From the comparison cloud we can infer that customers regard functionalities of products and fragrance as the most important. In addition the word “recommend” shows up as a commonly used word in the reviews dataset. Consequently in my perspective word of mouth is a great marketing strategy for brands to focus on. TFIDF is a NLP technique which stands for “Term Frequency–Inverse Document Frequency” a numerical statistic that is intended to reflect how important a word is compared to a document in a corpus.For my search engine I utilize “tm” package and employ weightSMART “nnn” weighted schema for term frequency. Basically the weightSMART “nnn” a natural weighting computation counts how many times each individual word matches up with the document in the dataset. If you would like to read more details and check more weighting schemas please feel free to take a look at the .With TFIDF measurements in place products are recommended according to a cosine similarity score with the query. To further elaborate how cosine similarity works it is a measure of similarity between two nonzero vectors of an inner product space that measures the cosine of the angle between them. In the case of information retrieval like a search engine the cosine similarity of two documents will range from 0 to 1 because the term frequencies (TFIDF weights) cannot be negative. In other words the angle between two term frequency vectors cannot be greater than 90 degrees. Additionally when the cosine value is closer to 1 it means that there is a higher similarity between the two vectors (products). The cosine similarity formula is shown below.Most of the products belong to household brands.The more active users of the site are from California Florida Texas and New York.There is a negative relationship between the number of reviews and rating score.Functions and the scent of hair care products are of great importance.Even though “recommend” is a commonly used word in this project it is difficult to tell whether is positive or negative feedbacks. Thus I can conduct sentiment analysis in the future.The selfdeveloped search engine applied with TFIDF and cosine similarity concepts will work even better if I include product descriptions. By adding up product descriptions users can have a higher probability to match their inputs to not only product name but product description so that they are able to retrieve more related merchandises and explore new features of products.,NA,Web Scraping Influenster: Find a Popular Hair Care Product for You
https://nycdatascience.com/blog/student-works/redefining-cancer-treatment-predicting-gene-mutations-advance-personalized-medicine/,27,One of the most exciting frontiers for machine learning is the field of medical diagnosis where background information and nuanced decisionmaking are considered essential. When Memorial Sloan Kettering (MSK) released a Kaggle competition entitled “Personalized Medicine: Redefining Cancer Treatment” we took on the controversial challenge of creating models that could perform the same tasks as experienced genomics researchers. We were provided with a dataset of genes and cancerrelated mutations a text file of scientific literature related to each mutation and a 19 classification of each case which had been handannotated by a scientist at MSK after reviewing the data. Although we were not told in advance what each class represented our goal was to correctly classify new “test” mutations after training machine learning models on the original datasets and text files; we were informed that our work would ultimately help automate the work of distinguishing between driver (cancercausing) and passenger (neutral) mutations. Recognizing that machine learning methods are still far from perfectly replicating the work of skilled oncologists we put together a number of tools that cancer researchers can use in conjunction with the classification model to speed up this timeconsuming process.Our first step was to visualize the data and extract as much information as possible. We examined the relationships between genes and classes and found that one gene could fall into many different classes suggesting that mutations within the same gene could produce widely different effects. We also noted that the majority of mutations in each case were point mutations in which one amino acid was mutated to another. For these cases we created two new columns in our data for the original and final amino acids and then merged our data with another dataset on the physicochemical characteristics of each amino acid. Using scales of amino acid charge and hydrophobicity (relationship with water) we created new features measuring the absolute value change in charge and hydrophobicity caused by each amino acid mutation; we reasoned that if the new amino acids had very different properties from the original ones it was likely to lead to changes in overall protein structure and function. Though our feature engineering of the original dataset was helpful we noted that the genes included in the training and test datasets were almost entirely different and since the gene/mutation datasets contained limited information we turned most of our attention to the text data.Our approach to the text data was twofold. We aimed to 1) use our background knowledge of the scientific literature to create intelligent stopwords and filter the data meaningfully and 2) explore different natural language processing vectorization  methods (n_grams TFIDF Word2Vec Doc2Vec) to translate the text into numeric features that our classifier can work with. Since we were dealing with large blocks of scientific text our first goal was to extract only the relevant sections of text (e.g. include results remove methods and materials). We also selected sentences containing words that were likely to contain other words that would help in classification such as “passenger” “driver” “tumor” etc. We found that using these informed subsets of the text worked well  our models using them often outperformed models containing the full text data.The first text vectorization technique we tried was TFIDF which rewards words that appear more often in a text but penalizes words that appear in all texts to help filter out common medical terms. We also manipulated parameters such as n_grams which determined the lengths of grouped words that our model used. Word2Vec is a word embedding technique that was developed by Mikolov et al. in 2013 at Google. The model itself is a shallow neural network with three layers: an input output and a hidden layer in between. Upon initialization a word weight matrix is created with a vector for each word in the corpus. The length of these vectors is a hyperparameter of the model with the number typically ranging from 100 to 500. The values for this matrix are randomized upon creation. The inputs that are fed into the network are onehot encoded vectors of each word (called the center word) and the output for each is the probability that all the other words lie in the context of the center word. This context is a sliding window around the center and is another hyperparameter of the model. During each epoch of training these inputs are multiplied by the weight matrix. The weights are then adjusted by backpropagation through stochastic gradient descent so that the probabilities of the next epoch are closer to the true probabilities of the context. This weight matrix is what is used as the word embeddings in our model. There are two different training algorithms that Word2Vec can employ  continuous bag of words or the skipgram. The latter is better at predicting infrequent words due to the fact that there is no averaging of the word embedding vectors during prediction time. Doc2Vec is almost the same as Word2Vec the biggest difference being the addition of a paragraph matrix. In Doc2Vec the corpus is split into paragraphs or documents. This allows for more robust context inference since each paragraph may have different contexts for each word. The paragraph matrix serves as a memory for these contexts. For each center word present in each paragraph the paragraph vector is passed along with the word vector and the weights are calculated for it as well. Doc2Vec also has two training algorithms that function is about the same way. The skipgram algorithm (accessible through the ‘Distributed BagofWords’ model) was used for out Doc2Vec embeddings.Our approach to the machine learning aspect of this project was to try as many classification techniques and feature combinations as possible  in hopes that each method is likely to bring a new perspective in capturing features unique to each class. Here we discuss the benefits and drawbacks to each technique generally and how each performed in classifying mutations into MSK’s nine categories.Easy to interpret and popular across a number of industries logistic regression fits a linear equation of variables to predict log odds which is then piped through the sigmoid function to find the conditional class probabilities for each observation. Our best MLR model used 400 Doc2Vec features 20 gene and 20 variation SVD features the absolute value change in charge and hydrophobicity. 5fold cross validation led us to choose C  .1 L1 regularization and balanced class weights to prevent the model from overpredicting more popular classes. Given the sensitivity of classifying mutations and their effect on cancer tumor growth we decided that balancing class weights in our algorithms was essential even if our prediction accuracy dropped. This particular MLR model achieved 59.5% classification accuracy and was fairly good at predicting all classes (e.g. it did not overpredict popular classes).Multinomial Naive Bayes is often  as a fast effective text classification technique. In our case given the relative word frequencies or other vectorized text features seen in research papers associated with each class the model can predict the conditional probability of each class given a new observation’s word frequencies/text features. Rennie et. al.  that using TFIDF or other text vectorization methods (rather than bag of words) as well as balanced class weights helped improve MNB’s performance on text classification. These alterations helped MNB models overcome some classical difficulttoattain assumptions like as feature independence. We followed their lead. Our best MNB model used Doc2Vec and 20 gene/20 variation SVD features. We used scikitlearn’s MinMaxScaler to scale all features between 0 and 1 as scikitlearn’s MNB classifier cannot handle negative inputs. The best MNB model had an unimpressive 48.6% classification accuracy overpredicting the most popular classes and underpredicting the less popular ones.Two deep neural networks were created for classification: one fully connected network and one convolutional network. All networks were constructed using the Keras package.The fully connected network consists of seven layers the first five of which had 512 neurons the penultimate had 100 neurons and the output layer had 9. This architecture was devised through some crossvalidation testing in terms of how deep (number of layers) and wide (number of neurons) the network was to be. The activation function for all layers but the output was the popular rectified linear unit or ‘ReLU’; the output layer has a softmax activation function since the output had to be the probabilities of the nine possible classifications. There were dropouts in between the layer which remove a portion of the inputs in order to reduce overfitting. This network performed well with an accuracy of 63.2%.The convolutional network was modeled after a network used in a recent paper titled ‘ by Hughes et al. (2017). It consists of two sets of two convolutional layers with a max pooling layer after each set. There is then a fully connected layer followed by the nine neuron classification output layer. Convolution words by reading the inputs with a sliding window so that all the fine details of the vectors can be read. The max pooling layers shrink the inputs so that the memory use isn’t as extreme (convolutional networks require a lot of memory). This network performed slightly worse than the fullyconnected network with an accuracy of 60.1%.The Support Vector Classifier (SVC) is part of the larger model class of Support Vector Machines (SVM) which work by representing observations as points in space and then drawing boundaries between them that provide the clearest separations possible. SVCs work well for text classification since they can handle dense concepts and sparse instances. In problems involving text classification models have to deal with many features most of which provide some information. This leads to a high dimensional input space with few completely irrelevant features. Overall SVMs can learn well independent of the dimensionality of the feature space making them a good candidate for text classification. For our SVC model we used TFIDF for text vectorization and selected an ngram value of 3 meaning that groups of up to three words could be selected together. Our SVC model predicted correctly across nine classes with 64.3% accuracy making it one of our top performers. The random forest method constructs an ensemble of independent decision trees each using a sample of training observations through bootstrap aggregation and selecting a feature for each node split using a random subsample of features. Once all trees are fitted the algorithm averages the class prediction probabilities for each featurevalue combination to deliver the final model predictions. Our best random forest model used TFIDF vectorization on the subset of text chosen through informed stopwords and ngrams  3  good enough for 56.9% accuracy.Gradient boosted models are ensembles of weak learning decision trees. Instead of averaging the predictions of hundreds of independent trees like in random forest models boosting takes an iterative approach  build a tree calculate the output of your objective function (incorporating training loss and regularization) and use this output as an input for the subsequent tree’s objective function. Boosting adjusts the weights associated with each split in a tree based on the error from the prior tree  slowly descending along the error gradient (gradient descent) until the algorithm can settle on a local minimum that optimizes your objective function. A “learning rate” hyperparameter lets you decide the rate of descent. For more on gradient boosting we love the XGBoost team’s description Our best GBM model had a classification accuracy rate of 67.8% and used the 400 Doc2Vec features 20 gene/20 variation SVD features absolute value change in charge and hydrophobicity. 5fold grid search cross validation helped us tune hyperparameters eventually landing with 600 trees .01 learning rate maximum depth of 15 and minimum samples per leaf of 5. Analytics Vidhya has a great tutorial on tuning hyperparameters .We also used XGBoost on the same features described above. XGBoost offers two primary advantages over scikitlearn’s gradient boosting classifier namely 1) it runs faster and 2) includes regularization at each step of the process to control for overfitting. Our best XGBoost model achieved 70.1% accuracy. 5fold grid search cross validation led us to a .01 learning rate 1000 trees a maximum depth of 15 and L2 regularization. The boosted models were generally good at predicting all classes.,NA,Redefining Cancer Treatment: Predicting Gene Mutations to Advance Personalized Medicine
https://nycdatascience.com/blog/student-works/web-scraping-hackmageddon/,27,You must have heard it on the news: “Country X accuses country Y of launching a cyber attack against its infrastructure” or “Huge leak at Corporation X account information of millions of users leak.”Sometimes you don’t even need to hear it on the news but instead it is right there plastered all over your computer screen: “Your information has been encrypted and the only way to recover it is to pay us.”All of these are cyber attacks.Cyber attacks are malicious Internet operations launched mostly by criminal organizations whose goal may be to steal money financial data  intellectual property  or  to simply disrupt the operations of a certain company. Countries also get involved in socalled statesponsored cyber attacks where they seek to learn classified information on a geopolitical rival or simply to “send a message.”That’s more than 5 times Google’s yearly cash flow of 90 billion dollars.And that number is set to grow tremendously to around 2 trillion dollars by 2019.In this article we want to explore the types of attacks used by cybercriminals to drive up such a huge figure and help you understand how they work and affect you.There were a few challenges in pulling the data together. One is that it doesn’t automatically advance from table to table. At the end of the page  there is a click button that leads you to next table. But after scraping the current page I fetched that link from the next page button to get my spider to request to go to next page.  Another challenge is due to Hackmageddon changing the table schema every year. In order to achieve my goals I used control flow in my spider to deal with schema problem. I almost scraped an entire website. The information scraped included Date Author Target Description Attack Target Class and Country.Before I start analyzing dataI need to clean the data. As I expected there was a lot of missing data. As this dataset is about cyber attacks where sometimes motive of the attack or attacker identity would be unknown that was to be expected. I used Python's Pandas library to clean the dataset. After cleaning the dataset I started visualizing the data. Preliminary exploration and analysis of the data revealed some interesting observations. Most attacked country is the United States followed by '>1' means more than one countries has been attacked by the same attacker UK and India.After knowing the motive and target class next question arose in my mind was: what technique or attack vectors do attacker use to drive up such a huge figures? Another question I had was who are the most notorious hackers of all time? The next bar chart shows you notorious hacker of all the time. Anonymous leads the hacking race with more than 75 attacks followed by OurMine.The bar chart below shows which hacker use which technique to drive up such huge figures. Anonymous did more than 30 attacks with DDoS followed by OurMine which launched more than 20 attacks with account hijacking.This scraped dataset has lot of unstructured data. To makes sense of this unstructured data I decided to conduct text mining on it. I used python's Scikitlearn library for machine learning and NLTK library for Natural Language Processing.In above D3.JS interactive visualization you can hover over to each dot and it will pop up an organization name that has been hacked. If you scroll to the right you will see the Legend at the bottom right. Each legend name indicate top terms that appeared most in each cluster. There are 12 clusters and each cluster has a different color. According to above visualization Government organization are most likely to fall into cluster 78 and 11 because governments holds so many confidential documents that hackers might wants to steal them or reveal them. Healthcare is most likely to fall in cluster 6 where hacker wants to gain access to data or customer records. Financial firms are most likely to fall into cluster 12 because of the confidential financial databases. Online gaming websites are most likely to attack by DDoS to disrupt the websites. The most attacked organization is the government.Guccifer 2.0 was involved with 2016 United States presidential election.“Anonymous” performed most number of attacks.Account Hijacking is most common technique used by hackers.Most number of terms appears in last cluster(383): 'leaked claims database anonymous' that means most of the attacks leaks information hacked databases and anonymous did most number of attacks.,NA,Insights into the world of Cyber Attacks by Scraping Hackmageddon
https://nycdatascience.com/blog/student-works/nyc-airbnb-insights/,28,"“Hey YuHan. I am going to Time Square to celebrate New Year this December. Do you have any recommendation for hotels or shortterm rentals?” Once I heard my friend’s question I intuitively replied “Why don’t you check out Airbnb?” That exchange triggered my interest in exploring Airbnb’s listings in New York City.I sought answers to these questions: Is there a borough where one finds more budgetfriendly and highly rated rooms? When is the best time to place your order in high season? Feel free to play around to check out some cool functions. Also all the codes are available on my.To acquire the most uptodate data I used Airbnb New York City’s datasets from  which is an independent website offering noncommercial sets of data. For the interactive map I applied the full 2017 dataset that includes over 40500 listings composed of entire houses private rooms and shared rooms. To analyze the listings and price changes over time I collected data from 2015 to 2017 which contains more than 45 million observations.What is the overall location distribution of Airbnb NYC?Which borough has a better priceperformance ratio? (i.e. being able to enjoy great experience with limited budgets)What are Airbnb NYC’s price changes over years and months?There are over 340000 hosts and 40ooo listings in the May 2017 NYC dataset. The overall market value is estimated based on the multiplication of minimum nights and 12% of listing price. Airbnb receives a booking fee from guests ranging between 6 to 12%  and also charges 3% commission fee from hosts for every successful transaction. Therefore the total average percentage of profit that Airbnb earns is 12% of one complete transaction.To have a quick glance at the overall location distribution in NYC we can use the ""CLUSTER"" function to group up nearby listings. In this interactive map users are able to filter boroughs room types price range rating score and the number of reviews. Meanwhile there are bar charts illustrating the count of each room type and average price based on users' requirements on the lefthand side. After checking the big picture this shiny app allows users to zoom in the map and take a deeper look at every listing by selecting ""CIRCLE"" button and clicking on specific dots.To explore detailed information in each borough users can slide the price and rating score bar to check which borough shows more listings that meet their expectations.Last but not least finding a great time to book your room is another factor that travelers may consider. By choosing the price change per month you can see that the average price drops a little bit in July.New York City is the biggest market for Airbnb. According to 2017 data the estimation of Airbnb NYC's market value is over 27 million. In New York City alone Airbnb’s profitability is about $3.24 million. For expanding the market and retaining users Airbnb not only has to deal with regulatory problems but also needs to keep improving user experiences.For hosts who post up their listings in competitive areas it is always smart to keep looking up others’ price. They can also consider offering some extra service such as offering breakfast renting bikes and so forth to attract more guests.Guest should be aware of the differences in timing and location. During summertime July is a great time to place an order. For highlyrated yet budgetfriendly listings Brooklyn appears to be the best bet.",NA,NYC Airbnb Insights
https://nycdatascience.com/blog/student-works/facial-expression-recognition-tensorflow/,28,As in the study of Artificial Intelligence we want machines to be able to communicate and serve human.,NA,Facial Expression Recognition with Tensorflow
https://nycdatascience.com/blog/r/nyc-citi-bike-migration-visulization/,28,"Like all other sharing systems  Airbnb the housing sharing system Uber the car sharing system Citi Bike is the network of bicycle rental stations intended for pointtopoint transportation.Citi Bike is New York City's largest bike sharing system. It’s a convenient solution for trips that are too far to walk but too short for a taxi or the subway. The bike sharing system is combined with all other transportation methods available in the area for commuters.Any Citi Bike client has come up against two frustrating scenarios: the empty dock at the start and full dock at the end of the trip. Researchers call this as ""rebalancing"" problem as part of ""fleet optimization"" questions.  This problem has attracted the attention of data scientists to develop complex methodologies to optimize the available bikes and open docks.Following I attempt to utilize the shiny visualization app to provide a hint for the 3 questions:The visualization app is intended to provide a way to explore different comparative measures at the route station and system levels with spatial attributes and time series.Citi published Citi Bike Trip Histories  . I used the Citi Bike data for the month of March 2017 (approximately 1 million observations). The data includes:Before moving ahead with building the app I was interested in exploring the data and identifying patterns of rebalancing.  Location wise imbalance (Top 10 popular Station)On the interactive map each dot presents a station.  The visualization will also provide options to identify popular routes by selecting date and hour range. The top popular routes are marked in orange as the lines between the spatial points. The direction of the routes is indicated by moving from the more red towards the more green dots.Interesting patterns are observed. The most popular routes on the west side run through Central Park and Chelsea Pier. Grand central/Penn Station centered routes are also in the hottest route list. Outside Manhattan there are centers in Queen and Brooklyn initiating lots of popular routes. Riders bike more along the west and east streets than along north and south avenue. That makes sense in light of the fact that  there are more uptown and downtown subways than crosstown ones and riders do utilize the Citi Bike as a an alternative transportation option.While not enough bikes available in hot pick up stations the docks are lacking in hot drop off stations. The red dots are where outflow of bikes exceeds the inflow of bikes The green dots are where inflow of bikes exceeds the outflow of bikes. In the other words the green dots are the hot spot to pick up a bike(more inbound bikes) and the red(more empty docks) to drop them off. And The more extreme the color of dot is the higher percentage change of the flows this stations has. The size and transparency of the dot is represented by the volume of  both inflow and outflow of the stations. The more obvious the dot is the hotter spot the station is.What caused the balancing problem? The map based interactive app provides an insight for predicting demand. The information displayed is the accumulated hourly variables based on dates selected. Details of statistic numbers is also available for each stations by zooming in.New York has a classic peak commuter flow .  Most commuters ride bikes towards the center of the town from its edge in the morning. At the end of the day they ride the reverse way to the edge where they live especially at the edge sections with fewer public transportations options.   What about the rider's activities. Is there any pattern involved? The app provides insights of rider's performance for reducing rebalancing demands. By studying rider's activities it will provides suggestions for potential solutions. Below each bubble represents an age and gender group. The age is represented as the number on each bubble.  A negative correlation is observed between age and speed. The younger the rider is the faster he/she rides. In similarity the group in the thirties shows similar miles per trip. The performance between female and male group are also different.  The male groups in blue perform a higher speed level than female groups in red. Is there solutions for rebalancing to cut the cost and improve the efficiency instead of manually moving bikes via trucks bikedraw trailers and sprinter vans from full stations to empty stations?  The moving will take crews travel in pairs 45 minutes to load a truck. Citi Bike sought a way to get the riders to move the bikes themselves. In May it started the pilot Bike Angel. The reversecommuter would be perfect target member of the  What is so appealing about the program is the bike sharing system could self distribute its fleet with the proper incentives. The member can easily make 10 Amazon gift card with a few reverse trips. As a result the demand of manually moving bike around would decrease.The Visualization app provides the real time status of fleets: popular routes inbound/outbound net change time series of stations hot spot analysis and rider's activities. It supports the self distributed fleet by establishing a baseline for identifying ""healthy"" rebalancing within the bike share system. It provides a hint for a future transportation solutions.The interactive app is available on.",NA,NYC Citi Bike Visualization - A Hint of Future Transportation
https://nycdatascience.com/blog/student-works/beat-the-books/,28,The mission: making more informed “better” decisions when placing wagers on NFL football games.In order to accomplish this mission an interactive R Shiny Dashboard ()  was created so that the user can select a specific teamopponent combination to obtain historical matchup performance trends in addition to variables correlated with beating the team’s user specified opponent.  Once the user is able to gain information on how the selected team performs against the selected opponent better gambling decisions based on statistical analysis can be made.  R is an opensource computer programming language widely used for statistical analyses.  R is bundled with visualization packages that easily allows the programmers to display their statistical findings.  R Shiny is a web application framework which allows users to interact with the data set being analyzed to produce visualizations as per the input parameters set by the user.  R Shiny is a package bundled with the RStudio integrated development environment (IDE).  There are 2 main types of wagers when betting on NFL games: the point spread and the point total.  The R Shiny Dashboard was tailored to present the data with the point spread and point total betting types in mind but the data can be applied to other NFL wagers. Sportsbooks formulate a point spread (aka “line”) for each NFL game.  If the “line” is negative the team mentioned preceding the line is favored by the amount of the line.  For example if the New Orleans Saints are 9 vs the Atlanta Falcons and a gambler places a point spread wager on the Saints the gambler needs the Saints to beat the Falcons by 10 or more points in order to win the bet (aka “cover”).  If the Atlanta Falcons are +9 vs the New Orleans Saints and a gambler places a point spread wager on the Falcons the gambler needs the Falcons to lose the game by 8 or less points in order to “cover” the bet.  If New Orleans wins by exactly 9 points the bet is a draw (aka “push”); and no money is exchanged.  In reality sportsbooks charge the gambler a percentage (aka “vig”) of the bet to take the bet.  The gambler loses the percentage in a push bet but that concept is outside the scope of the topics covered.Sportsbooks also formulate a point total (aka “over/under”) for each NFL game.  A gambler can bet over or under on the combined points of both teams.  For example if the “Over/Under” is set at 46 points and  a gambler bets the over he can win if the two teams’ combined score is above that amount. For example if the Saints scored 34 points and the Falcons scored 20 points the gambler wins the wager since a combined 54 points were scored in the game.  Just as in the point spread bet if the combined score totals 46 points the bet is a draw and no money is exchanged.The data set used for the R Shiny web application was obtained from Pro Football Reference which is an NFL statistic aggregating website ().  Quarterback (QB) running back (RB) and wide receiver (WR) statistics from each game of the past five (5) NFL seasons2012 through 2016were compiled into a CSV file so that it could be loaded into the R Studio IDE.  Reference the snapshot below for the QB statistics collected:Reference the snapshot below for the WR statistics collected:Reference the snapshot below for the RB statistics collected:The Home page is meant to illustrate that teams generally do not have very big swings in win total from season to season.  The line graphs illustrate total wins (yaxis) by season (xaxis) with each color representing a team.  The graphs are separated by conference and division.  The user can select a number for the win difference from year to year and the table below will display the teams with a win total swing greater than the value selected.  For example if the user selects 6 as the input the table will update to display the 8 teams over the last 5 years that had a swing in total wins of 7 games or more.  The matchup tables submenu page under the betting menu is a way for the user to easily see tables containing the statistics of prior matchups between the selected team and the selected opponent.  The user can select the team for analysis and the opponent via the sidebar.  Reference the table below for a legend explaining the variable names in the tables:Receiving statistics were excluded from team analysis since the receiving statistics mirror the passing statistics.  For example receiving yards for each team is equal to passing yards.To analyze what variables are correlated to beating a specific opponent a “Winning” variable  was created.  A value of 1 or 0 was assigned so that correlations could be determined.  A variable correlation matrix was created/plotted so that the user can visualize how the variables are correlated to one another specifically to beating the selected opponent.  Additionally the correlations (values) to the “Winning” variable were extracted and presented in a table so that the user would readily have access to that information.  See below for an example:The final feature of this Shiny web application allows the user to analyze the current season performance of the selected team and opponent.  In the preceding features historical statistical information was analyzed and the variable(s) most correlated to beating a specific opponent were obtained.  This feature presents how the selected team is performing and how the defense of the selected opponent is performing for a specific variable that the user can select.  Since the 2017 football season has not yet started Week 12 of the 2016 season was arbitrarily selected and used in this example to present how the feature will operate.  There is room for improvement for this Shiny Web application.  The following items will be addressed:The path forward for this web application would be to input sportsbook betting lines and predict scores of future games based on historical performance (both current season and prior seasons) via machine learning techniques.  Once betting line information is received and scores of games are predicted the predicted score can be compared against the betting lines to determine which wagers have positive expected value (EV).    ,NA,BEAT THE BOOKS | An Exploratory Data Analysis into NFL Statistics
https://nycdatascience.com/blog/student-works/golden-age-television-scraping-tv-data-metacritic/,28,"I don't discriminate when it comes to television  I love everything from bad reality shows to prestige dramas.  But over the past few years I've felt overwhelmed by the sheer number of shows being heralded as ""mustsee TV"". I know I watch Atlanta and Mr. Robot but who has the time? I'm not alone here  the Financial Times in an  entitled ""Watch it While it Lasts: Our Golden Age of Television"" wrote  ""We used to complain that there was never anything good on TV. Now we complain about the opposite."" In addition new companies like Netflix are spending billions of dollars each year on original content which may contribute to the feeling that there is constantly something new to watch. I scraped  from Metacritic a website that aggregates reviews of media products. Most notably it aggregates both qualitative and quantitative critic reviews into a single ""Metascore"". It also contains detailed information about each product though the TV section was not launched until 2005. Metacritic does have data on TV shows from prior years but their listings are incomplete. I scraped several layers of data including:From this process I obtained 13 variables for over 2000 shows including Critic Rating User Rating Rating Description Network Season and Series Premiere Dates and Genre(s).For my analysis I wanted to investigate my hypotheses about the influx of quality mustsee TV in recent years. I attempted to answer the following questions:First I wanted to explore the relationship between Critic and User Ratings. Metacritic provides a Critic Rating for TV shows with at least seven critic reviews as well as an average User Rating for shows with at least four user reviews. I graphed both and colored the points based on the Rating Description which is a qualitative variable based on the quantitative Critic Rating.Based on the plot the relationship between Critic Rating and User Rating seemed fairly linear and sentiments seem to converge as the Critic Rating improves. For shows that critics rated ""Average"" or ""Unfavorable"" user opinions vary widely. Further analysis revealed a correlation of 53% indicating a significant moderate linear relationship between Critic and User Rating. I also tested my theory of consensus among higher rated shows by conducting several twosample ttests on Critic and User Ratings for each Rating Description group but I found that the population means were different across all four groups. We can look at the distribution of Critic and User Ratings as well:Critics are ""harsher"" than users  the median rating given by critics is 65 while the median rating submitted by users is 74. The distribution of User Ratings is also narrower and is concentrated towards the higher end of the scale.Critic and User Ratings may be correlated but I wanted to know whether they matter for a show's success. By graphing the total number of seasons for each show I found that almost half of the shows in the data set lasted for only one season. I also checked to see if there was any correlation between Critic and User Ratings and a show's total seasons but there was a fairly weak linear relationship in both cases. Most shows only last for one or two seasons regardless of how reviled or beloved they are.Next I wanted to know whether there really been an increase in the quantity and quality of TV over the last few years. I graphed the number of shows on TV by year as well as the number of highly rated shows (Critic Rating > 60).Looking specifically at the years between 2005 (when Metacritic launched its TV section) and 2016 (since 2017 data is incomplete) it is clear that the number of shows as well as the number of ""good"" shows has increased. However looking at the average Critic and User Rating by year does not show such a clear trend.Critic and User Ratings have fluctuated in recent years even as the total number of highly rated shows has increased. We can conclude that shows are not necessarily getting better  we may just have more shows (and therefore more ""good"" shows)!The increase in show volume may be related to the many networks that have entered the TV ring over the last few years (e.g. Netflix Hulu). We can see this increase by looking at the number of networks with at least one show on the air by year.The number of TV networks essentially doubled between 2005 and 2015  no wonder we feel overwhelmed by our options! Access to many of these networks may require additional cable packages or online subscriptions beyond the basic channels so how do we decide where to spend our time and money? To explore this I looked at the top 10 and bottom 10 networks based on average Critic Rating. Of all networks with at least 15 existing shows PBS and HBO have the most critical acclaim. A&E and NBC have the least.To incorporate User Ratings and show volume into my analysis I created a bubble chart. In the image below the size of the bubbles represents the number of shows each network has produced. Only networks with at least 15 existing shows are included.This chart shows that HBO is essentially the ""model"" network  it has produced a large number of shows that are beloved by both users and critics. Other subscription networks such as Netflix and Showtime also stack up well against the competition. On the other hand traditional basic cable networks are lagging behind in terms of creating ""prestige"" TV. This reflects the  of cable package cancellation in favor of streaming services and other online subscriptions.Finally I used the data I collected to cut through all of the ""buzz"" and determine what I should actually watch next. I averaged Critic and User Ratings across all shows with at least three seasons and selected the top 10 Dramas and Comedies. Hopefully this will help any of you out there looking for a new show to binge watch!Based on the data from Metacritic it is difficult to say whether we are really experiencing a ""golden age of television"". There are more shows being produced than ever before (and therefore more good shows) though the average quality of shows may not be improving. The influx of new networks has likely contributed to this increase though many of the new ""prestige"" shows seem to be coming from platforms like HBO and Netflix not traditional cable. Unless cable networks start producing ""mustsee TV"" to improve demand they will probably continue to see a drop in subscription rates among their customers.Link to my .",NA,Are We In The Golden Age of Television? Scraping TV Data From Metacritic.
https://nycdatascience.com/blog/student-works/housing-tax-los-angelas/,28,Californians who buy a house often experience sticker shock when they get their property tax bill. The reason for the dramatic spike in  in real estate tax is the execution of California's Proposition 13 of 1978 which equates the  tax assessed value with  the purchase price. In an increasing market that will always result in a steep increase of the real estate tax value especially when the house has not changed ownership for an extended period of time. While equating the the tax value with the market value may seem fair the lack of yearly assessments creates disadvantages for multiple parties. As indicated by these disadvantages the problem is a doubleedged sword that creates disadvantages for both the real estate owner and the governing body. A possible solution could be to reassess all real estate every year in order to ensure that the tax value equals the market value; however such a process would be very cumbersome and time consuming in light of the fact that there are 3.5 million houses in Los Angeles alone. Consequently in order to assist in determining the market value of real estate with the aim of setting the tax value equal to the market value the following research question is coined: ?In order to create a model that might assist in the prediction of the real estate tax value market information is required. A current Kaggle competition provides information on 2.9 million houses in three counties in the United States one if which is Los Angeles () That makes it an excellent source of information though we must begin by addressing the generalizability of the sample.The concept of generalization from an academic point of view implies that a sample on a number of characteristics does not significantly differs from the population. Additionally the concept of generalization indirectly assumes that the data is randomly sampled which is required for any statistical test. With regards to the comparison of characteristics of the sample and population (US Census bureau) chi square tests were performed on the building construction year and the estimated value of the house (the house tax value) where both test statistics were found insignificant. However even if these tests might indicate that the data is comparable to the population it’s not a perfect match. The fact that the data was conveniently sampled because transaction information was used to select the houses reduces the generalizability of the data to transferability. This difference is only slight as it implies that the conclusion from the sample dataset can be used for the population; however when the sample dataset would increase the sample data will not resemble the population more.The quality of data input is one of the key factors to ensure accurate machine learning prediction accuracy. In order to ensure that the data quality is sufficient the following  cleaning workflow was performed:Overall with regard to the missing values it is possible to indicate that 41.3% of the observations were missing. The variables with the highest number of missing values are the building class type the story type the size of the basement and the size of the garden. In most of these variables the value is not missing at random as the house could simply not have a garden or basement. However in other case like the building class type clearly the value is missing as every house can be classified by a certain building type. In order to process the information within machine learning models these missing values must be imputed as machine linear algorithm cannot handle missing value.To make appropriate and reasonable decisions on the imputation methodology and data type each variable was compared with the description provided by Kaggle. Additionally logical reasoning was used for imputation. The following methods were used for some of the manipulations:Next the winsorization technique was used on all numerical variables. This transformation is performed to eliminate potential adverse effect that extreme values (outliers) may have prediction model.  Observations that fall below the 97.5th quantile and beyond 2.5th quantile were replaced with the mean value.As part of any data analysis exploratory data analysis must be performed. This analysis will ensure that the researcher has a good understanding of the data and can use this understanding and possible findings as an input for machine learning modelling. One could therefore even state that exploration is a prerequisite for machine learning modelling. The exploratory data analysis can be split into two sections: (1) analyzing the dependent variables (2) analyzing the independent variables.Investigating the distribution of the dependent variable the real estate tax value indicates two distinct peaks an indication that might imply that multiple processes are driving the outcome. The 1st peak is the land tax which differs strongly from the house tax (2nd peak) and might even be referred to as zero inflated. This occurs most probably because the land tax in many cases is equal to zero which is true for example apartments or houses with a very low land value. Attempting to predict the real estate tax value in its current shape could result in problems as the data (log transformed) is not normally distributed. Therefore it is advisable to split the independent variable retail tax in land tax and house tax and predict these individually. However here we’ll focus on the house tax prediction in order to reduce the complexity and length of this article. By reviewing the independent variables it is possible to gain a glance of the real estate market within Los Angeles. With over 50 independent variables only a small selection will be presented within this blog post. For example the real estate total tax value is concentrated between $150000 and $200000.  Second most of the houses within the dataset are built before 1959 with a decline in the total number of constructions within later years. Another interesting variable is the available perimeter. The data indicates that the majority of the houses have 1500 square feet of living space. Additionally the data indicates that the house perimeter ranges between  200 and 2400 excluding outliers in the data. Based on these insights and insights from other exploratory analysis not described in this blog post hypotheses can be formulated.In order to predict the real estate tax value a dataset is required that catches the influential factors which combined result in an accurate prediction. However as indicated in the previous sections the large number of missing values has resulted in the loss of a large number of columns that maybe could have held valuable information. In order to retain some of this lost information a clustering analysis was executed by means of KMeans clustering in which eight groups were identified. These groups were determined by observing the reduction in the gradient of the function of the within cluster variation and the number of clusters. As this process can be arbitrary in the absence of a strong inflection point the overall reduction of the within cluster variation was observed for different K resulting in a K of 8. Further analysis of this new variable indicated that the groups significantly differ with regards to the tax value and could therefore provide additional information that was not available within the existing variables. This new variable can now be used within the real estate tax predictions models. In the presence of a robust data set that has been cleaned and investigated for uncovered patterns hypotheses formulation and testing can be performed. Within daily practice this process is mostly done based on exploratory data analysis; however from a statistical standpoint it should be performed based on causal relationships which can then be investigated through correlation studies. Therefore a set of hypotheses was defined and consequently tested through bivariate analysis.In order to evaluate these hypotheses the Pearson’s productmoment correlation Welch Two Sample ttest and the KruskalWallis rank sum test were used.It is interesting to note that the hypotheses for the building quality type which was hypothesised to be higher for higher levels of structural tax turned out to be negatively correlated. Consequently we were not able to reject the H0 hypotheses and could not add this variable to the model because it was not logical from a causal perspective. The other hypotheses except the unit count which showed insignificant results the other hypotheses indicated significant relations that allowed for the rejection of the H0 hypotheses. Consequently these variables will be added to the various machine learning models. In order to construct a machine learning model a researcher has a large number of options and therefore an initial selection must be made. This selection process first focusses on the type of variable one aims to predict which in this case is a numeric variable making this a regression problem. Within the regression family various models are available ranging from linear regression lasso regression random forests and boosted random forests. With the aim of constructing a parsimonious model that can predict the real estate tax as accurate as possible these four machine learning models will be investigated. It is important to note that for all the applied techniques training and testing was performed with Kfolded separation of data with a 80/20 ratio respectively. A multiple linear regression models is a model that aims to find the best linear unbiased estimators under the Gauss Markov assumptions. Within this model multiple variables can be combined to predict one particular outcome where the relationship between the independent variables and dependent variable are assumed to be linear. Initial results from a linear regression model where the real estate tax value is determined based on the size of the house the construction year the property land type the number of bathrooms and bedrooms the type of airconditioning the number of pools and the earlier introduced clustering variable indicates that this model can predict 66.4 % of the variance within the data which can be considered a medium fit. However condition verification of the Gauss Markov assumptions indicate that the assumption of constant variance is violated making the estimators’ significance unreliable and poses the probability of creating an overfitted model. Consequently with the aim to correct for this violation of equal variance which is driven by the violation of normality the box cox transformation is performed.The box cox transformation aims to reduce the level of skewness within the dependent variable. Reducing the level of skewness should reduce the level of unequal variance within the model. The model result indicates that the R decreases to 60.05 in comparison to the 66.4% without the transformation. This decrease can be explained by certain variables losing significance and no longer contributing to explaining variance within the model. Consequently the box cox transformed model can be considered more parsimonious than the model without transformation. In a further attempt to create the best parsimonious model automatic variables imputation can be performed. This modelling technique is based on a multiple linear regression model where the Bayesian information criterion (BIC) is used to determine the most parsimonious model out of all possible model combinations. The downside of this modelling technique is that the variables that are used within the model are no longer driven by underlying causal relations but only based on their contribution to the reduction of the residual. This can result in models that are parsimonious but prone to overfitting the data. Nonetheless the results from this model indicate an R of 60.08% making it as strong as the box cox transformed model though it is based on a different set of variables which in most cases do not have any causal relation. Consequently from these three models the box cox transformed models is the most reliable and parsimonious. In the previous section variable selection was performed by imputing variables through the BIC; however there are other options available for the selection of variables for a model like the Lasso Regression. In Lasso regression shrinkage/regularization is performed for variable selection where Lasso regression attempts to minimize the error while also minimizing the number of variables used for prediction. The balance between the goodness of fit and the prevention of overfitting is determined by lambda. To determine this tuning parameter lambda 10folds cross validation was performed (see the figures below). Through this technique it is possible to determine the best lambda that minimizes the mean square error which indicates the prediction error. Through Lasso regression it was possible to improve the prediction to and R of 68.1 in comparison to 60.08% from the box cox transformed model which is an increase of 8%.                  In the last two machine learning approaches the focus lied on using numerical variables for linear prediction where categorical variables are used as dummies. However aWith the aim of constructing a model that can predict the real estate tax value as close as possible the boosting machine learning model is used. The boosting machine learning model is based on tree bagging which is used to reduces the prediction variance but in addition uses the last model in order to construct the next model. This technique will enhance the prediction power on the training data set but is prone to overfitting the testing data set. In order to fit a boosting machine learning model three tuning parameters must be determined which are the shrinkage the tree depth and the number of trees. Through cross validation the calculation of the mean square error and the Boosting test error plot (presented below) the tuning parameters are determined to be a shrinkage of 0.001 and a depth of 4. Based on these tuning parameters the boosting model’s R is 88.1% which is a strong improvement in comparison to the random forest model. However as indicated earlier the boosting model is prone to overfitting the training data which implies that the model is weak in prediction out of sample. Consequently validation on the testing dataset only indicates an R of 37.8 which is a very strong decline in prediction power. Within this project a multitude of machine learning algorithms were used with the aim of predicting the real estate tax value in order to automate the real estate valuation process and reduce the bias within the California tax system.  as indicated in the discussion of the machine learning models where the Random Forest machine learning model presents the best results. This medium fit is the result of the poor quality of the dataset used within this analysis. If better information with the emphasis on the less missing values is available higher levels of accuracy can be reached. However analysis of the California tax system revealed one of the underlying problems which prevent accurate prediction of the tax value. The tax value of a house is determined at the moment a house is sold as indicate in the introduction. This implies that two identical properties of equal value can have a great amount of variation in their assessed value even if they are next to each other. Consequently with this dataset. Overall this research project can serve as a proof of value. It indicates multiple shortcomings within the California tax system and that predicting the real estate tax value might be a good approach to automate the real estate tax evaluation process. Nonetheless for further research a complete dataset that contains information on the actual market value of the house would be better in order to prevent the misclassification of the households real estate tax value due to the existing time dimension in the tax value assessment.,NA,Predicting Housing Tax with Machine Learning Models
https://nycdatascience.com/blog/student-works/ufc-data-analysis-shiny-app-ufc-data-analysis-part-ii/,28,After successfully scraping and cleaning all the data I needed from my last post ( I was able to do some analysis and see if there is any interesting findings. I will make the analysis part into a Shiny App which is more interactive and compact. This is just a brief walkthrough of my App.If you want to download my App the full project can be found at: My App has 4 section: the    .Most of the graphs are drawn by googleVis for better intractability. But in the actual code you can still find these graph plotted by ggplot2 (which is commented out).This page provides a summary of the UFC events data.This page provides a analysis on all UFC fighters.In the fights section I offer a brief analysis how to win a UFC fight.Last but not least I present you with a UFC fighter searching wizard.You can find the fighter you want by First Name Last Name Birth Date Age Country Class HeightWeight in the table above.After having the name of your fighter in mind. You can put his name in the text box below and click 'Search'. Voila! You can see the photo of him/her the information about him/her and all his/her statistic.,NA,UFC Data Analysis - Shiny App ( UFC Data Analysis Part II)
https://nycdatascience.com/blog/student-works/lessons-learned-2016-kaggle-zillow-home-data-competition/,28,"Our team had two prevailing theories concerning the ultimate goal of the “Kaggle Zillow Challenge” going into this project.(1) The  describes how exploring the error can unmask useful insights about a model.So maybe predictions relating to the error would help the Zillow home site to tune their model (adjusting for the results of this competition).But this first theory would make more sense if we were predicting error not “log error” as calculated by:              Log(Zestimate)  Log(SalesPrice)  LogError(2) Zillow was being secretive and protective of their data and using this phase one of the competition as a litmus test to weed people out during phase one of the Kaggle competition before providing access to better data in phase two.With 1.2 Million in prize money at stake you have to figure there is a logic to what they were doing.But after testing a myriad of linear models and having them all fail on the assumptions of “no collinearity” or normality of the distributions it was easy to think that maybe they were just messing with us.Initially we spent days on data cleaning.Looking at the data provided this begged the question “is anything not missing?”The answer was a resounding “no”.The only fields where the data was 100% present were the parcelID (unique identifier for each house) and the log error / transaction dates provided in the training data. A few highlights from our data munging experiments:These are just a few highlights.In the end it is questionable how much bang we actually got for our buck with respect to all the work that went into this.Then the real analytical experimentation began.Our team explored linear models and linear models with transformation including the “Yeo Johnson” transformation.This transformation was selected because it could address the issue that BoxCox can’t handle negative values. With high hopes our team added to these experiments with regularization using Ridge Lasso and Elastic Net.  The team also ran random forest gradient boosting and XGBoost models as well.A point of humor among the team was in connection with one of the first models we tried to test the Kaggle submission process.  Dubbed “Will’s Stupid Model"" it was a simplistic linear model calculating logerror in terms of just two fields:calculated finished square feet and year built.As poor as this model did it set our high score on Kaggle that we were almost unable to beat by any other experiment we tried until almost the end.And in the end when we finally did beat it it was only by combining the results of this very same model with GBM by averaging the predictions of the two models together.This final experiment raised our ranking in the Kaggle competition by 95 places boosting us to 1297th place.Million dollar prize money here we come! Educationally as data scientists this experience was an opportunity to experiment and try many different things even if at times it felt like the deck was stacked against us in terms of producing truly meaningful results.Along the way our EDA meandered into exploring the question of missingness itself as a predictor of logerror.  Though this experiment did not bear fruit in terms of producing a viable model it was an interesting exploration of the data (as shown above).So what’s the number one thing we learned from this whole experience?Though Data Science is about finding statistically significant evidence within the data maybe it still pays to trust your instincts.From the very beginning this whole competition felt like a fool's errand.The one team at the Academy who truly produced the best and most interesting results chose to ignore what the Kaggle contest was all about and just attempt to predict home value from the data points given.",NA,Lessons Learned in the 2016 Kaggle Home Data Competition
https://nycdatascience.com/blog/student-works/machine-learning-kaggle-competition-mission-zillow/,28,In the year 1959 Arthur Samuel a pioneer in the field of Machine Learning defined it as a field of study that gives computers the ability to learn without being explicitly programmed [1]. According to Professor Tom Mitchell at Carnegie Mellon the field of Machine Learning seeks to answer the following question: “How can we build computer systems that automatically improve with experience and what are the fundamental laws that govern all learning processes?”Machine Learning algorithms are broadly classified as supervised unsupervised and reinforcement learning. In particular supervised learning algorithms have been used to make predictions based on certain given inputs in a variety of applications. For instance Zillow revolutionized the landscape of the real estate industry by introducing Zestimate; it could predict the sale price of properties with an accuracy of about 95 %. With an objective to improve the accuracy further Zillow launched a competition on Kaggle that challenged the Data Science community to develop algorithms that can potentially outperform Zestimate. The competition is phased in two rounds and the objective of the first round is to predict the logerror defined as follows:Team Entropy’s approach to this challenge included Exploratory Data Analysis (EDA) Data Imputation and the implementation of a slew of Machine Learning algorithms (Logic based methods Elastic net regularization (Ridge and Lasso) Tree based models (Gradient Boosting Machine Random Forest Extreme Gradient Boosting) and Automatic Machine Learning (h2o) and will be discussed in detail in the subsequent sections.The team started EDA by looking at the missingness in the dataframe. Knowledge on the nature of missingness allowed the team to make plans for subsequent data imputation. The degree of missingness is shown in the plot below. Figure 2 shows that most of the variables have certain degree of missingness. There are many variables having missingness over 99%. Multiple imputation strategies were employed for accuracy and contingency of the Machine Learning models.A correlation plot (corrplot) with significance is extremely useful and helps in visualizing the relationship between the numerical variables in a large dataframe as seen in the figure below. For instance dark blue circles reveal a strong positive correlation while dark red ones indicate a strong negative correlation between the variables. Furthermore multicollinear columns are highly correlated and can be easily identified from the corrplot. For instance the tax columns (tax_building tax_total tax_land and tax_property) area (area_total_calc area_live_finished) and bathroom (num_bathroom num_bathroom_calc num_bath) are highly correlated in this dataset. Interestingly there is a strong negative correlation between aircon and region_county. Cells marked with ‘X’ highlight an insignificant relationship with 95 % confidence.The team applied another powerful visualization tool tableplot to explore the dataset. As shown in the figure below different independent variables were plotted against the dependent variable logerror with the intention of catching potential patterns between them. which could in turn give us insights on training machine learning models. Some independent variables most notably “build_year” showed a C shaped pattern. This indicates that the price of newer houses are more accurately estimated by Zillow than older houses and therefore having a lower absolute logerror. Some other independent variables such as num_bathroom area_total_calc and tax_total showed a slightly overall increasing trend suggesting weak positive correlations between logerror and these variables. Interestingly these independent variables turned out to be top important variables on our models to predict logerror.The original dataset has a lot of missing values. Imputation is very important during the whole process. The team used two different ways to impute the dataset.The first round data imputation was performed analyzing each variable individually by looking at its missingness type and common sense. The imputation strategies are shown below:In an attempt to improve the prediction accuracy following strategy was employed for imputation in the second round:In supervised machine learning the objective is to predict some dependent variable  in terms of several independent variables:Specifically for the Zillow competition the above equation can be expressed as follows: After getting a sense of the data based on Exploratory Data Analysis and predictions from logic based methods Team Entropy decided to hierarchically include more complexity in our models. It is well known that the coefficients obtained using linear models help understand the relationship between the dependent variable and each of the independent variables. Furthermore the mean value based prediction discussed in the previous section is a special case of linear model having only the intercept term. In order to determine the linear model the package ‘glmnet’ was employed in R that essentially minimized the following function:  λ Random forest is a popular machine learning algorithms extensively used in industries. It’s capable of dealing with both regression and classification problems and can be used for unsupervised machine learning as well. The training algorithm grows a number(controlled by ntree) of decision trees in which at each node only a subset of independent variables(controlled by mtry)  are considered for splitting. By randomly sampling rows(bagging) and columns (mtry) the trees generated are not correlated to each other. The result is reduced variance (nodesize and maxnodes control the variance of single tree) when the average of the trees are taken for prediction.The importance of variables in the final model was visualized in an importance plot shown below. Build_year tax related features num_bathroom and num_bedroom were among the top of the list. These independent variables play more important role in predicting logerror and may contain information/patterns that Zillow overlooked in its Zestimate model.Gradient boosting is a very useful machine learning technique for regression and classification problems which produces a prediction model in the form of an ensemble of weak prediction models typically decision trees. It builds the model in a stagewise fashion like other boosting methods do and it generalizes them by allowing optimization of an arbitrary differentiable loss function.The team used gradient boosting model for the first and second imputation dataset. Here is one of the grid search result for the first imputation.This is the comparison of variable importance of two imputation dataset. So the team can find the result that second round imputation works for gbm model. A lot of variables are considered by gbm model compared with the result of the first round imputation. The team submitted to Kaggle four times using gbm. These are the parameters for round 1 and round 2 imputations and the right plot is the Kaggle MAE for each submission. This left one is the parameter of our best score using round 1 and round 2 imputation dataset. And the right one is Kaggle MAE score of each time. Based on gradient boosting framework XGBoost is an optimized gradient boosting library. It is enabled with parallel processing which makes XGBoost at least ten times faster than any other tree based models. Regularization is provided in XGBoost to avoid overfitting. Another advantage of XGBoost is flexibility. Regression classification and userdefined objective functions are supported. An objective function is applied to measure the performance of the model given a certain set of parameters. User defined evaluation metrics are supported as well. Missing values are handled internally by the model so that it runs with the raw data.In the project the team first inputted the original dataset into XGBoost model and the importance matrix is shown below. Based on the importance matrix and the first round imputation results the team then ran the model with the cleaned dataset of 22 variables. The importance matrix generated using cleaned dataset is consistent with the model run by original data. However compared to RandomForest and GBM which put all tax variables at important places XGBoost omitted the tax variables. The team then ran XGBoost on the second round cleaned data. Onehot encoding was also applied. It converts the categorical variables into numeric by creating dummy variables for each level of categorical variables. Onehot encoding aims to generate new features thus increase the accuracy of the model.Looking at the Kaggle scores shown below the best performance was given by the first round cleaned data. This proves the effectiveness of our first round data imputation. On the other hand the second round data imputation and the onehot encoding did not perform well.The automl package from h2o possibly presents a perspective on the future of Machine Learning. The package automates Machine Learning to the extent that there is no need to clean up the dataframe or to decide on the modeling strategy. The automl package runs a slew of Machine Learning algorithms on the entire dataframe and summarizes the top models in a leaderboard (see TABLE 2). For this specific problem automl predicts that the Distributed Random Forest (DRF) is the best performing algorithm. The important variables for the DRF model is summarized in TABLE 3 and the corresponding Kaggle score was 0.0649128.In the course of this project Team Entropy implemented a variety of Machine Learning algorithms for predicting the logerror. In Figure15 we compare the performance of various models for the months October to December. It can be seen predictions from XGB and Lasso are pretty much flat highlighting that the monthly change in the logerror values are not captured in these models. The Random Forest as well as the GBM models are able to capture some trend in logerror as a function of months. Hence it is not surprising that GBM was our best performing model on Kaggle followed by Random Forest although all models predict significantly lower values for logerror for these three months.The team members collaborated equally throughout the entire project. EDA was performed to examine the structure and degree of missingness of the raw data. Imputations were being made while the models were running. First round imputation was done by studying each variable individually by looking at the type missingness percentage and common sense. In the second round imputation a different strategy was employed; numerical NAs were imputed with 999 and the categorical NAs were imputed with 1. Five models were trained which include Lasso Random Forest Gradient Boosting Machine XGBoost and H2O. The best results of each model are shown below:The best performance was given by GBM with the second round imputed data. The Kaggle ranking at the time of submission was 725. The secondbest result was given be Random Forest with the ranking of 753 at the time of submission.The following ideas can be explored in the future to improve the prediction accuracy and thereby the Kaggle rank:,NA,Machine Learning Kaggle Competition-Mission Zillow
https://nycdatascience.com/blog/student-works/house-pricing-estimation-netherlands/,29,Selling your house requires finding a buyer willing to pay the price you are ready to accept. This can be a difficult task especially in the Netherlands as the final price is determined through negotiation where buyers try to push down the asking price for the house. In most cases homeowners ask real estate brokers to assist them in the selling process and in setting an asking price that will maximize the selling  price. In the Netherlands this process is not different; however since a few years homeowners have begun selling their houses without the intervention of an established real estate firm. Home owners do this as this can easily save them a few thousand euros as the real estate broker’s fee is a percentage of the selling price (). Consequently  the services originally provided by the real estate broker like providing a tour through the house to potential buyers or advertising the house now have to be organized by the homeowners themselves.  The homeowners also have the challenge of setting the right  asking price for the house.Real estate brokers usually have some experience in assessing the market value of a home because they are familiar with the area and have access to private databases that allows them to easily compare houses on house characteristics and price. Your average homeowners doesn’t have this knowledge or access. Consequently there is asymmetric information between the real estate firm and the house owner and it is on that basis that real estate agents can offer a valueadded service. From a commercial perspective such a practice can be considered absolutely normal; however in this particular case the information on houses which is used by real estate firms is no longer completely private. Nowadays information on houses which are for sale on the Dutch housing market can be found on a large number of websites and are therefore accessible to the public. However website design still does not allow homeowners to perform easy comparison and therefore prevents them from valuating their house effectively and precisely.This problem raises the following research question: In order to access and store housing information Python web scraping techniques such as Beautifulsoup were deployed. Beautifulsoup creates the possibility for the efficient extraction of information from web pages which can be stored in comma separated files or Microsoft Excel files. To reduce the load on the server that contains the information only text fields were extracted. Additionally to maximize the scraping speed TOR network I.P. rotation was deployed. This technique prevents that the server might block the I.P address from which scaping is performed as the server observed load per I.P address is reduced to normal single user load (a maximum of 200 pages are scraped before the I.P address is changed). In the case that the I.P address is blocked the I.P. address is automatically changed and the scraping process continues. The TOR network provides access to 7000 proxies worldwide which makes the scraper very robust against antibot technologies. In total 72000 pages were scraped for 35 different house characteristics.In addition to the scraping of information the information was used to construct a prediction model for house pricing. This model uses characteristics of the house like the size of the house and the type of the house to estimate the selling price. In order to estimate this model a multiple linear regression model with White’s Standard Errors was used. This particular model was chosen to correct for the violation of the Gauss Markov assumption for orderly least square regression and to aim for the best linear unbiased estimators. The final model obtained an adjusted R2 of 0.87 and therefore provides an overall good estimation.With the aim of using the estimated model to improve the seller's position in the realestate market the model was embedded in a Shiny application. This allows house owners to determine the selling price based on their house’s characteristics.In order to estimate the selling price of a house within the Netherlands and to cross validate the valuation model multiple houses are selected from the best known housing website in the Netherlands ;  which represents 80% of the available houses on the housing market (). From this website 5 houses were randomly selected for estimation. The table below presents the results from the price estimation and indicates that the model predicts in line with expectations considering the R2 of 0.87. In most cases the model overestimates the sales price of the house except for houses in the higher price range as indicated by house nr. 5. Overall it can be concluded that having access to market information allows for a close approximation of the sales price of houses. This implies that for determining the sales price of houses that are considered average for the Dutch housing market a real estate broker is no longer required assuming that market information is available.Nowadays house owners are selling their house without the assistance of a real estate broker but are still dependent for the valuation of their house. This dependency exists as market information is not easily accessible for the public which makes it difficult to make comparisons among houses.. Research has indicated that with access to market information it is possible to closely estimate the selling price of the house without the intervention of real estate brokers. Consequently removing the dependency on real estate brokers and improving the position of sellers on the market as a valuation fee is no longer required. Therefore it is possible to conclude that the elimination of asymmetric information does improve the seller's position in the Dutch real estate market; however only applies to houses that are considered average in contrast to the existing housing market.,NA,House Selling Price Estimation
https://nycdatascience.com/blog/alumni/alumni-spotlight-jhonasttan-regalado-vp-deutsche-bank/,29,Jhonasttan Regalado worked in various technology roles at bulgebracket banks on wall street before joining Deutsche Bank as a VP leading their development and implementation of an IT Operations Support framework. He first became interested in data science as a casual interest taking self paced courses but hit a wall with the complexity of topics around Machine Learning. Rather than enrolling in a multiyear graduate program he attended a  at the .  After the intense bootcamp he immediately saw changes in his quantitative thinking and new opportunities in his work thanks to his deep understanding of data science.  I was introduced to the world of Data Science in 2013 by a Udacity course on Big Data. My attempts to grow my understanding through the MOOC experience hit a wall when I couldn’t make any further progress in my understanding of gradient descent for Machine Learning. I thought about potentially going back to grad school for Data Science to close my knowledge gaps through the traditional classroom experience but I didn't want to sidetrack my commitment to family (coaching my children’s basketball teams) and community volunteer projects. After having phone calls and facetoface sessions with instructors and students at the NYC Data Science Academy I decided to enroll in March for the Bootcamp cohort in the Fall.The experience was exactly what I was expecting. I returned to work with a new set of skills that include quantitative thinking and application research data processing automation exploratory visualization analysis and presentation. All of these skills are applicable to my new role as an IT Operations Manager for the Trading Floor in the Electronic Trading space. My team supports systems that include Algorithmic Trading solutions to clients. Thanks to the bootcamp experience and training in statistics I am very comfortable discussing the behavior of a trading algorithm that is expected to follow a daily volume curve. As well as setup experiments to capture metrics for evidence to prioritize automation projects that reduce manual efforts/costs.I was fortunate to find work as an IT Consultant for a Y2K project at Chase Bank during my last year as an undergrad at Baruch College. This was my opportunity into the world of IT. As my one year contract was coming to an end I realized through the job interview process that having a degree would not be enough to land a fulltime job at an established financial organization like Chase. The situation led me to signup for my first bootcamp. I spent two months immersed in classroom lectures and labs learning Microsoft Desktop and Server side products attaining my MCSE (Microsoft Certified Systems Engineer) credentials by the end of the program. It was no easy feat but I learned then the value of a wellorganized bootcamp training and the personal effort required to successfully finish the program with marketable skills. Soon after I was able to find my dream job at Nomura working as a fulltime Windows Systems Administrator for the Equities Trading business.My passion for learning the business new technology and building relationships opened doors to critical roles in IT Operations that were aligned with the Trading Desk. This foundation and practice led to opportunities at Macquarie and later at UBS where I had the managerial support and environment to develop a Client Services model that was aligned with the needs of the business.A willingness to adapt has been key to my career development in the Financial Industry. Each role has built up upon the previous to some degree. My approach has been to first assess my understanding of the role and expectations the systems process and procedures. I then determine if I will need to plan for offsite training to close knowledge gaps. I am fortunate to have started with systems administration where I learned about servers and network topology. I was working at Nomura in the World Financial Center when the unfortunate 9/11 event occurred. As a result we had to move all operations to Piscataway NJ which provided me with firsthand knowledge of BCP (Business Continuity Planning) and handson experience on rebuilding a datacenter. It was through the building of relationships across IT partners that I was provided an opportunity to move into development. Looking back I was very naive at the time and said yes to the role not knowing how painful the learning experience would be. It took me quite some time before I could wrap my head around the fundamentals of Object Oriented Programming. The handson experience in the workplace under the supervision of great managers accompanied with external training paid off significantly for me. I was fully engaged in a role where constant feedback from the business led to frequent code releases to production with application support provided by me and my supervisor. I grew my understanding of SDLC (Software Delivery Life Cycle) alongside good project and change management practices. These core skills would prove to be transferable to future roles in IT Operations.Please describe the recruitment process at Deutsche Bank including the interviews you did. Also what was the onboarding process like? Have you been promoted or taken on more responsibility there since joining?A friend in the industry was interviewing at DB (Deutsche Bank) for a Production Support role in Equities Cash High Touch in 2013. He was told his technical skills did not match the role. So he recommended me instead. As you can see it is important to treat others with respect as it is a small world. I was happy at UBS but felt a tug to consider the role as DB was undergoing cultural changes and was in the middle of a migration for a major trading application. I decided to embrace the opportunity and was quickly onboarded. As it turns out many systems were in mid migration phases so I dealt with a lot of bureaucracy until these systems related workflows were fully migrated. I was able to apply core technical skills and grow my management skills in the IT Operations role through the different challenges I would face for the next three years. My responsibilities have grown from developing the Equities Cash Client Services model to scaling the model to other Production Support teams in Equities Cash as well as building and developing the talent pipeline.   What are some of the pros and cons of working in IT at a big Wall Street bank?In my experience one of the cons of working in IT at a big Wall Street bank is that migrating from legacy environments is a complex process which requires constant planning and QA feedback from end users. The process is timeconsuming but critical in order to avoid major setbacks due to gaps in workflow implementations that result in financial regulatory and/or reputational impact to the organization. A significant pro for me is that if you are up to the challenge your consistent approach to assessing and understanding issues learning quickly from mistakes and bringing together a matrix team for problemsolving that leads to solutions this kind of effort is recognized and rewarded (e.g. new responsibilities promotions financial bonus). Your voice matters.How are data science AI and machine learning transforming banking in general and banking IT specialists’ roles and responsibilities in particular?For me the strength of Data Science is the use of the scientific method for validating your hypothesis through experimentation. You no longer rely on gut feeling alone to make strategic decisions that impact an organization for many financial quarters or years. AI and Machine Learning are helping to codify services traditionally provided by a systems engineer or a financial expert. I believe having a foundation in Data Science and Machine Learning is necessary to 1) improving your ability to problem solve with data and present your ideas 2) help make sense and align you with where jobs are headed (e.g. coding automation services) and 3) understand how to position yourself within an organization to add value. Whether as an Analyst Data Scientist or Engineer the ability to process and explore data identify patterns and predict/forecast trends are critical skills that help attain employment and/or expand career opportunities today.,NA,"Alumni Spotlight: Jhonasttan Regalado, VP at Deutsche Bank"
https://nycdatascience.com/blog/student-works/visualizing-trends-primary-education/,29,Primary education is a fundamental requirement for success. Regardless of how one might define the term “success” the skills attained in primary schooling are vital. Given that the nonpoor essentially have full access to primary education we can safely assume that those who aren't enrolled are also poor. These areas tend to be the poorest and are most affected by regional conflicts.Using available data visualization tools within R like plot.ly we can graph specific indicators that were provided by the World Bank () to better portray our findings. In the graph above comparing the cumulative dropout rate to the last grade in primary education indicates the proportion of pupils enrolled in a given grade who are no longer enrolled the following school year.Comparing GNI per capita using the PPP (Purchasing Power Parity) method we can find SubSaharan Africa and South Asia lagging behind the rest of the world. In the next graph we can then look at the net enrollment rate and find these two regions at the bottom end of the scale as well. This reaffirms our assumption that poorer countries and regions are most likely to lack access to primary education. While trends have shown some improvement there is much more work to be done.Now that we’ve identified where the problem areas are physically we can do a little more digging to understand some of the factors leading to the success (or failure) of primary education. Using Pearson’s method of correlation we can use R to calculate the covariance of x and y divided by the standard deviation of x multiplied by the standard deviation of y. This will better display how various indicators correlate to one another. Essentially we are quantifying the interdependence between two indicators in order to try and identify what drives primary education success.For the sake of the data we have available we can define success of primary education as the literacy rate in the adult population. In addition to serving as an indicator of primary education success literacy is an important factor in reducing poverty. A study by the World Bank linking education and poverty found that “in all cases where detailed analysis of household data has been carried out poverty rates are highest for households headed by illiterate people and decline with increased education of the household head.”The correlation plot identifies the drivers of increased (or decreased) literacy rates in the population aged 2564. Enrollment ratios and GPI (Gender Parity Index) have a positive impact on Literacy rates while Pupil to teacher ratios have a negative correlation. Gender parity displays the access of females to males in terms of education. The closer to 1 the more equal access is. Gender parity is especially helpful when looking at certain countries and regions that may not prioritize female education. Additionally we note the dropout rates rising with the pupil to teacher ratio. In other words as we have more students per teacher they are less likely to succeed and stay in school for the following year. This leads to another cycle of illiteracy and in turn poverty.If we want to get serious about tackling the lack of primary school to those in need we must use data and data visualization tools to shed light on the issues. It’s time to end the cycle of poverty and we can do so by providing basic primary education to all children. Ensuring that students are not vastly outnumbering their teacher proves to be an important factor to keeping students enrolled and improving literacy rates. If we can collect more data we can answer the questions we need and raise additional ones we should be asking. Further research shows that the highest returns in less developed countries come from primary education. We must prioritize our future generations and give them a fighting chance to attain something better.A child’s success in life should not be dictated by the region in which they are born.,NA,Visualizing Trends in Primary Education
https://nycdatascience.com/blog/student-works/welcome-new-york-metrocard-city/,29,There are two sources you can download the Fare Card History data starting from May 2010.One is the the MTA website:nhtesNY state open data website:.Either way you can get the latest dataset up to the previous week. For the shiny app the raw data set were downloaded at July 11th 2017. Since most of the downstream analysis are performed based on annual result  only data from 2011 to 2016 are used for current R shiny development. To understand the dataset I tried to figure out the dimension resolution and limitation of the data table. The MTA fare card history file contains mainly the date of swipe (first week day) fare type subway station ID station name and total swipe count aggregated by week. It is intriguing that the fare type in some degree provide demographical information of its user for example the Senior/disable fare type apply only to the Senior or Handicapped Rider while 30 days unlimited type is best for daily commuters.  The analysis will focus on  weekly activity.One way to narrow down the questions shiny app can possibly answer is to target the potential user or shark holder. By creating interactive data visualization I am targeting the user who is interested in swipe count information for any form of decision making. For example the app can help the manager determine whether to add extra service or/and maintenance to meet actual usage needs in a subway station. Another potential target stark holder would be the local business owner near a station who might benefit from adapting the business to according to the passenger demographical information.Here I designed an R shiny monitoring application to visualize the fare card swipe data in an intuitive way.Click the links  to open shiny:and source code at my repository:First I made an overall manual tab which contains a bar plot of both annual total subway fare card swipes and average monthly swipes from 2010 to 2016.  After calculating the total swipe count in each subway station or each fare type stations or fare types are ranked based on total swipe counts. Consequently it becomes much easier to find the top station or most common fare type using the ranked bar plots.There is  access to information of each subway station or specific fare type.  Click Station ~ Fare Type tub and use the filter menu to select individual station or fare type. Year based comparison are displayed at each quadrant. The next menu tab “Timeline” is designed to check a specific time window between 2011 and 2016 and  provide a detailed view if needed. The shiny app also made it easy to look at the swipe count changes through the years both overall and in specific fare types and specific station.Finally the raw dataset is provided for further analysis.This shiny app made it easy to extract MTA fare card swipe count information based on user's criteria. For example the overall card swipe count showed a slow increase of usage from 2011 to 2016.  While there wasa slight drop during 2015 that may have been due to the fare hike in November 2015. However checking by fare type reveals a clear decrease of Full fare type usage since 2011. There was  increased usage of Senior/Disable fare and Mail/Unlimited fare type from 2011 to 2016.  Overall MTA fare card data provided a straight approach to explore the MTA subway activity. Using the shiny app it become easier to monitor the passage volume change for a specific card type or in a specific subway station thus helps the decision making using filtered data and plot for both MTA passengers and business owner.[1] http://web.mta.info/mta/network.htm,NA,R Shiny Visualization and Insight of MTA Fare Swipe History
https://nycdatascience.com/blog/student-works/senti-meter/,29,The subsequent sections present the methodology including which data in the tweets are extracted visualized and subsequently analyzed to come up with a sentiment score used to rank every governor.The methodology employed in this study is summarized in Figure1. The first part of the study was focused on scraping tweets. Although Twitter provides tweets extraction using API it poses rate limits on a per user basis. Hence a decision was made to use Selenium for web scraping due to its reliability even though it can be slow at times. To filter out irrelevant tweets from individuals with similar names as the governors the tweets were sampled only if they had a reference to twitter verified @Governor handle. For example the official Twitter handle for the governor of New Jersey is @GovChristie. Once the Twitter handle was identified it was used as the Twitter search term and the resulting Uniform Resource Locator (URL) was used to scrape the tweets pertaining to the Governor of New Jersey. Tweets were not directly read from @Governor handle since this may be overwhelmingly positive or neutral. A similar process was adapted for the governors of the remaining 49 states. The objective was to extract roughly about 50000 tweets (1000 tweets per governor and 50 states) but the program ended up scraping a total of 43737 tweets (see Figure2). This is partly because some of the @Governor handles were not very active on Twitter and generated fewer than 1000 tweets. The extracted tweets were saved in a csv file with a tag name identifying each governor.Several visualization tools are employed to better understand the effect of each variable on the sentiment score and are discussed as follows:    We can draw  the following conclusions:,NA,Senti-Meter applied to US Governors
https://nycdatascience.com/blog/student-works/r-visualization/how-safe-to-walk-the-streets-of-new-york/,29,"As a New Yorker I walk to school and work and so have wondered about the safety of the streets I must go through. We hear about drivers plowing into pedestrians. In May 2017 this happened at Times Square and it resulted in the death of a young girl. Since then I have constant fear in my mind that what if someone rams car into the walkway? So with this fear on my mind I decided to visualize the NYC Motor Vehicle Collision dataset to determine some significant insights.In New York approximately 4000 New Yorkers are seriously injured and more than 250 are killed each year in traffic crashes. Being struck by a vehicle is the leading cause of injuryrelated death for children under 14 and the second leading cause for seniors. On average vehicles seriously injure or kill a New Yorker every two hours.This status quo is unacceptable. The City of New York must no longer regard traffic crashes as mere ""accidents"" but rather as preventable incidents that can be systematically addressed. No level of fatality on city streets is inevitable or acceptable. This Vision Zero Action Plan is the City's foundation for ending traffic deaths and injuries on our streets.The dataset provided is clean but it has missing values. So we have to remove all the NAs first in order to visualize the data. Each observation (row) in dataset represents one accident. The date column has no missing values. As the date is in character format we have to convert it to date format in order to extract day month and year from given date. Before removing NAs we can visualize how many collisions occurs in each year. The first visual is the line chart of Number of Accidents vs Year shows change in accidents from 2012 to 2017. We can clearly see that after vision zero initiative in 2104 the number of accidents are increasing and reaches its peak point in 2016. There is a huge drop in number of accidents from the beginning of 2017 until the present.The next chart  shows the number of collision in each year by borough. It reveals that Brooklyn has highest number of collisions in each year and Manhattan ranks second closely followed by Queens.How about which day of week has highest number of collision?Friday! For each year Friday has the highest number of collision. We can assume that people are keen to go home after finally finishing the work week.According to above heat map of Hour of day as a function of Borough in Brooklyn around 4 pm to 5 pm has the maximum number of collisions. Manhattan’s highest number of collisions occur from 2 pm to 4 pm. Queens has the most collisions at 8 am as well as at 4 pm to 5 pm.In the maps above the heat map(left) shows the highest number of collision at almost all the avenues. It also shows the maximum number at the approach to Chinatown. Williamsburg Bridge. The cluster map (right) show that the  Lower East Side has maximum number of collision(1225) followed by Midtown(1060)m then comes  Chelsea with 971 collisions.In the above Bar plot(left) of pedestrians injured per  year Brooklyn ranks the highest followed followed by Manhattan. For Manhattan the number of pedestrians injured gradually decreasing over a year. But in Brooklyn we see about the same number of injured pedestrians in both year 2015 and 2016. In heat map(right) 42nd Street and 8th Avenue shows the maximum number of pedestrians injured. In second place is 14th Street which is a major cross street. Canal Street also shows more injured pedestrians than in  other places.The number of pedestrians and number of collision in Manhattan overall are gradually decreasing but the next line chart is quite shocking!The line plot above shows the ratio of injured pedestrians to the total number of accidents per year. Surprisingly Manhattan has huge spike from 2016 to 2017. Even though the number of accidents are decreasing in 2017. The number of injured pedestrians is not. In 2016 Manhattan had 227763 collisions and 2085 injured pedestrians. In 2017 Manhattan had 115499 collisions which is less than half the amount  of 2016 and the number of injured pedestrians were 1046. That's shocking!Some major contributors to injury are identified in the heat map above. The  major factor is driver inattention and failure to yield right of way. Other major factor indicated by a yellow hue include backing unsafely and even pedestrian/cyclist/other pedestrians error/confusion.The number of collisions are decreasing through the year but the number of pedestrians getting injured is not decreasing.  Millions of tourist came to visit New York City every year and everyday millions of New Yorker use a walkway as daily route to home or work. Government has to take an extra step for the safety of pedestrians. Government should reduce the speed limit introduce slow zones and increased enforcement. For pedestrian confusion government has to make clear signs more stop lights for pedestrians and increase pedestrians crossing time. Government have to make walkway with metal safety poles because now a days anybody would ram the car into walkway or plow into pedestrians.",NA,How safe is it to walk the streets of New York?
https://nycdatascience.com/blog/student-works/web-scraping-glassdoor-insight-employee-turnover-within-financial-firms/,29,Financial institutions are facing a crisis. In the past banks had no trouble finding and retaining toptier Ivy league talent right out of college. The largest of institutions could outbid other industries and provide more career advancement opportunities. However since the financial crisis of 2008 the financial industry and as a result top tier organizations have had trouble attracting and retaining talent. Big banks are no longer the destination of choice as many of the best and brightest are looking to tech companies like Google Facebook and startups to launch their careers. Having worked at a few investment banks I have noticed a trend throughout my tenure. Employee turnover is a huge concern.According to research provided by Crowe Horwath employee turnover rates in the banking industry are at their highest point in the past 10 years.  The talent loss is costing banks in many ways. Not only is there a morale drain but there is a significant financial cost associated with this phenomenon that has been steadily increasing over time. Quinlan and Associates estimate that a 1% rise in voluntary employee turnover costs each global bank between USD 250500 million per year in replacement costs.  Costs associated with turnover include: temporary replacement/overtime recruitment training/loss of productivity and overall new hire costs. So what is causing such a significant increase? Can employers identify why their employees are dissatisfied and make meaningful changes to curb the continuous exodus?Glassdoor is a review website that allows employees (both current and past) to provide anonymous feedback about their employers. As you can see below a single review contains a good deal of information about an employee’s experience.       One of my central goals for this project was to identify whether we can learn anything significant about why employees leave financial institutions from employer reviews.In order to validate why employers should look at reviews on sites like Glassdoor I focused on Wells Fargo as a case study. Wells Fargo was at the center of a scandal in 2016 when over 2 million unauthorized accounts were opened on behalf of customers without their knowledge. These customers were then charged fees on the unauthorized accounts that were being collected by Wells Fargo. As a result over 5000 employees at the bank were terminated. Additional investigations highlighted that not only were senior management aware of these illegal practices but they were promoting them. When employees raised concerns they were reprimanded and in some instances even fired.If we looked at employee reviews of Wells Fargo could we have seen a problem like this arising before it was too late? Absolutely.The diagram below represents a word cloud of the ‘cons’ section of Glassdoor’s reviews for Wells Fargo. A word cloud highlights the most frequently used words as indicated by size. After cleaning up redundancies and common terms the top 150 words look like this:We can see the words: “sales” “management” “goals” and “pressure” were repeated quite frequently. Reviews containing these issues date back to 2009 (well before the scandal).  This is an unfortunate example of what happens when employers fail to recognize the concerns of their employees.In order to learn more about the overall sentiment of employees that work in Financial Institutions I scraped over 60000 employee reviews from 13 different types of  financial organizations (listed below). As part of the data selection process it is important to distinguish between types of firms. Not all banks are the same and we may see differences once we categorize them.Working at a bulge bracket bank is very different from working in a boutique firm. A bulge bracket bank is considered large and provides all services in all regions. In contrast a boutique is smaller tends to offer a few selected services (i.e. Asset Management or Mergers and Acquisitions) and is generally more flexible in terms of structure than a big bank. Furthermore we can break down these classes into subclasses. Using various rankings over the past several years we can classify Centerview and Evercore as the top boutique banks and Goldman Sachs Morgan Stanley and J.P. Morgan as the top big banks.As a side note technically Wells Fargo does not fall into the category of a bulge bracket given that it is not considered a global leader in M&A which is why it is absent from the analysis that follows.According to employee reviews big banks are rated higher than boutiques in terms of comp and benefits career opportunities and overall rating.Boutiques tend to outperform the bigger banks in the “softer” valued areas like work/life balance senior management and for the most part culture and values.When we further look by subclass we can see the top boutiques outperform everyone  even in areas where the big banks previously showed strength. As an employee you may have a more handson experience as there is less overall headcount. Subsequently you might learn a great deal more than working in a big bank where you may only see things from a siloed approach. A high overall rating implies that employees are satisfied for the most part. We can assume that an employee providing a high rating of an organization will be less likely to voluntarily leave than an employee providing a low one. Given the extremely high turnover in the industry financial firms should take notice of what values drive the overall rating the most. If we examine the correlation plot below we notice that higher overall ratings are mostly dependent on how employees rate career opportunities culture and values and senior management.If employers focus on these key areas they can help stop the outflow of top talent from their organizations. You might guess that employees leave mainly for one reason: money. And while it’s true that when employees leave they will almost always receive a bump in overall salary; it may not be the reason individuals look to leave in the first place.If we segregate the data on current and former employees we can compare their ratings to visualize why we are seeing increased voluntary turnover.We see from the above (surprisingly) that the ratings for comp and benefits have essentially been the same over the past few years. We do however find differences in career opportunities as well as culture and values. From the correlation plot above we know these are main drivers to the overall rating as well. As a result based on our data we cannot assume that employees are leaving financial firms because they are unhappy with their pay but rather they are unhappy with their opportunities for advancement and the culture within the organization itself.If we take a further look at the ratings provided for career opportunities we can analyze how significant the difference is in terms of how employees view their CEO. An employee can rate their CEO in one of three categories: approves of no opinion of or disapproves of. The yellow band in the box plot below indicates the mean of each group while the black line represents the median.The result of the oneway ANOVA test provides a pvalue of ~0.0 which identifies that there is a significant difference in means between at least one of the groups. In order to determine which one(s) we must perform some posthoc analysis. In this case we used Tukey’s range test for pairwise comparisons.Per the above there is a significant difference between the means of each group; as we can see the reject column is true for all three comparisons. This signifies that we reject the null hypothesis for each comparison and confirm the alternative hypothesis that the mean difference is statistically significant between groups.In other words an employee’s view of their CEO is related to how they feel about their career opportunities.The financial industry is facing a very real talent crisis that is both prevalent and costly. They are no longer attracting top talent as they used to and are failing to keep current employees happy enough to stay. This boils down to institutions struggling to accurately address the concerns of their employees.Financial firms can make improvements in the following areas: career development culture realignment and continued employee engagement.According to a survey by Quinlan Associates employees were not satisfied with their organizations’ promotion process. Employers must make strides in transparency which includes the sharing of KPIs (Key Performance Indicators) around what it takes to be promoted. Given that junior staff have a higher turnover rate financial institutions must also make an effort in rebalancing their topheavy hierarchical structures to ensure midlevel employees have room to grow vertically. Additionally employers must ensure that employees are continuing to grow. After a few years in a particular role one might become a subjectmatter expert and be heavily relied upon within the organization. While this may have some short term benefit from an employer vantage point for the employee it could lead to a longterm problem of plateauing from a growth perspective. In order to prevent that result banking organizations should promote crosstraining and internal mobility across their firm.Since the financial crisis the prestige of the financial industry has taken an enormous hit from a reputational standpoint. Scandals at big banks are costly not only from the potential loss of customers but for employees as well. An individual should be proud of their career and in turn the firm where they choose to spend it. Financial institutions have failed to live up to their personnel standards in this area. Corporate greed can no longer go unnoticed. Organizations must ensure that the environment they have created adequately lives up to the requirements of their employees. A major problem faced by big banks is bureaucracy. While there are certain implementations that can take a great deal of time acknowledging employee feedback is an important step. In order to make meaningful change employee voices need to be heard and recognized. Ensuring there is open communication can significantly alleviate the talent crisis financial institutions are facing. Not only is it vital to gather feedback but taking strides to incorporate these observations is imperative.Unless these institutions make meaningful changes in their culture and career opportunities the banking industry will continue to face the trend of voluntary employee turnover in the wrong direction.,NA,Web Scraping Glassdoor: An Insight into Employee Turnover within Financial Firms
https://nycdatascience.com/blog/student-works/khrushchev-kaggle-russian-real-estate-market/,30,In April 2017 Sberbank Russia’s oldest and largest bank created a  with the goal of predicting realty prices in Moscow. Given Russia’s volatile economy this was a unique challenge. Sberbank provided Kagglers with a rich dataset of over 200 variables depicting both housing features and macroeconomic information to address this challenge.With only two weeks to generate results from this daunting dataset our team had to quickly explore the data and implement machine learning techniques to best predict housing prices. After a couple of days of individual research into Russian real estate and investigation of Kagglers’  our team set out to manage our project as depicted in the workflow below.The process was fastpaced and iterative as each model provided new insights and questions into the data. Below we describe the exploratory data analysis cleaning and imputation feature engineering and model selection and performance. The primary models we used for this task were:To have an overview of the percentages of missing values in each of the dataset we constructed the barplots below.In the train dataset out of the 291 features 51 have missing values. Out of the 290 features 48 have missing values in the test dataset whereas for the macro dataset 91 features have missing values out 100.With reference to the extensive EDA prepared by Troy Walters (another fellow Kaggler) we identified the train/test features which have strong correlation with price_doc (target variable) and started exploring each feature to get a better grasp of the nature and distribution of the data.A histogram plot of the “price_doc” indicates that the distribution of price is rightskewed as shown below. We logtransformed the “price_doc” to obtain a normal distribution.Exploring further and analyzing each feature revealed substantial amount of data quality issues. The below boxplots for “material” and “state” columns against the “price_doc” show unusual distribution for material value 3.0  and state value 33.0 indicating potential issue with data. Further inspection showed the presence of only one observation for material  3.0 which made sense to assign it to NA. “state”  33.0 has been most likely wrongly recorded and is corrected to 3.0.The “built_year” column contained erroneous entries such as “20052009” “4965” “71” which we corrected as “2009” “1965” “1971” respectively using best logical judgment. Also there few observations where the “built_year” values were entered under “kitch_sq” column and few where the values were being recorded with single digit number. We placed the values under the correct column and  assigned NAs for those records with single digit built year value.For records with “life_sq” greater than “full_sq” and “full_sq” greater than 1000 we reduced the “life_sq” and “full_sq” values proportionately by a factor of 10 based on the distribution of the data values. For records with “max_floor” > 60 the “max_floor” values were assigned to NAs as they are likely potential outliers.For NA or 0 “life_sq” values we imputed the missing values by the mean of the “life_sq” column values if the mean is less than the corresponding “full_sq” value of each record. Otherwise the values are imputed with (mean10). Missing “max_floor” values were imputed with the median value of the “max_floor” column. For “material” and “num_room” we used mode of the respective column values to impute the missing data.“built_year” and “state” had substantial number of records ( 13605 and 13559 respectively in the train dataset; 1049 and 694 respectively in the test dataset) with missing values as shown in the missingness matrix below. We did not think it was a good idea to just consider complete cases as there might be potential information loss given the high number of missing records for these two features. We used other features with available data as predictors and set these two features as “target” variable (one at a time) and generated predicted values using XGBoost.Since the dataset had quite a few categorical features we used Label encoding to convert the categorical values into numeric values. Along with the original features we combined features that were showing up as important and created new features. To calculate the importance of the variables in predicting the housing prices we used Ridge and LASSO regularization as generalized linear models. The model was validated using 100 values from 0.01 to 100 for the shrinkage penalty lambda. Before this the features with skewed values were normalized so that they follow a normal distribution. Since the magnitude of coefficients distort the output of the model we standardized the values using a standard scalar. The coefficient plot of the ridge regression shows the features in the descending order in which they converge towards zero. However the L2 regularization does not select a subset of important features and hence we proceeded with the LASSO. We fed the model a range of lambda values similar to the ridge regression. The benefits of using the L1 norm over the L2 norm is that after a certain value of the shrinkage parameter the coefficients drop to zero. The LASSO model eliminated 363 features and selected 95. The rmse values of the Ridge and the LASSO regression occurred very close to each other. For an initial linear regression we took Lasso’s top 30 important features and only the complete cases within the dataset so as to not deal with any imputation. This amounted to about 14000 observations to train the model. Using R’s lm function we trained and tested the linear model on an 8020 split of the training data. Unfortunately further investigation proved that the data violated the assumptions of linearity. The QQ plot (the first graph below) is in violation of the normality assumption of the residuals. Additionally we found that a couple of the observations were outside of Cook’s distance showing they are highly influential outliers.Taking these results into consideration we eliminated those two outlying observations from the dataset and ran a backwards stepwise function measuring BIC to narrow down the number of features to 20. Next we tested the VIF (variance inflation factor) of the features as a way to target multicollinearity and eliminated three more predictors. With 17 features the model had an R squared of 0.3366 and our predictions had an RMSLE of 0.46986.We then ran a second multiple linear regression in python using the cleaned imputed data and 46 predictors. Some of these predictors came from the macroeconomic dataset as well. Dropping 17 variables after running a VIF test left us with 29 predictors and an R squared value of 0.3631. This model improved our RMSLE score significantly to a 0.39653.We also employed a random forest model on our training set with the hopes of yielding more accurate predictions. Random forests are powerful models because they theoretically can’t overfit the data but we had to be wary of parameter tuning and predictor variables included in the model. We used scikitlearn’s RandomForestRegressor module and experimented with different minimum sample leaf sizes number of estimators and model features.An important aspect when improving the random forest models was deciding which variables to include in the model. Since the data was a time series we wanted to make sure that we had variables which captured the change over time for select variables. In our case we looked at the data in the macro dataset to get a high level picture of how the Russian economy might affect the housing prices. From the macro dataset we included a one month three month six month and one year time lag to show the rate of change for oil prices US dollar to Russian ruble exchange rate and inflation rate. We then included these features as predictors in our random forest regression model.In the end our best model gave us a root mean squared error of about 0.326. Unfortunately the feature engineered variables which were mostly variables included from the macro dataset did not actually improve our model in any way with our most important features consistently being variables that described housing attributes and location attributes. Although a standard random forest model won’t outperform an XGBoost model it is easier to understand which features might be important to the model and with more time I would attempt to make the feature engineered variables improve our model.Having obtained a score of 0.326 with a Random Forest model we then began training a model using XGBoost. XGBoost short for extreme gradient boosting is a powerful variant of  algorithm and can be consistently found in Kagglewinning models. In short Random Forest models use random subsets of your data at each iteration whereas XGBoost iterates depending on each previous iterations outcome.As is often the case with training supervised machine learning models we optimize an objective function (consisting of a loss component and a regularization component) to determine the parameters that will give us the strongest model:Obj(Θ)  L(θ) + Ω(Θ)In XGBoost the parameters tune the regularization component of the objective function with each playing some role in the complexity of the model. With our parameters tuned we were able to obtain a score of 0.3131 outperforming the rest of our models (details on what changing each parameter does can be found ). After two weeks and forty submissions our team landed in the top 6% of 2000 Kaggle teams with an RMSLE score of 0.3131. Although less interpretable the blackbox XGBoost model performed the best. Surprisingly imputing with the median (rather than any sort of regression or nearestneighbors algorithm) and eliminating outliers before training the models gave better results as well. As further investigation into our results we graphed the density plots of some of our predictions for the test set.As shown in the chart above our best score seems to be the most normallydistributed about an average price around 7400000 rubles.  We believe that running a principal component analysis for dimensionality reduction and testing out further feature engineering ideas may lead to better results. Additionally practicing Bayesian Optimization for finding the best parameters would be much more timeefficient than a grid search. Now that we have seen how the models work in practice and learned how to best communicate our results with one another we can better plan how to connect our code and possibly put together an ensemble or stacked model. Additionally given the timeconstraint of the test data implementing time series analysis would lead to a better interpretation of the problem.,NA,From Khrushchev to Kaggle: The Russian Real Estate Market
https://nycdatascience.com/blog/community/nyc-data-science-academy-hiring-partner-event-june-28/,30,There are no fees for graduate recruitment. If you are interested in hiring data scientists from us there is more information at  or send us a message via .,NA,NYC Data Science Academy Hiring Partner Event June 28
https://nycdatascience.com/blog/r/pursuit-of-happiness/,30,As per the  dictionary happiness is defined as a state of wellbeing and contentment. Over the years several attempts have been made by philosophers and researchers to quantify happiness (for instance see [12]). Such measures have then been used to determine the overall happiness levels of nations in the world. One such significant survey was conducted by and the data from this study was compiled in the . The data from this study is available on  for the years 2015 2016 and 2017. As per this study six key variables play a role in determining the overall happiness of a nation. These include—Data for variables 36 (above) were collected based on binary responses to Gallup World Poll questions. The purpose of this blog and the  is to provide life to the available raw data with the aid of visual graphics and to employ a bit of statistics to assess the importance of each of these variables on the overall happiness of various countries around the world.The raw data from  was available as three separate Comma Separated Values files. The data in these files did not contain the latitude and longitude information that was necessary for constructing the Geo map using the gvisGeoChart function. Therefore the package ‘rworldmap’ was used to import the latitude longitude information. Finally the package ‘dplyr’ was used to clean up the data and the function ‘rbind’ was used to generate a single dataframe that was stored as Happiness_Data.Rdata file for data visualization and statistics.With regards to the design of the shiny app the cerulean theme from  was chosen for its simplicity. The app allows users to select a variable and year to interactively display various visualizations. All the source codes for this app are available on .Several visualizations tools were employed to better understand the effect of each variable on the happiness of the nation as described below—An interesting feature of the motion plot is to enable the trails feature to track the progression of specific countries (bubbles). For illustrative purposes Burundi Syria Nigeria and Switzerland were chosen for the trail plot solely based on the GDP. It is evident from the trail plot that the trajectory of these nations from 2015 to 2017 had a negative slope indicating that although the GDP increased during this time frame the life expectancy values have come down. This observation holds true for most of the other nations as well. The negative slope is steep for Syria highlighting a significant decrease in life expectancy since 2015. Also Syria and Nigeria appear to have similar scores for the GDP but the life expectancy in Nigeria is significantly lower than Syria. Despite this Nigeria seems to be relatively happier than Syria. It can be a good exercise for the reader to  further and determine if this data along with proximity of Syria to Europe provides a rationale for the influx of refugees over the past three years.Visualization and Statistics bring life to the raw data and offer unparalleled insights into the relationship between the variables contributing to the happiness score.[1]  Kalmijn Wim and Ruut Veenhoven. “Measuring inequality of happiness in nations: In search for proper statistics.” 6.4 (2005): 357396.[2]  Frijters Paul. “Measuring happiness The Economics of WellBeing.” 53 (2016): 180183.,NA,Pursuit of Happiness
https://nycdatascience.com/blog/student-works/college-scorecard/,30,"  Given the advantages of a college degree paired with rising costs it is important for prospective students to be able compare costs across schools as well as assess their postgraduation outcomes.I used the  released by the Department of Education for my analysis. The Department began releasing the College Scorecard in 2015 to improve transparency in higher education and hold colleges accountable for measures like value and quality. The full data set has information on almost 8000 institutions in the United States including community colleges undergraduate schools and postgraduate institutions like law and medical schools. It also contains over 1500 variables including:It is important to mention that many of these variables including median earnings and graduate debt only apply to student borrowers of federal loans and may not be representative of students who have private loans or no student debt.For my analysis I specifically looked at institutions that offer fouryear undergraduate degrees and focused on variables related to cost and postgraduation outcomes. I attempted to answer the following questions:First I wanted to get a sense of earnings and employment prospects of former students and compare that against the average cost of each school. For prospective students considering loans to pay for college it might be valuable to understand where they can get the best ""bang for their buck""  schools with low average costs but relatively high earnings among former students. I graphed average costs for each institution against 10year median earnings separated by school type.Some insights from the graph included:Next I was curious to see what average costs debt and earnings look like across the United States. I used the College Scorecard data grouped by state to create a heat map of each value with leaflet.From mapping the data I found that:Finally I wanted to see whether there was a difference across school type based on several different variables. Specifically I wanted to look at demographic and outcomes data for private forprofit colleges which have  in the United States for their predatory recruitment practices and poor postgraduation opportunities.Insights included:The College Scorecard data definitely has its shortcomings  much of its data is based on students who have federal loans and it may not completely represent the full undergraduate population. However it also provides a trove of information that was previously unavailable including data on student outcomes. While no single data point can capture a school's ""value"" the College Scorecard is a very useful resource for prospective college students to understand and compare different schools across a variety of important metrics. I invite you to interact with my  to further explore the data and my insights.Link to my .",NA,What school is the best
https://nycdatascience.com/blog/student-works/ny-public-school-water/,30,            Beginning of this year we received a letter from my daughter’s middle school regarding water sampling from all the water outlets in the school for lead level  in the water. The test was done during 2016 school year. We were very surprised to know that seven outlets had more than 15 ppb(parts per billion) which is the Environmental Protection Agency’s threshold for action. Highlevel of water lead is known to have cause numerous health problems specially to younger kids and unborn children.  This is obviously very concerning for all parents of New York public schools. I immediately asked my daughter if they were also informed by school staffs and know which are those seven outlets are and she said “Yes”.       But what about other schools? I wanted to know. That is the motivation of this shiny project. This shiny app is created using R language with various R packages specially ggplot2 shinydashboard plotly and leaflet in shiny web framework. The main purpose of this app is to locate the New York public schools  and provide available data about 2016 water sample testing for lead level on the leaflet map. This app can also be used to generate various plots to visualize the data to see the distribution of counties with highest number of schools with most water outlets above federal lead limit for action. ,NA,How safe is New York public school drinking water?
https://nycdatascience.com/blog/student-works/r-shiny/premium-airline-performance/,30,Have you ever found that the flight for which you paid a premium fare did not live up to your expectations? Maybe you would like to be able to select an airline that has a better track record. But where to find such information?As airlines transported 3.6 billion passengers in the year 2016 which is about 800 million more than in 2011 (IATA 2016). Over the last 40 years global air travel has increased tremendously as it has become more affordable. This decline in fare price is the response to the airline deregulation act from 1978 and to the dynamics in the elasticity of demand for air transportation. This decline in the fare price is desired by most passengers as research indicates that the ticket price is still the most important feature on which passengers choose an airline despite other investments such as loyalty programs and the overall improvement of the customer’s experience (Skyscanner 2009).  However booking the flight with the lowest fare is not always the best strategy. low cost carriers are often also leaders in late arrivals low customer satisfaction uncomfortable cabins and unsatisfactory frequent flyer programs (NBC News 2017). While one group of passengers aims to minimize flight fare a second group has other priorities. For example a flight is far more valuable for a salesperson who suddenly has an opportunity to visit an important client than to someone visiting a friend or relative abroad. Consequently the salesperson is more willing to pay a higher fare in order to make the appointment (Airline Economics 2017). As the price of the flight is of secondary concern the salesperson is interested in other performance indicators for flight selection. That raises the following research question: To assess airline operation a wide variety of indicators are available; however not all of these indicators are relevant for the passenger. Relevant indicators of operational performance might be: percentage of flights delayed average minutes delay per flight percentage of flights delayed per reason and for example the Number of flights to a particular destination (NWDS 2017). In order to present such insights to passengers a reliable longitudinal data set that allows for the analysis of multiple airlines over an extended period of time is required. The time period should be chosen carefully as airline performance tends to sway over seasons which could sequentially influence the results (Bureau of Transport Statistics 2002). Consequently the time period is set to the last month of flight information in order to prevent seasonal bias and to ensure that passengers are informed about the current state of affairs. A dataset that allows for such an analysis is the U.S. Airline OnTime Performance dataset from the Bureau of Transportation Statistics U.S. Department of Transportation () .In order to inform passengers on airline performance indicators an application was constructed using R and Shiny. The application can be found at the following internet location:.To evaluate the functionality and usability of the shiny application a flight scenario consisting of a flight from Atlanta Airport to San Francisco Airport on the 16th of Aug returning on the 23rd of August was designed.. The possible flights options were determined by using Sky Scanner as well as the pricing options for these flights. Sequentially in order to determine the “best” flight the presented performance indicators and statistics from the Shiny application were used.For the trip from Atlanta to San Francisco there are three possible options for direct flights: Delta Airlines Frontier Airlines and United Airlines. These options were found using Sky Scanner and align with the information available in the Shiny application. For these airlines the following airline wide information is available:From these results one could infer that the performance of Delta Airlines with regards to delayed flights is better in comparison with Frontier Airlines and United Airlines. This might be a first initial indication of the airlines performance for the route from Atlanta to San Francisco; however does not necessarily mean that Delta Airlines performs on average for that particular route. Consequently route specific analysis should be performed.The Shiny application also provides the functionality to compare specific routes as route performance might deviate from overall performance. When the airlines are compared for the route Atlanta to San Francisco the following route performance is presented:For this route the presented route performance indicates that the flight duration (taxi out + air time + taxi in) is the longest for Frontier Airlines and comparable for Delta Air Lines and United Airlines. It is also possible to infer that the overall delay for Frontier Airlines is longer than Delta Air Lines and United Flights which is in line with the earlier findings from the average airline performance. However from this information as Delta Airlines and United Airlines are somehow comparable it is not yet possible to conclude which airline performs better. Consequently the flights are compared using ANOVA and a TuckeyHSD posthoc test (a test to compare the average flight duration for more than two groups). The table below presents the results and indicates that Delta Air Lines flights and United Airlines flights are both statistically shorter than Frontier Airlines but that there is no difference between United Airlines and Delta Air Lines.At this point of the selection process the date airports and airline have been selected which leaves the flight time. As Delta Air Lines provides seven opportunities on the 16th of Aug to fly to San Francisco. The passengers can select the flight time while taking into account factors that might influence the delay of the flight. Delay information for Atlanta International Airport indicates that as the day passes more flights become delayed. Consequently you’re better off flying earlier in the day if you want to reduce the probability of a delayed departure This selection can be made without having to take into consideration any price changes as the flight fare is the same during the entire day.Since the era of mass transportation the price flights have become ever more important for commuters. However another group of travelers exists that next to the price of a flight also highly ranks the punctuality of a flight. To facilitate this group of travelers in the flight selection process a Shiny application was constructed using data from the Bureau of Transportation Statistics U.S. Department of Transportation. The results from a scenario study indicate that information on flight performance can assists in the selection of airlines for specific routes within the United States and assist in the selection for specific flight times in order to reduce the probability of delay. However the findings presented within the document are limited to U.S Airlines for the period of April 2017 and therefore do not reflect future airline performance,NA,Are Premium Airlines Also Premium in Performance? / A Shiny Application
https://nycdatascience.com/blog/student-works/r-shiny/match-skill-job-simple-job-recommendation-system/,30,"In 2012 Harvard Business Review declared that the s data scientist was  the ""sexiest job of 21st century."" The demand for good data scientists has risen in large industries because big data has become mainstream. Businesses are aggressively looking for ways to use the massive amounts of data they are collecting and storing to gain new insights. McKinsey estimated that by 2018 the U.S. economy will have a shortage of 140000 to 190000 people with analytical expertise.As the job title 'Data Scientist' is used fairly loosely few people knows what it really means. As a result there could be quite a gap in skillset requirement between two job listing . A 'Data Scientist' could be someone who only knows crunching data with Python but it also could be someone like Andrew Ng who leads the  Baidu AI team in developing great AI products. While one has some data science skills the other  has to possess not only solid theoretical/programming knowledge but also keen business acumen and strong leadership ability.  The difference in the skillsets  will obviously leads to a salary gap. To gain a deeper insight of this gap I decided to scrape the information on data scientist jobs info from Glassdoor.com.Job hunting websites like Glassdoor Indeed and Monster are all good resources for acquiring job info. The reason I am choosing Glassdoor is because of its standardized webpage format. Different from Glassdoor Indeed and Monster redirect job postings to original recruiters' websites. From the perspective of webscraping it is hard to guarantee the dataset quality by scraping web pages with different structures.It is always interesting to find correlations between two variables that appear completely unrelated. Before diving into the website I brainstormed some questions:With these questions in mind I picked the following features:The scraping process can be divided into the following 2 steps: Code:In real estate the median salary ($116.94K) is  almost double the median salary ($62.42K) for the notforprofit industry. The following figure delivered a comprehensive image of how data scientist are positioned in each industry. Considering both the facts of number of positions and median salary the  IT industry outperformed all the others.  Within the IT industry most hiring positions came from Walmart eCommerce. Netflix provides the higher median salary around $180K.Users can upload their CV and input skillset to find job postings requiring similar skillset. To facilitate user input the skillset input bar is categorized into different buckets including education major programming skills business intelligence and big data skills. When they click the search button the shiny app will call help.py script  to calculate the Jaccard similarity score between user skillset and job requirements.  The Jaccard index is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets.In this scenario A represents user skillsets and B represents job requirements. Example:The similarity between the user and the job is 0.1428 (1/7). Code:",NA,Match job with your skill: A simple job recommendation system
https://nycdatascience.com/blog/student-works/imdb-runtime-vs-success/,30,"Years ago students of film writing were advised to make their feature length movie scripts between 90 and 129 pages.  A page is expected to be one minute of runtime.  Today these numbers have been revised to a simple target of 110 minutes.  A little fun fact connected with this:  my professor tells the story of an insider’s view from development and how the decision makers at one of the giants used to pick up a screenplay weigh it in his hands and say “too light” or “too heavy."" Then the script was rejected without ever getting a single page read.  Are these page length numbers arbitrary or is there something to these guidelines?With the bin size set very small in the above visualizations we can see movie releases increasing over time with the rate accelerating much more starting in the 1970's. Increasing the bin size from 2 to 28 further illustrates how in recent years there are so many more movies then in the past. A quick look at the history may shed a little light on what's going on here.  The first motion pictures were created in the 1890’s and were little more than short black and white moving images.  As the novelty of this waned next came serial shorts from which the term “cliff hanger” originates.  When real fulllength stories started to evolve they were  expensive and hard to produce.  As a practical matter only small numbers could be made per year. Today the bar has been raised but technology keeps lowering the cost of what it takes to meet that bar.  Independent and ancillary markets have widened the distribution channels and even YouTube is becoming a part of this expanding network.  It’s just a matter of time before low budget movies get made on smart phones if they haven’t been already.  Nothing earth shattering here but the visualization does help show that the runaway escalation started during the time when “” “” and ""” all made their debut.  Many see this time as “the beginning of the blockbuster.”As shown here the data used in this application is organized into 5 subsets of data:There is some overlap in the data sets shown on the application's ""Data"" tab:Click on the ""Visualization"" tab of  to obtain basic stats on each of the aforementioned  datasets (Min Max Mean Median and Quartiles) for these 4 numeric variables:This tab also provides a line plot using  linear regression which we can analyze for the general trend in movie runtimes that we are looking for.  If we start with the plot for “all the data” in our application outliers are mostly clustered pretty close to the general confidence interval for the model.  No outliers outside this range appear after 1990 and only a small number of points barely outside the confidence interval appear from 1960 to 1990.Since there is a limited outlier effect the mean seems like a reasonable metric.  It is 109.2 minutes. Of more interest to the original question this plot essentially reflects a 5000+ record sample of what got made.  The hills and valleys of the line seem to range between 105 and 120 minutes up through the 1970’s.  Then the line becomes a slightly downward sloping trend up through the present with our data points mostly appearing at around the 110 minute mark. Though anecdotal in nature the original 110 minute recommendation for movie scripts would seem to be supported by the data.  The confidence interval around the line though might suggest a range from about 100 to 120 minutes.  If our movie gets made we may then be concerned with what are its chances of making it into the top or bottom? Starting with the Top 250 Movies:The mean for the Top 250 movies was 130 minutes.  The curve trends upwards over all with a range in recent years (1990 to the present) that fell between: 130 and 140 minutes.  There are a larger scattering of outliers on this plot but based on how the outliers mostly cluster not too far from the confidence interval after 1990 the mean still seems reasonable to use.  If we think about why these better received movies are often longer than the general trend for what gets made I’m sure there are many factors involved.  For one thing big name directors and  producers have the kind of clout to make us sit through longer movies.  Consider “” saga which collectively was over 9 hours long with each of 3 movies lasting around 3 hours a piece.We don’t want to end up in the bottom so we'll take a look at this too:The mean for the Bottom 100 movies was 92.26 minutes.  This curve is also showing a distinct upwards trend over all but with a range in recent years (1990 to the present) that was from about 90 minutes to 115 minutes.  There are fewer outliers but there is also less data in this grouping.With more time and development a more thorough analysis could be developed from IMDB data. (available on Git) were used to gather source data.  A full workflow from initial input files to what got used in the application is provided in the .  As this process was experimentally developed and modified while creating the source data for this project R markdown was used to step through the process and keep notes on what was going.  High level:The CEO of a company I used to work for would tell stories of how senior management of various IT areas under him would plan and design solutions on a napkin in a bar which formed the basis of software project requirements.  For this shiny app the “napkin” was actually a piece of notebook paper captured as a  for your amusement.  Only a small piece of this plan has been realized so far.",NA,Runtime vs. Success (Using IMDB)
https://nycdatascience.com/blog/student-works/kaggle-competition-russian-housing-price-prediction/,31,We divided features into 16 subgroups (i.e. demographics). We ran random forests and Lasso on each subgroup.Interpretations: In most groups LASSO would provide that all the features in the group are significant at the minimum MSE level for lambda parameter.In order to select features we have chosen parameters that would go to zero slowest as lambda increases.35 features came out to be ideal in this case with MSE of 0.31 or RMSE of 0.56 for training data set and 0.46 on Kaggle’s testing set score 0.35. We have also tried utilizing Multiple Linear Regression using select features that would make sense in making housing price predictionFor the simplest model we have used ‘full_sq’(size of the unit) ‘ttk_km’(distance to the Third Ring) and ‘public_transport_station_min_walk’(minutes to walk to public transportation station)Result was that this simple model gave a superior result to LASSO model with 35 features with RMSE of 0.499 on training set and Kaggle’s score of 0.37535Using 15 features we were able to lower RMSE a bit further to 0.466 on training set and Kaggle’s score of  0.35189Macro data may not be as helpful as it is time series data and if year/month are included as independent variable it would incorporate the time elementFeatures Selection: 11 main features + 28 selected features +macro features,NA,Kaggle competition (top 3% ):  Optimizing Russian housing price prediction by a deep dive into the model selection and feature engineering
https://nycdatascience.com/blog/student-works/forecasting-economic-risk-eu-2020/,31,In the last decade the European Union (EU) economy has been negatively impacted by a series of events most notably the global financial crisis (2008) the European debt crisis (2009) and the Brexit vote for the UK to leave the European Union (2016). In this era of political instability investors and companies alike are curious to know whether this is a time of financial opportunity or risk. This analysis aims to assess the economic state of the EU into 2020 using a data science approach.To answer our business question we determined that forecasting GDP growth is an appropriate proxy for predicting the overall economic state of the EU. To simplify our analysis we selected EU countries that 1) are leaders by total GDP and 2) demonstrate unstable GDP growth in the last decade. We believe this subset of countries is pivotal to the shortterm future overall economic state of the EU.The gross domestic product (GDP) is one of the primary indicators used to gauge the health of a country's economy. It represents the total dollar value of all goods and services produced over a specific time period. GDP is often expressed as a comparison to the previous quarter or year. Accordingly a yeartoyear GDP growth of 3% can be read as the economy having grown 3% during the oneyear period. Forecasting GDP can provide valuable information to different groups. For exampleThe European Union (EU) is an economic and political partnership among 28 European countries. It was founded after World War II to foster economic cooperation with the idea that countries which trade together are more likely to avoid going to war with each other. The Eurozone on the other hand spans across 19 of EU member countries that use a common currency called the Euro.Since its establishment the EU grew from 6 to 28 members added 300M to its population and grew GDP by more than sevenfolds. ()Germany France United Kingdom Italy and Spain lead the EU with both the largest GDP and population in 2017.GDP per capita a proxy for measuring average individual wealth in a country varies greatly among the EU members.Unemployment rates in the EU are at record highs in the past decade including Italy and Spain who contribute a large portion of total GDP in the EU.More than half of the EU members exceeded a debttoGDP ratio of 60% which is alarming for developed countries.The data was restructured to a tidy format where:Storing our data in a MySQL database through Amazon Web Services Relational Database Services (AWS RDS) facilitates more effective collaboration and allows for reproducible research.As mentioned previously this analysis focuses on EU countries that 1) are leaders by total GDP and 2) demonstrate unstable GDP growth in the last decade. While the former can easily be determined kmeans clustering was used to assess the latter.Along with using GDP growth as an input we determined the five most important macroeconomic features to predicting it (process discussed in the Feature Selection section). Using these six features kmeans clustering was used to group each observation into one of five categories. We interpreted each of these five groups as different economic states a country has shown for a particular year ranging from very bad to very good.Of the top 5 EU countries by GDP in 2017 three countries (UK Italy and Spain) demonstrated unstable economic states over the past decade (i.e. fluctuations in economic states and/or high number of years with poor economic state.) These countries are selected for further GDP forecast analysis.The main application of an Autoregressive Integrated Moving Average (ARIMA) model is in the area of short term forecasting requiring at least 40 historical data points. It works best when the data exhibits a stable or consistent pattern over time with minimum number of outliers. In our analysis we have at least 50 historical data points for each of the macroeconomic indicators which makes ARIMA time series forecasting appropriate to use.Prior to forecasting GDP growth we transformed all 460+ features to growth rates. This process acts as a form of standardization to make comparison possible between countries. Additionally standardization is necessary to mitigate unwanted bias when conducting feature importance such as Lasso shrinkage and predictions using linear regressions.The example below shows the transformation of the population feature to growth rates.To forecast GDP growth using an ARIMA model the only feature required is the GDP growth itself.“A stationary time series is one whose statistical properties such as mean variance autocorrelation etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary.”Prior to forecasting both DickeyFuller and KPSS tests were performed to validate the stationarity of the GDP growth rates for each of the three selected countries.An ARIMA model is comprised of three components: AutoRegressive (AR) Integration/Differencing (I) and Moving Average (MA). These components correspond to the parameters p d and q respectively when configuring an ARIMA model.Provided that our GDP growth rates are stationary (d  0) cross validation can be performed across a range of p and q values to identify an ARMA model minimizing the Schwartz Bayesian Information Criterion (BIC) metric. Essentially the model with the lowest BIC is more efficient in predictions as it favors models with high accuracy but penalizes models that are complex.Choosing the best model with the lowest BIC we forecasted the GDP growth rates for each of the three countries into 2020. The forecasts can be found toward the end of the blogpost.When choosing appropriate machine learning models we took into consideration of the limitations of using a small sample size. As we are limited to less than 60 observations for each feature across each country it was important to use simpler models to avoid the issue of overfitting.We determined that using multiple linear regression and logistics regression models were most appropriate for our circumstance. Because these models do not handle complexity very well we perform feature selection in the next section to limit the number of predictors used to forecast GDP growth.Of over 460 macroeconomic indicators the 5 most important predictors of GDP growth were chosen to conduct the forecasts of GDP growth.Lasso regularization works by weeding out less important features by shrinking their coefficients to zeros using the L1 penalty.The chisquare test measures dependence between stochastic variables so using this function weeds out the features that are the most likely to be independent of class and therefore irrelevant for predictions.The screenshot below on the right side shows the feature importance as determined by the chisquare test.Using lasso regularization and chisquare test for feature importance we selected the following five features for predicting GDP growth.Previously GDP growth was forecasted using itself as a predictor (i.e. ARIMA time series forecasting). In this section GDP growth is forecasted using the top five predictors identified during the feature selection process. In order to forecast GDP growth using these predictors we used the ‘auto.arima’ function from the R ‘forecast’ library to forecast each of these five features into 2020. These forecasted values will help us forecast GDP growth values using a multiple linear regression and logistics regression.The illustration below shows the process of forecasting each of the five selected features for predicting GDP growth.To choose the best multiple linear regression model for forecasting GDP growth we conducted a cross validation across different training window sizes. Since the features and target variable at hand are time series using a traditional kfold cross validation will not consider the time series trend. Instead we implemented a moving window cross validation. To forecast GDP growth using a logistic regression we needed to transform GDP growth rates to categories. We did not need to transform the predictors as only the target variable needs to be in categorical form.In classifying the GDP growth rates the median GDP growth of the past 12 years for each country was used as the benchmark. Each year was categorized into either “below median” or “above median.”The illustration below shows how the GDP growth rates are transformed to categories assuming a median of 0.12.A grid search cross validation using l1 (lasso) and l2 (ridge) penalty is used across a range of  λ values to identify the model with the best accuracy. This model is then used to forecast the GDP growth categories.The table below outlines the forecasts using each of the models discussed for the United Kingdom Italy and Spain. The first three rows (2015 to 2017) are actual GDP growths based on historical data from AMECO. The following three rows (2018 to 2020) are forecasts provided by each of the three models (i.e. ARIMA Multiple Linear Regression and Logistic Regression).In summary our three models predicted positive GDP growths in the next three years for the United Kingdom Italy and Spain. Assuming a stable political climate in the near future foreign investors and companies can expect economic conditions to grow in low singledigit rates.,NA,Forecasting Economic Risk in the EU into 2020
https://nycdatascience.com/blog/student-works/yelp-recommender-part-1/,31,"This is the first part of the Yelper_Helper capstone project blog post. Please find the .This is the capstone project sitting at the end of our 12 week journey in the bootcamp. For this project we would like to work on something that:Based on these criteria we decided on the ‘Yelper Helper’  a realtime restaurant recommender system using Yelp open source data. We believe the experience we gained from this project will be widely applicable.Included in the dataset were five json files which are encompassed users checkins tips reviews and businesses. Yelp also included 200000 photos with their associated labels though we did not incorporate any image analysis in our end product. The total size of the text data was roughly 5Gb. While this barely qualifies as 'big data' our desire to use big data tools and techniques is justified. The richness and variety of the data gave us the freedom to apply a wide range of machine learning techniques to create what we called “Yelper Helper.” The goal of Yelper Helper was to build a recommendation app for users based on keyword inputs location social networks and reviews. This will be explained in more detail in the following sections. Knowing the user’s location and other optional information (user ID keywords) our engine can recommend nearby restaurants and visualize them on a map. The engine is a  recommender. For new or anonymous users we would be able to provide basecase recommendations using only location information. With additional keywords like ""spicy"" or ""tacos"" an NLP (Natural Language Processing) module is turned on that can offer similarity based recommendations; finally with user ID as input the collaborative filtering and social network modules will provide more personalized results based on historical rating activities and friends’ opinions.Before building the app and training the model we wanted to investigate a subset of the data to explore. Among the cities that were available in the Yelp data was Las Vegas which our team thought would be a good training ground due to the number of restaurants visitors reviews and variety in the data. Once the relational database was set up we dove in and performed some exploratory data analysis on the reviews.To get a better understanding of how the words in a review related to the rating that a user gave we performed a highlevel sentiment analysis to identify which words were associated with positive and negative reviews. We took a sample of 1000 negative reviews and 1000 positive reviews from restaurants in Las Vegas and analyzed words or phrases that were used most frequently for those reviews.Words most often associated with bad reviews are on the top left. They include:     and . Words most frequently associated with good reviews are on the bottom right. Among them:     and .Additional exploratory data analysis could be performed such as restaurant trends unique attributes per location etc. Since the data provided by Yelp was already clean with minimal missingness we did not impute any data. Finally because we were able to convert the data from json to csv easily we loaded the data into MySQL database for easy storage and extraction.Our team believed that it would be wise to load the converted csv files into a MySQL database hosted by Amazon’s Relational Database Service. Having the data stored in a relational database would then allow us to extract and access the data with efficiency. MySQL provides many advantages for our recommendation app. First some of the data files contained millions of rows so we spun up the MySQL instance and loaded each dataset into separate tables. This made data extraction easy and fast depending on what we wanted to analyze. Initially five tables were created one for reviews businesses tips checkins and users. A visualization of schema can be seen here:From there we created subtables which only included the relevant Las Vegas restaurant data. An additional advantage MySQL provided was that it allowed us to easily establish a connection via sqlalchemy and MySQLdb to Spark and Python thereby making it unnecessary to create multiple intermediate/temporary csv files.First we decided to analyze the Las Vegas subset using Natural Language Processing (NLP) a machine learning technique that aims to understand human language with all its intricacies and nuances. The 800000 reviews were divided into low ratings of 1 and 2 stars and high ratings of 5 stars. Our goal was to create a supervised learning neural network that could predict the sentiment of a review as positive or negative based on the language used. Restaurant reviews with ratings of 3 or 4 stars were thrown out due to the lack of consistency between reviewers (i.e. one reviewer’s 3 star review could be another’s 5 star review).The low rated reviews were given a rank of zero and the high rated ones were given a rank of one. The text of each review was then tokenized and converted to a sequence using the keras package. A neural network was set up in keras as well using a convolutional filter pooling filter and both ReLU and sigmoid activation function to predict the sentiment of each review as either 0 or 1. The final model was 94% accurate and was spotchecked on newlywritten reviews and on the remaining 3 and 4 star reviews. The model did quite well for our purposes and so we fit all 800000 reviews using the sentiment analysis neural network and then averaged the reviews by restaurant and used the results as a new feature in the locationbased recommendation (See section 6.4). The overall process for the sentiment analysis is outlined below.We also wanted to use NLP to make contentbased recommendations for our app. Itemtoitem similarity is calculated to make these types of recommendations. Think of when you read an article online: there is usually a side menu or a section at the bottom of the page that recommends new articles based on your interest in the current article. Often these new articles are written on the same or a similar topic. That’s a contentbased recommendation.Contentbased recommenders can take a user’s profile and past ratings to make new recommendations to the user. This however presents the cold start problem an issue that arises for a brandnew user who has no rating history. To combat this our NLP recommendation is based on a keyword soft match (similarity calculation) rather than the user profile. So typing “tacos” into our app should bring up a ranking of Mexican joints with the possibility to also recommend some nonMexican places serving tacolike food.To make this happen we once again decided to implement a neural network to understand the language used in the reviews. We used the Word2Vec function from Spark MLlib to create the model. Our process is outlined in the figure below.To preprocess the data we concatenated the reviews for each business together for a total of 6199 restaurants. Then we tokenized the reviews and removed all stop words. Stop words are common words in the English languages that provide function within a sentence but no context such as ""of"" ""the"" and ""has.""Word2Vec then translates each word into a vector of 100 features. These vectors are located in a feature space of 100dimensions with similar words closer together and unrelated words farther apart. For example the vectors for “ice cream” and “frozen yogurt” should be pointing in nearly the same direction but the vectors for “delicious” and “disgusting” would be far apart.Once the words are translated into vectors in order to check if the result makes sense (and have some fun) we can perform some word algebra. We can add or subtract the vectors from one another to find a new word.The word vectors above have been flattened into a 2D feature space. Beef and filet mignon are both foods from a cow while seafood and lobster tail both come from the sea. Lobster tail and filet mignon are exquisite and expensive types of beef and seafood. If we have filet mignon take away the fact that it is beef and add in a new category of seafood we end up with lobster tail.In addition to using word algebra we can determine how similar two words or documents are based on cosine similarity. Mathematically cosine similarity is the dot product of the two vectors divided by the product of the magnitudes of those two vectors. This calculates the angle between two vectors. The smaller the angle the more similar the words the closer to 1 the value becomes. Larger angles are more dissimilar and are calculated closer to negative one. Vectors perpendicular to one another will have a cosine similarity of zero.In the figure above ""burger"" and ""sandwich"" point in somewhat similar directions and have a similarity of about 0.6. Below we can see the results of a similarity search for the word ""Chinese.""Since the business reviews are more than a single word and a user may want to search using multiple words as well Word2Vec averages the vectors of all the words together and then calculates the similarity between the user’s keywords and each of the available restaurants’ reviews. The results are then ranked from most similar to least and returned to the user on the map. Collaborative filtering (CF) is commonly used for recommender systems. These techniques aim to predict user interests by collecting preferences or taste information from many users. In other words CF fills in the missing entries of a useritem association matrix. The underlying assumption is that if person A agrees with person B on one issue A is more likely to have B's opinion on another issue than that of a randomly chosen person.Below is a great visual example from Wikipedia. In order to predict the unknown rating marked with ‘?’ we rely more on the opinions from other users with  rating histories (green rows) thereby arriving at a negative rating prediction (thumb’s down).Mathematically this is done by lowrank matrix factorization combined with a minimization problem (see picture below). The oftensparse useritem rating matrix R is approximated as a product of user matrix U and item matrix PT which are built of latent factors. We then form the cost function J and try to minimize it. Currently in the  library the alternating least squares (ALS) algorithm has been implemented to learn these latent factors. Additionally since we directly rely on the user rating itself our approach is often referred as ""explicit.""For our project we pretrain the model and save it on our Amazon S3 server. When the recommendation engine boots up it will load the model from S3 and use it for prediction. This architecture is designed so that we can keep training multiple models offline as new data comes in. Once a new model is ready the recommender engine will make the switch by editing one line of code.This is a digital version of the classic WordofMouth recommender system  what people have been using for thousands of years.The Yelp dataset is unique in that there is an embedded social network. In the user json file each row describes one user in a dictionary format. For the “friends” key the value stored is a list of encrypted user IDs. In order to quickly convert the unstructured social network data into a structured ""node"" and ""edge"" set (often required by graph theory related packages) we employ the Spark distributed computing ecosystem. This allows us to finish the task within a minute compared to hours in a single machine Python environment.With the social network database now structured and easily searchable a few SQL joingroupbyaggregate commands can quickly answer questions like: “What are the average ratings of the nearby restaurants ?” This interesting feature has the potential to both provide conversationtriggering recommendation and improve user stickiness to the app. As an extension of the algorithm one can easily come up with other intuitive rating estimation schemes such as expanding the network to second and thirddegree connections and applying a weighted average.Users who come to our webpage may want a quick suggestion of all possible restaurants based on only their location. Unlike the methods mentioned so far this requires no additional information and returns recommendations fastest to improve user experience. While yelp provides aggregated ratings for each business these are not always indicative of a restaurant’s quality. For example a restaurant with one fivestar rating would be ranked ahead of a restaurant with ten ratings averaging 4.9 stars. Another problem is that a star rating varies from person to person and is integer based. Finally do we want to take into account reviews that could be irrelevant due to their date e.g. from over ten years ago?Our strategy for dealing with these problems is as follows:How old is too old for a review to matter? As this dataset spans 2004 to present we need to define a reasonable cutoff. Instead of a hard filter we weighted each review based on its age with a sigmoid function centered about 2012 so a new review receives a weight of 1.0 while a review from 2012 receives a weight of 0.5. Because the rate of review writing is steadily increasing approximately 60% of all reviews are unaffected by the time weighting. Our filter effectiveness is demonstrated in the figure shown below.Based on the NLP sentiment analysis described above each review was assigned a sentiment value. This is indicative of how positive a review is with a range of zero to one. In the final rating scatterplot below it is clear that sentiment and star rating given by the user are dependent variables.On the business level we need to address the popularity measures: review and checkin counts. A high review count is indicative of either a popular business or an exceptional one. Checkins are a more direct measure of popularity but it is a more recent feature so even the most popular businesses have counts around 160. With this in mind checkins are assigned approximated twice the weight of reviews.These distributions are extremely skewed as most businesses having a limited number of reviews and checkins. By taking a log transformation we can get a reasonable multiplicative factor that includes but doesn’t overstate popularity.With our final score metric fully defined we can remap stars to percentiles (in Las Vegas). In the chart below it is clear that our new stars need a different interpretation from what is usually assumed. In this system four stars is considered one of the best restaurants in the area and three stars is a good if not great restaurant. Though skewed perhaps this final score distribution is more realistic than a uniformly or normally distributed score. In reality there are only a handful of exceptional—and an enormous amount of average—restaurants.Regardless of the model used we don’t want to check every restaurant in our database if the user is requesting information about a specific area. The most accurate metric for this is haversine distance: the minimum distance between two points on the surface of a sphere. For reference the defining equations are:Putting all of these individual parts described in section 6 together our recommendation engine can react to whatever user input is passed in. In its fundamental locationonly mode we can return recommendations within half of a second. As keyword and userbased models require computation they require 8 and 20 seconds respectively.See blog post Yelper Helper is a userfriendly interface powered by a robust and varied recommender and supported by an efficient data pipeline making it a quick and easy way to find nearby restaurants the customer will love.Having recommendations available for all levels of interaction with the app provides quick suggestions for the casual digital passerby and yet promotes more consistent user engagement with the benefit of more personalized results. The locationbased recommendations aim to provide a quick and dirty service for passing users. The NLP neural networks of the contentbased recommendations allow users to filter restaurants by cuisine or dish. Collaborative filtering and social network recommendations provide individualized recommendations based on personalized taste and friends’ opinions.Building the machine learning models using Apache Spark and setting up a FlaskKafkaRDSDatabricks pipeline creates a powerful and scalable system robust to working with big data and a continuous stream of user requests.  With more time we would improve Yelper Helper with the following ideas.In a short twoweek period we learned a great deal about working effectively as a team by using an agile approach to divide up the labor and regularly meet up to discuss progress and problems. We gained experience in SQL queries Amazon Web Services PySpark programming language and Kafka streaming. We implemented several machine learning techniques and built an entire data pipeline to create a useful and professional product for the modern consumer. Guidance from Shu Yan Yvonne Lau Zeyu ZhangInspiration from Chuan Sun and Aiko Liu",NA,A Hybrid Recommender with Yelp Challenge Data -- Part I
https://nycdatascience.com/blog/student-works/tribots-read-and-write/,31,Word2Vec is a method that would try to represent the document into a vector formGeneral idea is that for the vocabularies used in the document one would come up with a probability distribution of words that would be used in a nearby expression 2 Methods for doing this is continuousbagofwords and skipgramContinuousbagofwords tries to produce probability distribution of a word given a list of wordsSkipgram tries to provide probability distribution of words that show up in similar content given a vocabularyWe have also tried to apply authorship attribution analysis to the reportsIt seems that doc2vec analysis is not quite effective at distinguishing authors unless the writings are drastically differentOne methodology is to use key features that would give characteristics about the authors and use kmeans clusteringLexical Features:average number of words per sentencesentence length variation(standard deviation of words per sentence)Lexical diversity(number of unique words/words used in document)Syntactical:Frequencies for common Parts of Speech types(singular/plural noun proper noun determiner preposition/conjunctionadjective)Bag of Words:Count the most common words in the documents and apply clusteringIf the company’s stock price  has decreased by 50% we have considered them “bad”If the company’s stock price has increased by more than 100% we have considered them “good” (over 5 years)Neural Network has been around since 1940s but were not so useful until about 5 years agoWhereas regular Neural Networks does not have a sense of time Recurrent Neural Networks try to capture time element while working with neural networks by using previous output as another inputRecurrent Neural Networks by itself is not as effective so people have incorporated LSTM to forget or remember previous outputs10K reports were too large for us to train using RNN Following the example of writing a similar story in Aesop’s fable we have written summary using company profiles in CNBC“the bell outwit met . nobody will all mouse got up and said that is all very well  but he thought would meet the case . you will all agree  said attached chief'”(Aesop’s Fable LSTM RNN output)“other service offerings for play and retailers through the consumer and sale beverages. Hasbro The Investment segment focuses sales and facilitates that sectors. Wholesale in food countries in China its in fundamental storage equity and”(somewhat makes sense),NA,Tribots read and write
https://nycdatascience.com/blog/student-works/creating-real-time-streaming-platform-identify-top-influencers-twitter/,31,The goal of the project was to This is why we decided to help marketers understand the top influencers for their brand by creating a platform TwitterTalker on which they could see the top influencers for a specific keyword. When a brand aligns with an influencer not only do they bring their audience but they also bring their audience’s network. Because of the loyalty of their audience an influencer has the ability to drive traffic to a company’s site increase its social media exposure and sell its product through their recommendation or story about their experience.That’s why it’s important for marketers to understand the dynamics of influence and how influence is changing all the time.The methodology we followed was an  from business understanding to solution deployment (see figure below). It illustrates the iterative nature of the problemsolving process. We believed the application should not be left in place unchanged after being created. Through the iterative process of feedback refinement and redeployment the application continuously evolved.To manage our resources as effectively as possible we used a shiny app and Asana a webbased project management tool for task assignment and progress tracking. For urgent information sharing we used Slack to communicate with each other. TwitterTalker was designed and developed in the course of 2 weeks. The final workflow is described below:We used Flask a socalled microframework to take raw data from RDS through SQLAlchemy and used templates to convert it into a viewable form. Like a magician we mixed some ingredients (data) according to a recipe (template) to create a potion (website).The formats that the browser can display include HTML CSS and JS triple. By combining all three elements a browser is able the render a nice looking interactive web site web page web application etc.Below is the homepage of the interactive web application. After clicking the ‘Find out more’ button users are invited to sign up. This is designed to collect user information. All input is stored in our RDS database. For future work we would like to collect more data on how users interact with our platform which would be used to train models to provide more sophisticated recommendations.The realtime analytics platform consists of 4 core parts: The Influence Score Dashboard provides a clear breakdown of what influencers are saying about a hashtag how many people the influencer is reaching and whether their view is positive negative or neutral. We expect marketers to leverage positive influencers in their campaigns and transform the negative and neutral ones into brand ambassadors. Below is an example of the dashboard for influencers with a positive influence sentiment. The dashboard includes the name of the top 5 influencers their influence score their total number of followers the number of times they got retweeted for the searched hashtag and their tweet for the searched hashtag.In order to define and measure influence we first identified which aspects gave an individual the power to influence others within their social sphere. There are 3 components called the “Pillars of influence” that marketers want to consider: Reach Resonance and Relevance. We measured it by looking at: We measured it by analyzing: This is when the Flask app comes in handy as we let marketers filter influencers based on specific keywords. The score of an influencer will be different for each keyword.The criteria in the influence score needed to be weighted differently. The challenge was that we were working in an unsupervised environment where we had a lot of criteria but no output variable. In other words we did not know whether a user for whom we had scraped data was an influencer or not.We used the . Below is the importance plot generated by random forest that we used to choose and assign weight to the variables in the influence score.Once marketers have identified the top 5 influencers by influencer sentiment the next step is to locate them and identify possible trends. The Flask app displays where the top influencers are and differentiates influencers who demonstrate a positive sentiment towards the hashtag from influencers who express negative feelings towards it.This information is valuable as it enables marketers to  For retailers it is an opportunity to open a new store next to one of these hot spots or display an ad showcasing one of the local influencers.Our real time app generates three different word clouds for the hashtag specified by the marketer one for each sentiment: positive neutral and negative. They are several ways marketers could use the hashtag word cloud. However the main goal would be to evaluate how effectively they are conveying their brand messaging by identifying the top words associated with their brand (or whichever hashtag they specified). Are the industry buzzwords part of the word cloud? Are the words they are targeting for SEO (Search Engine Optimization) showing up? Which words have a positive or negative connotation?Marketers can also use the interactive app to search for their competitors’ hashtag and see which top words are associated with them. Finally in a social media marketing campaign it is important to know a topic’s popularity and people’s sentiments towards it. Our realtime twitter analytics platform tracks the number of tweets and average sentiment score for a specified hashtag. This helps marketers track how much conversation their brand (or whichever hashtag they specified) is driving and gives them a clear understanding of their brand in the competitive landscape. The average sentiment score is another component that helps marketers understand how the social media audience feel about the hashtag they searched for. Overall this .To gain a deeper insight of the twitter data we had collected we did an analysis on retweet pattern sentiment and influence power. The goal here is to We selected 3 key words : ‘Friday Feeling’ ‘Fathers day’ ‘Trump’ and scraped over 100K tweets within 72 hours.In the hashtag event ‘Friday Feeling’ we can see an ongoing topic popularity starting from Friday 2:00 pm. Approximately 2750 tweets were posted during peak hours. The popularity gradually came down from 2:00 am. As for the sentiment the average scores remained positive at all time. The sentiment scores slightly decreased during peak hours however increased after 4:00 am.This is just an example of hashtag popularity tracking. In a real marketing campaign a marketing managers should always keep an eye on popularity. If a sharp drop in tweet volume appears one will be able to immediately react to the problem by implementing the following strategies: Understanding what twitter users are talking about is an important component of a marketing campaign. The sentiment tracking and word cloud functions allow marketing manager to know how audiences feel and what are the frequently mentioned words. If the average sentiment score drops significantly it means that a potential public relationship crisis might be happening. Our product can be used as a powerful tool for crisis management as it can reveal the direction of public opinion in realtime which allows leadership to take action faster.Take United Airlines as an example; UA CEO apologized 24 hrs after the video of the incident had gone viral on Twitter. The late and insincere apology caused UA's stock price to drop by 1.1% the equivalent of a $255M loss. If the crisis manager had identified the increase in tweet volume and negativity against UA then the CEO of UA could have responded much more quickly and the crisis management team could have effectively responded to the viruslike videos which spread at an early stage. Comparing with viral marketing the traditional marketing seems to be far less costefficient. Just like the ‘#FathersDay’ pattern companies need to pay big twitter influencers to post tweets about their products. However the retweet counts are much lower since people are not interested in sharing the topics.The product we created is a Minimum Viable Product (MVP) meaning that we built the minimum set of features possible to be able to deploy our product. The goal is to put it in front of real users and keep improving upon it as we gather feedback. This is based on an agile approach building a product quickly measuring each iteration of it and upgrading it often (as depicted below).For future work we’d like to collect data on the usage of our product and feedback from our users to keep providing scalable actionable and datadriven insights to marketers. We also want to help marketers to be proactive by. Finally we want to support decisionmaking by giving recommendations based on the information our platform provides. This is where a data scientist would want to collaborate with the marketing team to bring the product to a new level.,NA,Creating a Real-time Streaming Analytical Platform to manage social media marketing campaign
https://nycdatascience.com/blog/student-works/r-shiny/scraping-mobafire-com-for-best-champion/,32,: The map to the left shows an aerial view of Summoner's Rift while the map of the right is a simplified version of the same map. ,NA,Scraping Mobafire for the best champion.
https://nycdatascience.com/blog/student-works/capstone/u-s-residential-energy-use-machine-learning-recs-dataset/,32,The residential sector accounts for up to  representing a large opportunity for energy efficiency and conservation. A strong understanding of the main electricity enduses in residences can allow homeowners to make more informed decisions to lower their energy bills help utilities maximize efficiency/incentive programs and allow governments or NGOs to better forecast energy demand and address climate concerns.The Residential Energy Consumption Survey  collects energyrelated data on a nationally representative sample of U.S. homes. First conducted in 1978 and administered every 45 years by the  it is a leading data source for residential energy analysis. Methodology for the study is welldocumented  a comprehensive overview of RECS can be found in the program’s .This project applied machine learning methods to the most recently available RECS dataset published in 2009. The primary goals were twofold as is common in many regression exercises:RECS 2009 consists of 12083 observations (each representing a unique housing unit) and over 900 features encompassing physical building characteristics appliances occupant behavior/usage patterns occupant demographics etc. These features serve as independent variables in predicting the outcome variable annual electricity usage in kilowatthours (kWh). Because RECS aims to cover a comprehensive overview of many types of residences for a variety of analytical purposes (beyond energy use prediction) many of the features are sparsely populated collinear or uncorrelated with kWh usage. Therefore a preliminary and recurring task throughout the project was dimension reduction.Since most of the independent variables were collected through occupant interviews  as opposed to exact physical examination of the residence  the values of many are binned as factor/categorical variables. The dataset’s 931 original variables had the following characteristics: Missingness was common in the raw dataset with 73 features having NA values in more than 95% of observations. To quickly gauge whether these features correlated with the outcome variable kWh (and therefore should be retained/imputed) I made use of flexible EDA graphing functions from the  R package. An example is shown below on continuous variables revealing generally low rstatistics in correlation to kWh and having nonsignificant pvalues (not shown).  also accommodates similar exploratory analysis on factor variables via pairwise box plots. Overall although a more complex imputation strategy could have been employed to address these 73 features they were dropped due to their high missingness and little evidence of predictive power over the outcome variable. As previously mentioned the majority of the dataset features were factor variables many of which needed recoding in order to correctly capture the variation being described. As illustrated below nominal factor variables such as WALLTYPE have no intrinsic ordering; it would not make sense to order “stucco” higher than “wood” for example. On the other hand ordinal factors such as AGERFRI1 (age of mostused refrigerator) have a clear order to their factor levels; each level denotes a numerically higher value than the previous. Information contained in variables like AGERFRI1 is often more appropriately formatted in integer/numeric form which can better convey their continuous range of values aid interpretation of the variable coefficient and reduce standard error by decreasing the number of coefficients for model estimation (many algorithms generate a separate model coefficient for each factor level via onehotencoding).In the image above WALLTYPE was left as a factor while AGERFRI1 was recoded as an integer value (using the mid range of each bin as number of years). This factortointeger recoding was applied to additional binned variables relating to age of appliances and frequency of use of those appliances. New factor variables were also created by consolidating information from existing factors into onehotencoded binary values such as presence of heated pool use of electricity for space heating etc.We can visualize multicollinearity among the integer and numeric features using correlation plots again made with R package . In the plot below a conceptual group of collinear variables stands out i.e. those directly or indirectly representing a residence’s size for example total square footage (TOTSQFT) cooling square footage (TOTCSQFT) number of bathrooms (NCOMBATH) number of refrigerators (NUMFRIG) etc. To confirm the degree of multicollinearity variance inflation factors were calculated based on a linear model with the above numeric features  those with high VIF scores (loosely following the rule of thumb VIF > 5.0) were dropped.Despite variable recoding and decorrelation using the methods previously described a large number of features remained in the dataset the majority of them being factor variables. Although principal component analysis (PCA) is a powerful method for dimension reduction it does not generalize easily to categorical data. Therefore feature reduction was continued instead through the use of an exploratory LASSO regression model utilizing the L1 regularization penalty to drive nonsignificant variable coefficients in the model’s output to 0. This was helpful in identifying and dropping features with very little predictive power over kWh usage particularly factor variables that were not covered in the multicollinearity exercise above.The processed and cleaned dataset was migrated to opensource online machine learning platform  which offers highly efficient and scalable modeling methods. In contrast to machine learning conducted in slower native R packages ( ) in the local R environment R package  facilitates API calls to h2o’s online platform sending the given dataset to be distributed and parallelprocessed among multiple clusters. H2o offers an array of the most common machine learning algorithms (glm kNN random forests gbm deep learning) at very impressive run times  lessening the large burden of computational speed in the model fitting and tuning process. It also provides handy builtin functions for preprocessing such as training/validation/test set splitting in this case chosen to be 70/20/10.To best understand variable importance and interpret linear effects of the features on kWh usage a simple multivariate regression model was fit using h2o’s  function with default parameters (no regularization). A variable importance plot (see below) ranks the top 15 features by zstatistic all of which are quite large  an observed zscore of 15 translates to a pvalue of 9.63e54 or virtually zero. Hence all 15 variables show extremely significant statistical evidence of a relationship to kWh with an additional ~100 of the features (not shown) also meeting significance at the p  0.05 threshold. Variables beginning with “hasElec” were feature engineered (previously described) validating the approach of using domain knowledge to add or recode valuable information in new features. Coefficient estimates are denoted on the bar for each feature. We can observe that the presence of electric space heating at a residence (onehotencoded as a factor variable) increases yearly electricity usage at the residence by about 2722 kWh; each additional TOTCSQFT (air conditioned square feet numeric) adds 1.146 kWh/year.While default h2o  provided interpretability and pvalues adding regularization to the linear model enabled an increase in predictive accuracy. H2o allows flexible grid search methods to tune the main  hyperparameters alpha (type of regularization) and lambda (degree of coefficient shrinkage). Because grid search can quickly become computationally expensive I utilized h2o’s powerful  and  functionality to ensure that computation time is not wasted for very small marginal gains in validation accuracy.A primary purpose of grid search is to choose optimal tuning parameters according to a validation metric   in this case rootmeansquarederror (RMSE) of kWh prediction on the validation set  and then train a new model using the optimal values discovered on the full dataset (no train/validation split) to maximize sample size. In 5 minutes of training h2o fit all ~440 model combinations specified in the grid search with the best RMSE model having alpha  1.0 (LASSO regression) and lambda  1.0 (small amount of regularization). These parameters were then used to fit a final model on 90% of the data (combining 70% train and 20% validation sets) and performance was evaluated on the 10% holdout test data which had not yet been seen by the model.To increase predictive accuracy over generalized linear modelbased approaches I used h2o’s implementation of gradient boosted machines. Boosting a nonlinear treebased approach sequentially fits many decorrelated decision trees to slowly reduce overall predictive error (in the regression setting often RMSE). Grid search was performed using the same methods as above and with the following tuning parameters:Following the same process as with GLM parameter values were then chosen from the trained model with the best validation set RMSE. The model was retrained on the full 90% of training data and tested on the final 10% holdout split. Results from the two modeling strategies are summarized in the table below.Final predictive accuracy was similar for both linearbased (GLM) and treebased (GBM) models on the RECS dataset. The two models’ RMSE values represent a large improvement over an RMSE of 7560 which was established as a baseline accuracy by initially fitting a default GLM on the raw dataset (before any feature engineering or reduction). This improvement validates the approaches used  using domain knowledge to feature engineer highly significant variables and grid search for hyperparameter tuning.Aside from the regression setting future analysis could be applied to RECS in a classification context. Since the majority of the dataset’s features are factor variables energy providers could use the rich training set to attempt to answer important energyrelated classification questions about their customers  type of space heating presence of major appliances or whether the home is a good candidate for solar installation. Greater predictive ability over residential electricity use will contribute over time to a smarter cleaner and more efficient power grid.,NA,U.S. Residential Energy Use: Machine Learning on the RECS Dataset
https://nycdatascience.com/blog/student-works/data-scientists-guide-predicting-housing-prices-russia/,32,The  provided consist of a training set a test set and a file containing historical macroeconomic metrics. As with any teambased project defining the project workflow was vital to our delivering on time as we were given only a twoweek timeframe to complete the project. Painting the bigger picture allowed us to have a shared understanding of the moving parts and helped us to budget time appropriately.Our project workflow can be broken down into three main parts: 1) Data Assessment 2) Model Preparation and 3) Model Fitting. This blog post will follow the general flow depicted in the illustration below.Given a tight deadline we had to limit the project scope to what we can realistically complete. We agreed that our primary objectives are to learn as much as we can and to apply what we have learned in class. To do so we decided to do the following:In order to better predict Russian housing prices we first need to understand how the economic forces impact the Russian economy as it directly affects the supply and demand in the housing market. To understand how we can best apply macroeconomic features into our models we researched for Russia’s biggest economic drivers and also any events that may have impacted Russia’s economy during the time period of the dataset (i.e. 2011 to 2016). Our findings show that:These two factors initiated a “snowball.” negatively impacting the following base economic indicators:  The poorly performing Russian economy manifested a series of changes in Russia’s macroeconomic indicators including but not limited to:The change in these indicators led us to the understanding that Russia is facing a financial crisis.As we continued to explore the macroeconomic dataset we were able to further confirm Russia’s financial crisis. We observed that the Russian Central Bank had increased the banking deposit rate by over 100% in 2014 in an attempt to bring stability to the Russian economy and the Russian ruble.Next we looked for the effect of the Russian Central Bank’s move on the Russian housing market. When raising banking deposit rates the Russian Central Bank encouraged consumers to keep their savings in the banks and minimize the public's’ exposure to financial risk. That was seen clearly in the Russian housing market following the raise in deposit rates. We noticed that rent prices increased and the growth of commercial investment in real estate dropped in the beginning of 2015. This analysis led us to the understanding that the Russian economy was facing a major financial crisis which significantly impacted the demand in the Russian housing market. This highlights the importance of the usage of the macroeconomic features in our machine learning model to predict the Russian housing market.The first step to any data science project is simple exploration and visualization of the data. Since the ultimate purpose of this competition is price prediction it’s a good idea to visualize price trends over the time span of the training data set. The visualization below shows monthly average realty prices over time. We can see that average prices have seen fluctuations between 2011 and 2015 with an overall increase over time. There is however a noticeable drop from June to December of 2012.  It is important to keep in mind that these averaged prices include apartments of different sizes; therefore a more “standardized” measure of price would be the price per square meter over the same time span. Below we see that the average price per square meter shows fluctuations as well though the overall trend is quite different from the previous visualization.The data set starts off with a decline in average price per square meter which sees somewhat of an improvement from late 2011 to mid 2012. Again we see a price drop from June to December of 2012.Some other attributes to take into consideration when evaluating the price of an apartment are the size of the apartment and the size of the building.  Our intuition tells us that price should be directly related to the size of an apartment and the the boxplot below shows that the median price does go up relative to apartment size with the exception of “Large” apartments having a median price slightly lower than apartments of “Medium” size. Factors such as neighborhood might help in explaining this anomaly.Apartment price as a function of building size shows similar median prices for lowrise medium and highrise buildings. Larger buildings labelled “Sky” with 40 floors or more show a slightly higher median apartment price.Another interesting variable worth taking a look at is the type of transaction or “product type” mentioned in the data set. This feature of the data categorizes each transaction as being either an Investment or an Owner Occupied purchase. The barplot below gives a breakdown of the transactions for the top subareas (by frequency) based on the product type. This visualization clearly shows that certain areas are heavily owner occupied while other areas are very attractive to investors. If investment leads to increasing prices this trend will be important for making future predictions. If we shift our focus to transactions related to the building’s build year based on product type we see that older buildings generally are involved in investment transactions possibly due to better deals while newer constructions are predominantly owner occupied. Simple exploration and visualization of the data outlines important considerations while training our machine learning model. Our model must consider the factors behind the price dips in 2012 and learn to anticipate similar drops in future predictions. The model must also understand that low prices attract investors as can be seen with older buildings. However if investment leads to an increase in price for a particular building the model must be able to adjust future predictions for that building accordingly. Similarly. the model must be able to predict future prices for a given subarea that has seen an increase in average price over time as a result of investment.To simplify the feature selection process we fitted an XGBoost model containing all of the housing features (~290 features) and called its feature importance function to determine the most important predictors of realty price. XGBoost was very efficient in doing this; it took less than 10 minutes to fit the model.In the illustration below XGBoost sorts each of the housing features by its Fscore which is a measure of variable importance in its predictive power of the transaction price. Among the top 20 features identified with XGBoost a Random Forest was used to further rank their importance. The %IncMSE metric was preferred over IncNodePurity as our objective was to identify features that can minimize the mean squared errors of our model (i.e. better predictability).In order to determine which macroeconomic features were important we joined the transaction prices in the training set with the macroeconomics by the timestamp and then repeated the process above. The top 45 ranking features are outlined below. In this project we have used experimented with different subsets of these features to fit our models.Often times it is crucial to generate new variables from existing ones to improve prediction accuracy. Feature engineering can serve the purpose of extrapolating information by splitting variables for model flexibility or dimension reduction by combining variables for model simplicity. The graphic below shows that by averaging our chosen distancerelated features into an averagedistance feature we yield a similar price trend as the original individual features. For simpler models sub areas were limited to the top 50 most frequent with the rest classified as “other.” The timestamp variable was used to extract date and seasonal information into new variables such as day month year and season. The apartment size feature mentioned earlier during our exploratory data analysis was actually an engineered featured using the living square meter area of the apartment to determine apartment size. This feature served very valuable during the imputation of missing values for number of rooms in an apartment. Similarly building size was generated using the maximum number of floors in the given building.Among 45 features selected outliers and missing values were corrected as both the multiple linear regression and gradient boosting models do not accept missing values.Below is a list of the outlier corrections and missing value imputations.Outlier CorrectionMissing Value ImputationPrior to fitting models it is imperative to understand their strengths and weaknesses.Outlined below are some of the pros and cons we have identified as well as the associated libraries used to implement them.Multiple Linear Regression (R: lm Python: statsmodel)Gradient Boosting Tree (R: gbm & caret) XGBoost (Python: xgboost)Despite knowing that a multiple linear regression model will not work well given the dataset’s complexity and the presence of multicollinearity we were interested to see its predictive strength.We began by validating the presence of multicollinearity. In the illustration below green and red boxes indicate positive and negative correlations between two features. One of the assumptions of a multiple linear regression model is that its features should not be correlated.We continue to examine other underlying assumptions of a multiple linear regression model. When examining scatterplots among predictors and the target value (i.e. price) it is apparent that many predictors do not share a linear relationship with the response variable. The following plots are one of many sets of plots examined during the process.The Residuals vs Fitted plot does not align with the assumption that error terms have the same variance no matter where they appear along the regression line (i.e. red line shows an increasing trend).The Normal QQ plot shows severe violation of normality (i.e. standardized residuals are highly deviant from the normal line).The ScaleLocation plot shows a violation of independent errors as it is observed that the standardized residuals are increasing along with increases in input variablesDespite observing violations of the underlying assumptions of multiple linear regression models we proceeded with submitting our predictions to see how well we can score. ).  “” as the score on the public leaderboard is only representative of the predictions of approximately 35% of the test set.Models A2 and A3 were fitted by removing features from model A1 that showed VIFs (Variance Inflation Factors) greater than 5. Generally features with VIFs greater than 5 indicate that there is multicollinearity present among the features used in the model. Removing these did not help to improve our Kaggle scores as it is likely that we have removed features that are also important to the prediction of price.Models B1 B2 and B3 were trained with the features in A1 with the addition of identified important macro features. These models did not improve our Kaggle scores likely for reasons mentioned previously.The second machine learning method we used was the Gradient Boosting Model (GBM) which is a treebased machine learning method that is robust in handling multicollinearity. This is important as we believe that many features provided in the dataset are significant predictors of price.GBMs are trained using the bootstrapping method; each tree is generated using information from previous trees. To avoid overfitting the model to the training dataset we used crossvalidation with Caret package in R. Knowing that this model can handle complexity and multicollinearity well we added more features for the first model we fitted.In Model A1 we chose 47 features from the original training dataset which gave us a Kaggle score of 0.38056. Next we wanted to check if reducing the complexity of the model gives better results. We decided to run another model (A2) with only 20 features. This model performed better than Model A1.In Model A3 we added to the previous model 5 macro features that were chosen using XGBoost feature importance. As expected the macro features improved our results. The model gave us the best Kaggle score of all the gradient boosting models we trained.For each of the GBM models we used crossvalidation to tune the parameters (with R’s Caret package). GBM models on average provided us better prediction results compared with those using the multiple linear regression models.An extreme form of gradient boosting XGBoost is the preferred tool for Kaggle competitions due to it’s accuracy of predictions and speed. The quick speed in training a model is a result of allowing residuals to “rollover” into the next tree. Careful selection of hyperparameters such as learning rate and maxdepth gave us our best Kaggle prediction score. Fine tuning of the hyperparameters can be achieved through extensive crossvalidation; however caution is recommended when fitting too many parameters as it can be computationally expensive and time consuming. Below is an example of a simple grid search cross validation.The table below outlines the best public scores we have received using each of the different models along with the features used.This twoweek exercise provided us the opportunity to experience firsthand what it is like to be working in a team of data scientists. If given the luxury of additional time we would dedicate more to engineering new features to improve our model predictions. This would require us to develop deeper industry knowledge of Russia’s housing economy.We thank the entire team of instructors at NYCDSA for their guidance during this twoweek project and also our fellow Kagglers who published kernels with their useful insights.,NA,A Data Scientist's Guide to Predicting Housing Prices in Russia
https://nycdatascience.com/blog/student-works/retailers-heres-employees-saying-project-web-scraping-indeed-com/,32,This is the second of four projects for the NYC Data Science Academy. In this project students are required to collect their datasets using a method called webscraping. Essentially webscraping is the process of collecting information (i.e. texts) from a series of websites into structured databases such as .csv files. Webscraping enables analyses to be done on data that is not already provided in a structured format.In this analysis we will scrape employee reviews for the 8 retail companies that made it to  list. See below for additional details regarding each of these 8 retail firms in discussion. The rank and overall score corresponds to the employer's rank and overall score as they appear on Indeed's website.Given a twoweek timeframe the scope of the analysis was limited to these three primary objectives: (a Python framework) was used for collecting employee reviews for each of the retailers on the 50 Best Corporate Employers List. Each review scraped contains the following elements:As seen in the illustration below not every review contains a pro and a con as those are optional fields for the reviewer. To see an example of a real review you can visit Indeed's Walt Disney Parks and Resorts review page .In total we have scraped over 20000 reviews. The number of reviews scraped for each firm varies significantly across the board. This is likely due to the difference in the number of employees hired by each firm across the nation.Employee reviews for these eight retail firms tend to peak around the beginning of the year (January through April). This is expected as we know that retail is a cyclical industry in which the majority of sales happen during the holiday season (November through January). Because employees are oftentimes hired only for the duration of this period it makes sense  that the majority of the reviews come in around the beginning of the year.Among the eight retailers on the 50 Best Corporate Employers list we see that the review ratings skew toward a score of 5 which is the highest score a review can get.There is no significant differences in average rating across retailers.There is no significant differences in average rating across months.When we consider reviews of all ratings (i.e. reviews with ratings 1 to 5) retailers receive the highest average ratings during the months of  Note that the ratings have been scaled to 0 to 1 to allow for comparison across firms and months.If we only consider the  (i.e. reviews with ratings 1 or 2) retailers on average receive the most negative reviews during the months of  (i.e. these months scored higher across retailers)If we only consider the  (i.e. reviews with ratings 4 or 5) retailers on average receive the most positive reviews during the months of (i.e. these months scored higher across retailers).In summary while the highest average ratings concentrate toward the end of the year the highest count of both positive and negative reviews are given during the beginning of the year. This aligns with our understanding that the retail industry is cyclical. In other words employees are oftentimes hired specifically to aid sales around the holiday season (i.e. Nov to Jan). When their work concludes around the beginning of the year that's when we can expect employees to write the majority of the reviews (i.e. both positive and negative).A sentiment analysis was done using  library to visualize the emotions across employee reviews. In the illustration below each graph represents the intensity of the an emotion from January to December on a positive scale (i.e. 0 and above). For example for BuildABear Workshop's 'positive' graph we can see that there is an uptick in positive sentiment toward March and November and each of those months scored approximately a 6. This is a high score relative to the scores recorded across all other emotions.Generally the sentiment observed align with our understanding that the majority of the employee reviews are positive as we are narrowly focused on analyzing employee reviews among the best 8 corporate retail employers according to Indeed's ranking. As an aside it is interesting to see that BuildABear Workshop and Trader Joe's recorded more volatility in its scoring across the negative emotions (i.e. negative sadness fear disgust anger).Topic modeling is a form of unsupervised machine learning and it can help us identify patterns by clustering similar items into groups. In this analysis  a Python library was used to analyze the groups among positive and negative reviews (using pros and cons of the employee reviews). The drawback of using topic modeling as with other clustering techniques is that the user has to specify the number of groups to cluster the documents into. This presents a catch22 issue as the initial objective is to understand the number of topics or groups among your inputs. Given this hurdle the user must use their business judgment as well as running multiple trials with different number of topics as inputs to determine results that are most interpretable. More sophisticated clustering libraries aid the users by automatically selecting the best number of topics but this should only be used as a guide to begin the analysis.In the illustration below we can see that words like 'great' and 'benefit' among many other words are oftentimes seen together among pro reviews.Two topic modeling analyses were conducted using pro reviews and con reviews. Below are the most common combinations of words appearing in each of the groups for each analysis. It is not a surprise that the groups overlap in the common words identified among them as we have chosen a relatively high number of topics. Nevertheless the results provide a useful start for understanding what employees are thinking. library was used to further aid the visualization of what employees are saying about these firms. Essentially the library takes a list of words along with their frequencies and produces a word cloud based on those inputs. The higher the frequency a word is associated with the larger it appears in the word cloud.The positive reviews word cloud was created using only the pros from each of the reviews. (Note: Each review contains the main review a pro and a con.) Some of the common words we see among pros are 'great''work' 'job’'company' and 'customers.'Similarly a word cloud was created using only the cons from each of the reviews. We can see that common words appearing include and.Beyond the scope of this project below are other interesting areas that are worth looking into if additional time allows.,NA,Retailers: Here's What Your Employees Are Saying About You - A Project on Web Scraping Indeed.com
https://nycdatascience.com/blog/student-works/scraping-fiverr/,32,"When deciding on a website for my web scraping project I thought of Fiverr's captivating subway ads. Fiverr is a digital marketplace for buying and selling services or ""gigs."" In today's economy with many talented individuals seeking employment or supplemental income Fiverr offers a means for people to advertise their services and make some money as a freelancer. The prices on Fiverr can range anywhere from five dollars (hence the name Fiverr) to whatever amount the seller believes appropriate for the gig.Fiverr is not only a great resource for individuals seeking help on a project but can also serve as a valuable tool for startups looking to outsource some of their work. Such companies can use data from Fiverr to guide their research in deciding where to outsource their next big project. This aspect of Fiverr is the main focus of my web scraping project.I used two Scrapy spiders to scrape Fiverr. My first spider was deployed to parse through the Category and Subcategory levels of the website. The individual gigs that I needed to parse were on the subcategory level page. However I realized that Scrapy was unable to go deeper into the rabbit hole as it was unable to access the links to these ads. I decided to  incorporate Selenium into my spider to get these links and write them to a CSV file. After parsing through eight categories and seventyfour subcategories I was able to gather links to 16403 gigs. It was then the job of my second spider to scrape the data I needed from each of these links. This spider crawled in increments of 200 links at a time with a delay of 710 seconds between requests to avoid errors while crawling. The most common errors were the 301 Redirect error and the 403 Forbidden error which was a result of too many requests to the website in a short period of time. Because requests in a short time were disallowed  it took nearly seven days to scrape this data. The information scraped included category subcategory gig title and seller information such as name starrating location positive rating languages spoken and average response time. There was however a limitation to my data set since in the interest of time I decided to use the first five pages of each subcategory. The default order of gigs on a Fiverr subcategory page is by average customer rating. As I used gigs from the first five pages of each subcategory the gigs I scraped are mostly those with higher ratings.  Consequently the starrating and positive rating variables are of limited use in my analysis.Preliminary exploration and analysis of the data revealed some interesting observations. The word cloud below shows common word patterns from gig titles. The heavy use of the word ""will"" implies confidence as sellers emphasize how they're not saying that they ""may"" or they ""can"" but rather they ""will"" provide the service you need. It's also not very surprising to see that English is the standard language spoken by sellers on Fiverr with Spanish and Urdu coming in as distant second and third.One of the things I wanted to learn in scraping Fiverr was where these gigs were being advertised.. I wanted to know if these sellers were located in the United States or were  from all over the world. While the United States was the clear leader in the list of representation by country I was surprised at how many gigs originated abroad. The table on the right shows that there is a significantly large number of observations in my data set from other countries such as Pakistan India and Bangladesh.The barplot below shows the top fifteen countries overall represented in the data set. Beneath that we can see barplots of the average starting price and the average number of languages spoken by sellers advertising gigs from these countries. We can see the United States Pakistan India and Bangladesh lead the list in terms of quantity. As far as starting price is concerned India seems to be at the higher end of an average starting price greater than $14. This doesn't necessarily mean that India should be avoided but it can very possibly mean that gigs from India are on average more expensive due to the technical or specialized nature of the service. One might look into Bangladesh Sri Lanka or Indonesia if money is a concern (which it almost always is).  Morocco Venezuela and Serbia should be considered in multilingual services are required.Besides getting a general overview of the representation of countries in the overall data set I believe it's more interesting to see if there is a link between Category or Subcategory and Country. In other words are  certain countries better for Digital Marketing or Graphics and Design as compared to other countries? Before generating visualizations to check for any trends on category (or subcategory) and location I decided to run some Chisquared tests of independence.The left shows chisquare results using Python giving us extremely low pvalues that Python quantified as being close enough to zero to be considered as zero. On the right Pearson's Chisquared test using R yielded similar results of pvalues less than 2.2e16 suggesting a statistically significant dependence between the category/subcategory and location variables.Now that we have statistical evidence suggesting a link between Country and Category let's see what the representation of country is for each category. The barplots below show the top five countries for each category.An interesting observation revealed by these plots is that the United States Pakistan India and Bangladesh are present in most of categories. The information revealed by these plots can serve as a starting point for a business that can narrow down its choices based on which category of work it is looking to outsource. For example Pakistan looks like a great choice for Graphics & Design. For Music & Audio the top choices include the US UK and Venezuela. And the best bets for Programming & Technology appear to be  Pakistan India and Bangladesh.Now let's go a little deeper and analyze the representation of countries at the subcategory level. The visualization below explores the subcategories of Programming & Technology.Analyzing the data at this level is even more insightful than the category representation of different countries. Using subcategory level data allows  someone to distinguish  more specific outsourcing requirements. Either Pakistan or India would be a good choice for Ecommerce needs. India seems great at Quality Assurance related services while Pakistan seems to be a good fit for most of the other Programming & Technology subcategories.After narrowing our choices down to a few countries other variables from the data set can be used to help further refine our choices. The boxplot visualization on the right shows the average response time in hours for each of the top five Programming & Technology countries for each subcategory. For Quality Assurance the United States stands out with its larger range of response times in Quality Assurance while if you recall from the previous paragraph India was leading in this subcategory. This is a good indication to seek talent in India if the work is Quality Assurance related.It would be helpful to scrape more data from all pages of each subcategory to eliminate any convenience bias. This would allow regression modeling of starrating or positive rating on variables such as average response time or starting price. International economic statistics can be incorporated into this data to see how the economy of a country relates to representation on Fiverr. Such data would definitely be helpful in predictive and prescriptive analytics.",NA,Scraping Fiverr To Analyze Freelancing Market Trends
https://nycdatascience.com/blog/student-works/analyzing-evolution-rap-music-1989-2016/,32,"The current state of rap music today is something that is discussed in many hip hop and rap communities. Numerous people myself included believe that rap music has slowly been deteriorating especially since 2010. That is because today’s rap artists rely solely on beat and not on good lyrics. In my view the lyrics favored today have little or no word play and  the vocabulary has been severely dumbed down.  I was willing to put that impression to the test of data analysis.One of the perks of being a data scientist is being able to identify metrics to quantify a question like this. Some of the questions I wanted to answer were:The assumption for this project was:In order to get a general trend of the lyrics of rap music I decided to scrap the billboard website for top rap songs from 1989 to 2016. Although this is by no means a comprehensive list of the rap songs from the selected time period it is a good location to start. I decided to use the python class beautifulsoup to scrape the billboard and stored the results in two dataframes one dataframe for the artists per year and another for the song titles per year.  Once the artist name and song title for the top rap songs were obtained I used the unofficial API tswift to obtain the lyrics based on the artist name and song title. In order to accomplish this a lot of the artist name and song title had to be reformatted in order to successfully use the tswift API. Nonalphanumerical characters had to be removed and spaces had to be replaced with “”. Below are tables displaying the artist name and song titles from 2011 to 2016 after they have been adjusted to function with the tswift API. The lyrics  were then put in a pandas dataframe where they could later be manipulated or utilized for any analysis. Note that the tswift API was unable to identify ~ 10% of the songs. For future work on project I will create my own API to obtain song lyrics from a website with a bigger database of songs or from different websites.  The first Natural Language Processing (NLP) technique I wanted to try was latent dirichlet allocation (lda). This is an unsupervised method that aims to iteratively identify the probability of a word in lyrics ( a document) connected  to a particular topic as well as  the combinations of topics touched on by  a particular  word. As the number of topics is a very important tuning parameter I played around with different topic lengths (234 and 5) to see if I could identify topics that broke down the lyrics per year into different factions. I also tuned some parameters known as alpha and beta for the lda model. Both values are  usually between 0 and 1. A higher alpha value corresponds to each document containing a mixture of most topics and visa versa. A high beta value corresponds to each topic likely to contain a mixture of many words in the all the documents and visa versa. Unfortunately I was unable to adequately identify different topics in the lyrics per year. What the model did tell me was the usage of derogatory words after 1996 spiked up. Some of the topics per year definitely centered around love but apart from that the lda model provided little insight into the topics. Below is an example of the words belonging to the top three and top 4 topics in 2015 and 2016.What I hope to do in the future is pass a list of all the lyrics and see if the model can identify different topics in all the lyrics rather than per year.  Since I was unable to identify different topics from the lyrics I decided to look at the top 25 words per year. I did this using the natural language toolkit class in Python. I tokenized the lyrics and found the stem of each word and passed into a natural language toolkit function that identified the top 25 words per year. Rather than display barplots of the top 25 words per year from 1989  2016 I displayed the top 25 words per year in 1996 2006 and 2016 as examples. As can be seen from the barplots above words like what that said they etc. can be ignored since they provide no insight into the topics. Once they are taken out it is obvious that derogatory words become more prevalent in the top 25 words per year.  Again a lot of the words obtained did not help in differentiating the lyrics from 1989 to 2016 but what I did notice was the usage of derogatory words in the top 25 words per year. From this I created a list of derogatory words and decided to plot a trend of the number of derogatory words used per year in the lyrics. Apart from the spikes in the plot the general trend in the plot states that there is an increase in the usage of derogatory words especially from early 2000s to 2016. Another issue I wanted to see was the number of times the words like money power and sex were utilized. As can be seen from the figure above apart from the spike in 1998 the trend is relatively flat maybe even decreasing. It goes to show that that rappers might consistently talk about money power and
 sex but the usage of derogatory words is definitely rising.One point to note is that the slang used today is completely different from slang used 10 or even five  years ago. So although the usage of words  like “money” “power” and “sex” has decreased different slang could have been used to refer to the same words. What I hope to do in the future is identify all word similar to “money” “power” and “sex” and check if there is a major change in trend displayed above. Although this is by no means a comprehensive list of the rap songs from 1989 to 2016 the lyrics from the billboard website was a good place to start. Given more time more tuning of the LDA function will be necessary to identify if there are topics to be extracted from the lyrics. Although I was able to prove that the usage of derogatory words have on average been increasing I was unable to find a correlation that corresponded to a decrease in the lyrical word play and just overall better storytelling of lyrics. What I hope to do in the future is create an API capable of accessing a wider database of rap music lyrics creating code to measure the vocabulary level of the lyrics and identifying measures of quantifying lyrical superiority.",NA,Analyzing the Evolution of Rap Music from 1989 to 2016
https://nycdatascience.com/blog/student-works/kaggle-competition-predicting-realty-price-fluctuations-russias-volatile-economy/,32,The Exploratory Data Analysis (EDA) for this project was not as extensive as the other projects. A lot of Kagglers shared their exploratory analyses on the platform sparing us the need to do an extensive EDA.Aside from the high number of variables the main issue with the dataset was the missingness.The matrix below helps visualize patterns in data completion. The columns represent the variables in the dataset while the rows represent the observations. Except for full square footage (the first column in the matrix) none of the variables were fully populated as denoted by the white spaces. It is also hard to find observations that have values for each variable. In fact The dendrogram below helps to more fully correlate variable completion; it groups variables against one another by their nullity correlation.Cluster leaves that are grouped together at a distance of zero fully predict one another's presence. One variable might always be empty when another is filled or they might always both be filled or both empty and so on. Cluster leaves which split close to zero but not at exactly at zero predict one another very well though still imperfectly. For our dataset we see that the nullity of the build_count variables are correlated which makes sense but does not provide much insight. However we see that  We learned later in the process that these variables were important in predicting prices accurately. However as shown in the dendrogram if an observation is missing one of these variables it is probably missing the others making this observation useless. This is something to keep in mind when imputing.  The latter approach seemed more accurate. This is why we considered the observations that were complete in some situations.Our priority for this project was to uncover as many insights as possible. This is why we decided to stick to models that are easily interpretable and avoid models such as XGBoost which may help get a high score on Kaggle but are still essentially black boxes.In this blog post we'll highlight our . Below is a diagram that represents our feature selection workflow. The main goals were to:It should be noted that in general one shouldn’t expect that We trimmed down the list further from . The main goal was to get a good sense of our dataset by playing with it a little bit before refining our model. Our results are summarized in the table below:It is interesting to note that . It is also noteworthy that the Kaggle Error (error on a test set) is considerably lower than the error on our training set which is surprising.  We suspected the housing prices in the test set to be distributed quite differently than those in the training set.  from the model to see how this would change our results. (since in the training set housing prices trended upward with time so eliminating time yields lower price predictions). To investigate this more closely we turned to the macro data to see if it could be used to effectively understand/predict housing market changes. Our approach to the macro data was to try to find a relationship between the Russian economy and the average Russian housing price level.  If the macro could predict the average price level to some extent the goal was to combine the macro features with the house specific data via model stacking to come up with a final prediction.First we needed create a response variable (as we did not have any price information in the original macro data set) and looked at the daily average. However the time series was really noisy as it was dependent on the particulars of the houses sold on a given day. This is why we took a 30 day rolling average to smooth everything out (the blue line in the graph below) and looked at price per square meter to mitigate the effect of size on price (the green line). The two series were highly correlated (as depicted in the graph below) meaning that we could use them interchangeably with similar results.The next step was to identify macroeconomic factors that best predicted the price and could be used in our final model. The macroeconomic variables were highly correlated which narrowed down the selection significantly. In reality we already had an idea of what we wanted to see from the data. We had run some trials on Kaggle where we submitted the same price for every house in order to determine the average price level. We realized that  This is why we were looking for a negative coefficient in the macro data to predict a decrease. Unfortunately there was none. What we saw in the macro data was a paradigm shift where an endogenous event like an action by a central bank was shaking up the relationship between the variables which probably happened right on the border between the test and training set.    With these insights in mind (i.e. fewer features is better and time is misleading) we decided to run additional models on our main dataset that would exclude any notion of time and use a limited number of variables.We started with a multiple linear regression. Aside from its high interpretability a linear regression is more powerful when it comes to extrapolation. This is important when the predictions for the test set are outside the range of values of the predictor in the training set used to generate the model. The latter scenario was relevant to our dataset. For example some subareas were present in the test dataset but not the training and vice versa.The Variance inflation factors (VIF) revealed some multicollinearity between our initial features. Our final selection (after accounting for multicollinearity) included 7 variables. None of them were time related. Our score on Kaggle brought us in the top 50%.The next steps were to improve the accuracy of our model. There are multiple ways to do so:  doing feature engineering deciding on ways to treat missing data and outliers refining our selection of feature tuning our algorithm doing ensemble methods etc. We decided on two strategies:We also employed different methods to treat missing data and outliers. However the score did not change much. Doing feature engineering by creating new variable(s) from existing ones did not help much either. We’ll go into more details for the two main strategies we ended up choosing:Principal Component Analysis (PCA) works best on datasets with more than 3 dimensions as it becomes harder to make interpretations with a high number of variables. This was perfect for our dataset. and the biplot below explains why.The points in the graph represent the scores of the observations on the principal components. Points are close together which shows that observations have similar scores on the components displayed in the plot. This is an indicator that the components did not help differentiate the observations as well as we would have expected. The variance explained by each principal component was in fact pretty low (as depicted in the table below) and the principal components were loaded equally on the variables. This is why the components were not necessarily better predictors than the raw variables.The last strategy to improve the accuracy of our model was to run a random forest. We tuned our algorithm by playing with the mtry hyperparameter.We used a mtry of 53 (which was equal to the number of features divided by 3 usually best for regression) and a mtry of 12 (which was equal to the square root of the total number of features). The mean squared root error was very low (0.07 for the random forest with 53 mtry and 0.15 for the random forest with 12 mtry) indicating an overfit (which is confirmed in the graph below). Our score on Kaggle was indeed lower than the multiple linear regression by a slight margin (0.35346 vs. 0.35198).There are a There is no doubt that using XGBoost would have helped boost our score. However the main goal of the project for our team was to stick to simpler models that are highly interpretable rather than rely too much on black box models (granted random forest belongs to one of them). Although using random forest did not work for us it did work for other teams when using different parameters. However as mentioned above tree methods are not relevant when extrapolating and the competition hid the weakness of tree method since the housing price fell in the testing set (but stayed in the same range as the price in the training set).,NA,Kaggle Competition: Predicting Realty Price Fluctuations in Russia’s Volatile Economy
https://nycdatascience.com/blog/student-works/cluster-analysis-twitter/,32,"Most of the data readily available in the real world comes unlabeled. Getting the labels often entails manual classification which can be a tedious and expensive process. While dealing with unlabeled data we are limited to using unsupervised machine learning techniques since we do not have a definite response variable to work with. Unsupervised learning is generally used as an exploratory data analysis tool used to understand the data. I decided to work with unlabeled Twitter data in the  form of tweets to attempt  to understand  human thoughts and interactions. A business or organization can benefit a great deal from understanding the individuals in its  target audience and then leveraging this to connect with them on a more personal level. Thus having an automated classification system of such unlabeled tweets can serve as a valuable tool for business improvement.The classification of tweets is a multistep process starting from the preprocessing of unlabeled tweets before applying unsupervised machine learning algorithms for clustering and topic modeling. Once the desired level of accuracy in tweet grouping is achieved one can proceed to manually label them based on similarity in context. Now these labelled tweets can be fed into supervised machine learning models to train the model to understand the underlying similarities of tweets belonging to each category. A fully trained tweet classification model can then be applied to unseen data in an automated tweet labeling system. A visualization of the steps involved can be seen below.The steps outlined in red form the focus of this blog. Only once reproducibility is achieved in the clustering and topic modeling of tweets can one proceed to the supervised machine learning model.The data for this project were obtained directly using Twitter's Streaming API which allows access to Twitter's global stream of Tweet data. The API gives the user control over downloading by specifying the language as well as keywords being used in the tweets. For the purpose of the project I was interested in English tweets.  For some of my data I used the keyword search option to narrow the focus of the tweets being downloaded. The data conveniently downloads in JSON format which can easily be parsed into a Python dictionary.For analysis the data was then transformed from dictionary format into a Pandas dataframe. Preprocessing of the tweets was somewhat of a trivial process since they tend to be slightly different in structure from standard text documents. There tends to be more use of slang and shorthand to get the message across in limited 140 character working space. For that reason one must be careful in removing unnecessary elements from each tweet while retaining features which would contribute to the meaning and context of the tweet.I used regular expressions to remove the characters ""RT"" in any tweet because keeping it would lead to clustering retweets together which would be at odds with the purpose of my analysis. I also removed any user mentions from tweets which can be identified with the preceding ""@"" before the user's name. Similarly I removed any links that were present as well. Finally I used Python's NLTK package to remove stop words like   etc. Stop words are words that are grammatically essential to structure but contribute very little to the context of a sentence. Once the data was cleaned up it was now ready for machine learning.The unsupervised learning algorithms used for this analysis include Latent Dirichlet Allocation (LDA) and Nonnegative Matrix Factorization (NMF) for topic modeling and Kmeans for clustering of tweets. Conveniently all three are available in Python's scikitlearn package.Kmeans clustering algorithm essentially grouped individual tweets into only one of the specified number of clusters which could be problematic if a given tweet fell into more than one category. Despite this limitation Kmeans did surprisingly well in grouping most tweets based on similarities. However there would always be a ""catchall"" cluster which would contain any tweets which the algorithm was unable to properly classify. In this regard the topic modeling algorithms allowed the flexibility of assigning multiple topics to tweets by weight of how likely it is to be a part of each of the topics. The following sections of this blog will show how these algorithms performed with different sets of tweets.The first set of tweets I tested were obtained using different keyword searches related to health & fitness travel sports news/politics art music video games lifestyle dating/relationships wedding/marriage and other life changing events. This was a test to check if the unsupervised learning methods would group the tweets based on the keywords that were used to get them. A word cloud of the hashtags associated with this set of tweets can be seen below. we have the top words used to determine topic using LDA.  Topic 5 is clearly related to US politics as it includes words as trump comey and president. Topic 12 generalizes to entertainment by including the words music video sports and gameplay. Topic 8 seems to be a confused mixture between travel and politics that could partially be due to the mention of President Trump's travel ban.For this set of tweets NMF performed substantially better than LDA as can be seen with the topic assignments below. The top words for each topic relate very well to the keywords used in my search. Topics 1 3 5 7 9 and 13 clearly show relevance to jobs/hiring wedding art sports health & fitness and politics respectively. There were some unclear classifications with certain topics but overall NMF performed very well in this case.Clustering using the Kmeans algorithm also outperformed LDA in coherent grouping of tweets. With the exception of a few ""not so good"" clusters grouping based on sports politics music and health can be seen in clusters 5 8 11 and 13 respectively.The next set of tweets for cluster analysis were streamed with the only restriction of being in english. Given the absence of intentional underlying categories it was interesting to see how the algorithms would perform in grouping randomly obtained tweets. Below is a  word cloud of the hashtags of these tweets.For the openstreaming tweets LDA performed much better as compared to the previous set of tweets. Topic 2 is related to music entertainment and topic 3 is specific to tweets about the elections in the United Kingdom. Topic 28 seems to talk about jobs hiring and careers. Topics 4 and 7 are somewhat unclear in determining the context of the tweets.Once again NMF outperformed LDA in topic assignment as can be seen with the clear context of most of the topics determined using this algorithm. Topic 0 2 7 8 and 10 clearly relate respectively to entertainment UK elections career sports and giveaway advertisements. Topic 11 refers to the unfortunate tragedy of the Arianna Grande concert in Manchester UK and topic 29 speaks of events related to FBI director Comey's testimony regarding President Trump and Russia.Kmeans did a good job at clustering tweets by category as can be seen some of the clusters below. Cluster 5 grouped tweets mentioning different genres of music together in a musicrelated cluster. Cluster 14 and 25 group on the discussion of the UK and US politics respectively. The final set of tweets I obtained were from users following specific organizations and companies. The purpose of narrowing my focus to user tweets was to detect if multiple tweets from the same users can provide insights into topics of interest to the user. For this set I obtained about 100 tweets each from 2000 followers of organizations/companies such as Barclays Fitbit Tinder Democratic/Republican National Party and the Economist. For this blog I will discuss my analysis of tweets from followers of the Democrat or Republican Party.Using LDA topic modeling and visualizing some of the tweets associated with the assigned topics we can see the opposing views from followers of two competing entities. Followers of the Democratic Party seem to be more critical of President Trump in their tweets while followers of the Republican Party are more supportive of President Trump and more critical of Hillary Clinton and former President Obama.Future work on this project will involve cycling between cleaning the text of the tweets to obtain optimal topic modeling and clustering using the algorithms discussed in the blog. Afterwards supervised learning techniques can be used to train a model to classify new tweets based on preexisting or newly added categories.This scope of this project and its methodology can be extended to include image analysis and video analysis to include posts from other forms of social media such as Facebook Instagram and Snapchat. The more data that can be added to this model the better the model will serve as a valuable resource to organizations to understand their current and potential customers.",NA,Cluster Analysis of Twitter: Understanding Human Interactions for Business Improvement
https://nycdatascience.com/blog/student-works/simple-medicare-hospital-recommendation-system/,33,This is implemented in R using Shiny. This shiny App is intended to help patients to find hospitals where they will receive highquality care with reasonable copayment. Users can see direct outofpocket costs of certain procedures and the quality index of hospitals to identify their best choice for  for their personal health conditions and budgets.  Medicare is the federal health insurance program for people who are 65 or older certain younger people with disabilities and people with EndStage Renal Disease.The overall rating summarizes up to 57 quality measures over 3000 U.S. Hospitals. The overall rating ranges from one to five stars. The more stars the better the hospital performance on the available quality measures.The data include hospitalspecific charges for the more than 3000 U.S. hospitals that receive Medicare Inpatient Prospective Payment System payments for discharges paid under Medicare based on a rate per discharge using the Medicare Severity Diagnosis Related Group (MSDRG) for Fiscal Year (FY) 2011 2012 2013 and 2014.The top  performing hospitals aren’t necessarily highpriced though the worst performing ones sometimes are.The report ''  released by CMS has shown huge inequities in hospital charges. Take total joint replacement as an example; prices of this procedure ranges from $5304 in Ada Oklahoma to $223373 in Monterey California. [1]   Also there is no correlation between price and r quality. When conducting analysis on the CMS quality and cost data one find that that best quality hospitals aren’t necessarily highpriced while hospitals with the worst quality of care sometimes are.Patients are an insecure group of people worried about the risk involved in their treatments along with breathtaking sixfigure hospital bills. The whole healthcare market is like a 'Black Box'   The price information on healthcare is almost entirely blocked from customers. Although quality information is available online it is hard to guarantee the authenticity. Also as cost and quality information are never put together  patients don’t get to see the data they need to make wise decisions.Based on aforementioned problem an application allowing patients to compare cost and quality information at different hospitals seems to be an effective solution. This Medicare hospital recommendation system assembling 100+ DRGs' cost information and quality rating information over 3000 hospitals is able to bridge that knowledge gap that will  allow patients to be informed about costs and the quality of care to expect..EDA is crucial in the process of application development. To gain a deeper insight into the data  we need an  analysis on hospital type cost  quality comparison and  DRG cost comparison.Before diving into the cost analysis some terminologies and questions need to be clarified:[1]: [2]: [3]:,NA,Getting higher quality & lower cost procedure: A simple Medicare hospital recommendation system
https://nycdatascience.com/blog/student-works/predicting-baseball-hall-fame/,33,The Great Bambino. The Big Unit. Joltin' Joe. Henry Rowengartner. If you're familiar with the sport of baseball you might recognize some of these names from real life or the movies. Since baseball has been engrained in the fabric of America for almost 200 years and since it is my favorite sport I decided that I thought it might be fun to take a look back at some of the best players to ever play the game to see how modern day players stack up against them.Sports analytics have progressed dramatically in recent years. With the wealth of data available for Major League Baseball many teams are employing analytics departments to extract value from their statistics. I decided to scrape the hall of fame players on  to investigate these statistics and determine how good a player has to be in order to be inducted into the hall of fame. Additionally I took a sample of data from players that have played since 1989 in order to predict whether or not they might be eligible to make the hall of fame.The parent URL I used to extract Hall of Fame player statistics was on  a baseball database that has all the baseball statistics one could ever want. I had to write two separate spiders to take into account the different statistics used to measure a batter's statistical output and a pitcher's statistical output. All in all there are 163 batters in the Baseball Hall of Fame which translates to a file of roughly 3500 rows (including all their seasons played). There are 77 pitchers in the hall of fame which translates to a file of about 1600 rows (includes all their seasons played). The data for more recent players was downloaded and filtered to include only batters that had over 500 plate appearances per year and pitchers who pitched over 150 innings per year in order to normalize numbers.First I took a look at batters. I wanted to get a sense of the distribution of number home runs are hit per season for hall of fame batters as well as number of hits per season. The histograms look like this:Hitting a high number of home runs don't appear to be a huge indication of making it to the hall of fame although it probably doesn't hurt. We do see that on average there are roughly 160 games hit per year most frequently which translates to about one hit per game. A batter who bats .300 for a season or fails to get a hit 70% of their at bats is considered a great hitter. I also plotted the total number of strikeouts against number of walks and noticed that many hall of fame hitters had tremendous plate discipline indicated by walks being greater than strikeouts.Finally I plotted the batter based on OPS or On Base Percentage plus Slugging Percentage a general statistic to measure the overall value of a player. Suffice to say I was not surprised to see Babe Ruth at the top of that list.For pitchers I mostly wanted to see their performance in terms of outcomes they could control which are home runs allowed strikeouts and walks. I plotted the strikeouts per nine innings against the walks per nine innings. We see an interesting linear trend in which the higher the strikeout numbers the higher the walks. This could say something about the pitcher’s ability to spin and curve the baseball in order to deceive hitters which might lead to a lot of strikeouts but also less control.In order to measure overall value of a pitcher I measured their FIP or fielding independent pitching. This takes into account number of home runs allowed number of strikeouts number of walks and a constant number to normalize the statistic. The lower the FIP the better.According to FIP Ed Walsh is the best pitchers in the hall of fame followed by the more contemporary Pedro Martinez.Since I had the hall of fame statistics I figure that I could use them as a baseline and try to fit a logistic regression model that would take data for more recent players and predict whether or not they would be included among the players immortalized there. I combined my hall of fame data with the separate subset of more recent players and then used crossvalidation to train the model. Then I predicted and tested it on the test set.For predicting players I used backwards selection and tested the correlation between each variable to make sure there was as little multicollinearity as possible. I then created a binary variable to categorized players that are in the hall of fame versus those who aren't. For batters my best model had hits per season as the most significant predictor (the higher the better) followed by overall strikeout rate (the lower the better). The accuracy of this model was 97% which beat the baseline of 83% by a wide margin. The accuracy was very high and variance was very low though I definitely could have used more data to obtain a better model.For pitchers my best predictor was walks and hits per inning (the fewer the better) and home runs per 9 innings (the fewer the better). FIP also was included in the model though it might have not been necessary as it is essentially a total measure of home runs walks and strikeouts. The accuracy of this model was 95% which beat the baseline model of 90%. Again this was highly biased due to lack of data and crossvalidating my model.,NA,Predicting the Baseball Hall of Fame
https://nycdatascience.com/blog/student-works/make-goodreads-com-top-400-list/,33,"I am a bookworm. I read 34 books a month (nonfiction books only) and start 80% of my sentences with ""I read somewhere that...""The goal of this project was to analyze the  and predict the success of a book based on its reviews.I divided my analysis into three parts each guided by a set of questions:The first two parts are intended to guide an author BEFORE he/she writes a book while the last one is designed to predict the success of a book AFTER it was published.I used Python for my analysis and Python’s Bokeh package for visualization.. Note that there may be an overlap between the genres as a book could be classified under multiple genres.:The heatmap below addresses the first question: it shows the distribution of the Top 16 genres by ranking category (e.g. Top 100 Top 200 etc.). The color depicts the number of books.An interesting insight: the genre Literature is not part of the three top genres. Yet it is highly represented in the Top 100 list.As raised above the Top 400 list is meaningless if we can't put it into context by comparing it to the worst 400 list. Unfortunately there is no such list...The histograms below compare the distribution of genres between the Control list and the besteverbook list.An interesting insight: while the Classics genre is underrepresented on goodreads.com it is highly represented in the Top 400 list suggesting that On the other hand the Fiction Fantasy and Young Adult genres represent a significant proportion of the books on goodreads.com. Their likelihood of being in the Top 400 is  higher than an underrepresented genre like Classics.The short answer is no. As illustrated below there is a similar trend between the Control and Top 400 lists. The number of pages is not a predictor of success.For this part I looked at the distribution of books in the Top 400 per year and investigated a possible A significant proportion of the books in the Top 400 list were published between 2003 and 2006. Although a few peaks in the average number of pages published coincide with peaks in the number of books in the Top 400 there is no correlation between the two variables.It is interesting to see that the trend for the Control list (graph below) differs from the trend for the Top 400 list. Though a lot of books were published after 2010 they  are underrepresented in the Top 400 list.For those who are curious about the details below is a list of the Top 10 books in 2003 2004 and 2006.Is there a correlation between the distribution of books in the Top 400 per year and the number of books sold that year? It does not seem to be the case as illustrated below.However the dataset is biased (as explained below) so the results are to be taken with a pinch of salt. We see a clear trend showing that the Top 100 books receive more reviews than the Top 100200 books which themselves receive more reviews than the Top 200300 etc. Or is the relationship between the number of reviews and the score an inverted bell curve where the best and worst books get more reviews? After all an average book does not spark strong emotions (whether positive or negative); only the best and worst ones do.The Control list does not confirm the assumption of an inverted bell curve. On the contrary the fivestar books get fewer reviews than the fourstar book while the one or twostar books receive barely any reviews.I started my sentiment analysis on the Reviews column using the .The output of the AFINN method is a float variable (the AFINN score) that if larger than zero indicates a positive sentiment and less than zero indicates negative sentiment. A high level of granularity was needed to be able to identify which words could predict a book's ranking in the Top 400 list. Before applying the AFINN method to the Top 400 list I checked its reliability on a broader list i.e. the Control list.The AFINN Lexicon seemed to be a good predictor of success (as illustrated in the graph above). The next step was to see if it could help differentiate a book in the Top 100 list from a book in the Top 300 list…Unfortunately as illustrated below the  (where by definition all the books are successful).The scatterplot below offers the same insights as the box plot except that I used a continuous variable (the score) instead of a discrete one (the ranking category) and differentiated the ranking categories by color.In the scatterplot above the scores do not drop below 1.5 which makes sense as we don't expect the 'Best Ever' Books to have highly negative sentiment score (i.e 3 4 or 5). However there’s still a ; some Top 100 books have a negative sentiment score.My sentiment analysis needed some finetuning:There was  as illustrated in the graph below.It is interesting to see that some words with a low AFINN score appear quite frequently in the reviews of successful books. This is because the AFINN lexicon is constructed with  meaning that “like” in the sentence “I don’t like” will be interpreted positively.I was not very successful with the AFINN lexicon and decided to look at the Vader lexicon. Its algorithm seemed to be more granular as it output 4 classes of sentiments:I used a pairplot to analyze the correlation between:The graphs with a pearson's r score of 0.5 or higher signaling a correlation are in green. Pearson's r can range from 1 to 1 with:There is a negative correlation between the Vader neutral and positive scores which makes sense: the scores are from the same lexicon after all. However there is no correlation between the Vader positive and negative scores which is surprising.Lastly there is My analysis suggested that a . There is no clear pattern indicating a preferred year of publication or number of pages although most of the Top 400 books average 300 pages. However it does not help differentiate the fourstar from the fivestar books. That being said the fivestar books probably sell better than fourstar ones which was not part of my analysis.Finally the sentiment analysis part was unsuccessful at predicting the success of a book within the Top 400 list. The  helped predict the likelihood of a high score on goodreads.com. However .",NA,How to Make Goodreads.com Top 400 List?
https://nycdatascience.com/blog/student-works/travellers-guide-skies/,34,Maybe you’ve been here before. After a long flight you make your way down to baggage claim ready to grab your bags and finally make your way to your destination. A few minutes pass and you see other passengers grab their bags. After twenty minutes the crowd around the carousel starts to thin. Finally you’ve had enough. You speak to the nearest TSA agent and then find out after some investigating that your bags didn’t make the trip with you.United Airlines and other airline companies have recently captured the nation’s headlines because of terrible flying experiences caught on camera. Two weeks ago  on a United Airlines flight and just last week it was reported that a passenger’s pet giant bunny was . Such incidents as well as my own flying experiences piqued my interest. I wanted to use data to find which airline and airport are the worst in terms of TSA claim frequency and why.The data used to investigate each TSA claim event was taken from the . The data spanned from 20102015 with a total of just over 30000 claims. Some of the variables that proved most valuable were:Additionally data was pulled from the  for information on number of flights per top airlines and airports in 2014 and 2015. From this data we are able to get a rough estimate of how often a claim might be filed per airline and airport.First I wanted to get a sense of how many claims happened per airport. I use latitude and longitude data to created a map with googleVis and leaflet which shows different sized circles in each airport location proportional to the number of claims filed at each. We can see that regardless of year JFK has the most claims by far followed by LAX and then Chicago. This doesn’t seem too surprising however since we’d expect to see much higher claim counts from major metropolitan cities. The map will prove to be more useful once we have a better insight as to how many flights go in and out of each of these airports. That will allow for a more accurate assessment of  the claim rate.Next I was curious to see if there was a seasonal pattern involved in the frequency of claims. However surprisingly enough there was no clear seasonal trend according to the time series visualization.As we have an actual count of how many items were claimed per event I thought it would be cool to visualize how many specific items were lost or damaged by the top airports and airlines. The heat maps allow us to filter by disposition but for sake of brevity we’ll look only at approved claims (claims that were paid out in full by the airlines). Some insights were:For airlines in 2015:Though I went into this project assuming that United Airlines or Spirit Airlines would be the worst airlines in terms of number of TSA claims the data did not indicate that these airlines which dominated the headlines in recent weeks were responsible for the most passenger dissatisfaction. Incorporating the BTS data showed a different angle however based on the number of flights each airline flew per year in 2014 and 2015. In 2015 American Airlines Southwest Airlines Delta United and JetBlue rounded out the top 5 most flown airlines. To my surprise Atlanta was responsible for the most flights in 2015 followed by LAX O’Hare Dallas/Fort Worth and then JFK. Now the map actually tells us something. JFK was clearly the number one airport for most claims from 2010  2015 yet was only the 5th most traveled in 2015. As for the claim rate by airlines United claims the top spot in 2014 and 2015 followed by JetBlue and then Spirit Airlines. Maybe the news headlines were onto something!Admittedly my claim rates calculation should be taken with a grain of salt. The rate was calculated based on the overall average number of passengers per flight in 2014 (89) and 2015 (94). Interestingly 2015 saw about 25000 fewer flights than in 2014 but 2015 recorded more almost 50 million more passengers than in 2014. Perhaps the increase in passengers and decrease in flights in 2015 caused a higher rate of claims but I believe more data would be needed to confirm this claim.Another notion worth investigating is the number of connecting flights and passengers per airport. Atlanta leads all airports in number of flights by a wide margin but has a low claim rate suggesting that maybe passengers who fly to Atlanta don't actually file their claims until they arrive at their final destination.Lastly although this is a very high level analysis at domestic airport claims it does make me feel a little better that recent articles like  come to similar conclusions as my findings. I hope I may be of service by allowing you to interact with my  which you might find useful in deciding where your next flight might be and which airline you may choose to get there.Link to my .,NA,A Traveler's Guide to the Sky
https://nycdatascience.com/blog/student-works/movie-lovers-guide-new-york/,34,". apartment from Sex and the City.Have you ever wondered where your favorite TV shows and Films were shot?”
Last year my best friend came to New York to visit me We are both massive film fans and I thought it might be a good idea to book a tour taking us round some of the most memorable filming locations in the city. For example he's not a fan of Sex and The city or Gossip Girl which most of the ones I've come across seem to focus on. He is however a huge Science fiction movie fan we both love Spider man and The  Rises. However I'm struggling to find something suitable.
Like most of the New Yorkers I love New York City and I love movies. Then  why not combine them two together?If there’s one place outside of Hollywood that is inextricably connected to the movies it is New York City. “Spiderman” King Kong ，“ The Dark Knight Rises” “Breakfast at Tiffany’s” “Spiderman” ” “Sex and the City”: from romcoms to scifi thrillers New York City has been home to many iconic movie sets. According to the  New York is among the world's most filmed cities—in 2011 for example 188 films were shot on location in the five boroughs along with 23 primetime episodic television series.",NA,A Movie Lover’s Guide to New York
https://nycdatascience.com/blog/student-works/investigating-nba-shot-data/,34,naïvely expect that accuracy would be a strictly decreasing function of distance reasoning that farther shots are strictly less likely to be made than closer shots (all else equal).   So why isn't accuracy significantly decreasing as shooters get farther from the hoop between the interval of 10 to 20 feet?  Certainly many factors could be responsible but to list a few possibilities:,NA,Investigating NBA Shot Data
https://nycdatascience.com/blog/student-works/nba-teams-eventual-conference-finalists-perform-regular-season/,34,As much as Kobe Bryant was one of the greatest basketball players to have graced the game it would probably be easier (even for Kobe) to hit an open shot like the one in the first clip. Statistics could very well miss this.,NA,NBA Teams: How Do Eventual Conference Finalists Perform in the Regular Season?
https://nycdatascience.com/blog/student-works/interactive-geospatial-data-digestion-framework-implemented-r-shiny-us-county-example/,34,"Where are the best places to live? How do you answer this question?If you turn to Google there are many ""top 10"" lists generated by someone else who does  know your personal needs. If you have retired you might not care much about salary; if you do not have kids education costs might mean nothing to you; if you have strong political views you might not want them to clash with your neighbors'; if you have serious lung issues air quality might be more important than anything else. We don't all share the same concerns and one size does not fit all.The question is: how do we go about finding the answer that does fit individual need?How about choosing among a variety of available data assigning your priority and ranking the candidates based on your specific input?Here is a prototype of the described solution using R and Shiny. I invite you to play with this  and make your own judgement.",NA,Where to Live? An Interactive Geospatial Data Digestion Framework Implemented in R with Shiny
https://nycdatascience.com/blog/student-works/kaggle-otto-classification/,34,"The 4th NYCDSA class project requires students to work as a team and finish a Kaggle competition. The 2017 online bootcamp spring cohort teamed up and picked the .  It was one of the most popular challenges with more than 3500 participating teams before it ended a couple of years ago. Although high leaderboard score was desirable our primary focus was to take a handson learning approach to a wide variety of machine learning algorithms and gain practice using them to solve realworld problems.This blog post presents several different approaches and analyzes the pros and cons of each with their respective outcomes.The challenge boiled down to a supervised multinomial classification exercise. The training set provided by Otto Group consisted of about 62000 observations (individual products). Each had 93 numeric features and a labeled categorical outcome class (product lines). In total there were nine possible product lines. The goal was to accurately make class predictions on roughly 144000 unlabeled products based on 93 features. Since the features and the classes were labeled simply as feat_1 feat_2 class_1 class_2 etc. we couldn’t use any domain knowledge or interpret anything from the real world. We therefore sought a modeling approach centered around predictive accuracy choosing models that tended to be more complex and less interpretable.Without much realworld interpretability of any of the features an initial exploration of the dataset was essential.An inspection of the response variable revealed an imbalance in class membership. Class_2 was the most frequentlyobserved product class and Class_1 was the least frequentlyobserved. This gave us a rough idea that the data was biased toward certain classes and would require some method of sampling when we fit it to the models down the road.Next we took a look at the feature variables. A correlation plot identified the highly correlated pairs among the 93 features. The red tiles below show the intensity of positive correlations and the blue ones show the intensity of negative correlations. Using information gained from the plot we could eliminate or combine two features with high correlations.All 93 features were comprised of numeric values so we also looked at their value distribution related to the predicted outcome classes. As the plot below shows some of the features have a limited number of values and can be treated as categorical values when doing feature engineering. It might also be worth standardizing the value ranges for all features if we were to use lasso regression for feature selection.Principal component analysis and resulting scree plot revealed a ""cutoff point"" of around 68 components. This threshold indicates that in attempting to capture the collective variability among all feature variables a significant portion of the variability can be explained with only 68 principal components rather than the original 93 features. Although we opted to keep all features during the project principal components after the 68th (those not contributing much to cumulative variance) could be dropped from the model as a means of dimension reduction.Before the model fitting process it was necessary to understand the Kaggle scoring metric for this contest which would have bearing on the modeling approaches chosen.Kaggle uses multiclass logarithmic loss to evaluate classification accuracy. The use of logloss has the effect of heavily penalizing test observations where a low probability is estimated for the correct class. It also necessitates that the submission be a probability matrix with each row containing the probability of the given product being in each of the nine classes. Given this required format we attempted to develop methods to combine individual model predictions to a single submission probability matrix.We approached this multinomial classification problem from two major angles regression models and treebased models. Each of the team members tried different model types; several methods of ensembling were then attempted to combine individual model outputs into the final contest predictions. In order to conduct our own test before submitting to Kaggle we partitioned the 62000 rows of training data into a training set of 70 percent and a test set of the remaining 30 percent. Through the use of the set.seed() function/parameter in many R functions we made sure that all models were reproducible i.e. built and tested with the exact same training and testing sets and therefore could be accurately crosscompared for performance. While the kNearest Neighbors (kNN) algorithm could be effective for some classification problems its limitations made it poorly suited to the Otto dataset.One obvious limitation is inherent in the kNN implementation of several R packages. Kaggle required the submission file to be a probability matrix of all nine classes for the given observations. The R packages – we used  here – only returned the predicted probability for what it predicted to be the correct class not for the other classes. The inability to return predicted probabilities for each class made the model a less useful candidate in this competition.In an attempt to work around the issue we developed a process to synthesize the probabilities for all classes. We created kNN models using different values of K and combined the predicted probabilities from these models. The attempt didn’t result in accurate results though. The resulting Kaggle logloss score wasn’t at all competitive. The lack of true multiclass probabilities is almost certainly the cause of the poor performance of the kNN models. With only a predicted probability of one of nine classes for each observation there was an insufficient basis to predict probabilities well for the other eight classes. Having inadequate probability predictions for the remaining classes resulted in an uncompetitive model. Generating kNN models was also time consuming. The time required to compute distances between each observation in the test dataset and the training dataset for all 93 features was significant and limited the opportunity to use grid search to select an optimal value of K and an ideal distance measure.Given more time it might be better to use kNN in the process of feature engineering to create meta features for this competition. Instead of using kNN directly as a prediction method it would be more appropriate to use its output as another feature that  or another more competitive model could use as an input. Choosing different values of K or different distance metrics could produce multiple meta features that other models could use.Regression methods could be used to solve classification problems as long as the response variables could be grouped into proper buckets. For this problem we wanted to see if logistic regression would be a valid approach.Procedurally we broke the problem down into nine binomial regression problems. For each binomial regression problem we predicted whether the product would fall into one class and used stepwise feature selection (AIC used here) to improve the strength of the models. We then aggregated the probabilities of the nine classes weighted by the deviance of these nine models into one single final probability matrix.  Using the base R lm() function we found this approach to be extremely time consuming. Running one binomial regression model with stepwise feature selection could take up to an hour for the training set. The multi logloss score was slightly better than kNN but still not competitive enough. The weights assigned to the nine models seemed to have a significant influence on the accuracy of the model. We might be able to combine boosting and resampling to get better scores but the limited computational performance of the base lm() function prompted us to look for a faster and more capable alternative.Since highperformance machine learning platform can be conveniently accessed via an R package h2o’s machine learning methods were used for the next three models. H2o proved to be a powerful tool in reducing training time and addressing computational challenges on the large Otto training set as compared to native R packages.The  function mimics the generalized linear model capability of base R with enhancement for grid searching and hyperparameter tuning. This model was trained on the 70percent training set with a specification of “multinomial” for error distribution. Although grid search was performed over a range of alpha (penalization type between L1 and L2 norm) and lambda (amount of coefficient shrinkage) predictive accuracy was not improved while computation time increased. Ultimately no ridge or lasso penalization was implemented. The overall GLM strategy produced average logloss performance on the 30percent test set.H2o provides functions for both of these treebased methods. Despite sharing many of the same tuning parameters and using similar sampling methods random forest modeling on the Otto dataset – even at a small number of 50 trees – was computationally slow and provided only average predictive accuracy. The gradient boosted trees model in which decision trees were created sequentially to reduce the residual errors from the previous trees performed quite well and at a reasonable speed. This model was implemented with ntrees  100 and the default learn rate of 0.1.Developing a neural network model using the h2o package provided fast results with moderate accuracy but it did not match the most effective methods employed such as extreme boosting.The h2o package’s  function was used to construct a neural network model. The ability to compute logloss values and return predicted probabilities by class made the package suitable to provide results that could be readily submitted to Kaggle or combined with the results of other models.The function offers many parameters including the number of hidden neurons the number of layers in which neurons are configured and a choice of activation functions. Two layers of 230 hidden neurons yielded the lowest logloss value of the configurations. The activation function selected was the tanh with dropout function in order to avoid overfitting.While the neural networks model could be built in minutes it scored average accuracy with logloss values in the 0.68 range. It is not clear that further tuning of the model parameters would yield significant reduction in the logloss value.Combining high predictive accuracy gradient boosting without added computational efficiency the  package provided a quick and accurate method for this project ultimately providing the best logloss value of all models attempted. Cross validation was performed to identify appropriate tree depth and avoid overfitting. Then an  model was trained and applied the test set to score the logloss value.Numerous parameters had to be tuned to achieve better predictive accuracy. Due to time limitations we only tested the following parameters:The multilogloss value for the 30percent test set was 0.51 – the best from all of the models discussed above. When we used it on the real test data for Kaggle submission we got a score of 0.47.Although each model was submitted to Kaggle to test individual performance we also aimed to combine models for improved predictive accuracy. Two methods averaging and stacking were used for ensembling.Model averaging is a strategy often employed to diversify or generalize model prediction. Many models are fit on a given training set and their predictions are averaged (in the classification context a majority vote is taken)  diluting the effect of any single overfit model's prediction on test set accuracy. This method was used to combine test set predictions from our six individual models but did not improve overall accuracy even when attributing larger voting weights to stronger predictors like Stacking was used as a method in building the  and neural network models. We were interested to attempt stacking as the method was employed by the top teams on this Kaggle competition's leaderboard. Stacking involves fitting initial ""tier 1"" models and using the resulting predictions as metafeatures in training subsequent models.For this project we used the predictions from an  and neural network model as metafeatures for a secondtier  model. The multilogloss value obtained from our 30percent test set was 0.56 a worse test accuracy than the model alone.To conclude the best multilogloss value achieved from our experiments was at 0.47 using the  model alone. This put us around the 1100th position on the competition leaderboard as of the end of April 2017.We note the following key takeaways from this classification project perhaps also applicable to other competitive Kaggle contests:",NA,Otto Product Classification: Kaggle Case Study
https://nycdatascience.com/blog/student-works/global-life-expectancy-explorer-via-shiny-happened-floor-dropped/,35,"Robert Frost one of the great American poets of the early twentieth century once stated ""In three words I can sum up everything I've learned about life: it goes on."" If we can take his observation and apply it to the average life expectancy of humans across the globe as a single group we can say that life not only goes on but fortunately also up. (see graph below)",NA,Global Life Expectancy Explorer: What happened when the floor dropped out?
https://nycdatascience.com/blog/student-works/scraping-trulia-zillow/,35,For this project I have used Python's following packages:Beautiful soupScrapySelenium.Main goal of this project was to gather data preprocess it and prepare for farther analysis.To scrape real estate listing information from  I used Selenium Python bindings. Itself Selenium is appropriate for creating robust browserbased regression automation suites and tests. In other words it is an automated testing suite. Selenium Python bindings gives access to Selenium WebDriver which enables the user to directly communicate with the web browser and write functions and execute tasks in Python programming environment.When one goes to  and types in the area of interest to buy or rent real estate she is presented with an interactive webpage that has a map of the area dotted with locations of the listings and on the right side 20 listings per page. In order for me to understand what it is that I want to automate using Selenium I first had to brows the listings observe and register my own actions while browsing. This step gave me an initial idea of the algorithm to be written for automation.There are two aspects of scraping zillow.com with Selenium.In order for me to reach the final web page where there are all the descriptions and information for any one particular listing I had to go through several actions such as: This is the rough representations of initial chain of actions I wanted to automate with Selenium.The actual scrapping and writing of information happens mainly in step 3 and 4.Step 3 is required because when inspecting the webpage the xpaths to the information are hidden. They only become visible (hence ‘scrapable') when we click on the ‘More’ button.Step 4 mainly consists of finding the correct xpaths to all the different bits of informations of interest.Step 4 can be broken down into following smaller steps:.The website’s UI is similar to  with listings on the left half of the page and the map on the right side. The key trick to simplifying the scraping process was the following:If the website has it’s metadata stored in a JSON dictionary format thats a score!Steps of discovery:After inspecting each one of the search results I was able to find the tag that contained a relatively large json dictionary in it: a sign of useful information. Closer inspection revealed that it did actually contain all the information I was interested in regarding each listing on that particular page. To be more precise the tag contained several concatenated json dictionaries with different metadata information. That meant that after scraping this information I would have to use regular expressions and python’s string manipulation to extract the dictionary of interest.I used Python’s JSON package to help me with parsing the scraped information into a Python dictionary.,NA,Scraping Trulia and Zillow
https://nycdatascience.com/blog/student-works/using-nyc-citi-bike-data-help-bike-enthusiasts-find-mate-dating-app/,35,There is no shortage of analyses on the NYC bike share system. Most of them aim at predicting the demand for bikes and balancing bike stock i.e forecasting when to remove bikes from fully occupied stations and refill stations before the supply runs dry.This is why I decided to take a different approach and use the Citi Bike data to help riders find each other; a kind of Tinder for bike riders...As a bike enthusiast I wish I had a platform where I could have spotted likeminded people who did ride a bike (and not just pretend they did).The goal of this project was to turn the Citi Bike data into an app where a rider could identify the best spots and times to meet other Citi Bike users and cyclists in general.As of March 31 2016 the total number of annual subscribers to Citi Bike was 163865 and its riders took an average of 38491 rides per day in 2016 (source: )That adds up to more than 14 million rides in 2016!I used the Citi Bike data for the month of May 2016 (approximately 1 million observations). Citi Bike provides the following variables:Before moving ahead with building the app I was interested in exploring the data and identifying patterns in relation to gender age and day of the week. Answering the following questions helped identify which variables influence how riders use the Citi Bike system and form better features for the app:As I expected based on my daily rides from Queens to Manhattan 75% of the Citi Bike trips are taken by males. The primary users are 25 to 24 years old.However while we might expect these young professionals to be the primary users on weekdays between 8am9am and 5pm6pm (when they commute to and from work) and the older users to take over the Citi Bike system midday this hypothesis proved to be wrong. Also tourists seemed to have little impact on usage as the short term customers only represent 10% of the dataset.Looking at the median age of the riders for each station departure we see the youngest riders in East Village while older riders start their commute from Lower Manhattan (as shown in the map below). The age trends disappear when mapping the station arrival above all in the financial district (in Lower Manhattan) which is populated by the young wolves of Wall Street (map not shown).The map also confirms that the Citi Bike riders are mostly between 30 and 45 years old.Finally when analyzing how the days of the week impacted biking behaviours I was surprised to see that Citi Bike users didn’t ride for a longer period of time during the weekend: the median trip duration is 19 minutes for each day of the week.However as illustrated below there is a difference in peak hours. While the peak hours during the weekdays are around 8am9am and 5pm7pm when riders commute to and from work on the weekends riders hop on a bike later during the day with most of the rides happening midday.This is the first of the four projects from the NYC Data Science Academy Data Science Bootcamp program. With a twoweek timeline and only 24 hours in a day it was impossible to cover every data angle. Below is a quick list of the analysis I could have would have and should have done if given more time and data:,NA,Using NYC Citi Bike Data to Help Bike Enthusiasts Find their Mates
https://nycdatascience.com/blog/student-works/r-shiny/soccer-betting-analysis-use-betting-agencies-odds-predict-match-results/,35,"As a soccer fan with 3 years of work experience as a live soccer match analyst I have thousands of soccer game hours in my repertoire. I follow European soccer on a weekly basis and know most of the teams and players in the major leagues of Europe.Even with all my knowledge and experience I find it hard to predict soccer match results. Like in any other sport the best team doesn't always win. There are many parameters that affect the outcomes of any given game. The skills of the players the tactical formation and teamwork may be the most important ones. But are meeting all these parameters enough to win all the games played? If so we should be able to predict match results pretty easily.The truth is there are many more factors that affect soccer match results  team motivation and spirit player injuries fan support chemistry between teammates and opponents reputation and win history are some of them. The complex interplay between these variables during the fastpaced activity of a game makes every match unique.  Professional betting agencies are making a lot of money from people who want to predict match results. It is safe to say that their betting odds are calculated in a way that maximizes their profits and minimizes their risks.In this project I wanted to examine the degree to which betting agencies' odds correlate with actual match results and to see if there is any way to maximize prediction accuracy. For this I looked at the betting agencies' reported odds for the basic 3way bet (home team vs.  draw vs. away team) and the level of favored outcomes within each game (high favored moderate favored and low favored) and compared them to actual match outcomes to see if betting agencies' odds had any value. I further categorized the match outcomes by the location of the teams (i.e. hosting vs. visiting) the stage of the season and the numerical difference between the payouts in order to find patterns that optimize prediction accuracy.In the end I found that there are three parameters can help predict the outcomes with up to 80% precision: 1) the agencies' high favored result 2) the location of the team and 3) the stage of the season. I used R shiny app and ggplot2 to visualize the data. You can find the full results on the app.
In most soccer competitions draws may be the final result of the game so there are 3 different outcomes to bet on between Team 1 and Team 2:  First outcome: team 1 wins  Second outcome: team 2 wins  Third outcome: team 1 and team 2 drawThe odds are translated into payouts. The result with the minimum odd is the one that is most likely to happen it has the least risk and therefore offers the lowest payout.The result with the maximum odd is the one that is the least likely to happen it has the higher risk and therefore offers highest payout.For example let's take the first match in this betting odds chart of the English Premier League and look at the odds for the full time result. In this game Arsenal is playing against Crystal Palace. For an Arsenal win any dollar you bet will give you $ 1.29 (a $0.29 profit). For a draw in the match a dollar will give you $ 4.98 ($3.98 profit). And a Crystal Palace win will give a return of $ 8.06 ($7.06 profit) for a dollar bet.The data sets were taken from Kaggle a part of a soccer SQLite data base.The data sets include data on more than 25000 matches from 9 different leagues in Europe over 8 seasons (2008/2009  2015/2016). The data includes: match results and dates teams leagues and match betting odds from 9 different betting agencies.The European leagues are: Belgium Jupiler League  England Premier League France Ligue 1 Germany 1. Bundesliga Italy Serie A Netherlands Eredivisie Portugal Liga ZON Sagresand Scotland Premier League and Spain LIGA BBVA.The betting agencies are: Bet365 Blue Square Bet&Win Gamebookers Interwetten  Ladbrokes Pinnacle Sporting Odds Sportingbet Stan James Stanleybet VC Bet and William Hill.It is important to note that there was always consensus between the agencies regarding the probability for each outcome (i.e. they all thought Arsenal had the highest chance to win); the only difference was the magnitude of payout that they offered. Therefore I considered the average consensus as a single entity. For data processing I used RSQLite package for R to convert the different SQL tables to CSV files.                As part of the data cleaning and preparation I deleted rows with missing values and ignored data from 2 betting agencies because their betting odds were uploaded to the SQL server as integers rather than exact numeric values. After this process there were 22434 observations left.Moreover I added columns to the data set to include the match winners  the agencies' average minimum middle and maximum payout and agencies' favored result. For the analysis I defined the result with the minimum payout as the favored result by the betting agencies. The success rate shown in the charts is calculated as the number of times the favored result was the actual final result of the match divided by the total matches played.The favored result level column is a breakdown of the matches to 3 groups using the difference between the payouts (as extrapolations of the odds). My assumption for this calculated column is that the higher the difference between the payouts the higher the chance for the minimum payout to be the winning outcome. Therefore the groups are categorized in the following way:A high favored result  max payout  min payout > 2A moderate favored result  2 >max payout  min payout > 1A low favored result  max payout  min payout < 1The betting payouts have a normal distribution. The maximum and middle payouts are skewed to the right. The minimum payout ranges from a little more than 1 to around 3. The maximum payout ranges from 2.5 to 40. The middle payout distribution looks similar to the maximum payout and range from 1.9 to 10. Below are the histograms of the payouts:   The minimum and maximum payouts are inversely related and the minimum and middle odds are also inversely related. This is explained by the fact that when there is a high favored result (for example one team has a better track record than the other) its payout will be low and accordingly the other outcomes' payouts will be high. On the other hand when there is no high favored result (for example the teams playing have same skill level) the payouts will be quite similar. This is a scatter plot of the minimum and maximum payouts of the matches observed: The first chart depicts actual match results. Here we can see that the home teams wins 46% of the time the away team wins 29% of the time and there is a draw 25% of the time.The next graph shows that the agencies favored the home team 73% of the time and the away team 27% of the time while they almost never favored a draw (13 out of 22434 matches). We can see that the agencies favored the home teams in most cases. This emphasizes the importance of location in the competition. This chart raises an interesting question: why do the agencies never favor a draw result when this outcome occurs in at least 25% of the matches? I did not come across any data explaining how the agencies determine their payouts however I believe that agencies prefer to favor one team over the other because it's easier for them to promote the bet among gamblers. It's just more interesting to have a faceoff. Next I wanted to check how accurate the favored result was in terms of predicting the match outcome. I found that the agencies' favored result (represented by the minimum payout) had an average success rate of 53%.  This was consistent for each of the seasons in the data frame.
  I also wanted to examine to what degree the location made a difference in the payouts given to favored teams. For instance I would expect the payout for a favored home team to be lower than a favored away team. After all it is widely believed that the home team has the higher advantage. I used two box plots which demonstrated that there is If the minimum payout for a favored home team and a favored visiting team are almost identical does this mean that the rates of winning them are the same? In fact no the rates of winning are not the same. As we can see from this bar chart the favored home team had a 55% chance of winning while the favored away team had a 50.5% success rate. Although these are not earthshattering numbers could this be an opportunity for the betting agencies to attract more gamblers? By raising the payout for the favored away team they are promising larger compensation when in fact the probability of the favored away team winning is actually quite low. One of my more interesting findings was that the accuracy of the predicted wins increased during the later stages of the season. The late stages of the season carry higher stakes than the early stages as this when the champion and the relegations are decided. In the chart below we can see a slight improvement in the success rate over the span of a season. 
The results of the away team demonstrate a different trend. While the high favored teams' success rate increases over time the low favored teams show inconsistent success rates throughout the season. It seems that there is no clear effect of the season stage on their performance.  ",NA,Soccer Betting Analysis - How to use betting agencies odds to predict match results?
https://nycdatascience.com/blog/student-works/machine-learning/predicting-player-performance-daily-fantasy-sports/,36,According to FanDuel and DraftKings daily fantasy sports are games of skill and not luck. The outcome of games are the result of the participants relative knowledge of the game and the accumulated statistical production of the athletes in the real world. Using the vast amounts of player and team data collected in sports it should be possible to predict player performance and gain an edge in this game of skill. In its current state participants will combine their domain knowledge of the sport and hours of research to put together a winning strategy for the game.In fantasy sports participants compete against each other by comparing the performances of players they select against the performances of players their opponents choose. Player performance is determined by a simple formula based on their in game stats determined by the host site. We chose to focus our time predicting the value of NBA players due to our interest and knowledge of the game. The platform used to play daily fantasy sports is DraftKings. DraftKings offer many games that differ in the how the payout is distributed. We chose to only play in “50/50” and “DoubleUp” contests because they had the highest chances of winning. In every NBA game on DraftKings the collection of players drafted must satisfy a salary maximum of $50000 and position constraints. There must be eight players selected one for each position of the game an extra guard an extra forward and an extra utility player that can have any position. The player’s stat line for the day will be converted to points for your team and your overall team points will be ranked against everyone else’s points in the pool.Players will accumulate points as follows:We used team and player game logs for predicting fantasy points for every game. When looking at the real time stats and comparing it with fantasy points you can see an obvious correlation between the player stats and fantasy points. For the most part the number of points a player scores and the number of minutes they play have a large effect on their fantasy points. But this poses a problem because we cannot use real time stats to make predictions because they wouldn’t be available until the game is over. We would need to use lagged stats to make predictions.Using the game logs we created lagged stats based on the averages of the previous games for differing lengths of time (i.e. last 3 games last 6 games etc.). Besides individual performances in the past we believe the opposing team has a large effect on how well a player does in the game. We created features related to opponent stats versus a player’s team stats and features related to how certain positions perform against certain teams.We tried a variety of models but the models that returned the best results were regression trees and time series. We wanted to create different models for every player because every player behaves differently. But because there is not enough data to make predictions we decided to use clusters to group similar players together. We created groups based on player averages for the season. For each group we narrowed down features to use for our predictions by only selecting features that were correlated to fantasy points in each groups. From there we fit a regression tree on each cluster.In this project we applied common prediction methods to time series data. To be precise we modeled the draft king points of each player using two common time series models AR(1) and EWMA(0.95). AR(1) is the autoregression model with parameter 1 meaning that each date's DKP is modeled to be a linear function of the DKP the date before. It should be noted that our date is constructed rather than the actual game dates. In another word if a player has played games on Jan. 1st Jan. 10th and Jan. 15th we transform the dates into Day 1 Day 2 and Day 3 ignoring the actual game dates while preserving the order. The second model we applied to time series prediction is EWMA. Instead of considering the actual DKP's we construct a different time series where each number is a weighted average of the same day's DKP and the previous day's DKP and we chose the weight on the same day's DKP to be 0.95. Then we applied AR(1) to the constructed sequence. Such model allows us to incorporate the effects of recent performance of the player besides the previous day's. This model seems to be more reasonable than the previous vanilla AR(1) and indeed it performs better for our training data. Both AR(1) and EWMA(0.95) are combined with other models' predictions as an ensemble and the predicted DKP's are further used in the knapsack stage where based on each players' predicted DKP and actual salary we make the choice of lineups. Winning in Draft Kings requires creating a lineup that will outperform a percentage of the contestants in a game. In the 50/50 and Double Up formats a winning lineup requires placement in the top 50% or 43% respectively. For example in a Double Up format pool of 34 players the lineups that rank in the top 15 get paid. In order to create a lineup with the best projected performance we would have to select a combination of value that would yield the best value. This involves selecting a combination of players that has the highest projected DKP at the most affordable cost. In optimization this is commonly known as a knapsack problem   Given a weight constraint and and set of items determine the number of each item to include in a collection so that the total weight is less than or equal to the constraint and the total value is as large as possible.A simple approach to this problem is to use a greedy algorithm. This algorithm attempts to create an optimal lineup by selecting the local optimal at each step. This involves picking the player with the best performance projection and cost combination at each position. Although this approach generates an an acceptable lineup it creates a potential problem of getting stuck at a local optimal.Another approach used was a genetic algorithm. First a population of potential solutions was created. From the population the best solutions (parent solutions) were selected and allowed to crossover to create child solutions. The best child solutions become the new parent solutions and they continue to cross over and create new solutions for multiple generations. To avoid getting stuck at a local optimum randomness was introduced by mutating child solutions randomly replacing players and by inserting random solutions into the population at each cycle. We have only taken part in a few games for a couple days straight. Although we have not placed very high in many contests we were pretty successful. Overall we placed in over 70% of the games.For our future work we would like to apply the model to other daily fantasy sports platforms and try to make predictions for different sports. We also need to improve the predictions by including more features and trying different models.,NA,Predicting Player Performance for Daily Fantasy Sports
https://nycdatascience.com/blog/student-works/numerai-hedge-fund-competition/,36,Numerai is a hedge fund that uses a machine learning competition to crowd source trade predictions. Competition is based on proprietary hedge fund data collected and curated by Numerai. Data is encrypted before being made public because it is highly valuable proprietary and its quality provides a competitive edge for Numerai. This enables Numerai to obtain machine learning predictions on private data without ever making it public.Numerai  that high quality proprietary data is expensive to collect and provides a significant competitive advantage. Hedge funds and other financial institutions are in an optimal place to collect and curate this data but they have a strong incentive to keep it private and guard it. But these institutions only employ a small percent of the world's machine learning talent pool. It Meta model construction and its bias over time can also be compared to financial markets. Financial markets are based on decisions made by individuals which when combined determine the market direction. Similar processes are at work with the movement of this meta model. It is build on the predictions of the individual participants and the direction is determined by them. Additionally by using a metamodel formed out of a bag of predictions Numerai is able to take a portfolio theory approach to predictions. Metamodel is made of individual bets made by many data scientists. Averaging of these bets significantly reduces the individual systematic bias and model variance. Resulting leftover bias can be interpreted as learning deduced from the data.Following table shows the time taken accuracy loss and f1score of multiple models. All the algorithms were executed on Google Compute HighCPU (64 core 60 GB memory and Ubuntu 16.04) instances.We optimized and calibrated individual models using bayesian hyperparameter optimization and came up with three ensemble models that consistently produced low variance and low bias. Models were ensemble by soft voting hard voting and using predictions from previous models as additional features. Deep neural network in Keras was used as a meta classifier. These same algorithms were used for predictions on the following week’s dataset. Soft voting ensemble model consistently ranked among top 5 for both the weeks. Other two models consistently ranked between the 5th and the 20th position. Give us a shout out if you want to chat about additional details.In the near future we would like to build a deep learning model using transfer learning along with full generalization and automation from week to week. Implementation using PySpark (for parallelizing) and by incorporating MongoDB (parameter tracking) will also be in the works.,NA,Numerai Hedge Fund Competition
https://nycdatascience.com/blog/student-works/capstone/instapredict/,36,An internal Instagram study showed that teens delete up to half of all their Instagram posts due them not receiving enough likes. In fact Instagram’s new “Instagram story” is in part an effort to counter the vanity imposed by the “likes” metric  . Gathering the data for this project was the largest challenge. In recent years Instagram has evoked an increasingly stringent API policy requiring developers to undergo a number of processes before being given a key. As such we needed to scrape the data. Instagram like many other large organizations has many built in tools to limit and trap web scrapers from getting too much data. Alternatively there are a number of Instagram web viewers that display instagram content without the same regulations. For our project we used two sources: Instagim and Instaliga.Instagim allows photos to be displayed by tag on a single page displaying username likes comments caption filter and some relevant hashtags. Due to the scope and timing of this project we focused solely on photos under the “nature” tag. While we had the photo and the likes it got we were still missing a key metric: followers. We needed to find a website that could be easily crawled for followers and following since they would be important features in predicting the amount of likes for a photo. Instaliga was easy to scrape since it allowed us to append usernames to the end of a URL and then crawl for this information using scrapy. Additionally we were able to scrape the metrics for the last 20 posts from a given user giving us a baseline for their standard level of engagement with their audience. However since each user is it’s own URL the script generated too many requests often being met with server errors. To auto throttle scrapy’s wait time turned out not to be time efficient so we moved forward by hammering their server with requests and getting what data we could.  Some features such as the filter applied to the photo seemed to have even less of an effect than originally thought. These features would likely have larger bias in subsets of photo type: for example selfie photos may commonly use a filter to make a person’s skin look healthier. However for our “nature” photos there seemed to be little correlation.Our plan was to supplement all of the user data with information from the photos to achieve a more accurate prediction. The logic follows that if a user’s followers and general account popularity defines a range within a photo should fall the features in that photo will aid in assigning a more precise prediction for “likes”. After turning the photos into arrays the PIL library was used to to extract summary statistics for each color band. Other features such as luminance were easy to calculate given this data. However we also wanted to extract more complex features. Using the OpenCV library we were able to use a pre trained model for facial recognition and assign a number relative to the number of faces in each photo. We also measured the blur of each photo Unfortunately few of these photo features seemed key in finding a correlation for likes in our subset of data. We compared many of the features against likes and likes/followers ratio to normalize likes but the correlations still seemed somewhat weak.In the future we plan to extract even more features and scrape data from multiple accounts to see if photo features matter more in the realm of a single user. Comparing some of these photo attributes against the mean and median likes for users may also have been beneficial.We began the modeling process by constructing a basic MultiLayer Perceptron with a single layer of input nodes and a single output node  using the Rectified Linear Unit as our activation function. After setting up this basic model using the Keras API with TensorFlow backend the hyperparameter tuning was initiated. Having tuned the model extensively other models were then constructed for comparison.Given the results of the MultiLayer Perceptron we sought to compare with less complex model. Utilizing the GradientBoostingRegressor in sklearn we were able to obtain much better results in our cross validation processes and predictions on our test data. Ultimately we were able to compare our predicted results to the actual amount of likes a post received in our test set and determined that 95% of the predictions were within 30 likes.In the future we’d like to get access to the Instagram API since a scrapy based web application is not a model for stability and scalability. It would alleviate the majority of issues currently present in our process and allow us to expand our models for different categories of photos. Currently we are limited exclusively to public accounts due to data access. Additional features such as time of the week follower involvement/network analysis and a greater variety of image analysis would further reduce the error on our prediction within a given subset of photos. With the right amount of data and computational power this applied model could solve some inherent issues with Instagram’s “likes” metric and even expand to other platforms.,NA,likePredict: A product to predict
https://nycdatascience.com/blog/student-works/capstone/metarecommendr-recommendation-system-video-games-movies-tv-shows/,36,Metarecommendr is a recommendation system for video games TV shows and movies created by    and .  It uses wordembedding neural networks sentiment analysis and collaborative filtering to deliver the best suggestions to match your preferences. It is part of our capstone project delivered at the end of the  program.Finding a piece of media today can be difficult. There are so many games movies and tv shows coming out every week that it is difficult to keep up with. It can take hours to look through blogs videos and reviews to determine if a new piece of media is something you will like. Finding a game from the past that you are sure you will like is even harder. Websites like metacritic.com attempt to simplify this process by aggregating reviews. However there are still some major flaws including:Hence for our capstone project we decided to address these issues by creating an application to improve your search for your next game (and even let you find movies and TV shows if you wish!).  Metarecommendr is a web application that combines a sleek and intuitive user interface with the powers of contentfiltering and collaborativefiltering in order to deliver the best recommendation for you.Metarecommendr was designed and built in the span of 2 weeks. The project workflow is summarized below:One of the reasons we opted to implement both content and collaborativebased recommendations was the distribution of ratings found in our dataset. There were in total roughly a million reviews  half from critics half from users. We found that for both critic and user reviews scores the distribution of ratings were negatively skewed. Hence relying solely on ratings (for collaborative filtering) would not offer enough granularity to produce sensible reviews as most products are perceived positively.Interestingly in our early exploration of the dataset we found that the number of reviews was not necessarily indicative of the quality of a product. I is among the most reviewed items and yet it has a very poor average critic and user review. This makes some intuitive sense. Games that skew either very positive or very negative create more discussion. Extremely bad games can be fun to talk about with others similarly to how bad movies can live on as cult favorites. Mediocre games where there isn’t much to say tend to have less discussion and therefore less reviews.There are mainly two types of recommendation algorithms: contentfiltering and collaborative filtering.Since a big portion of the dataset was composed of text data from reviews the chosen approach for feature engineering on contentbased recommendations was . This is an unsupervised algorithm to generate vectors for documents. It is an extension of the Word2Vec algorithm where a document (instead of a word) is turned into a vector representation.  Its implementation in Python can be found under .Doc2Vec is able to learn semantical similarities among words making its implementation more sophisticated than tfidf. An example output of our model on critic reviews shows that it was able to learn pretty well similar words to the word “Excellent” . Pretty good job!For metarecommendr two Doc2Vec models were trained separately on Summary and Critic Reviews. We opted for not using user reviews since there were not enough descriptive words to yield a meaningful recommendation. On the user interface a user selects a product they like. Products are then recommended according to a cosine similarity metric. The closer to 1 the more similar two vectors(products) are.A major challenge to implementing collaborative filtering on this particular dataset was the high dimensionality and sparsity of the useritem matrix. There were a total of around 27500 products and 63000 users with an average number of less than 3  reviews per user. To reduce the dimensionality of the useritem matrix truncated Singular Value Decomposition (SVD) was implemented. Consider a usertoitem matrix A where aij represents the ratings from user i for product j. SVD states that every matrix Anxp can be approximated by the following equation:where U and V are orthogonal matrices and Sis a nxp diagonal matrix with singular values of A along the diagonal. As S is a diagonal matrix we can obtain a more compact representation through SVD. Truncated SVD takes this approach one step further by using only the k most significant values of S instead of all values. Under this approach we compute a rankk approximation to A such that it minimizes the Frobenius norm error as follows:For metarecommendr the dataset was split into train and test and k was chosen to be 13 according to Cattel’s scree plot.Once we obtain the rankk matrix A' we can make recommendations according to the entries in the matrix.  In the context of our dataset A’ corresponds to a matrix of predicted user ratings where aij'is the predicted user rating from user i for item j. Compared to a baseline where all user ratings for products are simply predicted to be the average user rating (RMSE  7.50) truncated SVD improves 19% upon the error term on predicted user rating (RMSE  6.07) .To sum up for collaborative filteringSVD  a user inputs and ranks a few items. A useritem matrix is then generated and decomposed by SVD. For a given user i this approach allows us to get a predicted user rating for different items and recommend items with highest predicted rating.To better understand the relationship between item review scores we compared items against each other using a modified Pearson’s correlation formula. To help scale down this correlation matrix items with less than 3 overlapping reviews were disregarded and given a score of 0 or no correlation. This itemitem matrix approach also allowed us to make crosscategory recommendations since the algorithm was no longer bound to an item’s metadata(such as in collaborative filtering). On the user interface a user has the option to select a product they like and they receive products with the highest correlation metric. As mentioned in the introduction a major problem with Metacritic’s dataset was the fact that sentiment of reviews did not necessarily match the text data. To address this issue we performed sentiment analysis on the critic reviews. Positive and negative were defined as follows: reviews with scores of 55 and below were classified as negative and those with scores of 85 and above were classified as positive. Reviews with scores in between these values were not used for sentiment analysis. Sentiment Analysis used vectors from doc2vec as features. We attempted a few different machine learning models including: Logistic regression Naive Bayes SVM and different types of Neural Network. The performance of each model is described below:At the end  with the following features: 2 convolution and pool layers 2 recurrent LTSM layers and 3 dense fully connected layers. This model lead us to an accuracy rate above 90%. On Metarecommendr this sentiment analysis is showcased interactively:  a user types in a review and the text is evaluated according to our model. Users are able to receive feedback on whether the given score aligned or diverged from the text. We hope to continue with this aspect of the project to improve accuracy and use it as another preprocessing step for our recommendation systemThere are a few improvements that could be made to metarecommendr including:,NA,"Metarecommendr: A recommendation system for video games, movies and TV shows"
https://nycdatascience.com/blog/student-works/renthop-kaggle-competition-team-data-jedys/,37,interest_levelinterest_levelinterest_level.interest_levelprice/roomprice/bedcreatedYYYYMMDD HH:MM:SScharacterPOSIXctweekweekdayhourfeaturesphotosaptIDfeatureaptIDphotofeatureCountphotoCountlisting_idIn order to extract numerical features from the description content we use Tfidf and apply logistic regression models to predict interest levels based on Tfidf vectors for each apartment. The probabilities of the predicted interest level would become new features in our main model.The above figure shows the workflow of our description feature extraction. After cleaning the text we vectorized all nouns and adjectives which generated an apartmentword matrix. We selected the top 1000 terms according to their weights as the input for our logistic regression models.In order to predict interest levels we split the training dataset into equal parts. We trained two logistic regression models with those two subsets and predicted each subset with the trained model on the opposite subsets. For the test dataset we train a new logistic model with the entire training dataset and predicted interest levels with the trained model. Now we can get the probabilities of our predictions giving us three more columns of probabilities of   and  interest levels for both training and test datasets. Finally we used  and  columns as new features in our main model. descriptionOur model selection process began with an analysis of which models were possible to use and which would be best given the nature of the task.  The project is a supervised classification problem. The training set contained nearly 50000 observations and over 100 columns.  Many of those columns were engineered from other columns. While this produced novel and useful information it also created multicollinearity.  One example of this was the calculated cost per bed and apartment price. Our final constraints were time and computing power.  The models indicated that a treebased model was our best option but we tested a number of different approaches.Models we experimented with include multiple logistic regression.  Despite regularization manual feature selection efforts and tuning this model did not produce useful results.  While we could deal with multicollinearity through regularization this forced us to abandon useful information. Support Vector Machines may have been a better option with adequate time and computing power but these algorithms proved too computationally expensive to be useful given our time and computing power constraints.We focused our efforts on trees when a simple random forest model outperformed our other models without serious tuning and feature selection efforts. Ultimately XGBoost proved to be the most effective model. Gradient boosted trees outperform random forest models because they partition the sample space to minimize a userselected objective function and according to userselected regularization parameters. This is different from random forest which selects a random subset of features for each partition. XGBoost outperformed GBM due to more flexible tuning parameters and more efficient processing due to parallel computing.We created a spreadsheet to organize our tuning process and a model to assist in parameter selection. Each team member submitted parameters we tested to this spreadsheet. We then trained a random forest model on that spreadsheet to predict which features might minimize logloss. This model quickly reached local minima and did not prove to be very useful. Going forward a similar model may be useful if we could introduce a randomness parameter. Finally we experimented with a neural network.  We trained a simple neural network using TensorFlow and Keras. Training took 60 hours which precluded any time for any tuning. The resulting model produced a logloss of 0.6. We then ensembled this model with our XGBoost model using the geometric and harmonic mean. Had our results been uncorrelated we may have seens significant reduction in logloss. While the geometric mean did produce a logloss score very close to our XGBoost model (0.57) we were not able to produce superior results using these methods. Once we had determined that XGboost would give us the best predictive accuracy we started tuning the model with a focus on decreasing overfitting for the model. We decided to do tuning manually while keeping a Google Spreadsheet containing parameters for each model along with the training validation and Kaggle logloss error. By doing manual parameter tuning we were able to take advantage of multiple computers and adjust in an more quickly something not possible when tuning parameters using grid search.The parameters we changed from default values in the xgboost package were eta gamma max depth column sampling per tree and subsampling.Eta or the learning rate affects the rate at which feature weights are minimized every iteration. A smaller eta will reduce the amount of overfitting but increase the number of iterations that are required. We found our best results with an eta of .01.The second parameter we tuned lambda controls regularization of the XGboost model.Gamma controls the amount of error reduction required by new branches in the decision tree. The default value for gamma 0 produces a model without any regularization. Increasing this value produces a model that overfits the training set to a lesser degree but does not necessarily minimize validation or test error. We found that a value of .175 for gamma was optimal for reducing the logloss of the test set.Max depth is a parameter that controls the number of splits a tree is allowed to have. A greater max depth value results in more complex trees that are better able to predict the training set but do not predict a test set well due to overfitting. A max depth of 7 produced a model with complexity without overfitting.Our column sampling by tree was optimal at 0.8 which means that each tree sampled 80% of the features randomly which serves to reduce overfitting and produce trees that do not require all of the features.Subsampling was the last parameter that we modulated. We ended up going with a subsampling of 0.8 meaning that each iteration used a random 80% sample of the training set. Like the other parameters we adjusted this served to reduce overfitting while still providing a model with accurate prediction.  ,NA,Renthop Kaggle Competition: Team Data Jedys
https://nycdatascience.com/blog/student-works/renthop-kaggle-competition-team-null/,37,"Contributed by:Scott Edenbaum Ray (Xu) Gao Tommy (Yaxiong) Huang and Dodge Coates. With educational backgrounds ranging from Mathematics and Computer Science to Financial Modeling Team Null leveraged their strong analytical skills and programming proficiency to tackle the Kaggle challenge. The team implemented various machine learning algorithms including Stacking Models Voting Classifiers and Artificial Neural Networks during their journey joining the top 50% of submissions.Introduction:3/4th of all NYC housing stock consists of rental units and the NYC residential real estate market ranks as the second most expensive cost per square foot in the United States. Finding the right mix of housing accommodations in a new city is challenging enough but finding a good value can seem like finding that proverbial ""needle in the haystack.""RentHop utilizes a vast array of data to sort rental listings by quality to assist apartmentseekers in finding the perfect apartment. Two Sigma and RentHop a portfolio company of Two Sigma Ventures partnered with Kaggle to create a competition. Each team submits their model predictions and are ranked according to the outcome (aka. ""logloss"") of their predictions. The models are used to generate classification prediction for the ""Interest"" variable with three possible values: ""Low""  """" or ""High.""MediumApproach::Feature PreProcessThe first step in our execution was to make some base assumptions about which included features are most and least important for purposes of creating a benchmark. We assumed (incorrectly) that building ID manager ID and  listing ID would have  little to no positive impact on our models. For our purposes display address and street address were of no benefit to our model. There are ~124011 listings each with 0  13 images totaling ~700k images at about 83gB in size. We assumed that there would be a link between better quality images and higher interest property listings so we collected the average brightness and average luminance both on a 0  255 scale (0  dark 255  bright). In addition we used exif.py to gather metadata and collected average image file size and number of images for each listing to great success.We took a look at the basic numeric variables (including: Latitude Longitude Price Bathroom and Bedroom) in order to understand the apartment interest level. We decided to use these features as the foundation for our model and a benchmark for comparison of future models.Initially we made the assumption that the apartment interest levels will have some correlation with the location features (latitude/longitude). The reason behind this is that we had a natural inclination to associate higher price with higher interest and if that assumption holds it would naturally follow that the chart would have clusters of high interest areas in high income neighborhoods say areas near  Central Park. We were undeniably wrong. The chart below clearly depicts the lack of any clear clustering amongst the high and medium interest neighborhoods and an overall abundance of low interest properties.Additional derivative features include manager count building count word count word diversity and description sentiment. The next step in our process is EDA.EDA (Exploratory Data Analysis)First we conducted a brief analysis and visualization of the price bathroom bedroom and word count variables.This exercise was a great example of how one's biases can influence their analysis. We also made a point of deciding what interest level actually represent. Our team came to agreement that the interest level is a representative of ""buying pressure"" and a better proxy indication of value than price like we originally thought.In general when someone looks for an apartment price is one of the most important factors to determine if this rental is a good value. We'll continue by analyzing the prices in different interest levels.Something unusual is happening to the low interest apartments in the price kernel density chart. This is a result of having a substantial quantity of high priced low interest properties so we decided to do a logarithmic transformation of the data to normalize the price.Observing the various interest levels in the above chart we noticed that the average price of the high interest properties is less than the average price of the medium interest properties. The average price of the low interest properties is the highest amongst the group but that was due to the low interest high priced outliers.Next we looked at the bathroom data.One bathroom is the most common arrangement for the rental properties of all interest levels an expected result for the NYC housing market. However a few super high priced listings add the high bathroom count outliers that are visible in the graph. The medium and high interest properties are very similar so the bathroom values may be better suited as a binary indicator for low or not low interest properties.Visualization of bedroom data is to follow.An overview of the plot indicates that the bedroom could be a useful indicator to differentiate high interest properties from low and medium interest property listings. The median number of bedrooms was the same for both high and medium interest level.We looked into Manager_ID to determine if it could be a useful indicator to classify various interest levels.The x axis denotes the frequency count for individual managers. We notice that there is a peak for each interest level. This corresponding manager lists more apartments than others. We also notice that the high interest properties are spread among a rather small quantity of managers and one manager in particular has orders of magnitude more listings than their competitors.There are 18 user selectable features (Parking Space Elevator Doorman Cats Allowed Dogs Allowed etc). Those features can be good indicators for classifying interest levels but are often counterintuitive such as cats allowed and dogs allowed.It is interesting that low interest apartments have a higher tendency for allowing dogs and cats when compared to medium and high interest properties. This is a good indicator to classify the low interest properties but it may be an artifact of the large crosssection of low interest properties relative to the high and medium interest properties.Next we analyzed the derivative features starting with description length (word count).There is a bump at the beginning of the low interest curve right around the value of x  0. This corresponds to descriptions of length 0 or empty descriptions. Naturally apartment listings that omit key information such as the description are destined to be categorized as low interest. Buyers are ultimately interested in getting a good value regardless of price and the lack of data makes it almost impossible to determine whether the property in question is a good value or not.Lastly we analyzed the image related features.The chart depicts the derivative features generated from the image data. A common image format is RGB where R (Red) G (Green) and B (Blue) values range from 0255 and combine to form the color of each pixel. In this case we were able to determine the brightness of each image by averaging the R G and B values then we averaged the brightness values for each property listing. Image Width and Height refer to the average pixel count for the images for each property listing. Similar to the description length a lack of images is a strong indicator of a low interest property.Feature EngineeringWe did a trial feature selection by using Price Bathroom Bedroom Latitude and Longitude to form our benchmark. We also generated two logistic regression models by using the 5 variables and using 3 variables (omitting Latitude and Longitude). Omitting Latitude and Longitude was of little consequence to our predictions.We used the Extra Tree Classifier (ExtraTree Randomized Tree Classifier) a variant of the random forest classifier to measure the feature importance. Unlike a random forest at each step the entire sample is used and the decision boundaries are picked at random rather than choosing the optimal boundary. By averaging the various subsamples the Extra Tree Classifier improved the prediction accuracy and counteracts overfitting.Below is our initial feature importance plot:Price turns out to be the most important feature followed by average image size (for each listing). Word Count (description length) is the 3rd most important feature as we have seen throughout the EDA.We did more research on the image features RGB average image height and width existence of metadata etc. were added into the feature selection consideration.Here price drops to the second most important feature and average image size is now top of the chart. Manager Count is the fourth most important feature according to the chart above. The inclusion of the additional image features adjusts the weighting on the feature importance.In This Kaggle Project we have tried three types of 'high level' models: ensemble (aka stacking) models voting classifiers and artificial neural networks  each with separate tuning parameters. Each of these models comes with its own set of pros and cons and ultimately provide different logloss results on the test set.Initially we used the stacking model due to its simplicity. Since we created the code to implement the algorithm and our primary objective was precision and consistency it wasn't optimized for performance. This model's accuracy performed rather poorly even when compared to the benchmark  the top 5 most important factors in a simple logistic regression.Our experience with the stacking model left us looking for better options and we tried a voting classifier next. Voting classifier is another way to combine the results of different models. There are both hard and soft voting options. While hard voting obeys the majority wins rule soft voting is much more flexible for multiclass problems. It allocates the weight for each model and calculates the final result based on the basic model results and weights. Voting classifiers may be one of the fastest algorithms used to stack different models. However a major downside to the voting classifier as in our case is unstable predictions. This means that running a voting classifier model with identical parameters will yield different results each time it is executed.The last model we used in our project is the artificial neural network  a supervised machine learning algorithm growing in popularity amongst the data science field. This particular model is quite wellstructured in the python “keras” package. By using this model and selecting the top features we achieved the best score on the test set a logloss of 0.585. Unfortunately the run time for the neural network is extremely long without the assistance of appropriate GPU hardware. When operating our model each crossvalidation fold with 5 bags needed  approximately 7 hours to complete on a 12 core Intel Xeon server. In this project we set 10 folds so we were limited in the amount of times we were able to run the model within the project timeframe.Initially we omitted seemingly useless and highly correlated features such as building ID  manager  ID  description latitude longitude and created. Our intention was to decrease dimensionality and prevent overfitting problems. However after selection and testing of various models and parameters we found it is better to include those features however counterintuitive it may seem. Here is a chart of our prediction scores.  Over time we did manage to improve the score dramatically and end up in the top 50% of submissions.:Successful group cohesion through proper use of Github along with a preliminary discussions of individual strengths and weaknesses was key in facilitating concurrent development throughout the duration of the assignment. Concurrent work on feature engineering model development EDA and parameter tuning was necessary in order to complete the project within the deadline.When working on models that take hours to run it was very helpful to work out of a more traditional development environment (such as sublime emacs or text wrangler) rather than an iPython notebook. This development path allows additional portability and facilitates execution of code on multiple computers or remote servers. We found access to a 12 core Xeon cpu server running Ubuntu linux extremely useful throughout the development of our model. The models were developed locally on our laptops and the code was sent to the remote server through git. Then the code was executed in a terminal 'screen' which allows for the model to continue running even after the remote connection (ssh) to the server terminates.The conclusions we can draw from this project are many. It is extremely important to fully understand and define the value that is being modeled as well as keep an objective view in regards to the analysis. It was quite interesting and humbling to find so many seemingly contrarian indicators throughout this analysis  such as the surprisingly unimportant Latitude/Longitude features.",NA,Renthop Kaggle Competition: Team Null
https://nycdatascience.com/blog/student-works/kaggle-renthop/,37,"Finding apartments for rent is usually a challenging task.  one of many websites that try to make the process more convenient tries to help renters by sorting their listing by quality using data. To improve their methods and to better understand the needs and preferences of renters RentHop along with  hosted a competition on  to predict the number of inquiries a new listing will receive based on its features. We took the challenge to see if we could accurately predict the listing’s interest level based on the data provided.Below is an overview of what we did:The participants of the competition are given two datasets: one for training containing approximately 50000 listings and one for testing with approximately 75000 listings in the set. The goal is to predict the interest level of each listing based on the thirteen features provided. The features include information about the location of the apartment time and date the listing was created description IDs relating to the building and manager and basic apartment information. Photos submitted for each listing has also been included for analysis.The interest levels are split into three categories: high medium and low. As the graph indicates there are a lot more listings with low interest listings than medium and high listings and the number of high interest listings is very low even compared to medium.When exploring the data we always need to take into account correlation between predictor variables. Our basic  instincts tells us that bedrooms bathrooms and price might be related. After further investigation however we realized that it probably wouldn’t affect our model too much.One of the most interesting things we noted when learning about our data was that it seems like the hour when the listing was posted plays a role in determining whether the interest level would be high medium or low. It was also interesting to note that most of the listings occur during the middle of the night which might be due to companies automatically setting up their systems to post on renting sites. The proximity of the listings are to the start of the workday seems to correlate more with higher interest levels which is likely due to way RentHop posts their listings on the front page. The newer listings are likely to show up on the front page which in turn lead to a higher interest level.Although we were given a bit of information for predicting interest level we wanted to include more information to help our predictions. Because the rules of the competition do not allow outside data to be used we had to be a little creative when creating new features out of the old. Our strategy was to create as many new features as we could then slowly eliminate features that were not useful for predictions. We broke down the features into smaller related categories to simplify the process.When looking for an apartment every renter cares about the price and number of rooms. The dataset doesn’t specify the total number of rooms but it does include the number of bathrooms and bedrooms. Although the bedrooms and bathrooms are numerical we thought it might be better to treat the information as categorical data so we decided to have the option to dummify those features. Using the bedrooms bathrooms and price features we also created new features describing the ratio between all combinations of the three.The descriptions and apartment features are text data so they were trickier to work with. We used TFIDF to get the numeric values for the frequent words shows up in ""features” and ""descriptions"" then got the mean for every observation. We then used LDA Topic Modeling to get seven topics for every observation based on their TFIDF values. Subsequently we used Google's Word2vec tool to train every word in ""features” and ""descriptions"" and got 100 numeric columns for every word. Then we calculated the mean for all the words in every observation. We also obtained the word and character count for description and the count for apartment features.We grouped the manager and building ID by frequency. If the frequency was within a certain percentile they would be a part of that group. We also had the option to dummify the ID’s.Due to time constraints and limited processing power we decided not to focus too much on the photos provided. We included the number of photos for each listing and tried to extract basic information about each photo. For each image we obtained the minimum maximum and mean of the dimensional properties and the brightness.We simply broke up the listing created feature into month created weekday created day created and hour created. We also had the option to categorize and dummify these new features.For all our models we dealt with categorical features by dummifying them.To start we created a logistic regression model. We included all the numeric features such as numeric vectors from Word2vec TFIDF Topic Modeling. However after we submitted on the Kaggle our results were shockingly bad. After more visualizations we realized that there was too much multicollinearity between some columns like Word2vec. When we deleted those highly correlated columns our results improved. We also had many other variables that were highly correlated with each other because of the way we created our extra features so we carefully selected the features for our model.  We tuned our model using cross validation and grid searching by checking a range of parameters and also testing both L1 and L2 regularization.Although we started with linear regression we focused most of our time on classification trees. This was logical because this was a classification problem and our initial research showed that none of the features showed strong correlations with the response variables. As it appeared that logistic regression was not the best fit model for this problem we attempted to set up to a random forest using our full dataset we included columns that were similar; and used grid search and cross validation to tune the parameters. Our results were decent much better than the results obtained from logistic regression. When doing Kaggle xgboosting is a must. It provides better results in a way that is not computationally expensive. We did not have the computational power or time to tune the model so we used parameters obtained from a public kernel for this competition. We used the reduced dataset from our random forest to obtain our best results with xgboost. When evaluating the feature importance for our model  we see many obvious features that go into deciding interest level like the ratios of basic apartment features and also location. It is surprising to see that hour the listing was created played a significant role in determinine interest levels.We did a simple ensemble with our models using weighted averages. We manually picked the weights according to our score on Kaggle. Our xgboost models were weighted the most and the logistic regression model was weighted the least. This slightly improved our results.Our models worked out fairly well but can always be improved. Because of our allout approach for feature engineering we ended up creating a lot of similar features. We can limit the features more by carefully selecting the features for our models. At the very least we hope to improve our logistic regression model by reducing multicollinearity. We would also like to explore the use of  more unsupervised learning techniques like principal component analysis and possibly build a neural network to evaluate the photos. When we finish selecting our features we would like to optimize the parameters better for each model. Instead of using the standard grid search we would like to try to implement Bayesian Optimization for selecting our parameters. Of course our major goal for the future is to improve our predictions.",NA,Kaggle Renthop
https://nycdatascience.com/blog/student-works/machine-learning-xkcd/,37,"–The ""features"" variable which contained every distinct feature (e.g. Elevator Cats/Dogs Allowed etc.) for each apartment proved more challenging.  To analyze them separately we unlisted and tabled the features then exported a csv for visual inspection.Though there nearly 1300 distinct feature tokens many of these were effectively the same whether due to the use of synonyms or alternate spellings of the same words.  There were 13000 “fitness center”s but also a couple hundred “gym”s and a few dozen “health club”s.  There were “live in super”s “livein superintendent”s and “onsite super”s and so on.To correctly assign features to apartments we made 54 new variables and used regular expressions to match as many features as we could to the listings.  For example we assigned “balcony” and “terrace” (both in the Top 20) to the same ""private outdoor space"" variable.  We also pasted together the “description” and “features” variables since over 1400 listings had no separate feature breakdown.  Based on the table we estimate our approach matched at least 99.5% of features listed in the original data to their listings.To determine which features had any effect upon logloss we constructed a saturated Random Forest model with the new features (alongside the allimportant price variable) on a subset of the training set.  Next we ordered the features by Gini importance in that model then gradually pruned tabulating the model’s logloss on the rest of the set as we went.As the plot shows about 40 of the features manage to lower logloss though the reduction tapers at about 25.  The decline though is quite incremental after more than a few predictors have been included.  Proportionately the features that removed the most logloss were        and . To account for multicolinearity and efficiently incorporate as many of the useful features as possible we conducted a Principal Component Analysis.This screeplot shows that while most of the features are independent a handful seem to be covariant allowing us to get the predictive power of close to 40 features with only 30 principal components.  For the final model we incorporated a 30PC featureset.We assumed that residential area would have a large effect on interest. Living closer to the center of the city is desirable so long as the price isn't too high. As such we decided to convert the latitude and longitude variables to neighborhoods. By physically listing out the name of the area such as “East Village NY” you can use the ggmap library to get the latlongs for the center of the neighborhood. Once you have a list of areas and their respective latitude and longitude you can simply use KNN (k nearest neighbors) to assign the latlongs in the data to neighborhoods. A count of listings by neighborhood – note that this is not normalized for size of that area –  shows that Renthop listings are concentrated in midtown and downtown Manhattan.Next we can map the neighborhoods back to general areas of New York City and New Jersey. Below is a histogram of areas for the training set with interest level as the fill.Unfortunately these area assignments did not improve our score much. The histogram explains why: there seems to be little correlation between area and interest level. All general areas (and even the smaller neighborhoods for that matter) seem to have equivalent ratios of high medium and low interest apartments.Given that price was a strong predictor we also attempted to assign a median price for each neighborhood. After grouping by both price and bedroom we summarized with a median price and joined it back with the original data frame. Finally we assigned a binary variable (“expensive”) that was ""True"" if the price of an apartment was above the neighborhood median and ""False"" otherwise.Ultimately it didn’t improve the score significantly but it helped more than the neighborhoods themselves.  While this work may have been unnecessary in the end it was interesting to see a feature that seemed important turn out to have a small impact. Reasoning that apartment listings have their own specific sentiment language we also tried to generate a realestatespecific sentiment lexicon.  By using keyness to compare the total word frequencies in the descriptions against the Brown Corpus of Standard American English (1960) we were able to highlight words that are particularly common in listing language as seen in the WordCloud to the right.In addition to general positive language like “beautiful” and “great”  words emphasizing size (e.g. “spacious”) and culture/convenience (e.g. “central”) stand out in the visualization.  However despite several efforts to score and/or classify these words they did not improve the final model much.  Apartmentseekers likely see through agents' spin.Our model selection process went through multiple stages. Initially we used a collection of untuned algorithms to gauge a baseline for how these algorithms would work in terms of time accuracy and precision. For the Gradient Boosting Classifier Random Forest Classifier Support Vector Machine and a Multiclass Logistic Regression we used the Grid Search package in Python SKLearn in order to vary the range of values of the parameters as well as the set of features to be incorporated. Through the CrossValidation packages we were able to specifically optimize the algorithms to minimize logloss.We noticed immediately that the treebased algorithms like the Gradient Boosting Classifier and Random Forest performed significantly better at the baseline and devoted more time to tuning these algorithms. Of the four models used in the exploratory analysis the Gradient Boosting Classifier performed the best with an initial logloss score of about 0.59.  Given the success of the Gradient Boosting Classifier we wanted to see if we could push our results  further by using the XGBoost algorithm  in R. Ultimately we were able to successfully improve our results and chose this algorithm in our final model.The caret package has a grid search similar to the one in python. Using this it was easy to search through a few parameters.  However given the scale of the model and the number of final features columns it was computationally expensive to run for a number of hyper parameters. Consequently we didn’t use it to run a wide search but to narrow down options for parameters such as learn rate.Caret can also create cross validation folds. Instead of running a time consuming cross validation you can pull a fold out from the full training set and use it as your subtest in XGBoost. By adding it to the watchlist you can assess the accuracy of your model on a smaller test set while training it.  Of course since this doesn’t compare all folds against each other like actual cross validation it doesn’t account for variance between folds and is prone to some degree of error. Often the test score could be lowered to 0.55 in script but our submitted model received a Kaggle score closer to 0.6.",NA,Machine-Learning with Renthop
https://nycdatascience.com/blog/student-works/understanding-class-imbalance-ensemble-modeling-six-sigma-connect-rental-listing-inquiriesunderstanding-class-imbalance-ensemble-modeling-six-sigma-connect-rental-listing-inqui/,37,The TwoSigma connect challenge was to predict interestlevel—high medium or low—of RentHop apartment listings in the New York City area. This is a classification problem that could ideally be solved using supervised learning. To understand the data we plotted the interest levels to understand the relative number of each interest level. As can be seen in the plot below the data has a class imbalance with a significantly higher number of lowinterest listings than medium and highinterest listings combined.An apt example of this imbalance is the ratio of the height of the tallest mountain in the world (Mt. Everest) to the height of the tallest building in the world (the Burj Khalifa). The Burj Khalifa while extraordinarily tall is dwarfed in comparison to Mt. Everest as can be seen in the image below (the Burj Khalifa is in the orange box!). Thus the crux of the project is how to classify the high and mediuminterest properties while still capturing the mountain of lowinterest properties.One of the most interesting facets of the data was the effect of price on interest level. As can be seen in the chart below the higher priced properties were much lower interest and had a significantly larger variation than those in other categories.  This difference indicates that the price is likely to be a significant variable in the data.Along with price other “simple” features like a set number of bedrooms or bathrooms could be examined immediately. Those proved not to have as much impact on interest as price.There were also factors that while data rich  were not quite straightforward and so could not be examined immediately. We needed to conduct feature engineering before extracting value for factors like photos features descriptions and manager ID. While the number of photos number of features and word count of the description could be immediately calculated the human interpretation of photo content feature importance and description content called for the application of advanced techniques in image processing and natural language processing.To transform the photos into data we first classified images using the Inception Model found in the Tensorflow Neural Network. While we were successful at classifying items in the images the process was slow. To classify all ~300000 images would have required more time than was possible to complete the project. In a second attempt we used luminance but this failed to differentiate among the photos. Finally we used the mean and standard deviation of the red green and blue colors in the photos.The first step in extracting feature importance was standardizing names e.g. “hi rise” and “highrise” needed to be grouped together as a single term. This standardized list was then transformed to a count using termfrequencyinverse document frequency (tfidf). The result of this was 400 columns (for 400 terms) of tfidf values. However we had to reduce these 400 columns because with our limited resources our models would not run with so many predictors. To reduce these 400 columns in the final model we used logistic regression with the 400 columns as predictors of interest level to form three columns that were predicted probabilities of high medium and low interest. These 400 columns were then combined with the remaining predictors to fit models by.The description column was transformed using two methods. First was to separate standard terms in the description by ngram (a sequence of “n” contiguous words). The NRC library used to determine the ngrams. The most popular features included stainless steel appliances. The values were transformed using tfidf and categorized using an SVM. This model failed to differentiate the interest levels and was not used in other calculations. The second method used to transform the description column was sentiment analysis also based on the NRC Library and the tidytext R package. The sentiments used were positive negative anticipation fear anger trust surprise sadness disgust joy and anger. For each description the number of words corresponding to each sentiment was counted and added to each sentiment column. These columns were then used in future models.Another column that could have held significant value was the manager ID column. To use this column we followed a Kaggle Kernel that classified the managers based on interest level. These values significantly improved our predictions in the validation portion of the training set but significantly increased the logloss (reduced accuracy) for the test set. We believe the inclusion of the output interest level in the determining the values for the predictor resulted in leakage. The resulting models were overfit to the data.  These features were removed in subsequent models.Using a random forest model we calculated the relative importance of each variable (contribution to decreasing the Gini Index). The price variable as expected from the EDA was the most important variable while the location (latitude and longitude) followed. The number of characters in the description was also important and the hour and day the listing was created were also critical. The sentiment features tended to be the least important.To check for correlation between the variables we performed a Pearson correlation plot for the numerical variables. As can be seen in the chart below (strong blue is a higher positive correlation and stronger red is a more negative correlation) the photo variables have a significant correlation.The models chosen for analysis included logistic regression random forest gradient boosting and extreme gradient boosting. The use of increasing complexity was used to determine if simple models could predict the results and if increasingly complex models would capture different aspects of the data that could be combined. The logistic regression model used regularization including ridge and elastic net. As can be seen in the bar plot of the confusion matrix below logistic regression with elastic net failed to capture any of the highinterest values but performed well for the lowinterest properties.We used the Ranger implementation of the random forest algorithm due to its improved speed. This model using up to seven predictors for each decision tree also did not perform well.The gradient boosting method as implemented in the H2O Rpackage was also used. This model required some parameter tuning but changes in these values did not result in a significant increase in accuracy. The values are shown below. While more high and medium values are predicted correctly a significant number of low values were incorrectly predicted.XGBoost was implemented to due to its popularity and successful track record with previous Kaggle competitions. Although the model made decent predictions with its default settings. Tuning the model proved to be difficult due to their large number. To save time we experimented by manually tuning the learning rate and depth of trees until we minimized the error between the training and validation set. Like the other models XGBoost proved very accurate in predicting listings with lowinterest levels but did not perform as well in predicting highinterest listings.To improve these models by directly addressing the class imbalance we used upsampling and downsampling. These methods respectively increase the number of observations for the minority classes and decrease the number of majority classes by random sampling. The number of observations is equalized in the resulting datasets before model training and could result in better predictions. The predictions from up and downsampled data did not significantly improve most models. For logistic regression far more highinterest properties were correctly predicted but a larger number of lowinterest properties were incorrectly predicted. Final models only used the XGBoost method.The final prediction was created from an ensemble of weighted XGBoost models. A base model an upsampled model and a downsampled model. The base model categorized most of the lowinterest models correctly but predicted few of the highinterest models correctly. The upsampled was the reverse and the downsampled was intermediate. A weighted average somewhat improved the accuracy overall but was comparable on logloss to the base model.The lesson learned is that class imbalance and a lack of differentiating features resulted in difficulties in categorization. The use of upsampling tended to improve the correct prediction of highinterest properties but tended to be poor at predicting low and medium interest properties. While a significant amount of time was spent on feature engineering more time finding features that clearly differentiated between low medium and highinterest properties was critical to making the most accurate predictions. Model tuning stacking and ensembling did little to increase the accuracy of the predictions. For this competition it would have been wise to spend significantly more time on feature engineering and less time tuning models.,NA,Understanding Class Imbalance and Ensemble Modeling in the Two-Sigma Connect: Rental Listing Inquiries
https://nycdatascience.com/blog/student-works/billboard-hot-100-lyrics-1987-2017/,38,"The rankings are based on a formulaic approach not the subjective to the musical preferences of the individuals tasked with compiling the list. Airplay on roughly one thousand terrestrial radio stations are tracked to form the foundation of the ranking data. Nielsen provides song sales data for both digital and physical formats which are factored into the rankings. Most recently Billboard added music streaming data to be factored into the hot 100 chart rankings. My goal was to analyze the lyrics by year and find trends in the most popular words used . The Billboard Hot 100 chart data was scraped from  using a combination of BeautifulSoup and Regular Expressions.The twsift unofficial API for MetroLyrics was used to acquire the lyrics corresponding to each song entry in the Billboard Hot 100 charts. This API allows quick access to the lyrical content hosted by MetroLyrics with one major caveat  the song title and artist must be meticulously adjusted (removing non alphanumerical characters replacing spaces with '' and correctly identifying the title & artist) otherwise it wont return the correct lyrics.Click here for lyric scraping code Top 25 Words per Year 198820161989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016Wordcloud by year from 198720161987 19881989  1990 1991 1992 1993 1994 1995 1996 1997 19981999200020012002 2003  2004 20052006 2007 2008 2009 2010 2011 20122013 2014 2015 2016from os import pathfrom wordcloud import WordClouddef get_wordcloud_year(year): wordbag  words_by_year(year)words  remove_nonalphanum(wordbag) print 0 len(words)words  words.split() # Remove singlecharacter & 2character tokens (mostly punctuation) words  [word for word in words if len(word) > 2] print 1 len(words)# Remove numbers words  [word for word in words if not word.isdigit()] print 2 len(words)# Lowercase all words (default_stopwords are lowercase too) words  [word.lower() for word in words] print 3 len(words)#remove stopwords words  [word for word in words if word not in all_stopwords] print 4 len(words)#wordcloud  WordCloud().generate(words)# Display the generated image: # the matplotlib way: import matplotlib.pyplot as plt # plt.imshow(wordcloud) plt.axis(""off"")# lower max_font_size #wordcloud  WordCloud(max_font_size50).generate() (str(words)) plt.figure() #plt.imshow(wordcloud) plt.axis(""off"") #plt.show() print(len(words)) wordcloud  WordCloud(width  1000 height  750 font_path'/Library/Fonts/Verdana.ttf' relative_scaling  1.0 stopwords  all_stopwords ).generate(' '.join(words)) plt.figure(figsize(2012)) plt.imshow(wordcloud) plt.axis(""off"") plt.show()###Generates histogram of top 25 lyricsstopwords_file  './stopwords.txt'custom_stopwords  set(codecs.open(stopwords_file 'r' 'utf8').read().splitlines())all_stopwords  default_stopwords | custom_stopwordsdef get_wordfreq_df(year):wordbag  words_by_year(year).decode('utf8')#vocab.decode('utf8')#words_by_year(year) words  nltk.word_tokenize(wordbag) words  [word for word in words if len(word) > 2] words  [word for word in words if not word.isdigit()] words  [word.lower() for word in words] words  [word for word in words if word not in all_stopwords]fdist  nltk.FreqDist(words) d  Counter(fdist) word_df  pd.DataFrame.from_dict(d orient'index').reset_index() word_df  word_df.rename(columns{'index':'Word'0:'count'})df  pd.DataFrame(fdist.most_common(25)) df.columns  ['Words' 'Count'] df.sort_index(ascendingFalse).plot( kind'barh' x  'Words' title  ""Most Common Lyrics in: "" + year )",NA,Examining Billboard Hot 100 Lyrics from 1987 - 2016
https://nycdatascience.com/blog/student-works/individual-menu-items/,38,On the map there are markers colored by the price of the item Green for items less than $10 yellow less than $15 orange less than $20 and red for items that cost over $20The map also provides the number of search results as well as the minimum and maximum price,NA,New York City Menu Items - Web Scraping
https://nycdatascience.com/blog/student-works/ebates-web-portal-promotion-analysis/,38,"EBATES is an online coupon web portal website launched in 1998.  Sold in 2014 to Rakuten a Japanese shopping website for $1B EBATES offers consumers cash back who use the portal to find retail shopping discounts.To date EBATES has paid consumers $1B in cash back rewards since website inception.  The average cash back value on purchases made with retail partners is 5%.  EBATES has 6 million users over 2000 retail partners and continues to grow and acquire other coupon website portals.    Given EBATES' success as an online coupon aggregator do the discounts offered on the website impact purchase behavior or increase cash back to consumers who make more purchases with retailers?  This blog post will explore this theory using a web scraped dataset from EBATES and R code visualization analysis.EBATES partners with web retailers to offer discounts and cash rewards to online shopping consumers.  As an aggregator of retail offers EBATES requires consumer registration before using the website to capture useful consumer data.  Once registered EBATES consumers will search for their shopping destination click on a cash back offer and redirect to the retail partner website for online shopping.  Any purchases made with the retail partner is passed back to EBATES and calculated in the consumer's account as their cash back shopping reward.  Quarterly EBATES will send rebate checks or deposit cash into the consumers Pay Pal account.  EBATES generates revenue through paid commissions based on their web referrals to retail partners. The EBATES core consumer is female and college educated who shops online both at home and work.  Typically college educated females are employed therefore this specific demographic doesn't have a lot of time to go to brickandmortar stores to shop.  Instead they optto use EBATES to gain a cash back discounts on items they would likely purchase anyway.  The following chart illuminates the EBATES' core consumer:source: Alexa AmazonUsing scrapy and selenium as web scraping tools to capture data from the EBATES website the web spider crawled through a table of all current retailers to capture the following dataset: retailer name total coupons/discounts offered current coupon promotion and cash back since website inception.  Once that data was captured and cleansed additional variables were calculated as a function of % cash back and total retail purchases to enhance the data collected.  In total a full dataset was developed on 1500 retailers (2200 in total) with the balance maintaining secrecy around total cash back.  Without total cash back as a variable it was difficult to make any conclusions about a retailer or consumer behavior other than highlight the number of coupons offered since site inception.  The biggest retailer holding back proprietary cash back data is Amazon.  Prioritizing by total cash back to consumers the top retailers in the dataset follow in the table below.  Clearly these retailers are aligned with the core demographic described previously with Macy's being the top shopping destination through the EBATES web portal.Looking at a visualization of the top 50 retailers by total cash back and total purchase amount the visualization indicates there is a relationship between the two variables as demonstrated by the regression line in the chart below  However one would assume there would be a strong relationship between these two variables anyway; thus you cannot necessarily derive a meaningful conclusion other than when sales increase more cash paid back is paid to consumers through these top retailers. Taking the analysis to the next level the relationship between total coupons and total cash back was reviewed and visualized.  In theory if the retailer offered more coupons then there should be more sales and ultimately cash back paid to the consumer.
However based on the visualization below there is a relationship between the two variables however it is a weak one based on the slope of the regression line.  In some cases adding coupons or discounts help generate cash back to consumers but isn't a strong indicator that this type of promotion will drive business to one retailer or another.Looking further into the dataset the same analysis was generated on retailers at the mean of total cash back.  Two hundred observations were included +/ from the mean.As the visualization indicates below and similar to the top 50 retailers visualization there isn't a strong relationship between total coupons offered versus cash back paid to consumers.Based on the data collected and derived there were no strong relationships found based on coupon promotion and total sales when graphed against cash back paid to consumers.  However one point does stand out which is difficult to quantify and that relates to brand.  With strong established retail partners EBATES has been able to attract and grow into one of the largest coupon sites on the web and is ranked as the 383rd most visited site. More data and time would yield actionable promotional results for EBATES.  Such analyses include: analyzing segmentation by retail category length of retail relationship with the site consumer profiling promotional data performance on the home page and A/B testing of offer promotion position and creative.Further to this list there are many more analyses to be reviewed to truly optimize the promotional value of the site and drive incremental revenue for partners and EBATES as a whole.Following is the scrapy and selenium spider utilized to capture the relevant data from the EBATES website included in this analysis.",NA,EBATES: Web Portal and Promotion Analysis
https://nycdatascience.com/blog/student-works/web-scraping/analyzing-zoc-doc-doctors/,38,Internet startups have continuously replaced the herculean task of picking up the phone and calling an actual human with a few taps on the phone. Instead of calling a cab you tap a button and one arrives. Instead of calling for late night pizza tap a few times and the order is on its way. Founded in 2007 ZocDoc allows patients to avoid the horror of a phone call and book their doctors appointment with a few taps on their phone.As a lazy millennial I’ve always appreciated ZocDoc’s convenience and use it for the majority of my medical booking. ZocDoc allows me to book doctors based on other patient’s reviews easily reschedule or cancel and fill out forms beforehand by my appointment.Curious about the size of their business the best doctors and basically everything about their service I set to work web scraping every single doctor on their site. After spending 95 hours scraping approximately 1.2 million doctors on their site I discovered that most of doctors on their site are placeholders. See for example . ZocDoc has populated its site with placeholder profiles via data from the American Board Of Medical Specialties. After filtering out these placeholders there are 47363 doctors on their site. Let's dig in. Following the logic used  given about 47k doctors on their site and ZocDoc's annual fee of (as of 2016)   $3000/year annual fee from doctors that use its service we can roughly estimate their yearly revenue to be about 141 million/year. In 2015 in a Series D funding round  $130 million at a $1.8 billion valuation. Thus following the 2015 valuation they were valued in 2015 at 14x of their current revenue. ZocDoc was started and is headquartered in New York City.  So not surprisingly New York state is the home to the most ZocDoc doctors with 13053 followed by Texas with 4569 and California with 4454. The top five states for ZocDoc doctors (New York Texas California Florida and New Jersey) account for 60% of the doctors on ZocDoc. New York city  Brooklyn and the Bronx account for 15% of the doctors on Zocdoc. Other major cities include Chicago Houston and Washington D.C.Doctors By CityOn the night of Feb 11 I collected data on doctor availability for the next fortyfive days  (which includes 32 weekdays) of all the doctors on ZocDoc. The median ZocDoc doctor has 107 appointment slots available in the next 45 days or about 2.3 a weekday. By comparing the first names of doctors against the most common U.S. baby names from 1951 to 1992 (25 yr old to 65 yr old a rough range for when doctors were born) and filtering for first names with at least 20 doctors on ZocDoc  we see names that are more common with doctors vs  the general U.S. born population.  The top five names are Russian: Dmitry Inna Yelena Alla and Igor. Perhaps not popular baby names during the Cold War.,NA,What I Learned From Scraping Every Single ZocDoc Doctor
https://nycdatascience.com/blog/student-works/r-visualization/hut-hut-hut-scrape-scraping-35-years-college-football-players-statistics/,38,CFB QB Findings: ·High but not record high College QB Ratings led to the most successful NFL QBs Every QB·Drafted from 1985 to 2007 with a QB Rating of 150 or higher started 5 or more years in the NFL.·No strong correlation between College Passing Yards and NFL Success·Most Successful QBs averaged between 7 and 9.5 Yard per Attempt·Most Successful College QBs threw 7090 TDs in college·No strong correlation between College Interceptions and Success·No correlation between College Rushing Yards and Success but poor Avg. Yards Per Rush does correlate with poor NFL success CFB RB/FB Findings:·College RB/FB ended up at 8 different positions when they got to the NFL·>750 Rushing Attempts in College correlates with poor NFL careers or over 4000 rushing yards·Most Successful RBs/FBs average approximately 5 yards per carry·Rushing TDs do not correlate with NFL success·No correlation between receptions and success·Negative correlation between NFL Success and Receiving TDs CFB WR/TE Findings:·College WR/TE ended up at 9 different positions when they got to the NFL·Most successful WRs/TEs played 2040 games in college·Most successful WRs/TEs in the NFL had less than 100 total receptions·Most successful WR/TEs had <1250 total receiving yards in college·Most successful WR/TEs avg. 1020 yards per catch·Most successful WR/TEs had <10 receiving TDs in college·No correlation between scrimmage yards/plays and NFL success,NA,"Hut, Hut, Hut, Scrape! Scraping 35 years of College Football Player Statistics"
https://nycdatascience.com/blog/student-works/r-visualization/cleantech-news-scraping-analysis-online-articles/,38,continues to to advance with support from technological innovation sustainability projects financial incentives and political programs. Given the field’s large scope there is no shortage of media outlets covering the action. With the goal of tracking and analyzing recent cleantech news I developed a web scraping framework using Python’s  conducted natural language processing on the scraped data with Python’s  and visualized the results using R’s . is a leading provider of online media and research in the cleantech world with an indepth focus on renewables energy efficiency energy storage grid modernization green financing and environmental policy. I scraped 100+ online articles from the previous 3 months for the following information: is an annual ranking of the top 100 upandcoming companies in the cleantech community “most likely to have a big commercial impact in a 510 year timeframe.” The 2017 rankings were scraped from Cleantech 100’s website and the company names were used as anchors for text recognition in the Greentech Media articles.Python’s  framework is a fast and flexible web scraping method based upon the use of “spiders” (scripts with html parsing instructions) to gather online information and store it for further use. In this instance I used a specific  class called “crawl spider” created by defining start URLs a set of rules to inform the spider which links on the start page to follow and set of instructions (specific xpath references) on how to parse extract and save fields from the html at the destination page.Let’s take a look at the spider used to gather information from Greentech Media’s articles.With the scraped data stored locally I conducted text processing on the Greentech Media articles through the use of regular expressions word/phrase frequencies and sentiment analyzers from the Python framework. The following questions were explored:As illustrated below electric vehicles policy and energy storage are covered by 10 or more articles collectively making up about a third of all articles published on Greentech Media in recent months. Interestingly politics are usually of secondary focus on the website. Perhaps more policy coverage has been warranted lately given the current political climate and uncertainty around the Trump administration’s effect on environmental issues.Greentech Media’s readers have the ability to submit their own two cents on a newsfeedlike comments section. We can observe from the histogram below that although many articles go uncommented (far left bin of each facet) a healthy number of articles elicit anywhere from 10100 comments with some generating more than 150. That may be due to the fact that certain polarizing themes spark more comments than a more objective or nonpolitical topic. For example most articles tagged with “energy storage” are uncommented; most articles tagged with “donald trump” show ~60 comments.To get a pulse on the current “players” in the cleantech community I conducted text searching for groups of proper nouns in all the articles. Lists of cleantech countries people and companies were generated adhoc off of background knowledge and a list of cleantech startups as described earlier was scraped from the Cleantech 100 online website.Regular expressions were used to identify total word counts for each proper noun. Additionally using  sentences from all articles were tokenized and analyzed for the relative polarity and subjectivity of that sentence.  gives each sentence a numeric polarity score ranging from 1 to 1 with a value of 1 indicating highly negative content and a value of 1 indicating highly positive content (0 being neutral); it also assigns each sentence a subjectivity score from 0 (completely objective) to 1 (completely subjective). An average for each of these scores was calculated across all sentences containing at least one mention of the given noun.,NA,Cleantech in the News: Scraping and Analysis of Online Articles
https://nycdatascience.com/blog/student-works/web-scraping/year-2016-songs-germanys-popular-radio-station/,39,looking for a unique and interesting subject matter selectsubmit button?hour&date24 * 366  8.784LastName FirstNameLastNameFirstName,NA,2016 in songs on Germany's most popular radio station
https://nycdatascience.com/blog/student-works/television-trends-social-indicator/,39,"Links:      |   There are various indicators in disciplines such as economics and politics that measure the state of different aspects of their fields. That is why—when events around the country in the past few years have caused people to question the state of the US and how surprised they are about ""who this country is""—I am surprised there is no indicator that can tell us who we are and where we are going socially as a country; there are a collection of indicators that describe the social environment in terms of such things as poverty obesity and suicide rates but these largely describe outcomes and consequences rather than preferences and personality.Spoiler Alert! A full solution to such a complicated task is beyond the scope of this project; a full solution would require multiple scraping projects and continued feedback from professionals in social psychology. I will address this again in the next steps section. Instead I used this time to take a first step in building a social indicator by scraping and visualizing information about television shows.I used  and  to gather television data from Wikipedia and IMDb respectively. There was some information I could only get from Wikipedia and some only from IMDb.While show titles could be found in both I needed to scrape them off of Wikipedia in order to  For fields common to both Wikipedia and IMDb such as genre and start/end date I still retrieved their information from Wikipedia; Once the scraping was finished I filled in any missing data by collecting the same information from IMDb along with IMDb rating and number of votes. In  I have visualizations onThis information is displayed for each year from the 1940s until 2016 by genre and by network.   What we can get out of the genre plots is that the networks and show creators believe that audiences want more comedies and reality shows (shows that tend to require less thinking). Dramas have not spiked up as much. While the shows created in these genres have been on a consistent rise the number of shows created by the major networks has been on a decline since the mid1980s. I will need to look into this further.TV show data alone is not enough to answer ""who are we as a society?"" especially without viewership data. Some future steps I would take to build upon this project are:",NA,Television Trends as a Social Indicator
https://nycdatascience.com/blog/student-works/recommendation-system-spam-review-detection/,39,I. IntroductionII. Web Scraping on Google App StoreIII. Recommendation SystemIV. Spam Review AnalysisV. SummaryWhat's More:References:,NA,Recommendation System and Spam Review Analysis
https://nycdatascience.com/blog/student-works/pathway-hope-salvation-armys-new-approach-towards-breaking-cycle-poverty/,40," Poverty is not an affliction of thImportantly this increase reflects a radical shift in the face of poverty over the last 50 years. In that time .the United States has experienced a disproportional increase in both single parent households and children who reside in poverty.For over 150 years The Salvation Army has sought to combat hunger and meet the need for those in poverty. The Pathway of Hope initiative was introduced to Eastern Territory of The Salvation Army beginning in 2016 and seeks to be a real solution to help families break out of the perpetual cycle of intergenerational poverty.Pathway of Hope is targeted and intensive case management to assist families striving to break free from intergenerational poverty. The Salvation Army forms a crucial partnership with families in need. Families participating in the program possess the desire to change their situation and are willing to share accountability with The Salvation Army for planned actions. Through achieving increased stability these families find a newfound hope propelling them forward on their journey to This analysis takes a deeper look at the initiative running in over 25 local communities within The Salvation Army’s Eastern Territory. The intake process individually evaluates a family in crisis and identifies custom and critical goals ranging from securing employment to finding affordable childcare.Poverty in the U.S. is an epidemic – including one in five children according to the latest Census figures. Children who live in poverty for half their childhood are 32 times more likely to remain in poverty (according to The Urban Institute). The Salvation Army works with many of these families by addressing immediate needs. Pathway of Hope is the next step for  to help them break the cycle of poverty.After a series of conversations with individuals at the Salvation Army's Eastern Territorial Headquarters I received data sets relating to the Pathway of Hope (P.O.H.) with the intention to analyze the current effectiveness of The Pathway of Hope program. Unfortunately the data currently collected by the Salvation Army is insufficient to quantify and rigorously answer this very important question ""Is the Pathway Of Hope effective?"" Upon further inspection the data shows two things:The data in this program is gathered from 26 separate locations with differing computer systems varying structure for inputting client data. Also in its current state covering 26 geographies the Pathway of Hope initiative has been running for only a little more than 1 year. These factors played a major role in the  consistency validity and integrity of the data collected but these hurdles were ultimately overcome by adjusting the scope of the analysis. Rather than attempt to analyze the effectiveness of POH as a whole I analyzed the distribution of Client Goals and their relationship to various factors such as race gender geography and household formation. str(Poh) 'data.frame': . of : : int 14 15 16 17 18 19 20 20 20 21 ... : Factor w/ 134 levels ""1/10/2017""""1/11/2016""..: 68 68 68 68 68 74 74 73 73 74 ... : Factor w/ 30 levels """"""1/12/2017""..: 14 14 14 14 14 18 18 18 1 18 ... : Factor w/ 26 levels ""POH EPA Carlisle(164)""..: 13 13 13 13 13 11 11 11 11 11 ... : Factor w/ 3 levels """"""No""""Yes"": 3 2 2 2 2 3 2 2 2 2 ... : Factor w/ 5 levels """"""Female Single Parent""..: 5 5 5 5 5 2 2 2 2 2 ... : Factor w/ 185 levels """"""1001""""1013""..: 118 118 118 118 118 161 161 62 61 161 ... : Factor w/ 490 levels """"""1/1/1971""..: 333 372 431 401 123 19 442 442 442 427 ... : Factor w/ 9 levels """"""American Indian or Alaska Native (HUD)""..: 9 2 2 2 2 4 4 4 4 4 ... : Factor w/ 3 levels """"""Female""""Male"": 3 2 3 3 3 2 3 3 3 2 ... : Factor w/ 6 levels """"""Client doesn't know (HUD)""..: 6 5 5 5 5 6 6 6 6 6 ...: Factor w/ 11 levels """"""Data not collected (HUD)""..: 1 1 1 1 1 1 6 6 6 6 ... : Factor w/ 70 levels """"""0""""1000""..: 13 1 1 1 1 62 1 1 1 1 ... : Factor w/ 9 levels """"""No exit interview completed (HUD)""..: 4 4 4 4 4 5 5 5 1 5 ... : Factor w/ 6 levels """"""Black or African American (HUD)""..: 6 5 5 5 5 2 2 2 2 2 ...Upon deeper inspection of the Poh data it became apparent that the data was filled with blank incorrect and duplicate values. Of the initial 665 clients 102 clients were duplicated in the data set between 25 times and among duplicate entries the Household.Type Entry/Exit Dates Race and Head of Household entries varied dramatically. This left me with the task of filtering out duplicate entries and trying to determine which entries were a result of the case worker having difficulties inputting information into the database and duplicating the entry or creating a new entry to be used as a placeholder in the database system. Due to the limited amount of unique data for the child clients I focused this analysis on the clients with the attribute Head of the Household  ""Yes"" which corresponds to analyzing the clients on a per family basis. The result is a narrow narrow pool of 148 clients in this analysis hardly sufficient to determine the success of the program but useful to determine areas of need and perhaps influence the deployment of services by the Salvation Army.256713740str(GC)'data.frame': . of : : int 14 14 14 14 14 14 14 14 14 14 ... : int 58 58 58 58 58 58 58 58 58 58 ... : int 4 9 14 19 24 39 51 56 57 67 ...: Factor w/ 17 levels ""Case Notes""""Child Care""..: 8 9 5 7 10 7 16 6 13 8 ... : Factor w/ 89 levels ""Access legal aid""..: 18 4 21 72 54 62 43 34 15 18 ... : Factor w/ 183 levels ""1/11/16""""1/13/16""..: 96 96 96 96 96 143 143 135 135 138 ... : Factor w/ 3 levels ""Closed""""Identified""..: 1 1 1 1 1 1 1 1 1 1 ... : Factor w/ 5 levels """"""Abandoned""..: 3 3 3 3 3 3 3 2 2 5 ... Goals by locationPlease note the top charGoals By Household FormationGoals by GenderGoals by RaceAnalysis of Goal Classification EconomicEducationHousingHealthEmploymentHouseholdNecessitiesLegalOtherChild Care.As a result of removing duplicate and erroneous data the resulting pool of data is unfortunately too small to draw conclusions to apply to the entire population of American families in the cycle of poverty. However I believe that the results of this analysis can be useful in assisting the Salvation Army in fundraising and optimizing the deployment of their assortment of services.  Upon sharing preliminary results with the Salvation Army I was informed that my influence contributed to overhauling the data collection methods and policies for the Pathway of Hope initiative in addition to updating their database infrastructure. In the coming months I plan to continue my partnership with the Salvation Army to add additional data as it is gathered and hopefully deliver useful analysis to help further the mission of .",NA,Pathway of Hope - The Salvation Army's solution to break the cycle of poverty
https://nycdatascience.com/blog/student-works/analyzing-chipotle-pricing-food-shortages/,40,Chipotle has two ongoing food shortages: pork in the United States and chicken in Canada. Some Chipotle locations serve pork from the UK that doesn't meet all of their food standards. This message is displayed when a user visits the page:For simplicity when focusing on different prices we will just look at steak burritos. For the most part menu items prices change together so a model for steak burritos should match other burrito prices.There is a clear regional differences in price especially in New York.Prices also vary based on housing price population and other factors. ,NA,"Analyzing Chipotle Pricing, Food Shortages, and More"
https://nycdatascience.com/blog/student-works/lost-years-exploring-disparities-life-expectancy-u-s/,40,Among the policy proposals that the new administration and Congress are likely to consider are changes affecting Social Security and Medicare. It is often argued that Americans are living longer and raising the eligibility age for Social Security and Medicare is a logical necessary and fair. But does the U.S. today have a consistent higher life expectancy regardless of income geography or gender?This question is worth examining not only because of the possible changes to retirement benefits but also because it can help to uncover social differences worthy of additional inquiry. Data compiled by the Health Inequality Project and  provides an ideal starting point for this inquiry.Plotting female and male life expectancy by household income reveals that life expectancy rises sharply with income then tapers off. The gap between female and male life expectancies is also clear and persistent and rises as income levels grow.As with other life expectancy figures discussed here the figures in this graph are raceadjusted with differences in life expectancy associated with race or ethnicity which warrant additional research having been removed.With incomes concentrated near the low end of the graph there is sparse data available above about $250000 in household income. A logarithmic scale for income may be more revealing.The next graph updates the previous one by transforming household income on a log scale.For the bulk of observations which are found between roughly $20000 and $200000 it appears that a linear model could approximate the relationship well. However the slope would be steeper for men than for women. Note that the slopes are lower at the lowest incomes and especially at the highest incomes compared with the middle income range. It appears that statements about a rising U.S. life expectancy require greater nuance given the wide range of life expectancies and their relationship with income levels and gender.If life expectancy varies by household income level and gender at the national level are these patterns consistent from state to state or are they more pronounced in some states or regions? One way to evaluate this is a graph that compares the life expectancy of people in the bottom quartile of household income with those in the top quartile of income in the same state. To isolate the influence of gender two graphs can be used to illustrate the patterns for women and men separately and they can then be compared.In the graph of women’s life expectancy by state below the points for every state would fall on the diagonal line if women in the lowest income quartile had the same life expectancy (horizontal axis) as women in the highest income quartile (vertical axis). In no state is this close to reality. Hawaii serves as an example below and is indicated by an annotation. As the note on the graph explains the vertical distance from the diagonal line to the point above it represents 4.3 years of life expectancy that women in the lowest income quartile lose compared with their highincome peers in the state.There are considerable differences among states in the life expectancies of women of both income groups and some variation in the number of years lost by women in the lower income group.Regional differences though not clearcut may also be present. States in the Northeast shown in red are clustered mostly on the right due to higher life expectancies for lower income women than most other states. In addition states in the Midwest indicated in blue appear to be a bit farther from the diagonal line than other states meaning that the disparity in female life expectancy by income seems to be greater there. Further research would be needed to tell whether other groupings of states such as according to shared policies or leading industries rather than geography would be more meaningful.Male life expectancy by the state for first and fourth quartiles of household income reveals an even larger gap. The dashed line that connects the point representing Indiana to the diagonal line means that lower income men in that state have a life expectancy that is 9.7 years less than their highincome counterparts. There still appear to be some regional patterns such as higher life expectancies for men with first quartile household income in most Northeastern states but they seem to be weaker than for women.The number of years of life expectancy that women lose out on if they are in the first quartile of household income instead of the fourth can also be seen on a map. The closer states are to blue in this graph the smaller the gap in life expectancy between women of the first and fourth income quartiles. Clearly states such as California and New York have significantly smaller disparities than others such as Kansas.Similar patterns can be seen for men. However note that the scale is different because the disparity is much higher overall.Again California and New York appear to have smaller gaps but their gaps for men are comparable to the largest gaps of any state for women.For states such as Wyoming and Indiana lower income men lose almost a decade of life compared with higher income men.  Over the period from 2001 to 2014 female life expectancy has been rising. This is the case at both the 25th and 75th percentile of household income. However there remains a large gap in life expectancy between the lower and upper income groups depicted as the shaded area between the lines that is persistent if not growing.Some additional research into the reversal that appears in 2004 would be appropriate. Life expectancy figures around 86 years are found at percentiles just above and just below 75th percentile. It is not clear whether the anomaly is an erroneous entry or has another explanation.For male life expectancy the trend is similar with increases for both first and fourth income quartiles and a large gap remaining. For men the gap appears to be widening due to the modest rise in life expectancy for lower income men.Overlaying the two previous graphs we can see that while the overall life expectancy is rising large disparities remain based on income and gender. The life expectancy of higher income men appears comparable to that of lower income women. At the same time there is a wide and growing gap between the life expectancies of lower income men and the other groups (higher income men as well as women of either income group).It is clear that a story that states that life expectancy has now reached a high level for all Americans is far too simple to serve as a basis for public policy. Differences in life expectancy by income level gender and state or region are pronounced. Further analysis is needed to inform the policymaking discourse.Other questions are also worth exploring. These include:,NA,The Lost Years: Exploring Disparities in Life Expectancy in the U.S.
https://nycdatascience.com/blog/student-works/first-term-presidential-referendum-congressional-midterm-elections-shiny-app-analysis/,40,“Midterm elections are regarded as a referendum on the sitting president's and/or  party's performance.” (source: Wikipedia “United States midterm elections” )Utilizing data visualization through a Shiny app this analysis reviews the composition of the US congress (Senate and House of Representatives) in the first year of a presidential term and two years after the presidential election at the midterm elections.  The analysis includes the congressional composition and seat changes over time by state and by party represented in a plot encompassing the United States as a whole. Each house of congress is segmented in its own plot.The analysis also visually summarizes the change in congressional seats by party at the midterm elections thereby indicating whether a president has gained or lost party strength in the congress  either in the senate house or both.The gain or loss in congressional party seats is the referendum on the sitting president. The midterm elections indicate how well the president is doing his job. Any changes in party strength based on congressional seat changes has a profound impact on the power and ability of a president’s performance. The ramifications are widespread affecting every American and the world too.The data set utilized for this analysis was the Kaggle “Party Strength by State” data set available at https://www.kaggle.com/kiwiphrases/partystrengthbystate. The data set begins with President Ronald Reagan’s first term in 1980 moving through present day.The data set also includes state level gubernatorial and state legislature data. This information was not utilized for the sake of this particular data visualization analysis. Independents were also included in the data set. However the observations of independents were insignificant and were disregarded with the primary focus of the analysis on the two main political parties in the United States: Republicans and Democrats.The data visualization from the Shiny app shows two primary graphical renderings by congressional house. The first group of renderings illustrates a map of the US showcasing the composition of the congress during presidential and midterm elections. Utilizing the primary widget the date of the data corresponding to a specific president of interest may be changed using the dropdown menu.With Reagan’s first term as the start of the data set the second graphical rendering to the right of each US map shows the changes in congressional seat by party. To render this graph click the second widget “Select Congressional Mid Term Election” to highlight seat changes by party at the midterms.Please note: the coloring of these plots is consistent with other political plots: red represents the Republican party; blue represents the Democrat party; and white represents an even split between the two parties where senators or representatives are 50/50 in their party affiliation by state. The white representation is much more prevalent in the Senate than the House of Representatives due to the smaller sample size by state e.g. two senators to numerous representatives by state population.Following is the data visualization of the Reaganera midterm election results:Analyzing the data through Shiny app visualizations over time President Ronald Reagan the only president since 1980 gained strength in his party after being in the White House for the first two years of his first term. Given his positive midterm referendum the change in party seats from democrat to republican indicates that the electorate felt comfortable with Reagan’s leadership providing more voting seats within his party in congress. To this day history looks back upon President Ronald Reagan as one of the greatest presidents of the 20 century.Reagan was the last president to enjoy such a positive midterm referendum in his first term. Since Reagan all other presidents have consistently lost seats in the midterms thus losing power and finding increasing difficulty in working with congress to legislate for the electorate.A further summary analysis derived through the Shiny app by president and party strength (+/ congressional seats at the midterms) can be visualized at this URL: The following chart summarizes the data visualized in the Shinny app by president and election.D  Democrats; R  Republicans; Independents were excluded from this analysis given insignificant observations.Based on the data visualized in the Shiny app and summarized in the chart above there are two keytakeaways:One fewer voters take a stance at the midterms whereas in main stream presidential elections there is greater voter turnout. Essentially the midterms are biased as the ‘motivated voter’ turns up to vote whereas others do not.  Second historically presidents lose ground with voters once they begin making policy. Simply put a president cannot support all of those campaign promises and the electorate lets him know by culling his power through congressional party seat changes at the midterms.Any data visualization analysis requires iteration to fully tell the story of ‘why'. In this case there are several data points that may be added to this analysis to make the data more robust and to take the analytical conclusions to another level. Those data iterations include:The following code represents the R code derived to create multiple data frames calculating the difference in congressional seats  core to this analysis. Other R code was utilized through the ui and server files to plot the geographical maps based on the primary data set.Calculating the congressional seat differences required the creation and mutation of multiple data frames. Once created the difference between the data frames was calculated and new integers were stored for analysis and graphical use. As a result new visualizations of the congressional seat changes by presidential midterm election were charted and rendered using googleVis and the Shiny app.The baseline code for these graphical renderings follows:,NA,First term Presidential Referendum through Congressional Midterm Elections: Shiny App Analysis
https://nycdatascience.com/blog/student-works/price-may-vary-us-hospital-charging-comparator/,40,In the State Overview tab users can select the year and DRG then the distribution maps and histograms will show the update result. The same data is also displayed as ordered bar charts with corresponding discharges. And the results are also listed in the table where users can filter to see charges of all DRGs in one state.The DRG Comparison displays boxplots of average charges and payments for selected DRGs in selected year. And the average charges and medicare payments seem to increase as complication increases for example intracranial hemorrhage w mcc charges twice as much as intracranial hemorrhage w/o mcc.Year trending of selected DRG in selected hospital is also included in this tab users are able to identify the changes in both hospital charges and Medicare payments in certain hospital.,NA,Your Price May Vary: US Hospital Charging Comparator
https://nycdatascience.com/blog/student-works/tracking-exercise-trends-nhanes/,40,The National Health and Nutrition Examination Survey (NHANES) is one of the foremost assessments of health statistics for children and adults in the United States. Sponsored by the Centers for Disease Control  combines interviews with physical examinations and laboratory tests for approximately 5000 Americans each year. Results are compiled anonymized and made publicly available at the program’s website on a rolling basis. These datasets are a major source of information for further studies ranging from simple national averages for physiological measurements (height or weight) to epidemiological trends for public health policy reform.The 20132014 NHANES publication is made accessible through a  application providing an interactive environment for users to explore data trends. Because of the wide range of information gathered by the survey the app focuses on a subset of the findings drawing particular attention to the prevalence of exercise and associated health outcomes across demographic groups.The app is meant to be used as a starting point to explore highlevel trends in the latest NHANES survey publication. It is recommended however that the program's  be followed in order to draw any robust statistical conclusions from the underlying sample data.The “Overview & Demographics” tab allows users to explore this question. NHANES appears to be a nationally representative sample of the U.S. population across gender education level and income. However certain subpopulations are more heavily represented such as youths under 18 still undergoing much of their physiological development and at therefore at higher focus for many public health concerns.The “Explore Exercise Trends” tab provides exploratory visualization of who is getting exercise in what form and amount and the possible health outcomes associated. Density curves help to illustrate exercise trends and confirm preconceptions about some of the grouping (i.e. demographic) variables involved in those trends.Below we observe that with each drop in level of education – from college/advanced graduate down to high school dropout – the density curve’s average moves from left to right. This implies a direct inverse relationship between minutes per day conducting “vigorous or moderate” physical activity at the workplace i.e. manual labor and education.Minutes per day of physical activity – whether in the workplace recreationally; vigorous moderate or otherwise – are used as the predictor variable in a scatterplot to explore correlations with a variety of health outcomes such as weight BMI cholesterol or blood pressure.Above we observe a clear negative correlation between resting pulse (bpm) and average minutes per day of vigorous recreational activity. Through the use of the interactive graphing package  we gain further information from mouseover tooltips in this visual: among participants getting an average 180 minutes of exercise per day none had a resting pulse of over 98 bpm.Other relationships explored such as cholesterol levels versus exercise habits highlight the limitations of taking a bivariate approach in predicting health outcomes. For example we might assume a negative correlation between amount of exercise and LDL cholesterol levels but in the case below we observe this correlation to actually be positive. This suggests the study could benefit from other known associations to cholesterol such as diet (perhaps individuals with moderateexercise jobs tend to have highercholesterol diets) and genetic disposition.As alluded to in the previous section inclusion of more variables from the original NHANES dataset and the use of formal statistical methods could help to tell a more comprehensive story about the program data. Chief among these would be an exploration of dietary/nutritional and lifestyle habits (e.g. smoking drug use) since NHANES has collected that data for each participant. As NHANES is an ongoing and continuously developing program there is further potential to track the movement of these health trends over time in the general U.S. population.,NA,Tracking Exercise Trends with NHANES
https://nycdatascience.com/blog/student-works/case-missing-offense/,41,Links:      |    contains a wide range of MLB data including data on batting pitching and fielding. There are teamwide and individual playerlevel data for the regular season and playoffs and the latest database as of this writing has data ranging from 1871 to 2015.For the dashboard I used the  file which has team summary data by year including summarized batting and pitching statistics. I used only the latest decade (2005 to 2015) of available data because for the purposes of the dashboard I did not think it was necessary to look further back in time unless I could not find trends in the last decade.:* What this tells us is that on average from 20052015 playoff teams are scoring 69.338 more runs per year than nonplayoff teams. Not only are they scoring  runs they are scoring more runs as you can see here in the difference between the blue and red bars:Similarly to Runs Scored  and  topped the batting statistics while  and  topped the pitching statistics favoring playoff teams.Okay but this is expected right? Shouldn’t the teams that can score more runs and get more hits win? Not so fast!Examining these correlation matrices of hitting statistics (all insignificant correlations are set to 0)……it appears that:Not only can the previous correlation observations be useful to keep in mind when acquiring players they can also be useful in setting rosters and lineups. However the leaguewide trends in the MLB caught my eye more than the differences between teams of varying playoff status.I saw that runs and hits were down:…so I thought “Okay it probably has a lot to do with the performance enhancing drug crackdown”. Then I saw the home run numbers and it looks like home runs are not significantly trending one way or another. They have their ups and downs. 2014 just happened to be a low year. Finally I saw the strikeouts:and walks numbers:,NA,The Case of the Missing Offense
https://nycdatascience.com/blog/student-works/r-shiny/mapping-nyc-common-core/,41,"To construct my app I combined four data sources.  I downloaded NYC public school test scores from   I overlaid these results onto a map of NYC's official 2010 census tracts using a shapefile downloaded from the .  I found income data on these tracts from the official US Census Burea's .  Last I found the addresses of all public schools on the  then acquired their geographic coordinates by querying Google Maps' geocoding API.  I combined this material together in R and built the app in Shiny primarily using the Leaflet package.  The relevant code and data are .Let's take a look at the city as a whole first.  Here is the city map displaying test results on the math exam from 2016.  Each dot represents one school with the green end of the color spectrum representing a higher percentage of students who scored at the ""proficient"" levels (i.e. scored at Level 3 or 4) and the red end representing lower percentages.While the correlation between income and school performance is substantial it is not allencompassing.  If we filter census tracts by income quintile we can see plenty of schools that diverge from the general trend in their region.  Let's just look at the lowest income quintile.",NA,Mapping NYC Common Core Scores
https://nycdatascience.com/blog/student-works/hubway-bike-share-ridership-patterns/,41,Boston’s bike share program  has quickly become an enojoyable convenient and affordable means to navigate the city. Publicly owned by municipalities in Greater Boston and operated by national bike organization Motivate Hubway’s increasing popularity is consistent with national trends of greater attention to sustainable transport in metro areas.Environmental monetary and convenience benefits aside most bike share programs have embraced the influx of big data collected by their bike/station technology. Hubway in particular uses the plentiful ridership data for its own business insights and posts the raw datasets publicly for any aspiring data scientist to investigate. The purpose of this project was to explore general ridership and demographic patterns in Hubway’s historic data.  the  dataset contains information on over 1M bike trips spanning from the program’s inception in 2011 to mid2013. Due to long processing times this initial dataset was subsetted to include only the summer of 2012 (JunSept) which still represented ~300000 entries. Information about the variables collected (trip duration start/end station rider information) can be found at the site link above.  Each observation (trip) in the dataset above features the rider’s registered zipcode. A reference table mapping those numeric zipcodes to their corresponding neighborhood/towns was compiled using the following public sources:   R package .  public daily weather data for Greater Boston was accessed and tabularized from .Important features provided in the Trip History dataset are the timestamps (minutelevel granularity) of departure and arrival for each Hubway trip taken. These variables allow for analysis of ridership activity from the perspective of our normal temporal routines  commuting socializing weekend excursions etc.Density plots help to visualize hourly ridership throughout the course of an average weekday or weekend. Here we see an obvious “M” shape on the weekday density curve very likely due to commuting activity (roughly including weekday hours from 79AM and 47PM). Further analysis reveals that these commuting rides accounted for . Average weekend profiles exhibit an opposite daily profile with peak ridership occurring in the afternoons. A small but noticeable amount of weekend rides occur from 12AM3AM catering to more ‘adventurous’ bikerbargoers.,NA,Hubway Bike Share: Ridership Patterns
https://nycdatascience.com/blog/student-works/hedge-fund-machine-learning-challenge/,42,"Most stock market data is not publicly available even though individuals could have access to more market data through Yahoo Finance than ever before. Numerai is the first interface between machine learning intelligence and global capital which manages an institutional grade long/short global equity strategy for the investors in hedge fund transforms and regularizes financial data into machine learning problems for global network of data scientists. People do not need financial domain knowledge for machine learning model development. Numerai has an updated open data source which provides high quality encrypted stock market data for developing machine learning models.The data is clean and tidy and you could apply whatever methods you would like to apply.Firstly let’s take a look at what the data looks like which has been used for competition between Dec 14 Dec 21 2016 (21 features 1 target for prediction 136573 observations for training 21 features 13518 observations for testing). The data has already been scaled between 0 and 1.The model performance for measurement is logloss. Logloss is suitable for measuring the probability of a binary outcome. It considers the confidence of the prediction when assessing how to penalize incorrect classification. For example when you have a binary classification problem a prediction outcome of 0.99 has a more confidence level compared with the outcome of 0.59 through logloss measurement but you could only classify them as one outcome if you set a 0.5 threshold.Firstly we checked the distribution of the training dataset by using barplot boxplot and violin plot as shown in figures from plots we could see the data is evenly distributed and no significant difference among features and we could not extract a lot of information from those plots.Secondly we checked correlations among all the features it is found that more than half of the features are highly correlated and we could do some feature importance analysis to decide whether we could do dimension reduction or expansion.So for this project we have two plans for developing machine learning models for Numerai projects “Less” approach and “More” approach.In the “Less” approach lasso regression random forest have been adopted for feature exploration logistic regression random forest and XGBoost have been adopted for model training and development. In the lasso model for feature deduction the lambda is set as 1e3 and the result shown in the figure is that feature 4610131819 20 21 are significant and should be kept as important features. While in the random forest model results feature 6 20 13 21 10 2 7 9 5 1214 8 11 16 15 17 1 9 4 18 3 are significant which is slightly different with the result got from lasso. Anyway feature 4 6 10 13 18 21 are proved to be important features by both models. Due to the different results shown above and it is difficult to decide whether we should only keep some important features for modeling all features have been kept for the initial model development.
Then we tried random forest algorithm with 300 500 800 trees and crossvalidation the result we got is 0.69501. Finally we tried XGBoost which is famous for machine learning competition. We implement grid search for parameter optimization as shown in the table the process is shown in figure. The best combination of parameters is 0.6 for colsample by tree 0.8 for subsample 0.1 for learning rate 50 for number of estimators and 2 for depth. We put the grid search results into a XGBoost model the results shown in the leaderboard is 0. 69028. Based on the results of these models it is found that logistic regression has the best fitting of the model and the prediction model could be improved more with the more feature engineering work.Logistic regression has been selected for model training since it is easy to implement efficient to try so we applied it to get a quick check about the prediction performance. With the cross validation of the training dataset we got a logloss with a value of 0.68910 on the leaderboard.In this part Python scikit learn has been used for model development since it includes efficient supervised and unsupervised machine learning algorithms. After the cross validation the lambda was found close to 0 which seems that there is no need to add penalty term. If we change the cost function what will happen? Two more  kinds of cost function were utilized to tune the lambda the left graph show the tune with logloss function and the right graph utilize the class accuracy. From these two graph we found the lambda close to 0 too which result is very abnormal in logistic regression model.",NA,Machine Learning Application in Hedge Fund
https://nycdatascience.com/blog/student-works/machine-learning/machine-learning-retail-bank-marketing-data/,42,"IntroductionThe DataData Visualization and PreprocessingThe Models and Model OptimizationsThe ResultsThe algorithms used in the study consist of Logistic Regression Radom Forest Gradient Boosting Support Vector Classifier and Neural Network. Without oversampling and parameter optimization all algorithms show around 90% accuracy overall but about 20% sensitivity on fitting and prediction of custmer signups.  After rebalancing the training data and grid search cross validation all the algorithms except the Support Vector Classifier showed improvement on balanced sensitivities and the top two improvements were from Gradient Boosting and Neural Network. The following outputs are then generated from Logistic Regression Gradient Boosting and Neural Network. Here Logistic Regression  is served as a baseline other algorithm to be compared with.
:  :                                                         :                                            :
          precision recall f1score  support              precision recall f1score  support           precision recall f1score  support
 class 0      0.91         0.95     29229     class 0      0.95         0.89    29229     class 0   0.96         0.60     29229 
 class 1      0.67         0.35       3721     class 1      0.34         0.45       3721     class 1   0.16         0.27       3721  
avg/total   0.88   0.90      0.88     32950    avg/total    0.88  0.82       0.84    32950  avg/total   0.87   0.49      0.57     32950      :                                                           :                                               :  
           precision recall f1score  support             precision recall f1score  support           precision recall f1score  support
 class 0      0.91          0.95        7319     class 0    0.95        0.89       7319     class 0   0.96           0.60       7319
 class 1      0.66          0.35         919      class 1    0.34        0.45         919     class 1    0.16           0.26         919
avg/total    0.88   0.90       0.88       8238   avg/total   0.88    0.82     0.84       8238  avg/total   0.87    0.48       0.56       8238
Conclusion",NA,Machine Learning on Bank Marketing Data
https://nycdatascience.com/blog/student-works/18037/,42,In order to support the clients for range of financial decisions Santander Bank offers their customers personalized product recommendations time to time. Under current system not all the customers received the right product recommendations for them. To better meet the individual's needs and ensure their satisfactionthis challenge seeks to improve the recommendation system by predicting which products their existing customers will use in the next month based on their past behavior. Having a precise and strong recommendation system the sales of the bank can be maximized. At the same time the right products can also help the customers utilized their financial plan. The size of training data set is about 2.3 GB which has 13647409 observations. The test data set has 929615 observation. From column 1 to column 24 are the input features which contain 21 categorical features and 3 continuous features. The input features contain customers’ demographic and status with the bank information. On top of this the observations are in the time series format. The data contains each customer’s information from January 2015 to May 2016.From column 25 to column 48 are the output features which contains the product purchased information according to each customer from January 2015 to May 2016. Each column stands for one product and there are 24 products in total. The final purpose is to make a prediction on which products customers are going to purchase in June 2016. In this case the prediction is going to be multiclassifier. To measure the result the competition is using Mean Average Precision @ 7. From the formula where |U| is the number of the users in two time points P(k) is the precision at cutoff k n is the number of predicted products and m is the number of added products for the given user at that time point. @7 here means the evaluation only take the top 7 products into account no matter how many products are in the prediction. At the same time if the customer does not purchase any product the precision is also defined to be 0. In order to manage and implement a great model on this complex data within two and half week as a team the team were following the steps showing below. By doing numeric EDA we dicovered that there were 24 features contain missing value in the data set. Besides having missing value by columns the data also had missing observations. Missing observations means that some of the customers missing data for certain months in between the overall time range. After deeper investigation there were 5 features being dropped before imputation due to over 95 % of missing value and repetitive information of other features. There were about 4 kinds of imputation strategy implement for this data set. For the features in the ‘Unknown’ column the missing values were all labeled as ‘unknown’. The reason is that the features in this column are more customer’s demographic information. Therefore in order not to make any assumption labeling ‘unknown’ was the only way. For the features in the Common Type column the median of each features were imputed for the missing values because those were the features that described the relationship between the bank or continued variable. The features in the others were imputed by couple different methods. Beside the ‘age’ feature the missing values of rest of the features were fill in based on treating those observation as new customers. From the EDA we were discovered that those missing values were the same observations. And within those observations ‘all the account activities were under 6 months which were also the bench mark for being a new customers. For the ‘age’ features the after scaled mean were using for imputation in order to avoid some skewness in the data. Last but not least there were two kinds of products having missing values. Due to the evaluation penalized the false negative we would like to assume that the products havent been purchased yet. At first we would like to take a look at how the product owned related to customer’s demographic information at May 2016. We could see that no matter which segments of the customers the ‘current cash account’ was the dominated product among all. Since the data set was in a time series format it was important to look at the trend of the numbers of the customers. As the graph indicated there were a big amount of new customers appear in July 2015 and keep growing for a bit for 4 to 5 months.  When we look at how many products does each customer own in May 2016 we discovered that there were customers do not own product anymore. Also most of the customers own 1 to 2 products. The following graphs show that if the customers own 1 2 or 3 products which products have the highest popularity. Instead looking at the relationship between products and customers we also did some investigation on how does the product sales over time. Using these two products as example the first one indicates that there were almost no selling activities for the last 6 months. The second graph shows that that product was constantly sold over the time. In addition we also take a look at the income distribution by cities. The graph shows that the income varies in all the cities. From this interesting information we were using it as part of feature engineering later on. In this project we have several rounds of feature engineering which can be divided into 4 stages. In the first stage the input and output features are encoded from letters to numbers and only the original features in the data set are used in the model training therefore it is 22 features in total. Since the data set is way too large and using all the data will run out the laptop’s memory limit so in actual model training one month’s data  is used as training set. However in order to find the month which gives the best prediction three directions of month selection are performed: using the previous month to predict the current month using the month from last year to predict the same month of current year using one month to predict the situation of three months later. After performing all the combinations the pattern between months are not that clear and the scores based on  are not good.Then using the same idea of month combinations combining adding the previous month’s product information as input features meaning 46 input features in total the performance of models is improved. Also it is found that the best way in this dataset to recommend new products is based on the same month from the previous year. Since then the data of June 2015 is used as the train set to give the recommendation for June 2016.In the following steps what we did was adding or dropping features based on time series kmeans clustering and EDA.Our purpose of the model training is to let the model be sensitive to newly added products. Because machine learning is not that smart we cannot anticipate the model training process to understand what we want them to do we need to provide the model the information we want the model to know directly. Therefore we create a change feature. Change here means use the current month’s product information minus previous month’s product information. This change feature has two levels that is “1” and “0”. “1” represents newly added products “0” represents other statuses. The new features are selected based on the results of time series.Change features have a positive effect on the model to further improve the model we reseparate the change features back to ‘1’ ‘0’ and ‘1’ respectively representing close an account no change and open a new account. At the same time 5 products are dropped from the predicting list because the bank doesn’t sell those products anymore. Since the Kaggle system calculation penalizes more on false negative. Attempting not to miss any prediction class weight  is added for output features based on popularity of the products meaning give more weight for popular products.The month selection methods used in the first two rounds of feature engineering are actually to manually search the time effect between months. Combining the information from the manually searching and time series results based on the three levels of change features more product information from different months are added as new features. Here’s an example that how a certain product information of a certain month  is chosen  based on time series.This is the change of pension account through time. ADF test result shows this time series is stationary which is statistically significant the lag number is 4. According to this information since the data of June 2015 is as train set the product change information of February 2015 is added as a new feature. However again this dataset is weird for month 13 and month 14 there are sharp increase and decrease in the chart. At that time the bank has about 50000 pension accounts and in month 13 there were over 10000 pension accounts being closed and in the next month almost 20000 newly opened pension account. This also tells that randomly adding the change features into the model is not appropriate because it is hard to predict the erratic change in the time series.More feature engineering process involves in the model training procedure and more details will be discussed in the following paragraphs.With following new features: adding 5 previous months’ account history a marriage index (combination of age sex and income) removing city and 5 rare products. Our Xgboost model scored 0.02996 on Kaggle Leader Board and Random Forest model which scored 0.02946 is the second best among our single models.The key to build a good model in this competition is to use June 2015 as the train set because 5 correlated main account types (nom_pens nomina recibo reca and cno) show seasonal changes. Using June 2015 as the training month and account history from Jan 2015 to May 2015 enabled us to capture the time series aspect of the dataset. Removing 5 rare products (aval ahor viv deme deco) also contributed to improve our models we saw a 4% increase in our Kaggle Leader Board score by making this change alone.At the end of competition we were working on a new strategy to improve our model. The new customers who joined after June 2015 showed different product purchasing behaviors from the old customers. We could use their data from July 2015 which wasn’t in our training set to build models for them separately. Although the “newcustomer only” model did not improve the predictions on new customers (Kaggle LB score ~ 0.0297) combining them with the predictions from our Xgboost model trained on old customers could provide better predictions.,NA,Top 9% Open Kaggle Competition - Santander Products Recommendation
https://nycdatascience.com/blog/student-works/ninkasi-beer-recommender-system/,42,An ubiquitous drink throughout the ages beer is the source of pleasure and the complement for celebration when consumed in moderation. Ninkasi the ancient Sumerian goddess of beer exists to satisfy the desire and sate the heart. Likewise our recommender system named after the goddess aims to deliver personalized recommendations to beer lovers across the country. Our final product consists of web scraped data a contentbased natural language processing model two different collaborative filtering models using Singular Value Decomposition++ (SVD++) and Restricted Boltzmann Machines (RBM) all packed into an interactive Flask webapplication. Our purpose is twofold: to create a recommender system of something fun for others to use and in the process learn how to build a complex recommender system.We obtained the data from  a popular beer website for beer lovers to rate and review their favorite beer. The scraping tool we used was Scrapy a python based web crawling package. Two separate dataset was scraped one dataset containing all the information about the beer and the second dataset containing all the information about the user and the review. We decided to scrape US beer only limited to the top 2025 beers produced in every states plus District of Columbia a total of about 280 000 user reviews were scraped.The left image shows all the scraped beer information and the right image shows all the scraped review information. This is a detailed table for all the fields obtained Our dataset consists of  1269 beers taken from the top 2025 beers of each state and Washington D.C.  Of these beers there are 58 styles and 338 unique brewers: Of the statistics over half the beers had missing values for IBU and mean because the information was not available. Here are some violin plots:The weighted average looks to be fairly normal with mean around 3.7 while both style score and overall score are heavily skewed toward 100. Per the website the style and the overall score are scores relative to other beers and as we chose the top few beers of each state it makes sense that they are skewed as such. The % abv has a median between 8 and 9 and ranging from a bit under 2.5 % abv to close to 20 % abv. F or estimated calories we have two peaks one around 50 calories and one around 250 calories. This is perhaps a separation between socalled light beers and heavier beers.   Our dataset consists of  ~278000 reviews written by ~17000 users. ~9000 users over half wrote only 1 or 2 reviews. (Perhaps a recommender system would increase usage). The dates of the reviews range from 20161204 to 20000426. This means that some beers have stayed on the top 2025 list for quite a while in their respective state. Also there may have been quite some changes to which beers are in the top list. This combined with the number of users who have written only 1 or 2 reviews may not be indicative of the actual number of users who have used the website.After cleaning our data set consists of approximately 11370000 words or enough words to fill 150 350page books. This equates to about 40 words per review without stop words. There are only approximately 127600 unique words or 1.1% of all words.  The top 100 words by frequency are:And the top 100 distinct words by TFIDF score  in no particular order as a word could appear multiple times in the top 100:Lastly we ran a cursory LDA model with 5 topics and 40 passthroughs. Though we chose 5 topics only the first two topics had proportions that were not almost zeroed out:With 1300 beer information and 280000 review data in hand we started to build our recommendation engine. Our plan is to ensemble multiple recommenders to give users the best beer options based on their preference.In this section we expand the discussion on the techniques we used to build the system. As can be seen from the figure above our recommendation system includes a contentbased method which takes advantage of user reviews and a collaborative filtering engine based on user rating. Finally we combine all three techniques in a Flask web app.The idea of contentbased recommendation system can be summarized as follows: (1) User select a beer that he/she likes. (2) Recommendation system find a list of beers that are most similar to the user input. (3) Output results.Figure above demonstrates the workflow. The whole process involves Natural Language Processing (NLP) calculating Term FrequencyInverse Document Frequency (TFIDF) score and Performing Latent Semantic Analysis (LSA). Finally cosine similarity matrix for all documents are constructed.Given 280000 reviews from users we need to have them cleaned corrected and organized into meaningful pieces before performing numeric analysis. Therefore the goal of text preprocessing is to generate a group of words for each beer. Python NLTK package was used throughout the process.Above figure demonstrates the workflow of text preprocessing. For each review string it is first encoded by ASCII then all  garbled symbols and punctuations inside the string are washed off using regular expression. Having all unimportant symbols removed the review string is tokenized into groups of words. Next spell check and auto corrections are performed to each word. In order to facilitate weight calculation stopwords are removed due to having less importance. Finally every word is lemmatized to unify the tense for each verb. The final output is a list of corpuses with each corpus pointing to each beer name.TFIDF is a numeric statistic that intended to reflect how important a word is in a collections of corpus. The tfidf value of a word increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus which helps to adjust for the fact that some words appear more frequently in general. The overall score is by direct multiplying TF points with IDF points. The calculation is performed using the  Python package using the formula given below.TFIDF calculation generates a document(beer)term(word) matrix with each cell corresponding a weight of a word for a beer. Due to the size of the matrix we select the top500 terms according to their weights to build the recommendation system. Before creating similarity matrix among all beers Latent Semantic Indexing (LSI) was implemented to obtain a lowrank approximation of the documentterm matrix. What LSI does is performing Singular Value Decomposition to the documentterm matrix and pick out the first k rank1 components that contribute the most to the original matrix. The advantage of such approximation can not only reduce the noise of documentterm matrix but also alleviate synonymy and polysemy parsing in natural language.When working with ratings data it is often useful to look at the data as a useritem matrix where each user is a row and each item is a column. The matrix will have values in the spots where the user rated an item but will be missing a value otherwise. Because of the large number of items it is often the case the matrix be very sparse since it is unlikely that a single user have tried a majority of the items.In collaborative filtering the goal is to impute those missing values by analyzing the useritem interactions. If two users share common ratings for some items the recommendation system will assume that the two have similar preferences and will try to recommend items from each other’s list that the other did not already rate. This idea can be expanded to consider every user together as one cohesive group. One powerful technique is to factorize the matrix into a user latent factor matrix multiplied by an item latent factor matrix.                                   The idea here is to approximate a single value decomposition (SVD) that would contain the hidden features to explain why a user rated the items the way that they did. In other words the SVD is trying to extract meaning behind their ratings. This algorithm approximates the SVD by treating the user and item latent factor matrices as parameters and training them to minimize the rootmeansquare error for the known ratings. We used Spark’s MLlib ALS package on our data to get a benchmark of 10.75% error.From the winners of the Netflix Challenge in 2009 an enhanced version of this technique was developed to decompose the ratings even further. Instead of just user and item latent factors the model seeks to also model each user’s and item’s bias (deviance from the average of all ratings). For example if two user both like IPA beers but the first user just so happens to rate things 2 points higher on average the system will think that these two users have different tastes. The same is also true between items. In order to combat this the new algorithm explicitly models the biases in an effort to recenter each user and item at the global average. This allows the latent factors to be estimated correctly without worrying about bias. SVD++ goes one step further to also include the number of ratings a user has. The goal of this is to penalize users that have few ratings so that the predicted ratings are more conservative since there is less information for these users.Unfortunately we did not find any standard SVD++ packages publicly available. However after studying the algorithm further we realized that the algorithm is essentially matrix operations and therefore can be efficiently programed using TensorFlow Google’s computational framework for tensor objects and operations.Furthermore we were able to find a regular SVD script using TensorFlow on github so we did not have to start from scratch. Our implementation of SVD++ involved augmenting this script to include the biases and implicit feedback in the algorithm. Additionally we added in features such as kfold cross validation and early stopping. RBM is an unsupervised generative stochastic twolayer neural network. It is derived from the Boltzmann Distribution from statistical mechanics which is a probability distribution that measures the state of a system. It is “restricted” because the neurons must form a bipartite graph which the neurons form two distinct group of units visible V and hidden H. There are symmetric connections between the two group and no connections within each group thus effectively forming two layers.RBM performs collaborative filtering by reconstruct the useritem matrix filling out the missing rating for all user. Consider a dataset of M items and N users we are treating every user as a single training case for an RBM therefore we can initialize the visible layer to be a vector with M element where each element or neuron represents an item with value equals to the rating if it has been rated and zero if not. The visible neurons are modeled from a conditional multinomial distribution of K distinct rating scores thus a “softmax” activation is used. The hidden layer is a latent layer where the hidden neurons can be perceived as binary latent features therefore a conditional Bernoulli distribution is used to model the hidden layer with a sigmoid activation function.To train a RBM the initialized visible layer is propagated through the RBM parameterized by the weight W visible bias bi and hidden bias bj. After we obtain the For RBM all the weights and bias are constant across all user training cases for example if two users rated the same item they must share the same weight and bias for that item between visible and hidden layers.The two fully reconstructed useritem matrix obtained from the two CF algorithms are ensembled linearly to generate the final useritem matrix for new user prediction. To give recommendations for new user we used a neighborhood approach. After receiving the new user rating vector the cosine similarity between the new user vector and the useritem matrix is computed to impute the missing ratings in the user vector using a weighted average proportional to similarity matrix. The final recommendations is obtained from the items with the highest similarity scores.To make our recommender system accessible to the everyday user we decided to make a . Both the content recommender system and collaborative filtering system are accessible using the app. Fun fact: the earliest known recipe for beer is an ode to Ninkasi dating back 3900 years.,NA,NINKASI: Beer Recommender System
https://nycdatascience.com/blog/student-works/predicting-food-desert-via-social-media/,42,Food deserts are areas that lack access to affordable fruits vegetables whole grains lowfat milk and other foods that make up the full range of a healthy diet (CDC 2016). The definition of food deserts is that areas where at least 500 people and/or at least 33 percent of the census tract's population reside more than  from a supermarket or large grocery store (for rural census tracts the distance is more than ) (ANA 2011). The distribution of food deserts (green) is shown in Figure 1.  ,NA,Predicting Food Desert via Social Media
https://nycdatascience.com/blog/student-works/machines-machine-jobs/,42,Through use of TFIDF I was able to determine that there was one group that stood out. Though there appeared to be other groups by graphical examination upon numerical examination I determined they were not as consistently separated from other groups. Separating groups is possible but the ability for separation with a clear distinction among the groups requires further study. ,NA,Machines for Machine Jobs
https://nycdatascience.com/blog/student-works/orpheus-multi-user-music-recommendation-system/,42,What’s the recipe for the ultimate road trip? Companions who can make you laugh snacks to last the whole trip and of course a good music selection. It has long been an unwritten rule that whoever’s at the wheel has control of the music. This can get boring fast especially when the driver has a bland taste. Some passengers may tune out and start listening to their own music on headphones and disengage from the group. What is essential is a playlist that would cater to the taste of all passengers.In this project we address the challenge of creating a playlist for multiple users with different tastes and preferences and provide a uniformly fantastic listening experience.Imagine an app where upon you and your companions logging in to your music devices aggregates everyone’s listening history and automates a playlist that everyone would enjoy!We created just an app: . With Orpheus multiple users can login to their Spotify accounts and find songs they can all rock out to. The order of the tracks can be based on mood tempo and more. Orpheus was developed in Flask using the Spotify Web API to get user data. Check it out !Orpheus can also be used for parties or as background music for group workouts in the gym with your bros!In order to develop a model for recommending music we needed data. We collected user  website. At a high level the data was used to train a recommendation system for a single user. We employed collaborative filtering using ’s machine learning library to build a latent factor model. This model is then used by an aggregation strategy to determine preferences for multiple users in a group and recommend a final playlist. Finally this playlist is sent to the Flask app where users can get groovy to it. The entire pipeline can be seen below:The following describes each step in more detail.The dataset the recommendation model was trained on was from the Echo Nest Taste Profile Subset.  The dataset consisted of 5 GB of 1019318 unique users 384546 unique songs and 48373586 unique observations of user song playcount triplets.  On average 125 users listen to each song and less than 100 users are responsible for 80% of the songs listened.  Most likely there are a few songs that are highly popular and most songs are listened by a few.  With user listening history in hand the next step was in creating a recommender system. In general recommendation systems aim to predict the preference that a user has for a given item. Items that have the highest predicted preference can then be made as recommendations to the user.There are two commonly used approaches to building a recommender system: Contentbased filtering techniques and collaborative filtering.In contentbased filtering the system looks at the characteristics of the users or items to make predictions. For example in music the system would find songs in the same genre or have the same artist to determine similar songs. Using these characteristics the recommender could look at a user’s items and determine which items are most similar and recommend them.In collaborative filtering the idea is that users similar to you will like similar items. The content of the items is abstracted away and only the interaction between users and items is taken into account. A downside of collaborative filtering is that to make recommendations a user requires historical data with the items: This is known as the cold start problem.Within collaborative filtering there are two types of feedback received from users: Explicit and implicit. Explicit feedback occurs when users actively rate an item (e.g. the Netflix star rating). Implicit feedback occurs based on the consumption of an item for example when a user listens to a song. Our dataset consists of these implicit ratings.We chose to use the collaborative filtering approach known as the latent factor model due to its ability to handle implicit feedback its scalability and Spark’s Alternating Least Squares (ALS) implementation. A good overview of the implementation and its practical use can be found .The latent factor model attempts to reveal latent features about users and products in order to make recommendations. Specifically given a userrating matrix the model finds an approximate lowrank matrix factorization as seen below. The dot product of the user’s latent feature with an item’s latent feature represents the user’s predicted ratings. Predicted ratings for all items are computed and ordered to give the user a final recommendation with the highest predicted rating.Lambda is a regularization parameter used to avoid overfitting to the data.The implicit model works slightly differently. Rather than attempt to predict explicit ratings a confidence that user likes item  is given by the following equation:Where alpha is a tuning parameter of the model. The implicit optimization problem then becomes:Here p_ui represents whether the user liked the item while c_ui represents our confidence they liked the item. The regularization term is the same as in the explicit case.For more details see the paper that invented the implicit feedback collaborative filtering method .Now that we have a model how do we choose the parameters? For that matter how do we evaluate our model?Ranking metrics are a common approach to evaluating recommender systems. Briefly they allow us to assess the quality of a recommendation based on their ranking of predicted items. To evaluate our model we used the ranking metric  (MAP). This was the ranking metric used for the .Given a user’s item history and a recommendation the  (P) measures the proportion of correct recommendations within the topk recommendations. The (AP) is the precision at each recall point  Finally the (MAP) averages the AP over all users.In order to evaluate the model’s MAP we perform crossvalidation. Each iteration of crossvalidation splits the users into training and test users. The test user’s listening history is further split to a hidden and visible set. The model is trained on both the training user’s and the test user’s visible implicit ratings. After the model is trained the MAP score for the test user’s hidden set is computed to determine the quality of recommendations. For efficiency we selected a random subset of 100 users for computing the MAP.To determine parameters to the model we ran 5fold crossvalidation on a one dimensional grid for each parameter: The rank lambda and alpha. We compared the results to a baseline popularity model which simply recommends the most popular songs that a user hasn’t already listened to. The crossvalidation results are given in the plot below.We first ran the explicit model which can be seen in Figure a). The explicit model performs considerably worse than the popularity model which is unsurprising given that the data is not explicit and therefore this is not an appropriate model to use.Figure  b) shows the implicit model as a function of rank. Generally the higher the rank the better the performance as higher rank matrices provide better approximations of the useritem interaction. We chose rank  50 as a good compromise between accuracy and computational efficiency.Conversely alpha had a profound impact on the quality of the model. We found the optimal value of alpha  40 which is also the suggested values by the .We found the optimal parameters to be rank  50 lambda  0.1 giving a final MAP score of 0.1.When comparing this to the closed Kaggle competition we get approximately 25th out of 150 teams. This gave us confidence the model was performing well.We now have a working recommender system and it works great for individual recommendations. The next part is to make Orpheus recommend for a group of users. But how do we convert a single recommender system into a group recommender system?One technique is to get each group member’s recommendation and combine all the recommendations using an aggregation strategy.As much as possible we want Orpheus to come up with  a playlist which would satisfy all members of the group. Assuming there are a couple of different aggregation strategies to employ which one would work best for a small group? What if in our road trip scenario we had a minivan instead of a car would the same aggregation strategy work on a larger group? More importantly how would Orpheus recommend to a group with very dissimilar tastes?These were the questions we needed to answer to come up with the best recommendation for the group. To illustrate how aggregation strategies work we pick some users in our dataset. Suppose User 193650 and his friends User 84250 and User 92650 go for a drive Orpheus knows their listening history and has come up with individual recommendations for each of them. Table 1 shows a subset of recommended songs and how confident Orpheus is that the user will like the song. A confidence closer to 1.0 means the user will most likely enjoy the song. Which songs will Orpheus play first?The least misery aggregation strategy has been used for a . The movie recommender uses it on explicit ratings while here we use it on implicit ratings. For each song we get the smallest confidence rating and set it as the confidence rating of the song for the group. We then rearrange all songs from highest to lowest confidence rating. This is now the group recommended playlist.To measure the satisfaction of each member of a playlist we apply the formula below taken from  which in turn can give us the group’s satisfaction rating on the resulting playlist.Another variable we need to consider is homogeneity of the group. Kmeans clustering on the dataset would uncover similarities of each users and group them together based on their taste. This way we can just get members from the same cluster for a homogenous group and members from different clusters for a heterogenous group. However the main challenge with the dataset is its dimensionality and sparsity. Imagine doing Kmeans clustering for 1 million observations and close to 400000 variables! Luckily in the process of training a recommender system a reduced latent factor matrix is produced. We apply Kmeans clustering on the user’s latent features to come up with groups.To test our aggregation strategies we divided our groups into two categories: homogenous grouping and heterogeneous grouping. For each category we wanted to see how varying the group size might affect the group satisfaction so we made 3 5 and 7 member groups. We made 20 samples for each group for a total of 120 samples. The statistical results below show that the group satisfaction of homogenous groups using different aggregation strategies do not statistically differ from each other. This means that any of the aggregation strategies will result to a playlist where all members are happy in the homogeneous case. For heterogeneous groups we found that the average strategy was statistically significantly better than other methods as can be seen by the ANOVA and Tukey HSD posthoc test results below.We complete our project with a Flask Application designed to generate a Spotify playlist ordered in whichever feature chosen from the tracks of up to six different people.  The diagram below shows how the app works.After the necessary inputs are made the playlist can be launched with a player embedded inside the application. Users can order the playlist by metrics such as energy mood and tempo as can be seen in the pictures below.,NA,Orpheus: A Multi-User Music Recommendation System
https://nycdatascience.com/blog/student-works/project-1-first-term-presidential-election-impact-congressional-mid-term-elections/,42,As a budding data scientist my first project will look at the composition of the US Congress (Senate and House) by party in the first year of a presidential term and two years after the election starting with President Ronald Reagan.  I’m interested in knowing how the congressional composition changes over time by state (D vs. R) and project what congress could look like for President Trump in two years.  ,NA,Project 1: First Term Presidential Election Referendum on Congressional Midterm Elections
https://nycdatascience.com/blog/student-works/iot-news-headlines-ddos-attack/,43,The famous cartoon series “The Jetsons” predicted 53 years ago how our lives would look like in 2063 when all our “daily devices” such as home appliances or cars would be connected to the internet. In 2016 the Internet of Things (IoT) is still evolving to a convergence of multiple technologies including wireless communication realtime analytics machine learning commodity sensors embedded systems automation and more. On October 21 a series of Distributed Denial of Service (DDoS) attacks caused widespread disruption of internet activity across Europe and the US. This attack was also unique because it targeted IoT devices due to the fact they have soft security profiles. As the awareness of what impact connected objects would have on our lives and on our security. In this work we analyze IoT news headlines after October 21 (from October 21 to October 31) last week (November 1 to November 5) and this week i.e three weeks after the attack (November 6 to November 11).In this work I used Python’s library Beautifulsoup for pulling data out of HTML and XML files from Google News to compare IoT news headlines.In order to compare IoT headlines for both the first and second week after the DDoS attack Word Clouds are generated as a visual representation of keywords in a text as shown in Fig.1.Fig. 1 : Word Clouds for IoT headlines two weeks after Oct 21 (left) and three weeks after the attack (right)We can see that from Fig.1 that for both the two weeks after the attack(Fig.1left) the news were kind of neutral and focused on explaining the cause of the attack. We can find words such as: DDoS Botnet security and users. For the third week after the attack (Fig.1right) we start to see some negative headlines and words such as: bad afterthought overload as well as the word security which seems to appear more often.In next sections of this work we will use Sentiment analysis (also known as opinion mining) to analyze the IoT news headlines and extract some meaningful information to determine the emotional communication in a text document. Python’s Pattern library is used for this purpose.In this part the aim is to measure subjectivity in IoT headlines. Fig.2 shows the subjectivity of these headlines for the three weeks after the attacks.Subjectivity is measured based on the adjectives in a text document. A measure of 0 indicates that the text is not influenced (neutral) and a measure of 1 indicated the emotional effect the author wishes to have on the reader. In the case of IoT headlines we can see for the last two weeks before the attack subjectivity was a bit higher 0.45 than this week 0.35.The overall polarity of a document (from Python’s Pattern library) is also based on the adjectives in a text document. Polarity is a value between 1 and 1 where 1 is negative 0 neutral and 1 positive.Fig.3 shows that IoT news headlines are written in a neutral tone.In the next sections we use Python’s Scikitlearn library for text processing in order to determine the similarities separate text by topic and extract some of the most frequent words. This library has several modules that transforms text into a documentterm matrix.First we calculate the measure of similarity between headlines. Since each row of the documentterm matrix is a sequence of word frequencies in headlines it is possible to put mathematical notions of similarity (or distance) between sequences of numbers in order to establish the similarity (or distance) between different texts. The Euclidean distance is used as a measure of similarity between text documents.Fig.4 shows the principle of distance (similarity) between two vectors in space and the equivalent for news headlines in a text document.We can see from Fig.4 that headlines from the first two weeks after the DDoS attack are quite similar comparing to the headlines of this week.Using the Euclidean distance from section IV we use Ward's method to produce a hierarchy of clustering. Ward’s method starts with each text in its own cluster and finds closest clusters and merge them. When the clusters are merged the distance between them is then changed to the sum of squared distances (linkage distance).For our headlines Fig.5 shows the different clusters. We can see how headlines from the first two weeks are grouped in the same cluster.From the headlines we want to extract the number of topics we have. For that purpose a technique called Non Negative Matrix Factorization (NMF or NNMF) to extract topics within a text.Fig.6right shows that there were two main topics during the three weeks following the DDoS attack. For each topic we have the main key words. We can also see in Fig.6left the frequency of some of these keyword by week.In this work an analysis of subjectivity polarity headlines similarities and clustering  along with extraction of keywords has been performed. These analysis are helpful to assess the evolution of headlines from a week to another. For now the headlines about IoT security indicate it is still a “neutral” subject despite the numerous threats and the debate is still going on about the urgency for viable IoT security solutions.,NA,"IoT news headlines, before and after DDoS attacks"
https://nycdatascience.com/blog/student-works/optimizing-machine-learning-algorithms-model-allstate-loss-claims/,43,A viable business model for an insurance company has to involve some way to assess how much payout they can expect for a given customer. If the payout is larger than what the company has accumulated from the customer’s payments then a loss is incurred. Allstate Insurance is trying to determine if they can predict the loss to expect from a customer. To gain a better understanding of how this can be done Allstate submitted a  where the goal was to see if participants could accurately determine the loss incurred for a given customer given various features about them. We participated in the Kaggle challenge with the goal of learning how to best optimize machine learning algorithms use feature selection use exploratory data analysis and a wide range of machine learning techniques. By accomplishing these goals we could then earn competitive score.Through Kaggle Allstate provided a training data set and a test data set with 188318 and 125546 customers respectively. Customer features consisted of 116 categorical variables and 14 continuous variables. There was a lack of domain knowledge as the categorical variables were in the form of ABC  and the continuous variables were scaled to range from 0 to 1. As a preprocessing step for our models we created dummy variables from the categorical variables and we tested the data for nearzero variance. We noted there were 54 factors that were removed every time We eliminated these factors in advance to cut down on preprocessing time.The loss variable from the training data set was greatly skewed to the right indicating extremely large loss values. To transform loss into a more normally distributed form we performed a log transformation. This can help improve the accuracy of methods not robust to outliers. Figure 1 presents the two forms of the response variable before and after the transformation.The left density plot shows the original untransformed response and the right density plot is the log transform of the response. We chose to use the log response to satisfy the regression assumption of normality.A look at the linear correlation between the continuous variables with themselves and with the loss dependent variable in the training data set reveals two important results shown in Figure 2. First none of the continuous variables have a strong linear correlation with the log loss. This suggests that a linear model is not likely to perform well. Second there is a high degree of collinearity among some of the continuous variables. While one could remove these collinearities (for example by performing Principle Component Analysis) we did not need to do so as the nonlinear models we employed do not require such assumptions.In order to streamline our team’s workflow we wanted to make the process of trying out different models as easy and efficient as possible. Therefore we created a model maker pipeline which takes in parameters specific to a model and outputs performance results as shown in the diagram below. The model maker employs the caret package an ensemble suite of many different machinelearning algorithms implemented in R. It also provides convenience functions for performing feature selection crossvalidation and parameter tuning. Utilizing caret allowed us to automate these steps as a streamlined workflow. For example suppose we want to test gradient boosting. We then simply specify in a parameters file the method name “gbm” a grid of parameters to test crossvalidation settings (the partition split sizes number of folds etc.) as well as miscellaneous parameters for that run such as whether to take the log transform. An example model parameters file is given below.Given these parameters the model maker performs preprocessing and crossvalidation on the grid of parameters builds the optimal model and outputs a timestamped folder of results and a Kaggle submission file. Results included the estimated MAE from crossvalidation and the optimal model parameters. Allstate defined the score as the Mean Absolute Error (MAE) between the predicted losses and the true losses. A lower MAE results in a higher score.Parallelization was utilized in R and model tests were run on the server. The server we used was the Brandeis University HPC Cluster. The server had multiple servers available with each server proving 32 cores of computational power and several hundred gigabytes of RAM. This allowed us to attempt more complex models including KNN which requires a large amount of RAM.We trained and tested 4 nonlinear models in the R Caret package: kNearest Neighbors (kNN) Gradient Boosting Machines(GBM) XGBoost and Neural Networks. The advantage of kNN is that there is only one parameter to consider the number of neighbors. The disadvantages are that it takes a long time to run on large data sets (it runs in (n) where n is the number of observations) it requires a 100GB of RAM on this dataset and it is not as accurate as other methods we tried. The final Kaggle score that we got with kNN was 1507.53360 MAE. The advantages of GBM are that it is robust and generally performs well out of the box. We took our final set of parameters from tweaking suggestions on the Kaggle Forum and got a score of 1163.47311 MAE. The final model used 5000 trees with an interaction depth of 5 a learning rate of 0.03 and 20 minimum observations per terminal node. Advantages of XGBoost are that it is well parallelized in R and can yield results significantly faster than GBM while the drawback is that there are many parameters to tune (7 in total). XGBoost is often the most competitive method in Kaggle competitions and some variant is often utilized in the winning solutions. We looked to the Kaggle Forum for suggestions on the parameters and we crossvalidated among four parameters (Figure 3). It is illustrative to observe some trends in this figure. For higher learning rates (eta) higher depths in trees perform worse than shallower trees. Conversely for lower learning rates the higher depth of 12 can outperform the shallower tree models. In other words if the learning rate is high our trees should be simple as to not overfit. If the learning rate is low more complex trees can improve performance. Our Kaggle score for XGBOOST was 1148.65697 MAE and our final parameters included 3000 trees a learning rate of 0.01 a max depth of 12 and a regularization parameter or gamma of 2.Neural Networks have the advantage that they are very flexible and are robust to outliers. The disadvantage is that it is difficult to find parameters that yield the best global solution. We decided to use the  R package and to utilize one hidden layer which simplified the possible network topologies to consider. Apart from the number of nodes in the hidden layer the  package allows a second parameter to tune called the . The purpose of this decay parameter is to prevent individual large values of weights from dominating the network and to prevent overfitting. The use of the decay parameter in  can be compared to the shrinkage parameter in Ridge regression. We performed crossvalidation on the entire training data set and found that ideal values for the  as well as the number of nodes. We decided to use a  parameter of 0.1 and 25 nodes in the final network model. The Kaggle score for this neural network model was 1206.69697 MAE.A summary of model results is given by the following table.To summarize we found that for this problem XGBOOST performed best followed by GBM. In third place after the tree methods was the neural network model and kNN performed the worst. Also even the best submitted Kaggle result still had a mean absolute error of over 1000. This tells Allstate the limit of how much they can forecast is within about $2000 of accuracy. Clearly judging the validity of claims using a model can be difficult.,NA,Optimizing Machine Learning Algorithms to Model Allstate Loss Claims
https://nycdatascience.com/blog/student-works/predicting-allstate-insurance-severity-claims/,43,"Kaggle competitions are a good place to leverage machine learning in answering a realworld industryrelated question. A Kaggle competition consists of open questions presented by companies or research groups as compared to our prior projects where we sought out our own datasets and own topics to create a project. We participated in the Allstate Insurance Severity Claims challenge an open competition that ran from Oct 10 2016  Dec 12 2016. The goal was to take a dataset of severity claims and predict the loss value of the claim.The training dataset consists of approximately 180000 observations with 132 columns consisting of 116 categorical features 14 continuous features and 1 loss value column. The features are anonymized into ‘cat1’’cat116’ and ‘cont1’’cont14’ effectively masking interpretation for the dataset and nullifying any industry knowledge advantage. To get a sense of what we are working with we examined the distribution of the data by building a histogram. It was obvious that the data was very skewed to the right. To normalize the data we transformed the data by taking the log of the loss. Since there are so many categorical variables we wanted to find a way to see if we could perform some feature selection. We referred to the Kaggle Forums and saw that we could perform a Factor Analysis for Mixed Data (FAMD). This is sort of a Principal Component Analysis for categorical variables to see if we can reduce our dataset or discover some correlations between variables.
Only 4.08% of the variance in our dataset can be explained from the first five components which are the highest contributors to the percentage of variance explained. As a result there is no one particular variable that dominates nor can we reduce our dataset to only a few components. Since there are so many categorical variables we wanted to find a way to see if we could perform some feature selection. We referred to the Kaggle Forums and saw that we could perform a Factor Analysis for Mixed Data (FAMD). This is sort of a Principal Component Analysis for categorical variables to see if we can reduce our dataset or discover some correlations between variables.However a graph of the contributions of the quantitative variables above to the first two components depicts three different groups that may be correlated with each other: a group formed by the upper right quadrant the lower right quadrant and the lower left quadrant. Analysis of the categorical variables is not as clear.XGBoost and Neural Networks are known to be strong learners and we expected them to perform best amongst other machine learning models. However both as a benchmark and possibility for stacking weak learners we incorporated other models to compare the costcomplexity and overall performance between them. For the Kaggle combination the metric is mean absolute error and we report our cross validation scores and leaderboard scores as such.Lasso (least absolute shrinkage and selection operator) is a regression analysis method which can perform feature selection and regularization in order to improve the prediction accuracy providing faster and more costeffective predictors. To preprocess the data we first wanted to remove any highly correlated variables. Looking at a correlation plot of the continuous variables we saw that variables cont1 cont6 and cont11 were highly correlated with variables cont9 cont10 and cont12 respectively. For the categorical variables we dummified the variables converting them from categorical variables to numerical ones. This expanded the dataset’s dimensions from (188318 127) to (188318 1088).Let’s run the Lasso regression model to explore its ability in loss prediction and feature selection. The training dataset has been split in 80:20 ratio and applied 310 crossvalidation (5 is ultimately selected) to select the best value of the alpha (regularization parameter). We can see the regression coefficients progression for lasso path in the graph below  which indicates the changing process of coefficients with alpha value. From plots below (“Regression coefficients progression for lasso paths ” “Mean squared error on each fold”) the best alpha value is 5.377 which could help reduce the number of features in the dummy dataset from 1099 to 326. The error histogram shows the optimized Lasso regression model prediction of the 20% test dataset the R squared value of this prediction model is 0.56.We then fit our data using a KNearest Neighbors Regression model. The tuning parameter is K the number of neighbors for each observation to consider. We start with sqrt(N) which was approximately 434 as our initial guess and tune K from 425 to 440.  K  425 performed the best in this range though that leaves open the possibility of tuning K in a range around 425. The model gave a CV score of 1721 and a LB score of 1752.We also used  Support Vector Regression to fit our data . Preliminary cross validation and parameter tuning on our test set revealed that the algorithm was computationally expensive taking ~12 hours on our machine. Using the sklearn class SVR from the svm module we attempted to tune cost and epsilon using a radial kernel and setting gamma to 1/(number of features). Preliminary tuning revealed an epsilon value of ~ 1.035142  and a cost of 3.1662 giving us a CV performance of 1570. The performance varied greatly amongst the few parameters we chose to test from 10 to the power of [1 0.5 0 1] for C and 10 to the power of [0.05 0.01 0.015 0.02 0.03 0.1 0.5] for epsilon for our grid.The Random Forest algorithm is a good outofthebox model for most datasets since they are quick to train perform implicit feature selection and do not overfit to the dataset when adding more trees. For this model we used the scikitlearn’s package: RandomForestRegressor. To train and validate the random forest model the data was split using 10kfold cross validation. The parameters we tuned were the number of trees and the number of features considered. Initially the model was trained using all features considered per split. This is essentially equivalent to bagging which performed poorly scoring a MAE of 1312 on the leaderboard. To improve on this we decreased the number of features down to the square root of the number of features. This resulted in the random forest only considering 12 features per split. Using less features forces the model to consider different features per split which ended up improving the model’s MAE score to 1188.Training the model on an increasing number of trees improved the predictive power but took more computational power. To prototype the effects of a model quickly we used 50 trees to get a sense of the effect and then 200 trees for more computational power. AddIng more trees will help the predictive power but with decreasing returns. With 200 trees the best random forest MAE score was 1187. This is the baseline score which we wanted to beat with our more competitive models.The Neural Network model turned out to be one of the better performing algorithms. For this competition we used the Keras (frontend) and Theano (backend) Python packages to build a multilayered perceptron. Our best Neural Network consisted of threehidden layers with rectified linear unit activation.Additionally we added in dropout and batch normalization as methods to regularized the network. The model was then run with a 10 kfold cross validation and 10 bagged runs per fold to essentially produce an output that is the average of 100 different runs to minimize model bias. One downside of Neural Networks is that it is computationally expensive. It requires computing many large matrixvector operations. With an Nvidia GTX 1070 GPU our model required 5 hours to train. Furthermore when we tried to add layers or more neurons the model started to overfit. The model resulted in an average validation mean absolute error of 1134 and a leadership board score of 1113 that put us in the top 25%.One of the most popular algorithms currently among Kagglers that proved to be successful is XGBoost. This algorithm takes a linearized version of gradient boosting that allows it to be highly parallelized and computed quickly. This allows large number of trees to be produced per model. We chose a learning rate of 0.01 with a learning rate decay of 0.9995. This model also used an average of 10 fold crossvalidation with a maximum of 10000 trees stopping when the validation error is minimized. The XGBoost model proved to be our single best performing model with a validation score of 1132 and a leadership board score of 1112.In the Allstate insurance dataset the data was highly skewed right with outliers taking on large values. Looking at our validation predictions against the true values the largest errors accumulate around the outlier points. For this reason we wanted to see how well we can classify if an observation was an outlier. We chose the threshold that separates an outlier to be two standard deviations above the average loss value. Although the outlier region only made up 2.5% of the data it made up more than 90% of the range of values.We first tried to use a logistic regression classifier to establish a baseline. This model resulted in a 97.5% accuracy which sounds good at first until we realized that this is only as good as the model guessing that all observations were nonoutliers. Next we tried a more advanced model the XGboost classifier with AUC score as the metric to maximize. However when we looked at the output predictions the probabilities were very close to 0.5 for all observations which told us that the model could not confidently distinguish the two classes. Also the AUC score was 0.6 which was much less than desired.The way this problem was set up it turns out to be an imbalanced dataset problem where the minority class was much smaller compared to the majority class. One traditional method to deal with these type of problems involved oversampling and artificially synthesizing new minority class and undersampling the majority class. Here we used the ImbalancedLearn Python package to readjust our data ratio from 97.5:2.5 to 90:10. Afterwards we again used XGBoost classifier and achieved much better results. The accuracy was raised to 99.6% and the AUC increased to 0.80. Although this was very insightful this new information did not help our regression model much so we turned our attention to other methods raise improve our error rates.Ensembling is an advanced method that combine multiple models to ultimately form an better model than any single model. The idea is that each model theoretically makes its own errors independent of other types of models. By combining the results from different models we can “average” out the errors to improve our score and reduce variance in our error distribution. For this competition we chose to do three different ensembling methods with two XGBoost and Neural Network models: 1. Simple average of the test results.2. Use an optimizer to minimize error of model validation predictions against true values.3. Weighted average of test results. With our best scoring model with a MAE of 1101 we were placed at the top 2% of the leaderboard by the end of the two weeks. XGBoost lived up to its reputation as a competitive model for Kaggle competitions but could only bring us so far. Only after we applied neural network models as well as the method of ensembling we were able to get to the top 2%.While trying to perform competitively in the Kaggle was tough. The two week time limit for this project in the bootcamp definitely amplified the difficulty. In order to perform effectively we needed to have good communication and a good pipeline for testing each model especially since some models took hours to train. It took a while for the team to build this communication and pipeline up but eventually we were able to share knowledge and get multiple workflows running.Although the Kaggle competition was a great way to test our mettle against other competitors using a realworld dataset there were some detractions in this format. By having a dataset given to us in a clean format the process of taking data and churning out predictions was accelerated greatly. Moreover we lost out on attempting to interpret our dataset due to the anonymity of the variables. However this did allow us to focus on practicing fitting and training models — a huge plus given our limited time. In the future we would like to incorporate the method of stacking models to see if we could improve our score even further. In addition we would like to explore other ways in handling the problems with our uneven dataset using methods like anomaly detection algorithms rather than binary classification methods.",NA,Kaggle: Predicting Allstate Auto Insurance Severity Claims
https://nycdatascience.com/blog/community/nyc-data-science-academy-partners-carto-teach-location-intelligence/,43,"   As analytics big data and the Internet of Things are beginning to transform almost every industry there is a growing demand for data science employees with an understanding of how to process manipulate and interpret location and geographical data.   (NYCDSA) a leader in data science education has announced a partnership with  a leading developer of location intelligence software to teach and train future data science students in the field of location analysis.  Students will be trained using the latest data science tools  including CARTO's spatial analysis platform  to visualize analyze and draw insights from location data in addition to inclassroom guest lectures from CARTO data scientists on advanced spatial data techniques and realworld locationspecific use cases. The inclusion of location analysis and intelligence is a strategic and innovative approach to provide NYCDSA students with cutting edge techniques in the data science industry. ""Data scientists extract insights from complex data to tell stories and explain 'why'” said Vivian Zhang founder and CTO of the NYCDSA.  “A key component to forming and enriching those data stories is understanding the setting. In this case when dealing with data setting means location and answering 'where.' This is a crucial component to understanding a complex data story.""""We are thrilled to work with CARTO to provide their bestinclass software and technology to our students and also host expert guest lectures from their data scientists. At the NYCDSA our goal is to ensure students get the most rigorous and comprehensive data science education and can apply advanced analytic tools in the real world. Collaborating with CARTO enables us to continue towards our goal of strengthening students' ability to analyze various types of data including location and to make datainformed geospatial decisions.""During the NYCDSA’s 12week fulltime data science bootcamp students become proficient data scientists using the programming languages R Python Hadoop & Spark as well as some of the most popular data manipulation packages such as XgBoost dplyr ggplot2. Students work alone and in teams to create at least .  They will have access to  and a full suite of location data to build insights on their data science projects. ""With CARTO's APIs data scientists can take full advantage of the spatial analysis capabilities of CARTO's platform"" says CARTO Map Scientist Andy Eschbacher. ""Using integrations like our SQL API with Python pandas or R data frames workflows can easily be built to leverage the spatial structure in one's data. The outputs then can be mapped and analyzed further within an interactive dashboard in our Builder.""CARTO's distinctive focus on location intelligence revolves around scalable data analysis and geospatial interpretation adding significant value to NYCDSA’s robust program. The goal of the partnership is for students to become more involved with geographic data and have the opportunity to dive deeper into behavioral patterns and trends. CARTO's location intelligence platform allows for a more efficient use and handling of location data inspiring the desire to explore innovate and understand the real world.CARTO leads the world of location intelligence empowering any organization and individual to discover and predict key insights through location data and then makes it available to their organization or the general public in the form of location intelligence apps. The apps built by developers or data analyst using CARTO’s selfservice platform help optimize processes predict situations and overall improve performance by leveraging location data. https://carto.comFounded in 2014 the NYC Data Science Academy offers the highest quality in data science and data engineering training. Their toprated and comprehensive curriculum has been developed by industry pioneers using experience from consulting corporate and individual training and is approved and licensed by the NYS Department of Education.The program delivers a combination of lectures and realworld data challenges to its students and is designed specifically around the skills employers are seeking including R Python Hadoop Spark and much more. By the end of the program students complete at least five realworld to showcase their knowledge to prospective employers. Students also participate in presentations and job interview training to ensure they are prepared for top data science positions in prestigious organizations. 93% of NYC Data Science Academy students are hired within six months of graduation. For more information visit",NA,NYC Data Science Academy partners with CARTO to teach Location Intelligence
https://nycdatascience.com/blog/student-works/dendotrons-allstate-claim-severity-kaggle-competition/,43,"What do you do when as a member of a team called ”The Dendrotrons"" in a Data Science cohort have a twoweek timeframe to work on the  challenge (predict the loss for Allstate claims) and present your results and insights? This article will walk you through our team’s journey for the Allstate Kaggle competition covering our experience in:The team consisted of six members with individual strengths in business engineering development project management production support and academic research. This blend of skills and experience provided a good influx of ideas and early experimentation to determine if strategy and tasks were aligned with objectives. The role of PM (Project Manager) was assigned to a team member to ensure the project timeline and deliverables were being tracked and making progress.Having strong communication skills is core to achieving success either as an individual contributor or when working in a team. For our team we agreed that having a communication protocol was a primary focus to ensure deliverables met timelines during the twoweek project schedule. The topic was discussed during initial team meetings to ensure an agreed upon protocol would work for all team members. The communication process included daily scrum sessions (max 15minute meetings to sync up and discuss project status / updates) continuous feedback via a team channel in  and file management in  and .Working with a two week timeline required for the team to operate under assumptions and expectations that typically take more time to formulate in a team environment. In referencing Agile team development process the team had to transition quickly through the first three phases of Form Storm Norm by agreeing that trust / respect / accountability had already been established given our experience as peers and individual project contributors during the past two months of training and project work in our cohort. This precondition allowed the team to agree on strategies and operate within the Perform phase thus helping the team achieve the core objectives by the project submission times. The scheduling process maintained a continuous delivery of tasks with very little bottleneck across task interdependencies.The team was evenly split into two subteams of three working on two major tasks in parallel: EDA and the Kaggle submissions. This allowed for Kaggle submissions to happen within the first week. ML was used for exploratory analysis and XGBoost was the preferred model used for prediction. The early experimentation process between the sub teams produced continuous feedback within the team; thus influencing the next steps for EDA / prediction. The feedback loop led to synchronization discovery and insight.Given the relative large number of features and observations the first step in the EDA process was developing a method to visualize the dataset as a whole. To this end a novel visualization tool was developed which generated grayscale images from every 25 or 50 observations after the dataset was sorted by increasing log loss. This high level view easily allowed patterns in the dataset to be seen. Furthermore being interactive in nature it allowed the user to simple click on a region in the image to generate traditional box or scatter plots for more detailed insights. This process is illustrated in the figure below.The continuous features (cont1 to cont14) were plotted together with the response variable loss. All features exhibited skewness including the response variable loss. To handle the skewness of the response variable we log transformed the data and performed a BoxCox transformation for the continuous features.The train claims severity dataset has 188318 instances and 132 features or attributes. 116 of these features are categorical variables and 14 are continuous variables. For this case study we need to predict the 'loss' based on the dataset features. Since we can’t use unique categorical features from the test dataset to make predictions an interesting part of our analysis is to determine if our test and train set have the same categorical variables.  We found that 45 variables are presents in the test dataset and not in the train dataset. This analysis could be beneficial in the feature engineering FOR THE PURPOSES OF THE KAGGLE COMPETITION in order to incorporate this variables to our machine learning model to better predict these cases. Figure.1 shows an example of missing variables in the test and train dataset for the categorical variable cat111. The variable F is missing in the train data and the variable D is missing in the test dataset.We first determine the correlations between continuous variables. Fig.2 shows an overview of the correlation matrix of all continuous variables (the darkest color is for the most correlated variables) and Table 1 shows some of the most correlated variables.This correlation analysis represents an opportunity to reduce the features through transformations such as PCA.From the correlation analysis we can see that we there is a potential possibility to reduce the number of continuous feature set. We use PCA to determine how we can use describe our continuous features in a reduced dimension subspace. Fig.3 shows the cumulative explained variance or variability from our PCA analysis and the number of components describing the continuous variable data. We can see that with 7 components we can explain 90% of the continuous data i.e half the total of the continuous features in the dataset (total of 14 continuous attributes).As we can see from Table.2 and as we did for continuous variablesThis correlation analysis represents an opportunity to reduce the 2labels categorical features through some dimension reduction transformation. Since our 2labels categorical features are transformed to numerical 0 and 1 variables we use SVD for this kind of “sparse” data in order to determine as we did for continuous attributes the reduced components that describe these categorical features. Fig.4 shows that 90% of the of the 2labels categorical attributes are described with 26 components i.e 36% of the total number of the 2 labels attributes (in total we have 72 binary categorical attributes).As Allstate did not reveal the true meanings of their predictors any attempt to find relationships among those predictors against the outcome (i.e. the loss) seemed meaningless.  However it is reasonable to speculate that the categorical variable “cat112” is the “State” indicator as it had 51 different values.  With such speculation we attempted to find some possible grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss.  Visualizing the ""cat112” column with loss the group finds that 20 of the states have mean loss exceeding the national loss. On a business perspective this is a helpful indicator for AllState to consider calibrating its policy pricing in these 20 states to minimize future losses. With such speculation we attempted to find some possible grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss.We chose  as our unsupervised machine learning for this exploration.  Following is the dendrogram of average linkage into 5 groups:The first cluster (shown inside the second red box from the left) only has two states (Q & J as ALL is NOT a state).  When we compared the statistics of this cluster against the other four clusters we discovered it had the distinctively shortest distance/range from min to max. The fifth cluster (shown inside the first red box from the left) had only one state (AQ) When we compared the statistics of this cluster against the other four clusters we discovered it had the distinctively longest distance/range from min to max as it contained the min and max of the whole dataset.We thought we could fold the first and the fifth clusters into the second cluster (shown inside the third red box from the left) for two reasons:With such an arrangement we could attempt feature engineering based on only three groups.For details please review [] below.For this project we tested several models both  and . But due to the sheer number of features and observations we tried to reduce the size of the data set to speed up the processing time for a first pass of model training. For example we used Caret’s near zero variance function and we also leveraged a quickanddirty linear regression and selected features for more computingintensive models based on their . Because most of the features were categorical we also tried reducing the number of factor levels prior to . For example cat116 had over 300 levels but because it didn’t have much predictive capacity we reduced the number of levels to three.To reduce the number of rows we used random sampling. We first created an 80/20 train/test split out of the Kaggle training data in order to test our results before running the trained model on the official test data. However within this training set we further reduced the number of rows for initial tuning rounds of new models. For example we might only take 40 features of 30% of the training observations in order to do a quick first pass of a random forest or to test how efficiently  can parallelize the training on our 16core .  We tested a linear regression model boosted trees (of which XGBoost performed the best) and a singlelayer neural network. For classification we used logistic regression boosted trees and a support vector machine. Overall we found that XGBoost was the best single model with a Kaggle MAE of $1126 and a 50/50 average of the Neural Net and XGBoost gave us an MAE of $1124.It proved very difficult to deploy any regularization or observation sampling when finetuning a model. We found that after all but the most cursory training passes we had to use all the data and all the features to tune model hyperparameters. The below chart illustrates this point.The tuning grid which was run for XGBoost on about 50% of the training observations makes it appear as though a learning rate of 0.01 was superior to 0.05. However when we used all the training data we found that a learning rate of 0.05 with a smaller number of trees was superior for both outofsample testing and Kaggle results. This point is further illustrated by the below variable importance chart.The second column “Other” is comprised of the remaining 110 observations that are not depicted on a standalone basis. Though each of them individually is less important that cat113 on the right their collective importance is the second largest contributor to variance reduction. Therefore any reduction of features or observations (via increasing sparsity) is bound to reduce the predictive power of the model.After plateauing at about $26 away from the best ranking Kaggle competitor we tried to determine exactly where our model was underperforming. The below chart shows the cumulative absolute error against the observed log loss. We can see that most of the loss was accumulated against observed log losses of about 6 to 9.5. In other words there wasn’t much to be gained by trying to improve the model for very small or very large observed losses.Therefore when we first plotted this below chart we weren’t overly concerned with what first looks to be very extremely poor predictive power on large losses. For the purposes of the competition it wasn’t important.Below is a detailed view (on a log scale) of the range of observations for which an improvement in model tuning would have the largest impact on the MAE. This chart depicts two main points. First the overestimates are generally larger than the underestimates. And second the underestimates get worse for larger observed losses. Therefore a good strategy to follow would be to seek out a model that is skewed in the opposite direction. Also we tried to use this information to engineer a new feature to help the model compensate for itself. However this was unsuccessful.Ultimately though we were only $26 away from the best Kaggle score this best score was still a $1000 MAE. And this is on a data set where most losses are around $3000. To zoom back to the big picture of what we were doing we plotted a linear regression on 6 features (blue) versus our best XGBoost model on the total set of features. XGBoost is certainly better but its predictive capacity is still limited. This gave us an idea...This insight led us to a classification problem!!We revisited the original motivation for the Kaggle competition. This passage is part of the competition description:“When you’ve been devastated by a serious car accident your focus is on the things that matter the most: family friends and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. “If the goal was to use predictive modeling to reduce paperwork then we thought maybe a classifier question might be useful. After all a major reason you have to go through so much paperwork for an insurance claim is that it protects the insurer against fraud. If we could split the claims into “big” and “small” categories maybe we could identify claims that look “fishy” meaning claims that look as though they ought to be small but were in fact big (the opposite is also a problem but not for the insurance company). If you were reasonably sure that a given claim amount was in the right bucket perhaps Allstate could reduce the amount of paperwork that the lion’s share of customers goes through This improves service for the customers and a reduces cost for Allstate. However if you weren’t sure you could classify the claim correctly the customers would have to go through the same process they do now.We found that half the claims by value were made by 80% of the customers so (following the 80/20 rule) it seemed that creating a categorical feature “isSmall” and splitting the data at the mean was a good starting point.We ran a support vector machine a gradient boosting classifier and logistic regression. As we were running out of time and running into model errors on the logistic regression we pared down the number of features to about 30. We found that all the models had a high accuracy (and not much different from logistic regression with only 30 features!) but we needed to optimize for sensitivity (i.e. “fishy claims”). Logistic regression was the best choice under the circumstances because it expresses its classification as a probability. We found that a cutoff of 50% probability to distinguish a small claim from a large claim may not be suitable due to too high a number of false negatives (claims the model predicted as being large but were in fact not) because that cutoff point yielded only a 50% sensitivity. Rather we needed to minimize the false positive rate.The below plot depicts the tradeoff that has to be made in an intuitive way for a decision maker. Starting on the left we can see that if we set the cutoff very low for the logistic regression more people gain but Allstate is exposed to fraud risk on 20% of their customers. As we move to the right you can see that the fraud exposure is reduced but people move from the “gain” bucket to the “same” bucket. This rate of swapping buckets starts to increase significantly after about 0.70.8. At that point there needs to be a 7080% chance that a claim looks like a big claim to be classified as such. This means that some genuinely large claims may be misclassified as “fishy” and have to undergo the current paperwork process. To determine the cutoff point you would need a subject matter expert to express the “gain” and “risk” in dollar terms and then the decision of where to set the cutoff point would be more clear cut.Recognizing the strength of individual team members and establishing a communication protocol that promotes continuous feedback during the project initiation phase is crucial for mapping tasks to resources successfully integrate the contributions and deliver quality work and service on time. Working as a team on a Data Science problem was the first for many of the team members. Thus we walked away with many lessons learned to be applied to future teambased projects.Working on Kaggle competitions as a team allows for many ideas and experiments but it is important to manage timelines and expectations so that all team members remain on track progressing towards the same end goal. We discovered that using machine learning could lead to further model experimentation through feature engineering. Also that reducing crossvalidation folds is a mixed blessing: useful initially but becomes easy to overfit. The constant flow of communication is what led to the discovery of a business insight and further exploration of the idea with classification models.Given more time:As Allstate did not reveal the true meanings of their predictors any attempt to find relationships among those predictors against the outcome (i.e. the loss) seems futile. However it is reasonable to speculate that the categorical variable “cat112” is the “State” indicator as it has 51 different values. With such speculation we attempted to find potential grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss. We chose Hierarchical Clustering as our unsupervised machine learning for this exploration.We compiled a data frame with 52 rows (51 states plus statistics of the whole dataset “train.csv” denoted with row name as  “ALL”):We scaled above figures before we calculated pairwise distances.  Then we plotted three dendrograms with three different types of linkage (single complete average).  The Dendrograms of Complete linkage and Average linkage both visually show balanced fusions starting from the bottom.  As Complete linkage is sensitive to outliers we chose the Average linkage for further exploration and we cut that dendrogram into 5 groups:We found the first cluster only has two states (as ALL is NOT a state) and the fifth cluster has only one state:We further investigated the basic statistics of each cluster:Our findings:We thought we can fold the first and the fifth clusters into the second cluster for two reasons:With such arrangement we can attempt feature engineering based on only three groups.",NA,The Dendrotrons: Allstate Claim Severity Kaggle Competition
https://nycdatascience.com/blog/student-works/predicting-insurance-claim-severity/,43,In October 2016 Allstate launched a Kaggle competition challenging competitors to predict the severity of  insurance claims on the basis of 131 different variables. Better understanding the future cost or severity of a claim is of utmost importance to an insurance company and would enable Allstate to price their plans more effectively. Additionally knowing the relative importance of different variables would allow the company to more efficiently evaluate potential customers.For this competition we applied various strategies models and  algorithms to predict the severity of an insurance claim. As we will discuss we utilized a variety of supervised machine learning methods including multiple linear regression ridge and lasso regression random forest gradient boosting machine (GBM) and neural nets. We then used ensembling to combine our models and arrive at more accurate predictions.One of the challenges within the competition was that the 131 variables provided by Allstate were anonymized meaning there was no explanation as to what the various columns described. In all there were 72 binary categorical variables 43 nonbinary categorical variables (with 3 to 326 levels) 14 continuous variables and one dependent variable “loss”. The company provided a training dataset with 188318 rows and a testing dataset with 125546 rows.We first visualized the loss variable which ranged from 0.65 to 125000. However the histogram was hard to decipher due to many outliers with high “loss” values. When only plotting the first 95% of the data we were able to visualize the distribution more clearly although the underlying data was still heavily skewed to the right.To remove the skewness we did a log transformation on the loss variable which normalized the distribution as seen below.To prepare the data for analysis we first joined the train and test dataset to account for several levels that appeared in the test.csv dataset but not in the train.csv dataset. Because of the many levels within the categorical variables we created dummy columns for each level with binary values of 0 or 1. However to reduce the number of new columns we limited the dummy columns to categories that comprised at least 2% of the variable. Lastly we applied a log transformation to the response column in order to normalize the distribution.The resulting training dataset had a total of 280 columns consisting of 265 binary variables the 14 original continuous variables and the logtransformed loss variable.To get a sense of our data and obtain a baseline against which to compare our other models we first ran a multiple linear regression model using the R Caret package. However we noticed that due to the approach we took in preprocessing our data the resulting matrix of predictors turned out to be rank deficient. This prompted us to try several linear regression models to address the multicollinearity in the data and the potential problems of matrix invertibility and nonreliability of confidence intervals. Our original model included all the variables. To try to solve the rank deficiency problem we ran a second model which excluded seven variables that had resulted in NA coefficients in the first model. Excluding these variables however did not address the original problem.  Finally a third model excluded all the variables dropped in the second model as well as all the variables that had failed to reach significance at the 90% confidence level in the first model. This model resulted in an adjusted R^2 of approximately 0.52 a crossvalidation RMSE of 0.51 and a mean absolute error (MAE) score of 1249.45 on the test data set. Inspecting the related diagnostic plots we were able to ascertain that the errors followed a relatively normal distribution. Similarly we noticed no distinctive patterns in the scatter plot of the residuals against the fitted values suggesting that the residuals also had a constant variance. These diagnostics gave us confidence in the validity of the model’s F test however given the modest accuracy of the predictions resulting from the multiple regression model we proceeded to investigate further models.We next ran a ridge regression cross validating over a grid of a large range for lambda and then cross validating again on a smaller range.  However the Ridge model returned a very low lambda value close to zero in the tenthousandth place.  This low lambda indicated a nearzero shrinkage penalty yielding results very close to that of the linear model with all variables included.  The RMSE for this model was 0.507 and a MAE of 1232.In addition we also ran a lasso model with 10 folds crossvalidation. The model returned a similarly low shrinkage penalty value of 0.0007140295 suggesting again the lasso fit would also yield predictions that would be very close to the multiple linear regression model. The model produced a RMSE of 0.507 and a MAE of 1248 performing close to the multiple linear regression model but somewhat worse than the ridge regression model.Another standalone model that we evaluated for learning to predict the loss variable was gradient boosting. With this method we tried to get a more accurate model out of an ensemble of random forests by adjusting the following parameters:The main challenge with gbm is finding the best mix of parameters especially in the choice of n.trees and shrinkage. As with all our previous models we used the Caret package to make parameter selection easier. Caret enables parameter tuning by the use of a tuning grid during training. The tuning grid can take in multiple values of each parameter and train the model over each combination of parameter values. We trained multiple models to eventually end up with the best combination of parameters.We noticed that at interaction depth of 10 and 500 trees MAE was minimized without overfitting the validation set. Trying an interaction depth greater than ten makes the model overfit early on in the iteration process. The lowest MAE score we achieved with the best tuning parameter was 1161.49.As we have seen none of the individual models performed fairly in predicting loss. However by trying to reduce the MAE of each model we were able to derive valuable insights. To achieve a better predictive performance we next proceeded to combine each of these individual models in an ensemble.To stack our different models we used the H2O and H2O ensemble packages. Stacking in H2O works by using multiple base learners on the dataset. This original dataset can be referred to as the “levelzero” data. The base learners can be many different algorithms with different parameters for each. Each of these base learners then computes its own predictions using the levelzero data. Column binding these predictions and regressing them onto the original response variable will now be the “levelone” data. Another learning algorithm will then be used on this levelone data to come up with a prediction which is better than each of the individual models. This last learning algorithm is called the “metalearner.”Using this framework we tried using a range of combinations base learns and metalearners. We started by testing ensembles featuring the default linear model random forest gradient boosting machine and neural network available and coupling them with linear random forest GBM and neural network metalearners in turn. At this stage the ensembles featuring a GBM metalearner scored best at a MAE score of 1142.Next we started adjusting some of the parameters for the base and metalearners and adding more base learners of the same type but featuring different parameter values. In this step we obtained our best result by using an ensemble that included three GBM models with different numbers of trees and one customized model for each of the other base learners. This ensemble yielded a MAE of 1125.Finally we tried eliminating some of the weaker performing base learners like the linear and random forest models using multiple GBM and neural net base learners. Our best scoring ensemble used four gradient boosting machine base learners five neural net base learner models and a ridge regression for the level one model and yielded a MAE score of 1118. Looking back on the experience one conclusion we reached was that building an ensemble that would reach a high score is as much an art as it is a science and that parameter tuning is a central part of the enterprise. We also noticed that bigger ensembles tend to score higher even when including base learners of the same type with identical tuning parameters. For instance one of our ensembles which included three gradient boosting machine and three neural net base learners with parameters that yielded the best MAE scores among individual GBM and neural net models respectively and the same ridge regression as a metalearner reached a lower MAE score than our best model even though the GBM and ridge models had the same parameter tunings.Finally from our perspective it looks like the manner in which preprocessing is conducted plays a crucial role in the ability to build a model that is capable of yielding high accuracy predictions. ,NA,Predicting Insurance Claim Severity
https://nycdatascience.com/blog/student-works/machine-learning/kaggle-competition-allstate-claims-severity/,43,Allstate Corporation is one of the largest insurance companies in the United State. The main products that Allstate offers are car insurance recreational vehicles insurance home property condo renters insurance and so on. In order to provide better claims service for Allstate’s customers the company is developing automated methods to predict claims serverity. Allstate launched this challenge on Kaggle and invited all the Kagglers to tackle down this task with their technical skills and creativity. The goal of this challenge is to build a model that can help Allstate to predict the server of the claims accuratly. At the same time to provide the important factors that have strong impact on the severity. With these information Allstate can proposed or adjust more suitable insurance packages for their customers. On top of this as a team we wanted to implant the data manipulating features engineering and the machine learning skills that we have learn to perform a strategic process and established a model that has strong capability to achieve the best prediction possible. In order to tackle down this challenge within two weeks as a team we were following the pipline showing above from the begining. The workflow is dividing into three main parts: 1) Exploratory Data Analysis (EDA): Due to all the features are annonymous EDA became a very important stage for us to understand more insights of the data. This stage is also crucial for the next stage  Feature Engineering2) Feature Engineering: Feature Engineering is the stage that allow data scientists to exert their creativity. It is also the stage that can help to differenciate the models from others. For the initial features selections we decide to use unsupervised learning methods PCA and Kmeans to see is there any possibility to reduce dimensions and cluster the features. After the first attampt we also used three different encodi09ng methods: 3) The models we have used are:The complete training data set contain    observations 130 continuous and categorical input variables and 1 continuous output variables with no missing value. The density plot of loss below provide the information about the distribution of the output variables. It indicates that the observation is skewed to the right. To prevent high leverage from the outliers we did a shifting and log transformation on ‘loss’ so it can be more normal distributed without changing the order. Second density plot is the result after traformating the output variable. From the numeric graphic analysis it inicates that the data set has 116 categorical variables and 14 continuous variables for the input features. For the categorical variables the lowest number of levels of variables is 2 levels and the highest number of levels that appears in the variables is 326 levels. With such high number of levels in the variables this is the part will be focused on during the feature engineering. After further investigation another import information about the categorical variables is that some variables containing levels only in the test set but not in the training set. The chart below lists out the unique levels for the categorical variables that have this behavior. For the continuous inputs the correlaiton plot provides the insight among the variables. The plot indicates that there are some strong relationship between some variables. This is important information for building model later on. In order to explore the potential dimension reduction the unsupervised machine learning methods PCA and Kmeans were applied to the training set. Based on the resurlts of the scree plots from PCA and Kmeans the dimensions are not able to have a siginificant reduced. For PCA with about 48 princial components around 88% variance is explained under 95%. For KMeans even with 20 clusters the withincluster variance is still very large. As the result we have to move forward with other feature engineering methods to improve the accuracy and reduce the computation expenses at the same time.In this project we did three different types of feature engineering. The findings form EDA showed that there are more than 100 categorial variables in this dataset. Most of them have two levels which is fine for modling. However a few of them have many different levels; some of them have levels greater than 100. So we did a dummy transformation first. We broke categorical variables with many levels into several dummy variables. It means that for a categorical variable when one of its levels have one observation it becomes a dummy variable having (n1) zeroes. Hence after the first step there were many dummny variables which were near zero variance (NZV) predictiors. We could keep all of them or drop the features which had NZV.  we used both of them to build models however the results were not good. On one hand removing data should be avoided if possible. We were not sure if the NZV predictors wrere noninformative. Those NZV features could in fact turn out to be very inofrmation. However keeping all of features took us long time than expected to build models and it might result in overfitting as well.On the other hand we used the function nearZeroVar from the Caret package to remove NZV predictor. By default a predictor is classified as NZV if the percentage of unique values in the samples is less than 5%. The advantage was that this method saved us a lot of time to build models however the drawback was that it increased the error and resulted in losing some potentially important information.In order to balance between time consumption and error reducing this project regroups those categorical variables over 15 levels to keep as much information as possible. After regrouping this project dummifies the new categorical variables with the rest for the third round machine learning. The good thing in this way is that useful information is kept. However there are multiple ways to group variables.Let’s first explain the reason why 15 levels here is a good cutting point. When using near zero variance to drop features for categorical variables it removes those levels with less than 5 percent of observations. For such a large dataset 5% means around 9500 observations. Even if a level having 9000 observations which must have some useful information will be removed from the dataset. For variables like cat116 which has 326 levels only 3 levels are kept after near zero variance feature engineering. Even in an ideal case only 19 out of 326 levels can be used for the further machine learning which is certainly not desirable. Meanwhile this ‘ideal’ situation also tells the fact that 20 is not a good cutting point. To be more conservative this project determines to regroup categorical variables over 15 levels. In other words keep over 5 percent of observations in each level.Here are two graphs. Each graph represents one categorical variable. Each point here is one level. The xaxis is counts. That is how many observations in each level. The yaxis is mean of the loss for each level. An interesting finding is that in most cases the scatterplot is like two straight lines one vertical one horizontal like in the left graph. The right graph Cat112 is an exception.However the ways to regroup are using the same idea: first keep the original levels having over 5% of observations and then split them based on the average loss and counts regroup them into high loss low count group low loss low count group low loss low count group and so on and so forth.By doing feature engineering in this way it also manually builds a connection between loss and those categorical variables. The next step is the third round of machine learning.The first individual supervised model we tried was Multiple Linear Regression. We mainly used this model to check would there be any difference between different feature engineering methods. The Linear model had been tried on 3 encoding methods. The first encoding method did not go through due to high number of variables has 0 variance. After dropping the variables has near zero variance the model gave the RMSE around 0.57659. On top of using the third encoding method which was to relevel the categorical variables that had high number of levels we also droped the highly correlated continuous variables to test. The model returned  the RMSE around 0.56557 this time which indicated that the third encoding method performs better on the linear model. Based on three types of feature engineering the new group gives the best results. In ridge regression the best lambda is 1e05 and the RMSE is 0.56414. In lasso regression the best lambda is 1.592283e05 and the RMSE is 0.56415.For the random forest model we used the data set from second encoding method which is drop the variables have near zero variance. The default setting was used for the initial fit which is 500 trees and 51 mtry. However the model took two days to finish the training without cross validation. Due to high computational expenses we choose to move forward to other individual models. Three Gradient Boosting model were trained:Four parameter were cross validated based on the subtest data such as ntree (number of trees) shrinkage factor depth of trees and number of the minmun obersevation in the nodes(figure 3&4)From the models with the best parameter the model 2 show the best accuracy (RMSE 0.51)but expensive computation(3 days) the model1 lower accuracy(0.54) but efficient(1.5 days)and the model3 sufficiently appoach the balance of the accuracy(0.53)and the computation(2days).Wealso tried XGBoost from Caret package to build models. Similarly the new group shows the best results. Compared with running XGBoost package directly the XGBoost from Caret already reduces the total number of parameters people can tune to seven. After trying different combinations of parameters we found that there were two of them really affected the model which were nrounds and max_depth. The rest were less relevant. The left graph above shows the result of the cross validation which gave us the best model the corresponding smallest RMSE is 0.5436. Meanwhile the most important feaure is cat80D  (see also right graph above) also similar with the results of gbm.,NA,Kaggle Competition: Allstate Claims Severity
https://nycdatascience.com/blog/student-works/book-rating-prediction-recommendation-engine/,43,"For our capstone project the team decided to create  a book recommendation engine for Barnes & Noble a traditional brick and mortar bookstore to help them increase book sales and customer loyalty. We used hybrid ensembled machine learning models (Random Forest) with collaborative filtering to make BookLab's recommendation results more creative than a simple book and author matching. We envision  to be an app that helps traditional brickandmortar booksellers like Barnes & Noble customers find the books that meet their needs whether it is for school work or leisure reading. We feel that B&N is lagging behind Amazon the current marketleader in terms customer engagement and monetization using data analytics. Their website allows customers to search for books by title author or category. However it is not possible to get a book recommendation based on your personal preference such as your favorite books.In the example above a customer searches for Harry Potter and the search engine provides all Harry Potter books and memorabilia. However there were no similar books recommended feature unlike in Amazon. Searching for the same book in Amazon will also provide the customer a selection of similar books not necessarily from JK Rowling. “Diary of a Wimpy Kid: Double Down” by Jeff Kinney and “Magnus Chase and the the Gods of Asgard Book2: The Hammer of Thor” by Rick Riordan was part of the Top 5 of algorithm recommendation. Presenting these 2 books of similar genre but different authors motivates the customer to explore new books. This is beneficial for both the customer and for the bookstore. We want to help Barnes and Noble’s customers find great books that will inspire them make them laugh make them cry and invoke their curiosity. Books that will make them a booklover and continue to go to BN’s website to discover new books. To do this we will: We sourced book ratings data from the University of Freiburg’s Department of Computer Science which scraped data from the BookCrossing website with the permission of the website owner in 2004. The dataset contained   (anonymized but with demographic information) providing  (explicit / implicit) for  .Similar to real business scenarios our dataset had missing information and formatting issues. Upon inspection we saw that we had to deal with the following challenges:Our approach was to preprocess the data carefully ensuring that we preserve the original data structure as much as possible to avoid inducing bias. The appendix below details this process. Apart from this dataset the team gathered information from Google Books API and Goodreads API to gather the following features:To have a quick understanding of the reader demographics we created a geographical map to plot their location. BookCrossing has users from all over the world with majority of the readers coming from United States. There were also readers from the African continent namely from Egypt South Africa and Nigeria.It is interesting to note that on average most of the low ratings came from Bookcrossing users between the ages of 1618. On the other hand most readers in their 30s rated their books higher on average.It would be interesting to find out what drove these younger readers to rate books lower. That will be in another post.Fiction represented by green points seemed to be the most read genre by bookcrossing members. This needs to be taken with a grain of salt as it seems like there is sparsity of other types of books that are nonfiction. It appears that it is more common for shorter books with less than 250 pages to be rated low. The same pattern seems to be visible for books with 750 pages and above. After further research we found out that 
Since the team found more value in determining which books lead to high or low satisfaction rate books with no ratings in our overall dataset was excluded in the training dataset. Below are 2 graphs breaking down rated and unrated book titles count.The team decided to use Logistic Regression and Random Forest to perform a 10 multiclass prediction with the expectation that the Logistic Regression model will allow us to have a highly interpretable model which would be easy to explain to B&N's management. On the other hand the Random Forest seems more robust for our dataset type given that it is better in dealing with uneven distribution and outliers. Therefore theoretically giving us a more accurate prediction in terms of sensitivity and specificity.We saw that most of the reviews were 810 and realized that we were faced with an imbalanced target class challenge. First we performed a KFold crossvalidation split on the entire rated dataset.Performing crossvalidation and then model fitting on the rated dataset with 10 classes for prediction resulted to low accuracy rate low sensitivity and low specificity.To improve the predictive power of the models team revised the problem into a 3class prediction. With this revision the model performed better overall with a higher AUC.This updated model performs much better than random classification and the previous model. However a third adjustment can be done by the team using resampling methods that under samples the majority class in this case the High Rating class and oversampling the minority class Low Rating.In a separate blog post the team will test penalized models which imposes additional cost for making incorrect minority class prediction such as penalized SVM and penalized LDA to see if they will perform better than model 2.The team recognizes that it is essential that BookLab is able to offer higher sensitivities in detecting lower rated books. We want to decrease the chances of our users encountering lower rated books as part of its trustbuilding efforts.The team used a distancebased similarity scoring algorithm called  to build BookLab our recommendation engine. Since the  has many zero implicit ratings we replaced these ratings with average ratings when possible [for detail please review AppendixB]. Then we retested our engine with the enhanced dataset.And we observed that the number of ratings available to CF do impact the recommendations made by the engine.*** For detail please review AppendixC.In addition to basic ETL functions for loading data and transforming data structures we have three main types of functions:*** For detail please review AppendixB.We have tested the function  and  with:We cannot confirm if any recommendations made are valid since: But we do observe that the number of ratings available to CF do impact the recommendations made by the engine.Although Marcel Caraciolo’s collaborative filtering algorithm is simple it does provide us with many basic functionalities to build a book recommendation engine.We believe we can build a more sophisticated engine by combining the important features we identified in the machine learning part of this project. For example “category” and “publisher” are two important features we identified.  We can expand our data with “category” and “publisher”.Let’s say we have one million useritemrating available. BasicallyMoreoverWe wanted to have a userinterface for both the clients and the customers to experience BookLab’s recommendation system. This initial version uses Python language to perform Collaborative Filtering and show the results with a GraphLabUser Interface. The GraphLab syntax and objects are quite similar with Python making it faster for the team to learn it and design an interface very quickly. For example GraphLab has its own version of data frames and arrays called SFrames and SArrays.This initial version allows users to get a book recommendation using an ISBN or book title. The idea is users can type in their favorite books (its ISBN) and BookLab will provide them 5 new book recommendation.BookLab can be implemented by Barnes and Noble using their own proprietary dataset. We expect that with the rich dataset they have from their BN Members and over 6 million books BookLab will render more accurate book classification and recommendation results compared to the more limited dataset the team used for this capstone. The recommendation algorithm can also be used for more personalized email marketing campaigns to BN members wherein every month TopMatch books alerts will be sent instead of generic email ads.1. To provide recommendation for a user 2D prefs matrix will have users as rows and items as columns; i.e. rating stores as prefs<user><item>.  We provide two prefs matrices:2. To provide recommendation for an item 2D critics matrix will have items as rows and users as columns; i.e. rating stores as critics<item><user>.  We provide two critics matrices:",NA,BookLab: Helping You Discover New Books With Machine Learning
https://nycdatascience.com/blog/student-works/tree-troubles-predicting-sidewalk-damage-resulting-trees-nyc/,43,"The ultimate dataset consisted of the following features ""tree_id"" ""year"" ""tree_dbh"" ""health"" ""spc_latin"" ""spc_common"" ""root_stone"" ""root_grate"" ""root_other"" ""trunk_wire"" ""address"" ""zipcode""  ""boro_name"" ""longitude"" ""latitude"" ""block_code"" ""sidewalk"". Details on these terms can be found at the dataset link above. Clustering was done by first generating a dissimilarity matrix using the “” distance then using the “” function to find the best number of clusters. Using sample datasets (1000 obs) containing all the geolocation related features (i.e. address zipcode boro name longitude latitude) the optimal number of clusters found is 6. These clusters more or less corresponds to the boro the trees are located in. See image below.Removing all geolocation related features with the exception of longitude and latitude the optimal number of cluster is now found to be 2 which corresponds to the sidewalk condition of either damage or not damaged.",NA,Tree Troubles -- Predicting Sidewalk Damage Resulting From Trees In NYC
https://nycdatascience.com/blog/student-works/rate-hiking-not-investigating-one-model-predict-fomc-decision-interest-rate/,44,"Low interest rates have been part of the Federal Reserve’s monetary policy since 2007 when they were put in place for the postrecession recovery effort. At the end of 2015 the Fed decided on a rate hike for the first time since the crisis and it is expected to continue raising rates multiple times in 2017.However the Federal Open Market Committee (FOMC) doesn’t always follow market expectations when it makes its decisions which can cause huge global capital market volatility. It is valuable to foresee the fed's action and make use of the stock market volatility. So the big question : The interest rate in an economy not only reflects but influences economic activities.If the interest rates is too high people can't afford the price of loans to invest in new equipment new houses and new employees.Conversely if the interest is too low there is huge risk of overheating the economy which will cause bubbles (for example in the stock market) or the inflation that happens in a lot of developing countries now where prices go up as a result of too much money chasing too few goods.How can we avoid the market volatility?Usually the Fed tries to balance economic activity and inflation pushing the jobless rate as low as possible while also ensuring that prices don’t rise too quickly or too high. The question is: Assuming the Fed makes the decision on the US economic data I scraped 100+ of the most popular US economic data ( .csv files) from """" with Python (Scrapy and Selenium).In order to answer the questions above two kinds of models could be considered :The linear regression model can predict the interest rate based on US economic activities. Meanwhile the classification model can predict the Fed's action (0 means no change 1 stands for increase 1 for decrease).Most of the variables could be removed for several reasons:Figure 1 shows annual data creates many missing valuesThe interest rate is kind of oscillating variable that ranged from 020% so all the variables changing over time should be transformed to oscillating variables.The change rate was used to replace the number (right part of figure above)After feature engineering the linear regression model was built to predict the interest rate and 7 variables were significant.where is the bank loan rate when bank loans increase the interest rate should be raised to reduce the risk of bubbles and inflation; is the real export rate while the Real Export rate grows faster more money flows into the market and the Fed should raise the interest rate to withdraw the extra export earned.stands for the value of US dollar the more expensive the dollar is the higher risk of overheating the economic and the higher possibility the Fed raise the interest rate. is the personal consumption expenditures rate. The rate of consumption increase which always caused by prices going up faster. The Fed could raise the interest rate to avoid inflation. is the unemployed rate. The higher the unemployment rate the lower interest rates are required to encourage the investment.RGDP is the rate of GDP growth. The higher the rate of GDP growth the more money is required in the market for the economy to function. If real GDP grows too quickly the Fed could reduce the interest rate to lower the price of loans and lower risk of deflation.is the total vehicle sales. Vehicle sales relate to mortgages which means more and more vehicle sales require the lower the price of loans (decreasing the interest rate).The statistic of the model is below:The added variable plots above shows the contribution of each additional variable. Distinct patterns are indications of good contributions to the linear model.In order to evaluate the assumption of the linear regression four diagnostic plots (below) were brought up. The upper left plot shows the residuals (the vertical distance from a point to the regression line) versus the fitted values. Note that three points are numbered 667 373 and 706 which does not necessarily indicate a problem but does mean we need to pay special attention to them.The upper right plot is a Normal QQ plot of the residuals. Recall that one of the assumptions of a leastsquares regression is that the errors are normally distributed. This plot evaluates that assumption. Here three points 667 373 and 706 lie pretty far from the dashed line especially the point 706 indicate large deviation from the line.The lower left plot is identical to the upper left plot. The only difference is the square root of the standardized residuals on the yaxis which the residuals are rescaled so that they have a mean of zero and a variance of one. This plot eliminates the sign on the residual with large residuals (both positive and negative) plotting at the top and small residuals plotting at the bottom. The red line shows the trend and the variance in the residuals shouldn't change as a function of x which means the red line should be relatively flat. It is here except at the far left end where several points with near zero or negative fit values pull the line down and points 373 and 706 at the right end pull the trend up.The lower right plot shows the standardized residuals against leverage. Leverage reflects both the distance from the center and the isolation of a point. The plot also contours values of Cook’s distance which measures how much the regression would change if a point was deleted. Cook’s distance increases due to large leverage and residuals. If one point far from the center with a large residual can severely affect the regression. On this plot point 706 has a large Cook's distance (>0.5) and point 622 with large leverage has a big Cook's distance but still smaller than 0.5.As a result of the regression diagnostic four data point with larger residue or greater leverage need further investigated which happened in 19810101 20011001 20050701 and 20081001.  From the table below we can see large difference between the real interest rate and the prediction. :The Fed kept the real interest rate too high which caused economic recession.  : From 2000 to 2001 the Federal Reserve in a move to protect the economy from the overvalued stock market made successive interest rate increases but after  attacking the economy need lower interest rate to reduce the cost of investment and rebuilt. As the result of Fed kept the interest rate higher : In 2001 Alan Greenspan dropped interest rates to a low 1% in order to jump the economy after the "".com"" bubble comparing with the model that show the real interest kept too low which indicated the housing bubble in 20052006.  In order to rein the housing bubble the Fed started rate hiking with too fast steps from 2005 which caused . Also as the real interest rate is too high the  occurred in 2007. (the only data point with the Cook's distance >0.5): Since the advent of the global financial crisis of 2007–08 and since the riskfree shortterm nominal interest rates were either at or close to zero a policy termed 'quantitative easing' (QE) have been used by the United States the United Kingdom and the Eurozone. QE is essentially a new way of raising the money supply in the economy.  Since interest rates have been near zero the whole world has had trouble using just monetary policy to jump out of the .Also from the figure below comparing the prediction and the real interest rate over 40 years we can find the model distorted after 2008. Due to the irregular QE QE2 and QE3 and the material negative interest rate the real interest rate has trouble reflecting the economy which is abnormal in history with bubble in stock market but lower inflation and economic recession.If the points with large residue or great leverage were removed the Rsquared value would increase by almost 0.1 which means a great improvement on the model. Meanwhile this would then easily point out  these major policy events.The new model point out the larger singularity between the prediction and the real interest rate which can foresee the crisis while the gap beyond one threshold.Now the linear model can help predict the value of the interest rate which can foresee and direct the Fed's decision.Can we directly predict if the Fed take an action on the interest rate?Data were trained with best  but due to the unbalance of the data in each class large error occurred in the classes ""increase"" and ""decrease"".As the selected feature without multicollinearity multinomial loglinear models via neural networks was used to predict the Fed's decision.From the linear model and figure above we see the gap between the prediction and real interest rate which reveals the potential raise of the interest rate.but both the random forest and the neural network predict higher possible that the Fed will keep the current interest rate. The Fed has many economists making very advanced models about the economy. For this project I wanted to see if looking at a few basic features could model past Fed actions. The main features made intuitive sense but I would still trust the Fed’s econometric modeling processes to determine how to change a metric as important as interest rates going forward.",NA,Rate Hikes or not? Developing a machine learning model to predict Fed interest rate decisions
https://nycdatascience.com/blog/student-works/help-airbnb-hosts-set-renting-price-hint-hotel-price-tripadvisor-web-scraping-project/,44,Python Scrapy is  a powerful application framework for extracting structured data from web pages you could extract data and generate a data file from websites based on your interests and needs and also it offers the capability to easily postprocess and persist your data. This technology resolves a big problem for me especially when I could not find an ideal dataset for a data science project. The problem i am interested in is what is the difference between Airbnb and TripAdvisor? Could I use hotel price to help Airbnb hosts setting their listing price? Let’s do it. Airbnb as a popular peertopeer online homestay network enabling people to list or rent shortterm  in residential properties with the cost of such accommodation set by the  owner. So setting a correct rent price for Airbnb hosts is one of the keys to maximizing revenue. While hotels industry has already used dynamic pricing for decades for controlling prices and maximizing revenue how would hosts go about setting their prices? Could we get some hints from hotel price trends for helping Airbnb hosts adjusting their listing price? Let’s Scrapy some data. In this project the trends of the hotel price could be web scraped from TripAdvisor.com. I would like to compare the price differences between Airbnb and hotel prices and I also would like to measure the differences between services. If it is possible I would try to develop an Airbnb price suggestion model based on the scraped information.Let’s assume we would like to book a 2 people travel to Orlando FL during Dec 2327 2016.The information I would like to scraped from those websites are listed below. listing URLs in the search results locations of listings prices (night price cleaning fee tax service fee) room type ( private room entire home shared room) people capacity bed type amenities cancellation rules and review score. hotel names hotel addresses hotel reviews review tags amenities hotel stars and hotel price ranges. Due to the limitation that current hotel prices are updated from other hotel booking websites through TripAdvisor this information could not be acquired. Fortunately there is a hotel price range information in the TripAdvisor hotel listing which could be adopted as a good hint for price suggestions.Figure 1  shows the scrapy process of getting data from a website. In the “Create Spider” part a Scrapy project can be created with a working directory which stores a number of project files (scrapy.cfg items.py pipelines.py settings.py spiders).  Within the Scrapy project folder I need to write code in those files to realize different functions as below.: containers that used to collect scraped data. : create a spider.py define Python classes (start_urls Xpath) according to a certain site (or group of sites ) you would like to scrapy; construct appropriate XPath expression to extract the data from the websites here I use Chrome which provides a developer’s tool for inspecting the structure of web pages (right click and then inspect): define Python classes that sequentially process the dataAfter I finish and validating all of the coding files I could deploy the spider by using “scrapy crawl” command. In the “Scrapy Engine” part once an item has been scraped it is sent to the item pipeline here the pipeline is used to validating scraped data storing the scraped items in a file or a database in the “Data Downloader” part. Finally a csv/txt file with data could be generated by using Scrapy. Based on review scores of Airbnb listings the price range is as shown in Figure not big differences among listings with different review scores. The median prices with different review scores are around $50 the price range is wide among Airbnb with 4.5 and 5 in reviews. The review score is a comprehensive evaluation based on Accuracy Communication Cleanings Location Checkin and Value.Figure 3 &4 show price differences among different hotel stars.  are often used to classify hotels according to their quality price differences is obvious among hotels with different starts and generally higher than Airbnb rent prices. Statistics visualization of amenities is shown in Figure from which we could get a general idea that the top 10 amenities Airbnb hosts provide include Air Conditioning Wireless Internet Free Parking on Premises Heating Internet Shampoo TV Dryer Washer Family Kid Friendly. Whereas in the hotel amenities the top 10 ones are No Smoking Rooms Wheelchair Accessories High Speed Internet(Wifi) Free Internet Pool SelfService Laundry Family Rooms Multilingual Staff. The big difference in Amenities between Hotel and Airbnb is that Hotel has more advanced  amenities service such as Wheelchair Meeting Rooms International Drive Hot Tub Bar Lounge  Airport Transportation.I assumed the differences in price is caused by the amenities difference between Airbnb and hotel service an average Airbnb listing is just a less functioned hotel room. Based on this assumption I build a random forest model let’s check what features affects hotel price and Airbnb price most.,NA,Web Scraping Project: Experience Difference between Airbnb and TripAdvisor
https://nycdatascience.com/blog/student-works/behind-flightdeal-scrapy-project/,44,As I mentioned before there can be inconsistencies in the data you want to scrape. Structurally a website is usually wellorganized but without a schema or some sort of validation the data itself can be wildly inconsistent. Here are a couple of examples from Fare Availability on The Flight Deal:,NA,Behind the Flight Deal Scrapy Project
https://nycdatascience.com/blog/student-works/pains-growing-e-commerce-business-etsy/,44,28143 observation and 7 features available for analysis. After preprocessing the data I performed graphical and numerical exploratory data analysis using R. What follows are my initial findings.,NA,The Pains of Growing an e-Commerce Business: A Case Study on Etsy
https://nycdatascience.com/blog/student-works/scraping-nsf-awards-create-database-active-stem-researchers/,44,There a numerous use cases for having a searchable database of active STEM (Science Technology Engineering and Math) researchers. For example targeted marketing by companies interested in selling services and products or helping students select the best research institution and mentors based on their interest. Further it can to provide an additional means of getting an overview of active research areas. Unfortunately no so such database is readily available even though most of the needed information is readily on various websites i.e. faculty profiles journal sites etc.An obvious approach for creating this database is of course manually searching through the above mentioned websites. However this would be very time consuming. A more efficient method would be to use web scraping to automate the data extraction process. Unfortunately a small scale test indicated that there is just too much format variation in the relevant webpages to make web scraping practical. For example even within departments at the same institution there is often different formats used for faculty profiles. Moreover since these profiles are infrequently updated the information they contain is often out of date.,NA,Scraping NSF Awards to Create Database of Active STEM Researchers
https://nycdatascience.com/blog/student-works/can-avoid-mouse-clicks-starbucks-store-amenities-locator-web-scraping-project/,45,As I’m walking from Grand Central Station to the NYC Data Science Academy for my first day of class I’m thinking of which Starbucks Coffee store serves breakfast sandwiches. It is a journey in trial and error and wasted time as I walk into stores along my path until I find the right one.  Providing a solution to this problem was the basis for project three of the NYC Data Science boot camp. The scope of the project (solve a business problem using web scraping technology and present your insight) was a great opportunity to use Scrapy (a web scraping framework) for data capture R Studio for data analysis and CARTO to prototype a web based product. My solution allows end users to view the Starbucks Coffee Store amenities and their locations in one place. Go ahead give it a try .The Spyder framework integrates web scraping and Python programming for a flexible and adaptable solution to capture and process webbased content.  R Studio provides a smooth interface and great libraries for EDA to gain insight from the data. The CARTO dataset upload and mapping process is intuitive and allows you to visualize your data on base maps within minutes. Five least common amenities:Combining Open Source and vendor applications (Scrapy R Studio CARTO) allowed me to deliver an  that uses a website as the data source within a two week time line. The web app prototype enables end users to visually explore analyze and find Starbucks Coffee stores with the most / least common amenities. But most importantly you can view a store's amenities with a minimum amount of clicks.,NA,Which Starbucks Coffee Store Amenities Are the Most Popular?
https://nycdatascience.com/blog/student-works/r-visualization/europe-vs-usa-kyoto-protocol-successful/,45,"The Kyoto Protocolwhich was signed upon on December 11 1997 is an international agreement linked to the United Nations Framework Convention on Climate Change. Under the Kyoto Protocol signatory countries agreed to drop total emissions to 1990 levels. A total of 192 countries signed the agreement nearly all European countries. In the US the senate expressed total disapproval of the treaty by stating that it : “"".In order to compare the efforts of a Kyoto(Europe) and non Kyoto(USA) partner the data of the CO2 emissions have been extracted from the annual monitoring reports of the World Bank. Since 2007 marks the 10 year anniversary of Kyoto protocol our comparison will include CO2 emissions between 2007 and 2013 (the last available measurement). In order to assess the level of CO2 emissions I developed an interactive Shiny app to compare both Europe and the USA from 2007 to 2013. I define “Europe” as the countries of the European Union plus Iceland Norway Switzerland Ukraine Serbia Kosovo Moldova…. An explanation of the different features of the Shiny app is presented below.The main goal of Kyoto protocol was to drop the total CO2 emissions per country to the 1990 levels. Since the population of the world has increased and that some countries as China and India have significantly increased their level of CO2 emissions dropping these total emissions levels of CO2 as stated by Kyoto seems impossible regardless of the efforts of both Europe and the US. Recently CO2 emissions per capita have been considered as a ‘fair’ measure to assess the level of effort made by each country. Fig.1 shows an interactive map of both the US and for European countries comparing  CO2 emissions levels per capita for each year.Fig. 1 : Interactive map of CO2 emissions (metric tons per capita) for both the US and European countriesAnother aspect of assessing CO2 emissions for both Europe and the US is to look at the evolution of the CO2 emissions per category including: transport residential (households and private companies)industry and electricity consumption. Fig.2; shows an interactive bar plot of the evolution of the percentage of CO2 emissions per category for both the US and European countries per year.Fig. 2 : Interactive barplot of the percentage of CO2 emissions per category for both the US and European countriesCO2 emissions per metric tons per capita for each year between 2007 and 2013 with the levels Kyoto protocol per capita.Fig. 3 : Interactive barplot of the levels of CO2 emissions (per metric tons per capita) for both the US and the European region. It compares the total levels per year and Kyoto levels.From comparing the data of CO2emissions in both Europe(a Kyoto partner) and the US (a non Kyoto partner) we can see that both regions made an effort to reduce CO2 emissions per capita over the past decade.Even if the Kyoto protocol was about the total level of CO2emissions European countries dropped 1 tonne per capita but didn’t meet the target levels of Kyoto protocol CO2emissions per capita. On the other hand the US has been successful in reducing the level of CO2 by more than 3 tonnes per capita comparing to the stated goals of the Kyoto protocol.In order to better determine the efforts of each region of the world to reduce their impact on global warming a comparison of the evolution of investments and the share of green energies could be a better way to measure the commitment of each country to reduce CO2 emissions.",NA,Was the Kyoto Protocol Successful? A Comparison of Europe and the USA
https://nycdatascience.com/blog/student-works/healthy-life-world-health-development-indicators-explore/,45,Data DescriptionA development indicator is usually a numerical measure of the quality of life in a country which is used to illustrate the progress of a country in meeting a range of economic social and environmental goals.The World Development Indicators (WDI) (http://data.worldbank.org/indicator) from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world. It is the primary World Bank collection of development indicators compiled from officially recognized international sources. It represents the most current and accurate global development data available and is updated quarterly covers from 19602015. The categories in the WDI include Agriculture & Rural Development Aid Effectiveness Climate Change Economy & Growth Education Energy & Mining Environment External Debt Financial Sector Gender Health Infrastructure Labor & Social Protection Poverty Private Sector Public Sector Science & Technology Social Development Trade Urban DevelopmentThe data source is from Kaggle.()In this project we select 8 indicators from the Health Nutrition and Population (HNP) category for data exploration from 19952012. The data covers 186 countries divided into 7 regions (East Asia & Pacific Europe & Central Asia Latin America & Caribbean Middle East & North Africa North America and SubSaharan).: Improved sanitation facilities measured by % of the population with access.: Health expenditure per capita measured by 2016 US dollar.: Number of Physicians per 1000 people.: Life expectancy at birth total measured by years: Total PopulationFeatures related to a specific disease:: Tuberculosis treatment success rate measured by % of new cases.: Tuberculosis case detection rate measured by % all forms.: Number of incidence of tuberculosis per 100000 peopleStructure of the WDHI Shiny AppThis Shiny App adopts a Navbar style and includes three tabs to examine indicators: “By Year” “By Country” and “Reference”. By clicking the “By Year” tab the user could visualize a bubble plot revealing Life Span versus other indicators among different countries . The user can also see trends over years by adjusting the sliding bar. By clicking the “By Country” tab observations based on each individual country are revealed along with a histogram plotted based on year. According to the “Reference” tab  the user could get access to the whole dataset and search according to year or indicator name.  The tab also displays the data source link and a brief introduction to the indicators.How to Explore the Data with the Shiny AppBelow is a data visualization example of Sanitation Facilities and Life Span in 1996. In the plot each bubble represents a country and the size of the bubble depends on the population of the country. Clicking on the bubble shows the values of the improved Sanitation Facilities Life Span Region and Population.When the year slider moves the change between different years is revealed. We can investigate countries with improvements as well as poorly performing regions.We can also use visual inspection to find outliers. For example in the plot below Equatorial Guinea’s Sanitation Facilities value at 1996 is 81.1% while the Life Span value is only 50.7 which lags behind other countries with similar Sanitation Facilities. Countries in SubSaharan region also have a lower Life Span compared to other regions. People in Europe and North America countries have longer Life Spans. More indicators can be selected through the dropdown menu.Below is a data exploration example in the “By Country” tab. The graph examines Health Expenditures of the United States from 1995 to 2012.Observing the trend the value is always increasing and it arrives at nearly $9000 per capita in the year of 2012. For every year’s Health Expenditure distribution other countries lag far behind the United States. In the next example below  Chinese Health Expenditures also increased during those years. However they remained in the lower part of the world health expenditure distribution based on where the health expenditure rate falls on the histograms for each individual year.The last part of the Shiny App includes a data exploration tool (shown below). Country year and indicator name can be filtered individually to  look for insights on a more detailed level.The user could also use the Tuberculosis series indicators as a case study. It includes the success rate detection rate and number of incidence. The yearly granularity of charts can reflect a country’s health development performance in handling disease control since 1995. For another example users can check the number of physicians in each country and find its correlation to tuberculosis life span or improved sanitation facilities. ConclusionThis Shiny App provides several options to investigate the world health development indicators and impacts of specific indicators on other facts with years. These can be useful to develop some prediction models to develop specific strategies for countries and regions.,NA,Quantifying Uneven Health Development in the World—A World Development Indicators Data Exploration Shiny App
https://nycdatascience.com/blog/student-works/know-xbox-360-games-coming-xbox-one/,45,Microsoft surely takes many factors into account when prioritizing their games but I needed data to analyze so I could think more like them. I decided to collect data from websites using Scrapy a free Python framework designed to allow a programmer to use various methods to extract data from a web page.I grabbed the list of all Xbox 360 games and extracted every single piece of information I could find on their pages including publishers developers user review scores and their counts release dates Smartglass features demo availability multiplayer types sound features and the number of addons themes gamerpics and the like.I then scraped game names users and professional scores from Metacritic along with several other factoids like release dates in case another site is lacking.I know Microsoft has said in the past that they look at UserVoice votes to help make choices so I scraped the votes and forum comment counts.Microsoft has prioritized their first party titles and I believe exclusivity should increase the likelihood of becoming backwards compatible. I retrieved the differentiations of Xbox 360 exclusives (console and regular) from Wikipedia.I grabbed the names and requirements of Xbox 360 Kinect games (required or supported) from Wikipedia. Remember that this is important because Microsoft says they won’t support Kinect required games but games with optional Kinect functionality left me wondering.I manually compiled a list of games that required peripherals. This was greatly helped by UserVoice which includes games that will not be made backwards compatible for various reason.It is important to find how much an Xbox One version of the game in the form of remasters remakes or crossplatform releases had any correlation. I needed to make a manual list of remasters but I decided to scrape certain sites for references to keep me up to date. I started with the list of Xbox One games off Microsoft’s site and I supplemented it by scraping a GameInformer article that gets regularly updated with the latest remasters. Based off these and further research I manually made a new list of games that could be found on both platforms.,NA,I Know Which Xbox 360 Games Are Coming to Xbox One
https://nycdatascience.com/blog/student-works/pokemon-tracker-select-places-using-statistic-inferance/,46,Figure 2 select the city: Los Angels and adjust the zoom level to 16 and this show very small range and explicit spawn locationFigure 4  the zoomed central park map showing  the explicit pokemon spawn location and the traffic.,NA,Pokemon tracker in selected places using inferential statistic
https://nycdatascience.com/blog/student-works/r-shiny/can-learn-traffic-patterns-using-911-call-data/,46,Past occurrences can serve as a useful guide shedding light on trends and informing future decisions. In this blog post I present an interactive application that leverages 911 call data from Seattle to reveal patterns in traffic related emergencies. The application uses Shiny Dashboard as an interactive platform and a data set of 911 call data released by the Seattle Police Department.Trafficrelated emergency calls are the most frequent type of 911 call  placed in the Seattle metropolitan area. While the original data set contains information about other types of call such as disturbances liquor violations burglaries assaults and many others I limit the focus of this post to trafficrelated calls.The application creates interactive choropleth maps employing the sp and ggplot2 R packages where users can visualize the total number of calls by neighborhood and top locations according to specific parameters. A first panel of the application allows users to visualize the distribution of the number of calls by neighborhood and hour and at a much higher level of granularity  identify locations (hotspots) where over the time frame of the dataset 2010  2016 there have been at least 20 trafficrelated emergency calls registered. These smaller locations have been obtained by dividing the polygon created by the largest and smallest call latitudes and the largest and smallest call longitudes into 1000 equal sectors. One trend that becomes apparent by playing with the 'Hour' slider bar is that the number of hotspots with 20 or more calls during a particular hour varies throughout the day. As we can see from the screenshots above between 3 and 4 am there is only one such hot spot. By contrast between 8 and 9 am we observe 20 hotspots. As a trend the number of hotspots falls during the night and parts of the late afternoon and is high during the morning and evening.The information bar above the map provides the total number of calls registered at the hotspot with the highest number of calls as well as its geographic location. This feature reveals another interesting pattern. The number of trafficrelated emergency calls at the busiest hotspot decreases during the night and late afternoon and peaks during the morning afternoon and late evening. As the screenshots below indicate the lowest number of emergency traffic calls at the busiest hotspot occurs between 6 and 7 pm and the highest between 9 and 10 pm.It is useful to note that even though not visible from the screenshots above the neighborhoods with the highest numbers of calls do not stay the same throughout the day. In addition perhaps the most counterintuitive insight resulting from the analysis by neighborhood and hour is the discrepancy between hotspots location and neighborhoods with the highest number of calls. This suggests that while high in volume at a particular time in a neighborhood trafficrelated emergency calls may be well dispersed over the area of the neighborhood and also that even in neighborhoods with a relatively low number of calls there exist small where these calls are concentrated and which demand user attention. Finally the location of the top hotspot migrates from Northern Seattle in the morning to the city center in the morning and afternoon and further south to West Seattle in the evening.A second panel of the application enables users to visualize the distribution of emergency calls by neighborhood and weekday. Hotspot area is determined as before but this set of maps overlays the op ten hotspots by total number of calls. An interesting pattern observed here is that except for Sundays there is is only one or no trafficrelated call hotspot in Northern Seattle. In addition Saturday is the day when top neighborhoods in terms of volume of calls display the highest number of emergency calls.The application can provide similar insights for the different types in the Seattle 911 calls dataset. A next step in developing the application therefore will be to include these in different types of calls.,NA,What can we learn about traffic patterns using 911 call data?
https://nycdatascience.com/blog/student-works/mpgview/,46,there are 797 cars owned per 1000 people in America What is described here is a Shiny app built in R that provides multiple perspectives on gas usage in cars.This app can be useful when looking for a car.  For example while everyone know that SUV’s use more gas than a Tesla let’s say there are still choices to be made among SUV’s.  A Lexus RX Hybrid is a better choice than a Range Rover in terms of expected expenditure on gas.,NA,MPGView: Find the Vehicle Best for Fuel Economy
https://nycdatascience.com/blog/student-works/predicting-visualizing-pokemon-go-using-k-nearest-neighbors/,46,Motivation:Functionality:Visualization and Prediction:Density and Distribution:Data Source:Insights and Future Updates:Appendix:KNearest Neighbor Algorithm:,NA,PokeViz: Visualizing and Predicting Pokemon Go using k-Nearest Neighbors
https://nycdatascience.com/blog/student-works/r-shiny/taiwan-hospital-resources-dashboard/,46,Since Taiwan implemented its National health insurance program in 1995 the number of hospitals staff and medical resources has kept growing. However there are still a lot patients waiting for bed availability. Doctors and nurses are still working overtime constantly. The injured are not able to be delivered to the nearest hospitals in time when multiple casualty incidents happened.Health care is one of the topics that the public has difficulty understanding due to its complexity and low transparency. However it has a very tight connection with our daily lives. Therefore I would like to give a general overview about the hospital resource allocation in Taiwan including the distribution of hospitals staff and hospital beds. I will also investigate how the allocation trends changed from 2006 to 2015 in each county..All the data merging cleaning and calculating was performed in R and the project is created in a . All the code can be found on .The data in this project is from . The features used are :The first tab gives a general overview of hospital resources in Taiwan. Users can select multiple cities to compare the statistics in different categories including the number of beds the number of staff and the number of patients. The Shiny app will automatically compare different types of sub features in that category. Finally they can see the statistics of each year from 2006 to 2015.With the capability of a googleVis motion chart the app can perform more nuanced visualizations of the data. Users are able to select the data by total count or per Capita.Within the chart users can select different variables for both the Xaxis and Yaxis. They can also select different variables for dot size in the scatter plot and select different variables for colors in the scatter plot bar graph and line graph. Each dot/bar/line represents a different city and users can select multiple cities to track the trails for the yearly trend. Lastly the users can click the play button or drag the bar to see the movement as a trend. Therefore users can see the different associations between variables over time.The third tab shows the bed occupancy rate by region. Users can select occupancy rates for different types of hospital beds and the map will show the occupancy rate in different regions by colors. The darker the color is the higher the bed’s occupancy rate. Users can also see different results in different years by selecting the “By Year” menu.In this tab the map shows the relationship between geography and the bed occupancy rate.After exploring the app some of the most interesting findings were:To perform a more diverse and deeper analysis the app can incorporate more data.To understand the allocation of different types of staff and beds for different departments the subcategory data can be added in such as the number of doctors for General Medicine General Surgery Neurology Ophthalmology and so on.To assess the allocation of hospital resources the death rate of each city can also be incorporated into the app.Even more steps ahead the app can add in National Health Insurance data in order to analyze the efficiency of the use of financial resources.,NA,Taiwan Hospital Resources Dashboard
https://nycdatascience.com/blog/student-works/dashboards-decision-making-shinyapp/,46,The ability to see how each market performs and to compare them with one another is essential especially for globalbrands like Starbucks. Starbucks groups its operations in 3 different geographic regions: the Americas Europe Middle East and Africa (EMEA) and China and Asia Pacific (CAP). The 4th chart shows the consolidated sales growth rate for all 3 regions. Immediately the Starbucks executive can see that even if overall the global sales growth rate has been increasing EMEA is not contributing to global growth. There is no need to initiate a longarduousprocess of manipulating financial data.,NA,Dashboards & Decision Making: A Glimpse of What Shiny Can Do
https://nycdatascience.com/blog/student-works/bikes-go-analysis-nyc-citi-bike-station-capacity/,47,I have been fascinated by the number of people riding NYC Citi Bikes to and from work. I’ve asked a few Citi Bike subscribers about their experience and yes some include dealing with a pothole or two; therefore I was interested in the logistics. How are the stations replenished and how do users typically deal with bike capacity constraints when they really need a bike. Do they go to a nearby station or hail a cab? My second project at the NYC Data Science Academy boot camp (build a web based product addressing a business problem using Shiny and RStudio) provided me with an incentive to further research these questions and drive towards the creation of an interactive web based solution. My cohort peer James Lee told me stories of riding a Citi Bike during his undergrad days. And having to go to a station at certain times (before classes would finish) to ensure that he would get a bike. Otherwise he would be left to wander on to the next station to find a bike. My experience with the NYC Citi Bike Station Map website led me to think about how users determine if there will be enough bikes or docks at a given station or destination? And would a ride from point A to point B result in a surcharge if the ride duration were to go above the purchased pricing  (30 mins for day riders and 45 mins for subscribers)?locate feature allows you to look up one address at a time which results in a popup on the map displaying the number of available bikes and docks. You can filter on bikes or docks to reduce the number of markers on the map. However I felt these features were limited. Features that would allow users to determine bike / dock capacity at the station / destination use historical data to show capacity trends during peak times with awareness of the trip duration would improve the ability to plan ahead. I spoke with a few members of my cohort my instructor Chris Makris and a small circle of friends to confirm that the pursuit of a solution for this case would be worthwhile. So I went forward to build a Shiny app with these features which can be found .My original goal for the project was to analyze historical ridership data provided by NYC Citibike through their open data portal. However my cohort peer Joshua Litven referred me to an article that was mentioned by Andy Eschbacher of Carto (was a guest speaker at the Academy the week prior to the project assignment) that provided a detailed analysis on NYC Citi Bike ridership. The analysis by Todd W. Schneider covered ridership trends including peak hours months age and sex. Todd’s analysis provided me with an opportunity to move on to the product development stage using his analysis to influence my development of a prototype dashboard solution that would allow  users to track stations meeting their capacity requirements alongside trip duration and cycling path.The Shiny project technology scope is limited to RStudio and Shiny Dashboard for the development of a webbased solution within a twoweek timeframe. Given the time constraint I limited my exploration of options to popular libraries that also provided examples related to my project scope. My final working prototype met the project requirements by integrating multiple R libraries such as:The shinyapps.io free subscription service model limits you to five Shiny applications with 25 active hours per month. This option made it possible for change releases (as part of the Software Delivery Life Cycle) to be visible to users in the public internet domain. A small test group outside of the cohort was able to validate the  and offer immediate feedback. Present state Shiny development solutions are robust for rapid prototyping of  ideas with options to scale through a simplified model. But the programming syntax requires further simplification to reduce debugging time due to syntax errors. I believe this will further reduce the learning curve making Shiny a great alternative for rapid prototyping of webbased solutions to data related problems.I combined the methodologies from   and   principles and DevOps practices to undergo rapid change iterations with focus on hypothesis testing and user feedback and a fast transfer of technical knowledge from the collective expertise of cohort members and external advisors. As a one man team it meant that I experienced wearing multiple hats to bring the prototype from concept to fruition: Market Research Business Analyst Product Manager Project Manager Developer Production / Customer Support Sales. As a result the first week was prioritized on researching  and dynamic map rendering APIs (Application Programming Interface) with time invested into iterations of trial and error until I found a solution that worked with recently acquired knowledge of R programming. The second week was prioritized on rapid iterations of development and releases to allow for end user testing and feedback. This method was crucial to keeping me focused on developing a product that met initial requirements and was flexible to allow for alignment with the end users' expectations. My approach to dealing with setbacks and limitations after exhausting the collective knowledge pool was to park bugs / request into a 'followup' / 'would be nice feature' checklist so that I wouldn't get stuck on working to resolve an issue or perfect an implementation. In paraphrasing my instructor Chris Makris 'Park the pie in the sky feature requests for a future iteration and remain focused on the core needs of the application to meet the timeline' was sound advice I kept close to heart.The overall process has been an amazing experience for me. I wanted to deliver on a primitive working prototype. Given the twoweek timeline I had to park the historical peak trend analysis feature but am amazed by the significant growth in geospatial and programming knowledge attained and by the number of features I was able to integrate into the final prototype based on user feedback. I will not hide that there were many moments of failure and frustration (e.g. endless troubleshooting of routing coordinates rendering as a polygon shape instead of a straight path on the map) along the way. However my family friends instructors TAs and cohort peers continued to provide me with the support guidance and encouragement to see the project through with the result being a working prototype on the  to aid users in their research / planning. This type of environment was the deciding factor that led me to partake in the immersive.Who would have thought that by the end of my first month of training at the NYC Data Science Academy boot camp that I would have produced a functional webbased prototype that addresses a business problem? The training curriculum cohort collaboration alongside existing industry research in the public domain and my commitment and persistence with respect to  following best practices allowed me to step outside of my comfort zone and achieve a working  that includes the following features:,NA,Where Did All the Bikes Go? An Analysis of NYC Citi Bike Station Capacity
https://nycdatascience.com/blog/student-works/storm-shiny/,47,"This is the front page of the shiny app entitled “Storm Events on Map”  Four other tabs are also included: “Storms on Time Series”  “Fatality” “Word Cloud” and “About”.  The first tab displays the storm events that happened during the selected years on the map and one single point represents one storm event. Each color corresponds  to one of the storm types which includes flash flood flood hail marine thunderstorm wind thunderstorm wind and tornado. Hence a higher density of color means more storms in that area. The reason why the project focuses on this six types is because first those storm types had  a very high frequency in the past ten years. The order of storm types is based on  frequency. Second those storm types have destructive effects meaning great loss of life and high impact on the economy. Therefore the app also includes the data for fatalities injuries property damage etc. visualized on the bar chart at the bottom right.Why did I use ""selected"" years? This is an interactive app and selecting the time range is one of the interactive features.  Users can choose the start and the end years through the slide bar located at the bottom of the left panel. The user can use this panel to control the information displayed such as the storm types on the map and the loss or damage from  storms. In this interactive way it is very straightforward for the user to ""see"" the storm events in certain years and have a sense of their corresponding loss simultaneously. An example graph can be seen  below.Knowing the facts of what happened in the past and how many times of certain storm types occurred is important. However one more important thing here is to use the facts to find a pattern and furthermore to predict the next event. As shown in the following graphs this shiny app also provides an interactive output based on the roll period the number in the white square at bottom left and the time range the user chooses. Also as the cursor moves along each line the data will be displayed on the top right.What do these two graphs show to us? The answer is that storms have patterns through time. On the left graph there is a high peak every year around summer. Then we can give a rough prediction that every summer we are going to have a storm peak season. Although the right graph has one extremely high point the fatality in storms does have patterns too. The fatality changes with the trend of storm events. Then what about the high fatality point? We can use the year selector to limit the time range from 2010 to 2012 to take a closer look at this specific point and at the same time have a threeyear comparison to distinguish that ""unusual"" event. The two graphs will change simultaneously with the year selector.Through the interactive charts the first piece of information we can get is that the high fatality point is on April 2011. Observe the trend of each storm type through the left graph the unusual pattern hidden in the lines is the light blue line zoomed in the left graph. According to the legend the light blue represents tornadoes. It seems that tornadoes are more hazardous than other storm types. However even if we know the deaths caused by the Alabama tornado in 2011 we still need more evidence to prove the danger of tornadoes.This shiny app explores the relationship between fatalities and storm types  under the ""Fatality"" tab. See below. Tornadoes are dangerous! According to this chart storm type could be one of the possible factors of storm fatality. The app also provides other possible factors which relate to fatalities including location and both the age and gender of the person killed . Please go check it out in the app. The NOAA dataset records every single storm event with a description of the event. What are the words used most often in the descriptions? Users can generate a word cloud with  this shiny project to get an answer. Change the minimum frequency of word usage  and choose a different limit to the number of words displayed. In this way word clouds will vary.",NA,Storm Events in U.S. - Interactive Shiny App
https://nycdatascience.com/blog/community/nyc-data-science-academy-domino-data-lab-announce-partnership-advance-data-science-education/,47,New York NY. – November 1st 2016    (NYCDSA) and  are proud to announce a partnership to train the next generation of data scientists. The partnership will provide data science students with training and access to the tools they’ll need to succeed in the workforce. As part of NYCDSA’s data science bootcamp Domino Data Lab’s data scientists will give several guest lectures about realworld data science problems and solutions. Students will also get free access to Domino’s data science platform with sample projects they can work on to hone their skills. “We’re very excited about this partnership with Domino Data Lab” said Vivian Zhang founder of NYC Data Science Academy. “There is a tremendous need for data scientists who can apply advanced analytics skills to a broad range of problems. At the NYC Data Science Academy we make sure our students have these skills and can hit the ground running the moment they step out of our classroom. Domino strengthens our offering with an applied data science perspective and insight into collaborate reproducible research practices.”During the 12week fulltime Data Science Bootcamp students become proficient data scientists using R Python Hadoop & Spark as well as the most popular and useful open source packages such as XgBoost dplyr ggplot2 ScikitLearn and more. The full curriculum for the 12week NYCDSA bootcamp includes:Domino brings unique value to the NYCDSA program in two ways. First Domino’s data scientists are experienced in working with corporate data science programs and can help students understand the transition from academic to applied data science. Also Domino’s open Data Science Platform supports all the languages and packages students use during the course so they can spend more time learning and doing and less time downloading and configuring.“NYC Data Science Academy is among top ranked bootcamp programs giving people the skills they need to advance their careers in our knowledgebased economy and they're delivering potential employees to companies who are struggling to hire qualified data scientists” said Nick Elprin CoFounder and CEO of Domino Data Lab. “This is important work and we're very happy to be able to help them in their mission.”“We are very excited to see the results of our combined efforts” said Zhang. “If you’re looking to improve your quantitative and coding skills or you’re a datapassionate geek come see the effectiveness of this connection for yourself and apply to the NYCDSA Bootcamp.” Domino Data Lab is changing the way data science teams work delivering an open Data Science Platform that accelerates research and increases collaboration. Data scientists benefit from access to powerful compute resources automatic version control and other features that help them work faster. Data science teams get a central hub to collaborate establish best practices and build on past research. Organizations using Domino build better models and put them into production faster. Based in San Francisco Domino’s customers range from small startups to Fortune 500 companies. To learn more about Domino visit Founded in 2014 the NYC Data Science Academy offers the highest quality in data science and data engineering training.  Their toprated and comprehensive curriculum has been developed by industry pioneers using experience from consulting corporate and individual training and is approved and licensed by the NYS Department of Education.  The program delivers a combination of lectures and realworld data challenges to its students and is designed specifically around the skills employers are seeking including R Python Hadoop Spark and much more.  By the end of the program students complete at least five realworld  to showcase their knowledge to prospective employers.  Students also participate in presentations and job interview training to ensure they are prepared for top data science positions in prestigious organizations.  93% of NYC Data Science Academy students are hired within six months of graduation. For more information visit,NA,NYC Data Science Academy and Domino Data Lab Announce Partnership to Advance Data Science Education
https://nycdatascience.com/blog/student-works/breaking-elements-kickstarter-project/,47,"Got an idea for a great project but lack the funds to execute it? Want to spread the word about a project you think the world should know about? Want to invest in new innovative products or meaningful causes? If you answered yes to any of those questions Kickstarter is the site for you. With the  of Kickstarter ideas pitched on the site how does one cut through the noise and present a project in a way that makes it likely to be funded and successful? Why do some projects such as the infamous  get overfunded while some projects never get touched?To explore such questions I decided to make a Shiny app with a Kickstarter data set scraped by . The idea behind using this data set for a Shiny app is so that end users can explore whatever categories interest them within Kickstarter via some visual EDA tools. The data set is scraped on a monthly basis and my app is currently using the data set from 10152016. Although the data set contains all projects from many different locations around the world I limited the scope to Kickstarter projects within the United States in order to focus on finding meaningful insights within one geographic location. You can find the latest version of the Shiny app .When we reach the landing page of the app we see some high level statistics about the entirety of US Kickstarter projects. Examining the landing page of the app more generic categories have more Kickstarter ideas.As we can see the top 5 categories are music film & video publishing art and technology. As we go down the category list we see more specific categories; comics and photography which have 2.6% and 1.7% respectively could technically be considered art which has 10.2% of all projects. Journalism which is 1% of all projects can technically be put into publishing which has 12.5% of all projects. Breaking down the projects by category there are more project ideas for creative subjects (i.e. music film publishing and art). Conversely there are fewer Kickstarter users who try to fund their technology ideas. One can speculate that users with tech ideas may go towards more traditional venture capitalists or think that their ideas require too much funding to be done on Kickstarter.Now that we have a view into the overall usage of Kickstarter let's breakdown Kickstarters by geographic region. By going into the ""Crosstab"" section of the Kickstarter Explorer Shiny app any user can break down the number of Kickstarter projects by state and category. Before doing any analysis on this I had a preconceived notion that New York would have more art and film projects whereas California would have more tech project ideas. Let's look the actual breakdown:Looking at the data from this facet we can see that while CA isn't the highest in terms of technology ideas (they do have Hollywood after all!) they have a relatively higher percentage of ideas in tech. This also reaffirms the view that the usage of Kickstarter is skewed towards the film and music communities.So far we have seen what communities Kickstarter project ideas come from. The next logical step would be to take a look at these places and see which projects are successful. By going into the Project Explorer module a user can look at a bubble chart of projects compared by goals and amount pledged. This module is useful for viewing outliers in the Kickstarter communities. We can see which project ideas hit it big with large amounts of money pledged as well as the projects that asked for too much and never got going. One example of a community that I explored was Colorado's live and successful projects:Here we can see a project idea called  which gained a lot of backers and a much larger amount pledged than the expected goal of $15000. By identifying past and current successful project ideas one may be able to study mimic their attributes to try to repeat their success. This is also very useful to get a highlevel understanding of what is trending within a project category. Interestingly enough when I was doing this project I showed this Kickstarter idea to my friend and he instantly backed the project to get a Fidget Cube himself. This demonstrates that this is useful in finding the nuggets of data that may be masked behind the masses of other projects.Lastly this app has a 'Data' module which shows all the data in a searchable data table. This is the 'throw in the kitchen sink' module as it allows the user to browse the data and sort the data in any way they want. For me one question that I wanted to explore with this module was whether people were lowballing their goals to get successful projects. If so how many of these lowballed projects were extremely successful? By sorting the Goals column by lowest to greatest I actually observed a large number of projects with a goal of $1. More surprisingly a lot of these projects were extremely successful if you took into consideration the ratio of goals vs. pledged amount.While it's a no brainer that these projects are successful (they only require $1 to be...) they show a different side of Kickstarter. It almost appears that these people are using a ""pay what you want"" method as any amount pledged would hit the goal of the project. This may be useful if you do not know how much you need as a goal but feel like you don't need too much money to succeed.All in all this app allows you to explore different elements of Kickstarter projects which interest the end user. This app is great for identifying and examining outliers via the bubble chart and data table. It is also effective for examining the breakdown of communities and support within Kickstarter. In the future I would like to add more high level graphs in the landing page to help with getting an executive summary without going through all the tabs. I would also want to add more Javascript functionality and maybe implement some statistical tests as modules.",NA,Breaking Down the Elements of a Kickstarter Project
https://nycdatascience.com/blog/student-works/steam-data-visualization-using-ggplot2-and-r/,48,One of the most time consuming parts of this project involved merging the data because of the game’s names. The names of games on Microsoft’s sites were abbreviated or shortened due to character limits on their own website or are only available as “XYZ Edition”. Users often used abbreviations or modified names when submitting to UserVoice. Wikipedia articles sometimes used the international name of a game. Punctuation trademark/copyright marks and differences in spelling prevent one to one matching.The solution was to make a new column of names where I would remove as many obstacles as I could so that when datasets are merged there are the fewest differences possible. I began by removing words including “the” “edition” “special” “full game” “free to play” and many more. I then removed pluralization and possession from words and converted numbers to roman numerals (“Assassin’s Creed 2” to “Assassin Creed II”). I also removed all non alphanumeric letters as well as spacing and capitalization.I felt that the increase of owners better represented the success of a sale than sales numbers alone because it allows comparisons of smaller games such as indie games to AAA games. For example if an indie game sold only 2000 copies and increased to 2400 copies sold that is very impressive but a 400 sales increase for a AAA may be dismal. The above scatterplot shows how there seems to be a closer relationship to the increase of owners and the number of owners before versus the as opposed to a comparison using sales.,NA,Steam Data Visualization using GGPlot2 and R
https://nycdatascience.com/blog/student-works/16172/,48,The use of cosmetics is growing worldwide. Nowadays men use cosmetics almost as much as women. Also cosmetics usage among babies and children has also increased. However the wide range of products and the complexity of their ingredients raises the concerns of regarding the health effects.. Data visualization based on The California Safe Cosmetics Program can give consumers a quick comparison while making purchasing decision. collects data based on which requires companies that manufacture cosmetics to report any cosmetics products that contain ingredients known or suspected to cause cancer birth defects or other reproductive harm.Cosmetics covers a wide range of categories. Within the CSCP database the products are classified into thirteen primary categories which are .The first thing for consumers to consider is which category of  products they should be more cautious about while making a purchase. This analysis provides an overview of the products reported in each category.  has the highest count compared to other categories and  has the lowest count relatively.After understanding that NonPermanent Makeup has the highest number of products being reported the question becomes which of these products is  reported the most.After digging into the subcategory the analysis shows that  are the top three subcategories.Sometimes the products contain more than  one kind of chemical. From the violin plot below we can see  that most of the categories contain  kind of chemical but  NonPermanent Makeup such as Nail and Sunrelated products also may  contain  kinds of chemicals in a single product. The category one should be most cautious about is  because some of the products can contain up to  kinds of chemicals in them.In order to have a better understanding while purchasing cosmetics this analysis gives the top three chemicals that are used the most within each category.  With the exception of Personal Care Products  is the chemical that is  used the most for every category. and  are two more chemicals that appear in multiple categories.  The analysis below indicates  that are the top two companies with the  highest number of products being reported. The next three are:   and Looking at the top company L’Oreal USA let’s discover which brand under L’Oreal has highest number of products being reported. The word cloud shows that is the top one and following are  and Then  and Even though it seems like a lot of wellknown brands have products containing reportable chemicals some companies have been taking actions after being reported. From the Timeline of Reported Cosmetics Analysis from 2009 to 2016 it would appear that 2010 was the year with the  highest ratio of products first being reported. In the same year about half of the reported products’ chemicals were removed.However in 2011 about 2/3 of the products were reported again. Overall the number of products reported went down after 2011.  Despite a slight increase again in 2014 in general it would appear as if the California law is working to make cosmetics safer.  There is a clear trend showing a decrease in reportable chemicals over time.  As for how companies dealt with the challenge of having reportable chemicals in their products it looks as though there were initial efforts to remove the chemicals but subsequent to this products tended to be discontinued rather than altered.  This might due to the difficulty of the removal process or the product lifecycle.,NA,Harmful Chemicals in Cosmetics
https://nycdatascience.com/blog/student-works/nba-shot-log/,48,In the NBA a top player makes around a thousand shots during the entire regular season. A question worth asking is: What information can we get by looking at these shots? As a basketball fan for more than 10 years I am particularly interested in discovering facts that can not be directly seen on live TV. When I was surfing on web last week I found a data set called NBA shotlog from Kaggle. This data summarizes every shot made by each player during the games in the 14/15 regular season along with a variety of features. I decided to perform an exploratory visualization with this data.,NA,Visualizing the Game Style and Shooting Performance among Superstars via NBA Shot-log
https://nycdatascience.com/blog/student-works/key-insights-ames-iowa-housing-data-multiple-factors-behind-house-price/,48,"When people desire to own a house of their own ... What do they usually consider?When was the house built?  House style? Location? Building material?Are these the features that  affect house prices?  Based on an open dataset published through   we explore some insights behind the house pricing.This dataset describes the sale of individual residential property in city Ames Iowa from Jan 2006  to July 2010 which includes 80 variables(23 nominal 23 ordinal 14 discrete and 20 continuous) for assessing the house sale price. Those variables focus on property quality and quantity looking at the typical information most potential home buyers consider when making a decision. Although in the original data set observations with unreasonable values (sales data don’t represent actual market values) have already been removed general living area with more than 4000 square feet has also been removed from the data set according to the recommendation from paper [1]. Also data with house sold year at 2010 has been removed since it does not cover information for the whole year. Finally for this  R data visualization project 8 variables  1281 observations (containing categorical data and continuous data) are selected from the original dataset which includes Sale Price ($) House Built Year House Sold Year House Sold Month House Style House Location Zone House Foundation and Location Zone.Scatter plots above show the relationship between house sale prices and the house built year.  The price trend is generally increasing; the newer the building the higher the price. Different color dots represent different house style categories which are 1.5 Story 1.5 Story(U) 1Story 2.5 Story 2.5 Story(U).For example 1.5 Story(U) means one and half story with an unfinished story or basement represented by ""U"".The definition of a half story is the floor area partially or wholly built into the framing of the roof which is still livable and with a sloping roof  usually having lights from dormer windows.From the plots above the house price trend is increasing. The newer the house the higher is the price according to the regression line. Popular house styles are 1.5 Story 1 Story and 2 Story which are always needed in house market.   The price range distribution is wide after the year 2000 which means that house prices are  affected by other factors. Moreover 1.5 Story(U) 2.5 Story and 2.5 Story(U) house style disappeared in the market after the year 1960 and two new house styles (Split Foyer Split Level) emerged in the housing market with most house prices between $100000 and $200000.The split foyer house style is a 1970s 2 story house design featuring small front entryway between floors which has two short stairs one leads up to a main living level and another one goes down to a finished lower level. Many people have complaints about the small entrance of this house style especially in winter time and maybe this is one of the reasons why it has a lower price compared to Split Level style house even though both of them are featured as half a story difference in adjacent rooms.The split level house style is basically a “colonial” style house and could be 3 or 4 levels not just 2. Rooms in the house are somewhat above or below adjacent rooms and the floor levels difference is approximately half a story.Let’s move on to the house sale history regarding years and months. Plots below show the number of houses  sold in different years.  The median price is marked on the top of the bar.  It would appear as though house prices were  stable from 2006 to 2009. The number of houses sold peaks in July while the highest prices peak in September.  Location always matters. The plot below reveals the relationship between house sale price and location zone. Residential High (RH) zone has a stable house price while there is a large variance within the  Residential Low (RL) zone and the  Residential Median (RM) zone.   Floating Village(FV) is a special area where a retirement community was developed and have the highest median price.The last plot reveals that  people do not ignore house foundation material when purchasing a house. The plot shows the relationship between the overall quality house and the house foundation material. A highquality house which means the overall quality score above 6 is usually with a Brik & Tile foundation. Slab and Poured Concrete exist in lowquality houses. This information may be helpful for people to estimate the true house price.Based on above data visualization results some insights behind home values have been revealed. House built year is one factor in deciding the home value. Season and location play their role in affecting house prices and a high quality house based on its foundation material also affects pricing.  More research could be done on variables like house living area bathroom numbers and bedroom numbers related to  house prices to prepare a house price prediction model.",NA,Insights on Housing Data: Multiple Factors behind House Price
https://nycdatascience.com/blog/student-works/r-visualization/public-vs-private-institutions-cost-benefit-exploratory-visualization-analysis/,48,In 2008 I was a senior in high school applying to colleges around the country eager to start the next phase of my life. Unfortunately my application cycle fell right in the middle of our recent recession caused by the collapse of the housing market in 2006. Although I had gained admission to Northwestern University a prestigious private school I had to decide if it was worth spending my parent’s life savings as well as taking out large loans. My other choice was my native state’s public school State University of New York (SUNY) Binghamton. Although SUNY Binghamton was not as prestigious it would have cost me 4 times less and majorly reduced the financial burden on my family. Ultimately I had to choose between an expensive private college or go a cheaper public college. I ended up going to Northwestern University but if I had more data on the differences I might have chose differently.It is not uncommon for students to have to choose between more prestigious private schools and cheaper public schools. However as college tuition student debt and the need for a college degree are on the rise it is becoming ever more important to choose carefully. In fact regarding the student debt bubble billionaire entrepreneur Mark Cuban has said we are “going to see a repeat of what we saw in the housing market...”. Furthermore according to collegedebt.com the cumulative U.S. student debt is over $1.45 trillion dollars more than the total credit card and auto debt in the country.In this blog we will do an exploratory analysis of the data released by the U.S. Department of Education and look at the costs and benefits between public and private colleges. This blog will focus primarily on predominately bachelor’s degree granting schools and the latest available year’s data (2013). This analysis will focus on the cost debt and earning aspect. We will see a U.S. map of the cost and adjusted cost density plots of the median debt and median earnings after graduation and lastly a scatter plot of net cost vs. median earning. The cost of attendance is the college’s reported estimate of total cost needed per year this includes tuition living expenses fees etc. The public school map is dominated by green and yellow points ($10000 to $30000) whereas the the private school map is dominated by orange and red points ($30000+). It seems that private colleges are roughly about $20000 more expensive. In the above plots we confirm that most private institutions are more expensive than public institutions.However when we plot the adjusted net cost (cost of attendance minus average grants and scholarships) we can see the difference in cost is actually smaller where the private institutions provide more financial aid but still ultimately cost more. The public school map is dominated by blue and green dots ($0 to $20000) whereas the private school map is dominated by green and yellow dots ($10000 to $30000). Roughly speaking the public institutions give about $10000 aid whereas private institutions give about $20000 aid.In the median debt density graph the public and private graphs have different peaks and a portion that overlaps. From this graph it seems that most of the median debt from public schools are less than private schools. This makes sense since in the previous U.S. maps private school costs more so logically students will have to take out more loans to pay for tuition.Surprisingly in the median density graphs the median earnings (10 years after graduation) between public and private schools have little visual difference. The density graphs seem to fall on top one of another with the peaks almost aligned but the private density graph has a little more variance. It seems that regardless of private or public schools in general the earnings are about the same.To understand the distributions a little more clearly 2D density contours are overlaid on top of the scatter points to illustrate where the highest density regions are. The innermost contour line shows the densest region of each group. Here it can be seen that even though the two college types may overlap the peak of each group are separated.In general the cost of private institutions are roughly on the order of $20000 more than expensive than public schools. However private institutions give on the order of $10000 more financial aid resulting in private schools only be on the order of $10000 greater in net cost. On the other side of the analysis in general students tend to leave private institutions with more debt but earn about the same amount after graduation. Finally net cost does not seem to generate more earnings which results in public schools being cheaper but earning around the same as private schools. With all else being it is recommended to go to public schools to save money.In this analysis we focused on a high level overview of whether the monetary investment in more expensive colleges (private institutions) is worth it based on how much debt and earnings one comes out with after graduation. However college is more than just money in and money out there are many other factors that define a good and worthwhile investment. These factors may include faculty to student ratio school size location types of programs and many others. In the future a deeper analysis will be done to include these intangible factors. In addition this study only looked at predominately bachelor’s degree granting institution it would be enlightening to see how schools that grant different level of degrees such as associate degrees medical degrees etc. play into earnings and investment returns.,NA,Cost Benefit Exploratory Visualization Analysis of Public vs Private Institutions
https://nycdatascience.com/blog/student-works/feel-unsafe-truth-ot-rumor/,48,The Trump Campaign and the Hillary Campaign fight over the facts about crime rates with Trump insisting that people are less safe now than they used to be. Do Americans feel safer or less safe compared to decades ago? Every person has his own feeling and answer but here let us visualize the data from the to see whether people are safer or not.Looking at crime statistics (right) since 1992 the number of total crime has continued to drop for two decades. It fell both under Democratic presidents and during George W. Bush’s terms.Imagine two scenarios:Both crime cases were counted as one but which one would make you feel  more unsafe? Of course the answer is murder.In the right bar chart: the violent crime statistics tell you Donald Trump’s feeling is the truth. After 2012 President Obama's 2nd term the number of murder and rape victims went up every year at an increasing rate.The fact is the number of violent crime have risen since 2012.If you can't understand “what a disaster” it is right now left is the US Crime segmentation analysis.Comparing the violent crime ratio with the property crime ratio the ratio of violent crime is approaching the historical peak meaning the highest probability to be a victim of violent crime since 1994.  Meanwhile given recent trends it would appear as though we are at risk of exceeding the peak in the near future. In the US the percentage of women who were victims of violence has grown in the past 60 years. But in the past 8 years the ratio of rape in the crime segmentation doubled as much as in the past 20 years. How dangerous is this for women? In the past 30 years the possibility  that a woman would be the victim of a rape has  !!  However this recent increase in the data could be from more women coming forward and reporting rape.From the historical data of the past few decades you can find that Democratic presidents have been more successful against the violent crime. In the past 24 years one of the democrats’ achievements is that the violent crime was cut by almost 40%.The Hillary campaign claims that whenever a Republican ruled the US they ruined the country and the murder and robbery rates increased insanely.  Does the US actually not get any safer under Republicans?In the end as it turns out both Hilary and Donald are right.  ,NA,Is Trump Right? Do People feel less safe now than in “olden” days?
https://nycdatascience.com/blog/student-works/help-shelter-animals/,48,"Contributed by Chuan Hong. Chuan is currently in the NYC Data Science Academy 12 week fulltime Data Science Bootcamp program taking place between September 26th to December 23rd 2016. This post is based on her class project  R  Visualization.Each year approximately 7.6 million companion animals enter animal shelters nationwide (ASPCA). Of those approximately 3.9 million are dogs and 3.4 million are cats.  About 2.7 million shelter animals are adopted each year (1.4 million dogs and 1.3 million cats). Meanwhile about 649000 animals who enter shelters as strays are returned to their owners (542000 dogs and 100000 cats). Compared to these lucky cats and dogs finding their families to take them home many shelter animals face an uncertain future. It is estimated that 2.7 million cats and dogs are euthanized in the US every year. Given the differences in outcomes for shelter animals we can analyze the factors that make some cats and dogs more likely to get adopted.In this dataset there are ten variables which are  ""AnimalID"" ""Name"" ""DateTime"" ""AnimalType""(Dog/Cat) ""SexuponOutcome""(Neuteraed Male/Spayed Female/Intact Male/Intace Female) ""AgeuponOutcome"" ""Breed"" ""Color"" ""OutcomeType""(Return_to_owner/Adoption/Transfer/Euthanasia/Died) and ""OutcomeSubtype""(Other/Foster/Offsite/Partner/Barn/SCRP/Suffering/etc.).After a quick check of these variables I decided that""Color"" and ""OutcomeSubtype"" would not be included in this visualization project. This was because that there were 300+ unique colors in this dataset. It was way too many to visualize factor by factor. Meanwhile based on the Sankey plot below we can see that the ""OutcomeSubtype"" is a detailed explanation of the variable ""Outcome"". First let's look at how many cats and dogs we have in this dataset and how different outcomes are distributed. From the two graphs shown below we can see that both cats and dogs were commonly adopted but dogs are much more likely to be returned to their owners than cats and cats are transferred between shelters more often than dogs. It also appears that very few animals died or got euthanized overall.",NA,How to Help Shelter Animals
https://nycdatascience.com/blog/student-works/finding-influencers-twitter/,49,"Have you been followed on Twitter or Instagram by someone you don't know? I get this a lot. And so to avoid being thought of as rude I follow back. Eventually I got tired of following back when I realized that some of these accounts don't really do anything but collect followers. Now why would anyone go through all the trouble of following people in the hopes of being followed back? Why would anyone waste so much time on the internet for this?I eventually realized the answer when I saw that most of these accounts were not personal. A lot of these accounts I encountered were about food some about beach vacations and on some occasion accounts with risque content.Advertising has infiltrated the social network. It used to be just ads on banners but now companies hire personalities on social media to spread the word about their product or event. Companies spend big bucks on celebrities in an effort to publicize their brand and attract a celebrity's fan base. A sponsored tweet could net as much as $13000 as was the case for. Celebrities have multitudes of followers and get paid big bucks by sponsors. So people may have thought that creating accounts and amassing followers would eventually get them sponsorship deals with advertisers. In this exercise we see that sponsors might be looking for some other things other than the number of followers. In a social network a link could represent a relationship as in Facebook or the passing of a tweet as in Twitter. These links determine the flow of information and are therefore a good indicator of a user's influence. I will be presenting two methods of finding potential influencers in a network. One would be by extracting a user's influence measures and the other is by using network graphs. A large database was found on . The database contained a stream of tweets related to NASDAQ 100 stocks extracted from twitter for 79 days from 2016 March 28th to 2016 June 15th. This was selected because of a good mix of accounts representing organizations and personalities. The database also contained information about how many times a tweet was passed along and who the original tweet came from. This act more popularly known as retweeting can be identified in the stream as tweets having 'RT @user' or 'via @ user' at the beginning of the tweet. The stream also contained information about mentions. In twitter a mention is a public conversation between users. A user calls the attention of another user by mentioning them in a tweet. Mentioning is identified by tweets beginning with '@user'.The influence measures extracted from the stream were the following: indegree retweet and mentions. These measures were selected because of how they affect the flow of information in the network. Indegree measures the user's popularity. This was easily extracted from the database by the number of followers a user has. The number of followers shows us the size of the user's audience base. Retweet influence represents a user's ability to create content which other users find worthy of sharing. When a tweet is shared by another user a bigger network of users is exposed to the tweet. From the stream this was extracted by counting the number of retweeted messages for each user. The third measure mention influence was extracted by counting the number of mentions containing the user's name. This influence measure indicates the ability of the user to engage others in a conversation. This represents the topofmind value of the user's name.A total of 96613 users tweeted about NASDAQ 100 stocks during the timeframe. Between them over 680 thousand tweets were broadcast. A word cloud of the NASDAQ symbols most often mentioned shows that Apple represented by AAPL was the most tweeted stock among the group. Users were most active on April 27 where they broadcast over 20800 tweets. This coincides with the day when AAPL stocks slumped following speculations that iPhone sales may decline by as much as 60 million units compared to the same quarter a year ago. The slump in Apple shares dragged the techheavy NASDAQ into the red by the day's end.Users' activity on this day showed that activity was mostly during trading market hours which is 13:30 to 20:30 UTC.Each user's ranking over the three influence categories was assigned by using fractional ranking. For example in assigning the indegree ranking a rank of 1 was given to the user with the most number of followers. Users with the same number of followers receive the same ranking number which is the mean of what they would have under ordinal rankings. Table 1 shows the top 30 users across the three influence measures. Notice that minimal overlap can be seen across each influence rank. The first user to show up across all three measures of influence was ""WSJ"".To see how much users overlap across the three categories a Venn diagram of the top 100 users was derived. Figure 4 shows that among the 239 users in the top list only 10 users can be seen across all three measures of influence.Figure 5 below shows a correlation matrix which represents how a user's rank varies across the three different measures of influence. The correlation matrix represents the strength of the association between a pair of rankings. This matrix was derived by comparing the relative influence ranks of all 96613 users in the database.The users show a strong correlation in their retweet influence and mention influence. The low correlation of the indegree measure across the other two measures show that indegree ranking may not be related to the other rankings.A couple of conclusions can be derived from the correlation plot. First we can say that in most cases users who are retweeted often are also mentioned often and vice versa. Another one is that the most followed user may not be the most engaging user in the group. A user's popularity therefore is a weak representation of the ability to motivate the spread of information.Retweets and mentions have direction. A retweet is the path of an idea from User A to User B. User A broadcast a tweet which was read by User B. User B thought it was worth sharing and retweeted it. This retweet will eventually be seen by users not directly accessible to User A. When User A mentions User B this is again a link from User A to User B.  With this in mind we have enough data to convert our twitter stream into a directed network graph. All users will be a node in our graph and all directed links will be edges. The igraph library will be used to extract information from the resulting network graph.A quick look at the resulting network graph for the whole stream shows that we were able to create a graph with 96613 nodes and 168 519 edges. Because of this size the resulting network graph will not be shown. This is because of the amount of time and computational effort needed to come up with a plot. It would most likely be a crowded mess of dots and lines anyway. However we can still extract some information from the graph object.The density of a network object is the proportion of present edges from all possible edges in the network. Our present graph has a density of 2.799118e05. A very low density would mean that there is a very low interaction between our users.The diameter of a network graph is the length of the longest path across unique nodes and edges. Considering the direction of the links the diameter of our network is 14. This means that we are able to trace an unbroken path across 15 users.The hubs and authority algorithm was developed by Jon Kleinberg to examine the relevance of a web page's content. He categorized pages into hubs and authority pages. Hubs which have more outgoing links are the internet's catalog. This is similar to the early days of Yahoo where it touted itself as the internet's yellow pages. Authority pages have more incoming links presumably because of their highquality content. Translated to twitter activity hub pages would fit the description of a user with high retweet influence and authority pages would be similar to a twitter user who has high mention influence.The hub score and authority score of the network graph was derived using a simple igraph function call. The resulting top hub score went to ""markbspiegel"" while the top authority score went to ""Benzinga"". This is in contrast to the ranking tables where the top retweet and mention belong to ""philstockworld"" and""jimcramer"" respectively.To find out where the discrepancy came from each node were investigated. Although it showed that ""markbspiegel"" had more unique edges than ""philstockworld"" if we consider and sum the weight of each unique edge philstockworld still beats markbspiegel. The same is observed when looking at the edges of ""Benzinga"" and ""jimcramer"". The discrepancy is consistent with how web pages are rated wherein the number of links matter more over the number of times each link was activated. The hub and authority score also does not take into account the weight characteristics of the nodes.To see an actual network graph we narrow down our selection to a twitter stream of users tweeting about CA Technologies.Table 2 shows us the resulting top influentials derived from our ranking method. The first user to cross the three influence categories is ""Benzinga"". The resulting network graph of this smaller twitter stream comes up with 431 nodes and 131 edges. ",NA,Finding Influencers on Twitter
https://nycdatascience.com/blog/student-works/lending-club-loans-analysis/,49,"Technological disruption is affecting many industries and dusty old consumer lending is no exception. Peertopeer lending  private individuals lending to one another rather than from banks  has been growing exponentially over the past 10 years and Lending Club is a lead player. But first how exactly does Lending Club work? Say you have some spare cash. Lending Club now gives you an alternative to lending it to your crazy uncle. Instead you can lend it to a...stranger! That may sound scary but Lending Club handles the movement of money back and forth between you and the borrower. Rather than using Lending Club to borrow for new investment or consumption borrowers are typically trying to consolidate existing debt. The below word cloud illustrates the prevalence of ""debt consolidation""  and ""credit card"" in the free text field ""loan purpose"". Borrowers want to consolidate their debt at Lending Club because they think they can get a better interest rate than they pay on their credit cards. This leads to the other important service Lending Club provides:  they estimates the credit risk  the risk of not getting paid back  of each loan. They give each loan a “grade” (just like at school) and based on this grade assign an interest rate. The lender can choose from a spectrum of low risk low return to high risk high return loans. As you can see in the below chart this interest rate has changed quite a bit over time.  The interest rates on the high grade loans have been stable over time but the rates on lower grade loans have gone up considerably. You can also see that the difference in interest rates between loan grades is quite high. For example you may be paying 10% more interest if you have a grade C loan as opposed to a grade A loan.  That’s $5001000 on a $10k loan!
So how does Lending Club determine this allimportant loan grade?  You can see that back in 2007 Lending Club’s loan grade metric was highly correlated with FICO scores: high grades were associated with high FICO scores. However over time the range of FICO scores used to give both high grade loans and low grade loans increased. Particularly intriguing were the low FICO score people getting grade A loans and the high FICO score people getting lower grade loans. As context you would have trouble getting a typical mortgage at all if your FICO score were much below 650. But if you had a FICO score of 750800 you would have banks chasing you down the street with free toasters to get your business! I investigated this latter group further. How do these credit golden children end up with say a grade C loan paying 10% more than one would think they should? I called this group “outliers”.If these outliers were indeed unjustly given lower grade loans than they deserved we should be able to see that in the default rates of the loans. However the chart below illustrates that although the outliers’ default rates are a bit lower than peers with the same loan grade but lower FICO scores they are not generally low enough to justify being bumped up a grade. For example the red C grade bar would have too high a default rate to justify being moved to a B or A grade loan despite the fact that all its constituents have FICO scores above 750. In short Lending Club assessed the outliers' credit risk accurately despite their relatively lofty FICO scores. So how does Lending Club figure this out? How do they know that your crazy uncle with an 850 credit score really deserves a grade C loan? The short answer is we don’t know. However there are characteristics that suggest some possibilities for further analysis. For example in the above graph it's clear that the loan amount seems to be a little higher for outliers than for nonoutliers. Also it looks as though the outliers have more recent credit inquiries and are more likely to have recently opened a credit account than nonoutliers. This is reflected in the shape of the violin plots for Mths_snc_rct_inq and Mths_snc_rcnt_acct. Therefore it may be the case that a potential borrower applying for a larger loan than usual and opening up credit accounts recently may be interpreted by the Lending Club as warning signs that their credit risk is higher than that implied by their FICO score alone.In summary the Lending Club's credit rating practices have matured considerably over the past 10 years particularly with respect to their divergence from the FICO scorebased credit ratings. However the FICO score is still a useful metric for a lender: although the Lending Club has a more accurate algorithm for measuring default risk than FICO within a given loan grade borrowers with high FICO scores tend to default less frequently than borrowers with low FICO scores in the same loan grade. This is valuable info for a potential lender!",NA,Who to Lend to at the Lending Club
https://nycdatascience.com/blog/student-works/visualizing-economics/,49,Countries are economically different in various regions of the world.  The residents of wealthier countries are not only better off financially but they also tend to live much longer.  How does the living conditions compare? What could be a possible reason for this? This project visualizes the economic conditions of different income regions and identifies how these situations compare over time.   The data on this project was found in the databases of World Bank and the World Health Organization.,NA,Visualizing Economics and Mortality
https://nycdatascience.com/blog/student-works/r-visualization/health-insurance/,49,"Is it possible to purchase a health insurance plan with lower rates and more benefits? The answer is yes. This project provides some insights on this question. Firstly the project analyzes  the factors driving plan rates based on over 12 million records of health insurance marketplace data. Secondly according to over 5 million records of plan benefits data the project provides  information about different plan types: their benefits variety and specialty. Besides your name the first question when you try to get a health insurance rate quote is your age. In order to regulate the marketplace and to limit the amount that can be charged to older people the Department of Human Health Services(HHS) established the federal default standard age curve which has been in effect since 2014. For rating purpose HHS defines a single age band for children age 0 to 20 and older adults age 64 and over and oneyear age band for adults age 21 through 63. As shown in the chart above the health insurance plan rates increase with age and the maximum ratio for age rating is 3:1. That is for the same insurance plan if the rates for a 21 yearold adult is 100 dollars the rates for the older adults cannot be over 300 dollars. However age is a factor that we cannot change. Let's explore some controllable factors of plan rates.The second question in a rate quote might be the plan length. The choices are various like 3 months 6 months 12 months etc. Here's a graph for each age group showing how  plan rates are affected by the plan length. The project used month as a length unit and all the plan rates mentioned in the article are monthly plan rates.The graph shows that for each age group the average rates are lowest for 12month plans. Another good choice of plan length is 6 months. Why are the rates of short term or even longer term so much higher? One of the possible reasons is that insurance companies might consider customers buying a shortterm or longterm plan for a special medical purpose. 6month and 12month plans are considered as ""regular"" plans by default. For the plan length option 12month is the best choice.However the average rates shown in the bar chart seem too high. Let's take a look at the raw data shown in a violin plot.The plot above showing the raw data density reveals two main trends: one is the goingup trend the other is the flat trend at the bottom. What does this mean? It means that for each age group at least two price levels of insurance plans are available: cheap ones and expensive ones. The seemingly weird plots at the bottom actually provide  some good news. No matter what age group you are in plenty of free health insurance plans are provided by insurance companies.Another question most often asked in rate quote process is ""Do you smoke?"". Why asked? Because it could affect your plan rates! The project found that insurance company's policy about tobacco use meaning whether the company cares about customers using tobacco or not is another important factor of plan rates.A family option is the most effective way to cut expenses. According to the project's analysis buying with dependents or as a family will reduce plan rates massively. Since region like age has the same effect across different factors on plan rates here we combine region with a family option as an example.As shown above even the highest average rate is only $44.81 per month. Due to the limitations of this blog post the interactive feature of the chart cannot be shown below. New Jersey as an example is illustrated in the chart. In general rates vary across states and the darker the color the higher the average plan rates.It seems that the HMO has the largest marketplace. However that doesn't mean that HMOs provide the most benefits.In fact the PPO plan type covers more kinds of benefits. In other words what the graphs illustrate is that when people have no preference about benefits PPOs and HMOs are two better choices.If people have a preference about benefits which plan type should they choose? Below is a list containing the unique benefits covered only by specific plan types. Therefore when we have preference in the fields mentioned below we know which type to choose.Age plan length tobacco policy family option and region are possible factors of plan rates. If you choose a oneyear plan an insurance company that doesn't have a tobacco policy and  purchase with dependents you could get a lower rates.PPOs and HMOs cover more benefits than other kinds of health insurance plan types. However each type has their own benefit specialty.",NA,U.S. Health Insurance: Lower Rates and More Benefits?
https://nycdatascience.com/blog/student-works/r-visualization/clicks-grades-relationship-student-interactions-virtual-learning-environment-overall-grade/,49,"The recent growth in online courses has led to an increase in learning  options for both traditional and nontraditional students.  In these courses students use a Virtual Learning Environment VLE to simulate the experience of a real world classroom. Aside from the obvious benefits for students one that is just now being explored is the ability to assess in real time the level of interaction with the course material.  Recent  has shown that such information can be used to spot students in danger of failing and intervene in a timely manner. Such interventions however require human resources which are costly and not scalable. As such analytics data from the Online University in the United Kingdom was analyzed to see if there are any clear differences in usage between passing and failing students. The hope is that such information can be used in the creation of an automated tool to help students improve their overall grades.The  dataset as it's known consists of 7 csv files totaling more than 450 MB in size. There is information for seven courses each running from February to October. And there is two years worth of data is available. Student demographics and timecoded student interactions with the VLE make up the bulk of the data and were the focus of the analysis.The above histograms show the breakdown for student access clicks for the same class held a year apart. Clear from the plots is that most students show between 0 and 2000 clicks. So are there differences between the number of clicks of the best students and those that failed?Not only is the number of clicks per student important but also the mean time they take to access content relative to each other. Where most students seem to take similar times to access content a smaller percentage either take very long or very little time. The question is do the student who take the least time do the best?  Likewise do the students who take the longest end up failing the course?The relationship between the number of clicks and the overall student outcome can be seen in the scatter plot above. Students who failed green dots exhibit much less interaction with the VLE versus those who passed. Unsurprisingly the best students on average were accessing the VLE more than anyone else.From analyzing the mean access time scatter plot students who pass or do well have a much narrower range of access times vs. the student who failed. As a matter of fact students who failed were accessing the content earlier than those who passed which was a bit surprising.  One would assume that the earlier a student accesses content the better they would likely do.When the level of access by age category is compared the results were unexpected. One would assume that younger students (< 35 years) being ""more"" tech savvy would be accessing the course content more than the older students.  This isn’t the case according to the data. Students in the 55 and over category were the ones using the VLE the most. One possibility for this is that younger students may be saving the content to their local devices so they have less of a need to continually interact with the VLE.  As expected there seems to be little difference between the interaction levels between men and women though statistical testing might show otherwise.Students who interact more with the VLE tend to perform better.  In contrast students who access the materials early tend to perform worse.  Interaction level also seems to be age related but gender doesn’t seem to play much of a role. From these results a data driven widget could be added to the VLE to encourage students to interact with the VLE in meaningful ways based on their current grades and demographics. This approach would be both scalable and individualized.",NA,Clicks and Grades: Relationship Between Student Interactions With A Virtual Learning Environment and Outcomes
https://nycdatascience.com/blog/student-works/extramarital-affairs-factors/,49,"In order to determine some of the possible factors of extramarital affairs we will explore a well known survey from the paper “Theory of extramarital affairs”  published in the Journal of political economy in 1978. This survey has a sample of 601 individuals and measures 9 factors like: Having children how the individuals rate their marriage how many years they have been married and how educated they are etc..Although the number of participant is 601 450 individuals in this survey didn’t have an an affair. For practical purposes we will only focus on individuals who had at least one affair during their married life.Fig.1 : Marriage rating vs GenderThe individuals who answer the survey were asked to rate their marriage with a scale from 1 (Bad) to 5(excellent). From Fig.1 above we can see that the average rate for female participants is 3 and the average rate for male participants is 4. We can also say that for this survey the individuals appear to be quite happy with their spouses. In this case how do children influence the quality of their marriage and the number of affairs they could have? Fig.2 : Marriage rating vs Children for both gendersFrom Fig.2 we can see that individuals without children seem to be more happy with their spouses than the ones without children for both males and females. If we look at Fig.3 we can see that the people without children are more likely to have more affairs than the people with children. In addition the number of affairs for male participants with children are almost twice their female counterparts with children.Fig.3 : Number of affairs and children for both genders After examining both the children and marriage rating factors we will next explore some other factors as years of marriage age and education.

From Fig.4 female participants seem they don’t have affairs during the first year of marriage. Between 1 to 5 years of marriage they have more affairs than their male counterparts and we can see that between 5 to 10 years of marriage they have the same number of affairs.  For more than 10 years of marriage female participants have more affairs than their male counterparts.   a degree someone has",NA,"Extramarital affairs, some factors..."
https://nycdatascience.com/blog/student-works/predict-new-york-city-taxi-demand/,50,There are roughly 200 million taxi rides in New York City each year. Exploiting an understanding of taxi supply and demand could increase the efficiency of the city’s taxi system. In the New York city people use taxi in a frequency much higher than any other cities of US. Instead of booking a taxi by phone one day ahead of time NeWe use a joining dataset detailing all ~1 billion taxi trips (14G) in New York City from April and September in 2014 as provided by he NYC Taxi and Limousine Commission (TLC) including information of yellow green and uber taxies. The data associates each taxi ride with information including date time and location of pickup and dropoff trip distance payment type tip amount total amount. Also hourly  weather information is incorporated in the big taxi dataset. A small number of taxi pickups in this dataset originate from well outside the New York City area. In order to constrain our problem to New York City as well as to reduce the size of our data given our limited computational resources we only consider taxi trips that originate somewhere defined in the figure on the right.The information including taxi pickup date time longitude and latitude coordinates is selected which contains 1 billion observations and thus is hard for visualization and modeling. To reduce dimension and data set longitude and latitude variables are rounded to 3 decimals and condensed to 120000 records.We used a python library Geopy to find out zip code from corresponding latitude and longitude. The geopy is a client for several popular geocoding web services which makes it easy for user to locate the coordinates of addresses cities countries and landmarks across the globe using thirdparty geocoders and other data sources.In order to put the raw data into the same form as our input to the problem we group the raw taxi data by time (at the granularity of an hour) zip code temperature and participation of rainfall count the total number of pickups for each timezip codeweather combination and store these aggregated values as data points to be used for training and testing. For instance one row in our aggregated pickups table is “201404 01 00:00:00 49.0 0 10001 375” representing 375 pickups in zip code 10001 on April 1 2014 between 0 aM and 1 aM local time 49.0  and no rain. In total our data set consists of 710000 such data points.After preprocessing the data we did the exploratory data analysis by using Tableau.Firstly  we are exploring how the taxi demand is affected by the weather from two attributes: Temperature and Rainfall.From the temperature graph the overall trend of hourly pick up frequency is getting larger from middle 62 to the right and left sides.  Meanwhile the fluctuation is getting bigger from middle to the two directions as the temperature becoming colder or hotter. Thus the temperature has great influence on the taxi demand which is also very intuitive that people require more taxies to go out when the temperature are cold for example below 36. While when the temperature getting hot more and more people come to the city including travelers which increase the demand of taxies. While the influence from rainfall to the hourly taxi demand is counterintuitive. From the rainfall graph the hourly taxi demand does not increase as the participation of rainfall increase. But the fluctuation of hourly taxi demand is getting bigger as the participation of rainfall increase which indicate that the hourly taxi demand may affected by rainfall but may be not so much or it is a combination with other features to affect the taxi demand.Secondly  we are looking for the relationship among the different types of taxi. From the heat map Uber operation area focus more on Manhattan area. While the green taxi is not allowed to take passenger in Manhattan so there is no green points in the heat map. Last but not least yellow taxi has the most records and wide spread operation area.Furthermore we group the trip information by weekday and hour to see how the taxi demand changes.As you can see  Uber yellow taxi and green taxi follow the similar cyclical trend in both of the above plots. The total taxi demand peaks around Fridays and Saturdays while bottoms out around Sundays and Mondays. Meanwhile  it  peaks around 67 pm and bottoms out around 45 am within a day. It is more obvious by visualizing the hourly changes in animated maps below. The colour faded at the most around 5 am in the morning as we discussed above.By filtering the zip code within New York City our data set has 71000 observations.In order to evaluate the performance of our model we split the data into a training set (80% of data set) and testing set (20% of data set)  where the training examples are all ordered chronologically before the testing examples. This configuration mimics the task of predicting future numbers of taxi pickups using only past data.We chose RMSE to evaluate our prediction because it favors consistency and heavily penalizes predictions with a high deviation from the true number of pickups. From the point of view of a taxi dispatcher any large mistake in gauging taxi demand for a particular zip code could be costly ‒ imagine sending 600 taxis to a zip code that only truly requires 400. This misallocation results in many unutilized taxis crowded in the same place and should be penalized more heavily than dispatching 6 taxis to a zone that only requires 4 or even dispatching 6 taxis to 100 different zones that only require 4 taxis each. RMSE most heavily penalizes such large misallocations and best represents the quality of our models’ predictions.In comparing the results between our different models we also report the R^2 value (coefficient of determination) in order to evaluate how well the models perform relative to the variance of the data set.The multiplelinear regression model allows us to exploit linear patterns in the data set. This model is an appealing first choice because feature weights are easily interpretable and because it runs efficiently on large datasets. The result is showed below.To improve multiplelinear regression and introduce regularization ridge model is applied and the hyperparameters alpha used for ridge model are determined using Bayesian Optimization select parameter values. The result shows that ridge regression does not improve multiplelinear regression.Bayesian Optimization: uses a Gaussian Process to model the surrogate and typically optimizes the Expected Improvement which is the expected probability that new trials will improve upon the current best observation. Gaussian Process is a distribution over functions. A sample from a Gaussian process is an entire function. Training a Gaussian Process involves fitting this distribution to the given data so that it generates functions that are close to the observed data. Using Gaussian process one can compute the Expected Improvement of any point in the search space. The one gives the highest expected improvement will be tried next. Bayesian Optimization typically gives nontrivial offthegrid values for continuous hyperparameters (like the learning rate regularization coefficient and so on) and was shown to beat human performance on some good benchmark datasets.The tree regression model is capable of representing complex decision boundaries thus complementing our other chosen models.  Random Forest is chosen since it prevents overfitting and robust against outliers. And the hyperparameters maxfeatures(number of splits at each tree) and nestimators (number of tress) are determined using Bayesian Optimization select parameter values. Of the values we swept our model performed best with maxfeatures 14 and nestimators of 500.We listed the top 20 important features produced by random forest. As demonstrated in EDA  the hour weekday and temperature features are important. Also some zip codes  By further investigation the true value of taxi demand in these zip codes area have high value and high variance.XGBoost is an advanced gradient boosting algorithm. It is a highly sophisticated algorithm powerful enough to deal with all sorts of irregularities of data. The tool is extremely flexible which allows users to customize a wide range of hyperparameters while training the mode and ultimately to reach the optimal solution. For our model the booster parameters are tuned byBayesian Optimization to find the best combination of hyperparameters (listed in the table) where max_depth is the maximum depth of a tree learning_rate determines the impact of each tree on the final outcome ignorer to avoid overfitting  n_estimators is the number of trees gamma is the minimum loss reduction at each split  min_child_weight is the minimum sum of weights of all aberrations required at each split node  subsample is the fraction of observations randomly sampled for each tree and closample_bytree is the fraction of columns randomly sampled for each tree.Ensemble modeling is the process of running two or more related but different analytical models and then synthesizing the results into a single score or spread in order to improve the accuracy of predictive analytics and data mining applications. We combined our two strong models: randomforest and xgboost for ensemble modeling and used linear regression.The results of all the models are compared in the below table and ensemble modeling did not further improve the prediction as expected. Overall xgboost performs best.In order to visualize how well the models (randomforest xgboost ensemble) perform we plot the true versus predicted number of pickups for each data point in the test set in the Figure.The scatter plots in the figure suggest that the three models perform well on the test set. Most predictions lie close to the true values. The data points straddle the unit slope line evenly signifying that the models do not systematically underestimate or overestimate the number of taxi pickups. For three models as expected absolute prediction error increases as the true number of pickups increases. This effect can be visualized as a coneshaped region extending outward from the origin within which the data points fall.To take a deep look at how absolute prediction error varies the data is separated to 3 subsets based on the true number of pickups: subset 1 with pickups greater and equal than 1000 subset 2 with pickups greater and equal than 100 and less than 1000 subset 3 with pickups less than 100. From this table we can confirm that RMSE increases as the true number of pickups increases. Also xgboost performs best in different subsets.The below figure shows the comparison between the prediction of 3 models and the true value by randomly pick 10 samples from each subset.  At each individual value different models give different performance. So we did the predictions for the next coming week using three models and the result is visualized summarized and compared by shiny interactive application.can help user to compare the number of pickups of three different models (random forest xgboost ensemble) and across different locations in a given time zone  and also visualize the trend of the number of pickups in a 24hour cycle across different locations within New York City. Overall our models for predicting taxi pickups in New York City performed well. The xgboost regression model performed best likely due to its unique ability to capture complex feature dependencies. The decision tree regression model achieved a value of 35.01 for RMSE and 0.98 for R^2. Our results and error analysis for the most part supported our intuitions about the usefulness of our features with the exception of the unexpected result that participation of rainfall  feature is not important for model performance. A model could be useful to city planners and taxi dispatchers in determining where to position taxicabs and studying patterns in ridership. In the future we will implement the 2 below models.Neural network regression: We may be able to achieve good results using a neural network regression since neural networks can automatically tune and model feature interactions. Instead of manually determining which features to combine in order to capture feature interactions we could let the learning algorithm perform this task. One possible instance of features interacting in the real world could be that New Yorkers may take taxi rides near Central Park or when it is raining but not when they are near Central Park and it is raining since they may not visit the park in bad weather. Neural networks could be promising because they can learn nonlinearities automatically such as this example of an XOR relationship between features. In addition to our three core regression models we implemented a neural network regression model using the Python library PyBrain. However we would need more time to give the neural network model due consideration so we list it here as possible future work.Kmeans Clustering: In order to find nonobvious patterns across data points we could use unsupervised learning to cluster our training set. The clustering algorithm could use features such as the number of bars and restaurants in a given zone or distance to the nearest subway station. The cluster in which each data point falls could then serve as an additional feature for our regression models thereby exploiting similar characteristics between different zones for learning.,NA,Predict New York City Taxi Demand
https://nycdatascience.com/blog/student-works/horse-racing/,50,"Horse racing and data go handinhand.  The vast array of statistics about horses jockeys trainers lineage of horses and much more is impressive and the application of this data in determining odds of success is integral to the sport.  For centuries people have worked to understand the relationships among the data in an effort to better predict the success of a horse with a dream of “striking it big”.  With so much data and the possibility of immediate application of predictive models we became quickly enthralled with the idea of building a better model to predict outcomes.  Since half of the final project team was from India and due to the relative ease of obtaining the data we chose to focus on horse racing in India.  Our decision to proceed with this project was easy but was the last easy step in the process.The first challenge was finding data.  The entities that own the data control it tightly.  Results from individual races are findable on the internet but there is not a single location that has all the data nor is there a compiled database that was available to us.  Our first challenge therefore became creating a database upon which we could train models.  We used beautiful soup in Python to scrape over 3500 web pages of data.  We were able to build an initial database with records from the 5 major tracks in India spanning almost 10 years of data.The database was formatted as a single row for each horse appearing in each race totalling 210000 runnings.  The data in each record included information about the horse and its lineage the trainer and jockey and statistics including the horse weight speed rating racetime odds gate draw and age as well as the variables we might want to predict the finish place and finish time.The challenge with this project continued when we looked at the data.  The following illustration highlights the extent of the missingness in the data.The racetime odds were missing from nearly 35% of the data.  We struggled with how to handle this missingness debating whether more bias would be introduced by imputing or by dropping these records.  Ultimately the racetime odds proved to be vital to developing a model with predictive power and the range of odds between the favorite and other horses was sometimes very large.  Because of this we concluded that it would produce less bias if we dropped the records entirely.  We have to proceed with one large caveat: we do not have an understanding of why this data was missing and how it might affect the conclusions if it were available.  However we found it to be the case that odds info was missing from entire races or it was complete for entire races and it was not the case that individual horses in a race were missing odds info.The rest of the missingness was more understandable or was easily imputable without introducing meaningful bias.  For instance occasionally the rating would be missing.  In the case of a missing rating we set that horse’s speed rating to be the mean of all the horses in the race.  We ended up engineering features that had missing data for several reasons and we were able to use the same procedure of imputing those values with the mean of a race.Our first look into the cleaned dataset provided us with some clues about how to proceed.  The first thing that stood out was extent to which the odds alone were predictive.  The implied probability of the favorite horse winning the race was about 42% derived from the racetime odds.  The favorite horse actually won 43% of the time.  The mean odds of the favorite was 1.7/1 which meant that a simple “pick the favorite” strategy would have been profitable.  Our earlier caveat raises its ugly head at exactly this time… if there was some reason for the missingness that has to do with the odds then this would not hold true.  The following chart highlights the implied probability of winning for horses starting in each rank of the favorites table.  Also included is the actual win percentage and the number of observations.  For horses beginning a race as the oddsfavorite the mean implied win percent was the aforementioned 42%; the actual win rate was 43% in over 13000 races.Given the persistence of winning for specific horses trainers and jockeys we wanted to make sure that our featurespace included the history of each of these groups.  We created methods of grouping and filtering in Python to create historical records for each horse trainer and jockey for all races preceding the current race and appended to the appropriate record.  Each record now contained historical run times win place and show percentages for individual horses as well as win place and show percentages for trainers and jockeys.  We then applied procedures to group within races and added features that described the difference between each horse and the rest of the field  comparing prior race times speed ratings and weights. We now had a full featurerich dataset that included 20 more features than the simple data we had scraped.Individual models don't compare well to an oddsonly model:
",NA,Predicting Horse Racing Outcomes in India
https://nycdatascience.com/blog/student-works/finding-fare-uber-recommendation-system/,50,"Driving an Uber in New York City is a difficult proposition. Navigating the crowded streets of Manhattan competing with yellow cabs for fares and sometimes driving for miles just to get . The margins of driving an uber are razor thin and drivers can’t afford to spend their precious minutes looking for a new fare. While this is certainly an issue for its employees the problem of finding new fares quickly is also dilemma for Uber itself. Uber needs as many drivers as it can get in New York otherwise the response time to new fares will not be sufficient compete with yellow cabs and other apps. Drivers won’t want to drive for Uber if they can’t make money so they need their drivers picking up new fares as quickly as they drop them off. Uber also wants their drivers to head to areas that will be popular in advance so that their customers don’t have to wait too long for their ride.We attempted to solve these problems with this app. While our program won’t solve every problem with driving an Uber in New York we believe that drivers will get a great deal of benefit out of using it. We encourage you to try out the app and we hope that you enjoy it!We used two different sources for our data on taxi rides. The Uber dataset which was roughly 100 MB and 3 million rows long was taken from FiveThirtyEight’s Github account. The data was available to FiveThirtyEight as the result of a freedom of information act and thus only contained the longitude latitude and time of each ride. The yellow cab data was download from the city of New York’s website which contained many more variables than just latitude/longitude. We automated this process with the following script:
From the original format we grouped the data by neighborhood and split it up by time of day using ten minute intervals. We also created our variable of interest which was the sum of uber and taxi rides.We knew we needed to include weather data in our analysis as anyone who’s tried to hail a cab in a rainstorm can attest. We downloaded data from Weather Underground which contained daily weather statistics recorded in Central Park going back several years. It’d be better if we had weather broken out by hour or minute but this measure should give a rough indication for conditions on each day.We also scraped data from newyorkgasprices.com for a complete list of local gas stations and their prices. This site was a bit cumbersome to scrape but we accomplished it with the following script:Now that the data is collected and clean it’s time to implement a model to do the actual predicting. We tested two different models: linear regression and random forests.We implemented linear regression using the sklearn package in python. While linear regresion is not the most predictive model we thought it would be useful to test as it can quickly and efficiently return results (unlike other machine learning methods that take longer). When we tested the model it returned an R^2 value of .72 and an RMSE score of 112.1. Here is our final code for its implementation:We also tested random forests a model that is significantly more predictive and expensive than linear regression. We saw much better test results from Random Forests with an R^2 of .93 and an RMSE of 10.5. While it takes a bit longer to return results than linear regression we opted to use Random Forests in our model for its greater predictive power.All of these predictions wouldn’t be very useful without an interface to display them so we built a Flask app for drivers to use. We wanted to limit the amount of information that drivers had to input into the app so we only ask for the distance that they’re willing to drive and if they’d like to get gas.The visualization on the front page is meant to give drivers a rough idea of where most fares are happening throughout the day. This was built using R and ggplot2 and the data is grouped by latitude and longitude every half hour. We looped through every half hour and made a separate graph for that time which we output to a .png file. We then used gifmaker.com to turn those images into a completed gif.As we previously mentioned we wanted to limit the amount of fields that the user had to input when using our app. Our algorithm requires quite a bit of information to generate its prediction however so our app uses APIs to retrieve the relevant data. The user’s current location is retrieved from your IP address. Weather conditions are taken from Dark Skies which returns the current temperature and the precipitation rate. Travel times and traffic conditions are calculated using the Google Maps API and the current date and time are taken from the user’s computer. After the current conditions are collected the app takes the radius that the user has inputted and excludes any neighborhoods that are too far away. This saves a great deal of computational energy so we can return results faster and more efficiently. For each of these neighborhoods the app inputs the current conditions and calculates the predicted demand. It then returns the best neighborhood along with directions to that neighborhood from Google Maps.If the user checks the box indicating that they need gas directions to the nearest gas station are returned. This is simply done by calculating the distance to all available gas stations using Google Maps and returning the direction to the closest one. This project was a fascinating topic and we both greatly enjoyed working on predicting Uber rides in New York. This is clearly an important area and there is much more work that can be done. With better data from Uber we could learn a tremendous amount about where and when the next fare is coming. We sincerely hope they make more data available soon as it could lead to more fascinating explorations.",NA,Finding The Fare: An Uber Recommendation System
https://nycdatascience.com/blog/student-works/15212/,50,"In August 2016 Bosch one of the world's leading manufacturing companies launched a competition on Kaggle addressing the occurrence of defective parts in assembly lines. This post focuses on the machine learning pipeline built for the competition and how to preprocess the large dataset for a traditional machine learning modeling process.Manufacturing industry relies on continuous optimization to ensure quality and safety standards are respected while pushing the production volume. Being able to predict if and when a given part of a product will fail the standards is an essential part of such optimization as it leverages the existing massive amount of data recorded in the production line without affecting the process. This argument is particularly relevant in the “assembly” phase since it accounts for 50% to 70% of the manufacturing cost. Bosch among other companies records data at every step along its assembly lines in order to build the capability to apply advanced analytics and improve the manufacturing process.A quick check at the dataset header allows drawing a sketch of the assembly line used for this dataset.The assembly line is divided into 4 segments and 52 workstations. Each workstation performs a variable number of tests and measurements on a given part accounting in total for 4264 features. Different products may not share the same path along the assembly line nor there seem to be a common starting or final workstation. Each of the 1183747 parts recorded in the dataset follows one of the 4700 unique combinations. As shown in fig. 2 one observation  can be interpreted as a series of cells (yellow boxes) where the object is processed. Conversely features may be described according to their popularity (number of rows/parts for which the feature exists) and defective rate defined as the percentage of the parts being measured at a given feature and found to fail the quality test (see fig. 2). It is interesting to notice how features with high defective rate (>0.6%) are clustered around specific areas mostly in line 0 and 1.  
By just loading the first few rows of each dataset it is possible to check all the column names at the same time and discover whether there is any pattern in the structure. As the result each of the tables in training and testing set has the same number of observations respectively which suggests that the data was separated by column from a single table therefore the original table can be restored by simply binding all the three tables together without any advanced joining procedure. This answered the previous question about D codes  it turns out that the last digit of each column is just the column number instead of the feature ID. Each timestamp column is located next to corresponding F column which explains why D(n) columns are describing F(n  1) columns.Next there is a massive missingness within the dataset. To be specific only 5% of numeric values 1 % of categorical values and 7% of timestamps were NOT NULL. This is quite understandable  each observation only goes through a certain number of stations and will not be touched by most of other stations. Therefore the missingness was not at random. Thus if a proper transformation method can be applied to squeeze out those void cells from the data table the physical file size of the data file can be significantly reduced and make it possible to apply machine learning directly on the entire dataset.What are the reasons for producing a defective part? What is the likelihood of detecting an error in the assembly line? It is reasonable to assume that such likelihood increases with the number of steps required in order to produce a part. Similarly the higher the number of measurements the higher the time required to complete the part/product. By using this simple assumption we can use produce (at least) three new features related with the “process” rather than the individual feature. This is particularly relevant for time stamps (Date dataset) where most nonnull features for a given row show only very few (around 3) unique values. By calculating the time lapse (TMAXTMIN) the entire dataset can be reduced effectively from 1156 to 1 column. The other feature namely the number of steps (nonnull features) per row can be calculated for both numerical and categorical datasets.A second major “gain” in dimension reduction can be obtained on the categorical dataset by noticing a large number of duplicated columns (1913) probably referring to the same features measured at different stations. Furthermore the categorical features have only 93 unique values. Rather than encoding the features (preserving the original feature set) we chose to look at the appearance of each categorical value. Combining these transformations the original 2140 features shrink to 93 dummy variables. After feature engineering the dataset is ready to be fed into the machine learning pipeline. Due to the high correlation among variables a large number of observations  and nonrandom missingness patterns of the data the tree models are expected to perform better in this scenario because they are capable of picking up correlations among variables during the training process. However a logistic regression model was still trained and the performance of this model can be used as the baseline for measuring other models' performances. The metric been used to evaluate each model's performance is the Matthew Correlation Coefficient (MCC) which is equally valuing both true positive and true negative rates and the range of this score is from 1 (perfectly incorrect) to 1 (perfectly correct).Next a tree model shall be selected to better adapt to the missingness of the data as well as to achieve a better MCC score. Considering Random Forest is extremely computationally intensive and not able to handle missing values XGBoost was selected due to its high computation efficiency and capability of dealing with missing values automatically. Meanwhile the only hyperparameter of this model that has been modified was learning rate which was set to 1 in order to get fast convergence. A fivefold crossvalidation on training set shows that this basic model has achieved an MCC score of 0.24 which is a huge improvement!",NA,Bosch Production Line Performance
https://nycdatascience.com/blog/student-works/restaurant-recommendations-groups-people/,50,Have you ever wanted to find a restaurant for you and a friend to go to only to have trouble finding one due to conflicting tastes and tedious perusing of restaurant listings and reviews? Wouldn’t it be so much nicer if you could get recommendations based on both of your preferences?We asked ourselves if this could be done a better way when we began this project. The company Yelp collects information and reviews about businesses to better inform consumers. They posted a  from which we extracted relevant information for restaurants. Here are some details about the data:The Challenge Dataset:We set out to use this data together with machine learning to predict which restaurants a pair of people might find enjoyable. Our recommendations are shown to users in an app which provides personalized recommendations for multiple users. Our product Yelp Nearby is a multiuser restaurant recommendation engine. The front end is built on Flask a Python web application microframework in combination with templating languages including Jinja2 CSS and HTML. Javascript was also used to employ Google Maps API into our application. On the back end we used both advanced machine learning framework GraphLab and homemade modules to build our recommendation system. In order to allow users to get recommendations anywhere they want in the United States we also created functions to get a list of restaurants on the map from the Yelp API.On our Yelp Nearby application users can indicate what types of food they would like to eat and get restaurant recommendations in any US city. Yelp Nearby allows users to browse through recommendations without having to adjust to a new interface since it works like most major recommendation applications such as Tripadvisor and Airbnb. Users can either scroll the recommendation list on the lower left corner or click on markers to pick a restaurant. Detailed information such as address phone number price range rating and a link to the restaurant’s Yelp profile is displayed. Our recommendation system considers not only the general ratings and comments about restaurants but also the tastes of both users based on their Yelp profiles and their current mood. The detailed construction of the system will come up next. In this capstone project we use collaborative filtering technique to build our recommendation system. On the other hand the yelp academic data set does not cover the full 50 states in U.S.  In the dataset there are about 25000 restaurants but there are about 600000 restaurants in U.S more than 20 times above the Yelp dataset coverage.  We also like to make dining recommendations for a group of two or multiple users.  Thus we adapt a threestep pipeline in building our recommender. The collaborative filtering allows us to make tailored recommendation for individual users even if the particular user’s taste of dining is not fully reflected in her/his Yelp review history.In the Yelp data set there are detailed user attributes including the number of review counts the average star ratings she/he had given the reviewer’s social network on Yelp the annual Yelp elite status statistics etc. On the business (item) side the business review counts average star ratings the review texts the business location etc. Through a careful feature engineering procedure one can select the combinations of numerical or categorical features to feed to the graphlab factorization/ranking recommender.    The inputted side information helps to tailor the recommendations specific to the user’s taste.To analyze the multipleuser’s common eating preference in a dining location outside of Yelp academic data coverage (like New York city San Francisco etc) we cannot output the recommender’s recommendation to the user directly.  Instead we adapt a clustering technique known as density based spatial clustering of application with noise (DBSCAN) to cluster the 25000 restaurants into different clusters.  Finally the recommender’s output and the locally available restaurants are classified into the known restaurant clusters to compare their similarity.   We make recommendations choosing the local restaurants in the user chosen categories which are most similar to the output of the collaborative filtering results.This design makes our recommendation system the socalled hybrid recommendation system in the literature. Among the various designs of recommendation system there are three main types of recommendation systems relevant to us. The baseline popularity based recommendation the more traditional item (content) based recommendation the more advanced collaborative filtering recommendation.  Each has its own strength and its own weakness.The popularity based model gives out recommendations most popular among the aggregate user groups.  It does not need any user nor item information to work. But it completely ignores the different tastes of the users. In the context of dining the tastes of the food critics can be vastly different from those of the masses.  Even among the food critics or the generic citizens their favorite dining habits can be quite different. The popularity based recommendation which is the baseline of all the other recommendation systems is too static to satisfy the different demands from the different user groups.The commercial package GraphLab stands out in its implementation of the collaborative filtering not only in its varieties of algorithms but in that its ability to input the side user/item content information to improve the recommendation quality.  This partially alleviates the cold start issue of the regular collaborative filtering based system in that the users with no review history at all can still get personalized recommendations.The heart of the collaborative filtering is to factorize the useritem review matrix.  In the context of the Yelp dataset schematically we have the following UxI review rating matrix on the left.The user latent factor loading matrix records the weights the various latent factors’ influences on the user list. The item latent factor loading records the way the latent factors influences on each item’s rating.  The factorization of R into the product structure signifies the modeler’s attempt to explain the user ratings in terms of the confluence of ‘how does the individual user get influenced by the various latent factors’ and ‘how does the individual latent factor affects the ratings on the list of all businesses/items’.The GraphLab’s factorization recommendation system has several variations depending on the choice of the iterative procedures whether to include the side useritem content information into the matrix factorization etc. The key idea is to combine the matrix factorization with the more well known linear regression and coefficientwise L2 regularization.Our collaborative filtering system is based on the ranking/factorization model of GraphLab in which the optimization of the sum of the squared residual errors can be expressed as three main groups. The key terms gauges the squared error of approximating the review matrix by the latent matrix factorization. The second group resembles those of the multilinear regression of the side information. Finally there are L2 penalty terms to control the model overfitting. This step is essential because the number of latent factors and the amount of side information input into the model are determined by the modeler which can easily led to overfitted recommendation model if they are not chosen carefully.   The optimization of these three types of terms lead to a single system of equations combining matrix factorization linear regression and ridge regression together.The Yelp academic data set includes many attributes of the reviewers and the businesses. We selected some of them to include in the recommender model training.In the above table we list some of the important ones which we include as the side information.  Even though the collaborative filtering technique does not need the side information to provide user recommendation the inclusion of them into the model helps to make the recommendation more specialized to the particular user. The GraphLab package allows the modeler to determine whether to include the side information into the matrix factorization procedure or merely to use in the linear regression portion.The circa 25000 restaurants included in the Yelp dataset did not cover the entire United States and included only a small proportion of the over 600000 restaurants in the country. Making recommendations outside of the dataset posed a challenge. How could we provide a recommendation for a user who is in New York or San Fransisco if those cities are not included in the model? The answer was to cluster businesses and find similar restaurants in the local city. Extrapolation should not generally be done but this is not a case of trying to predict a stock’s value a year into the future; if a user would likely enjoy an Italian restaurant with seafood outdoor seating and high ratings with few reviews in Arizona it is likely that user would also enjoy a restaurant with similar qualities in Florida.Latent Matrix Factorization produced  the list of recommended restaurants for a particular user comprised of restaurant attributes and values. From the user app filters specified by the user are utilized to generate a list of restaurants in the target area. These two sets of restaurant lists with their attributes are then processed through the cluster analysis model to determine the recommendation list.The table below shows the different types of cluster algorithms that were considered by our team. We ran the data for both KMeans and DBS but decided to settle down with the DBS Clustering method.With more time there are enhancements that can be made. These include:,NA,The Restaurant Dilemma: Personalized Recommendations for Groups of People
https://nycdatascience.com/blog/student-works/capstone/yelper-collaborative-filtering-based-recommendation-system/,50,"""Getting information off the internet is like taking a drink from a fire hydrant"" (Mitchell Kapor). Information overload is a real phenomenon preventing us from making good decisions or taking actions. This is why recommendation systems are becoming common and extremely useful in products such as Netflix Amazon Echo and Facebook News Feed in recent year.Besides in the big data era realtime recommendations may become a new norm because realtime recommendations:Technically we use Spark MLlib to train the ALSbased collaborative filtering models in Yelper (). Below are several brief steps:Now imagine that there are one thousand or even one million of customers who need to access Yelper to get recommendations in different cities. Before we show how Yelper handles this challenging scenario let us first delve a little bit deeper into why this scenario is challenging.○ More graph analysis○ Improve recommendation",NA,Yelper: A Collaborative Filtering Based Recommendation System
https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-competition-2/,51,"Contributed by Shuo Zhang Chuan Sun Bin Fang and Miaozhi Yu . They are currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between July 5th to September 23rd 2016.Discovery of the long awaited Higgs boson was announced July 4 2012 and confirmed six months later. But for physicists the discovery of a new particle means the beginning of a long and difficult quest to measure its characteristics and determine if it fits the current model of nature.  The ATLAS experiment has recently observed a signal of the Higgs boson decaying into two tau particles but this decay is a small signal buried in background noise.  The goal of the Higgs Boson Machine Learning Challenge is to explore the potential of advanced machine learning methods to improve the discovery significance of the experiment. Using simulated data with features characterizing events detected by ATLAS our task is to classify events into ""tau tau decay of a Higgs boson"" versus ""background.""We have tried a number of different machine learning methods. Below is a quick review of the methods with a general description their pros and cons.Let us have a look at our first treebased model Random Forest(abbreviated as RF below). There are two parameters we need to tune for RF: ntree and mtry. Ntree controls how many trees to grow and mtry controls how many variables to draw each time. Due to the large size of the data my first try was very ambitious: ntree  [200050008000] and mtry  [3456]. However it turned out that the computation was tremendously large and led to crush of R. So my second try was less ambitious with parameters ntree[5008001000] and mtry  [345]. From the below graphs we can see that mtry  3 is the optimal parameter for each subset of data set.However the AUC curve looks abnormal because the graphs showed that the RF predicts every single observation correct. Why is that?In our case since weight is estimated from a probability space. In order to do prediction RF model bisect on the probability space continuously until reach certain criterion (eg. no more than 5 observation in each subregion) and use the mean of each subregion to predict. However our probability space is very sensitive and ntree  1000 is too large thus making the RF too accurate. What is more by looking at only 3 variables (especially the traversemass variable) we can clearly tell whether the label is going to be a 'l' or a 'b'. So the first few split of the tree model is already sufficient to make the prediction. By making ntree equal 2 or 10 we can get a 'normal' graph. However doing so is not very meaningful because it simply made a fine model less finer.To improve RF next step is to apply gradient boosting. There are 4 tuning parameters: interaction.depth n.trees shrinkage and n.minobsinnode. Interaction.depth controls maximum nodes per tree  number of splits it has to perform on a tree (starting from a single node). For example interaction.depth  1 : additive model and interaction.depth  2 : twoway interactions. N.trees means the number of trees (the number of gradient boosting iteration) and increasing N reduces the error on training set but setting it too high may lead to overfitting. Shrinkage is considered as a learning rate which is used for reducing or shrinking the impact of each additional fitted baselearner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous its negative impact can be easily corrected in subsequent steps. N.minobsinnode controls the minimum number of observations in trees' terminal nodes which is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances. Imposing this limit helps to reduce variance in predictions at leaves. We can conclude the 4 tuning parameters are related with each other and change in one parameter has impact on the others.Let's take subset 1(jet_number equal to 1) for instance. Due to the large size of data and computational cost our first try is to use small trees (200 500 800) with lower interaction depth (3 4 5) a wide range of shrinkage (0.1 0.05 0.01) and small values of n.minobsinnode (10 50 100). The best optimal parameters with the highest AMS value is 800 n.trees 5 interaction.depth 0.1 learning rate and 100 n.minobsinnode which lead to AUC equal to 0.88(as shown in the graph). Therefore our second try is to fix n.trees and increase interaction.depth to (5710). The best optimal parameters with the highest AMS is 10 interaction.depth but it produces a lower AUC value equal to 0.86. The possible reason is overfitting by introducing too much interaction between the features. So our third try is to fix interaction.depth to 5 increase n.trees to (800 2000 5000) and lower shrinkage to 0.01 in order to reduce overfitting. And 5000 trees results in a higher AUC value (0.91). So our last try is to increase n.trees to (5000 7500 10000) and fix interaction.depth and shrinkage to 5 and 0.01. Our best optical parameters (interaction.depth:5 shrinkage: 0.01 n.trees: 10000 n.minobsinnode: 100) give us the best result with AUC equal to 0.91.The same process of tuning parameters is applied to the other 2 subsets.For subset 0(jet_number equal to 0) the best tuning parameters are interaction.depth: 5 shrinkage: 0.01 n.trees: 10000 and n.minobsinnode: 100 and result in AUC equal to 0.91.For subset 2(jet_number equal to 2 and 3) the best tuning parameters are interaction.depth: 5 shrinkage: 0.05 n.trees: 800 and n.minobsinnode: 100 and result in AUC equal to 0.91.There are 6 parameters to tune for Xgboost.Our tuning strategy for xgboost is as follows: We combined the training results from the three split datasets into one submission file as illustrated in the handdrawn picture below.After the split each subset corresponds to its own optimal tuned parameter. Then we were facing two strategies to combine the three vectors of probabilities:We tried both strategies multiple times and submitted our results to Kaggle. However the rankings fluctuated around 1000 in the private leaderboard which did not meet our expectations. Indeed at the submission stage it is possible for us to further tune a ""magic"" cutoff value say the value ""C"" in strategy 1 above to obtain a great submission file that achieving relatively higher AMS score or ranking in the leaderboard by submitting perhaps hundreds of times to Kaggle.However we didn't step into this direction because we don't think repeatedly submitting results to Kaggle to achieve high ranking was an elegant solution. .We sit back and started to figure out the root cause. Soon our team realized that this combining process is actually mathematically flimsy.Let's again look at the examples in the handdrawn graph. Although P03  0.36 in the subset jet #0 and P12  0.36 in the subset jet #1 the two probabilities 0.36 has totally different meaning:In other words the predicted probabilities in each subset were all conditional probabilities a.k.a conditioned on jet numbers. Thus two identical probability values from two subsets mean different things and they were not necessarily comparable with each other because we had no prior knowledge about the internal distributions of each subset. Even if we do do we know the signal distribution of the testing dataset? Not at all. The testing dataset could contain 15% observations with jet #0 or completely no jet #0.To sum up splitting the dataset based on jet number is indeed a possible way to achieve relative acceptable ranking or AMS score. However based on our analysis it also has drawbacks:",NA,Team DataUniversalis' Higgs Boson Kaggle Competition
https://nycdatascience.com/blog/student-works/signal-noise-finding-higgs-boson/,51,An experiment was designed to find the last missing particle in the current model of physics the theorized Higgs Boson. This elusive particle was found in 2012 and the discovery warranted a Nobel Prize in 2013. The experiment was performed at the European Laboratory for Particle Physics whose abbreviation CERN is derived from its French name.The analysis of the massive amount of data generated by the experiment was a challenge. One primary task of the analysis was to discern the particle signal from the background noise in the data. Machine learning can help people make this distinction and in this article we will discuss which methods are best for this task and why we chose to use them.The data come from the Higgs Boson Challenge on the data science website Kaggle which hosts competitions for model creation and prediction.  to see the code we used for our models.Ensemble models provide better predictions than singular models because they reduce the bias of the model reduce the variance of the model and are unlikely to overfit the training data. Three possible variants of ensembling are averaging voting and weighting. Averaged ensemble models take the outputs from several models and average their results. Voting ensemble models take the categorical output of multiple models and allow the models to vote on the outcome. If an even number of models are included a way of breaking ties must be defined. Weighted ensemble models choose weights to apply to each of the model outputs enabling individual models to exact larger influence. We created a threemodel weighted ensemble and will be discussing the results the ensemble and the individual models in the ensemble in this blog post.The ensemble model was constructed of three models: an XGBoost model a random forest model and a logistic regression model. The output probabilities of each model were weighted by coefficients which acted as weights for each of the models. The coefficients were constrained to be between 0 and 1 and their sum must necessarily be one so that the output probabilities would also lay between 0 and 1. The equation used was:_XGB +  * _RF +  * _LOGIT  _ensembleA threshold  was then applied to the ensemble probabilities to classify the observations into signal and background. The four tuning parameters to the ensemble model  and  were trained with a grid search on bootstrapped subsets of the training data. The grid was constructed by splitting each of the four tuning parameters into 100 values between 0 and 1 and constraining the sets to those where the first three sum to 1. Before discussing the outcome of the ensembling we will first turn to each of the individual models and their performance.EXtreme Gradient Boosting (XGBoost) authored by Tianqi Chen Tong He and Michael benesty is ”an efficient implementation of (the) gradient boosting framework.” ().  As such it is a computationally friendly and extremely powerful learning algorithm that is used in many Kaggle competition to provide highly accurate predictions.  We chose to tune and train a XGBoost model despite its limited interpretability in an attempt to maximize our AMS score and ultimate place in the Kaggle competition.  We began with code from the authors that had been specifically written for the Higgs Boson Challenge.  Although optimal parameters had also been provided with the code we chose to write our own tuning gridsearch to seek for even better sets of parameters.  By varying the learning rate (eta) maximum depth of individual trees (max_depth) and the number of trees built in each implementation of the model (nrounds) we arrived at an optimal parameter set {eta  .05 max_depth  9 nrounds  120}.  Results were compared by calculating an AMS score on the training data and these parameters selected provided the highest AMS score.  It should be noticed that there are numerous other sets of parameters that could be finetuned and given more time/computing power we would have chosen to hypertune the parameters further.  However the ultimate results provide evidence of the power and predictability of the algorithm.  AMS scores on the training data were in the range of 2.5.  When applying the XGBoost model we built to the test data and submitting to Kaggle we were able to achieve an AMS score of 3.573 which would have placed us in place 661.The primary drawback of the algorithm however lies in its interpretability.  We were unable to speak to the contribution of individual features to the final prediction of the model.  However the goal of Kaggle competitions is primarily predictive power and the emphasis on predictability is negligible.  For interpretability we built a logistic model which will be discussed below.Our initial inspection of the dataset highlighted missingness in the data much of which was due to the number of “jets” for which there was data.  We trained the XGBoost model using datasets split by the number of jets to see if there would be greater predictability if the missingness was artificially dealt with by humans.  As it turns out the computer is much better at managing the missingness in the data especially when using an algorithm as powerful as XGBoost.  One of the main reasons for choosing random forests to be a part of the ensemble included the algorithm’s ability to pick a subset of predictors at each node split of an  individual tree. This behavior leads to a decorrelated solution once aggregated across several trees which reduces the variance of the final predictions. In other words it prevents the overfitting that other models may be prone to.We approached the random forest in the same way we did the XGBoost in that we ran it on the data set that had been split by the number of jets and also on the complete training set. Parameter tuning was done on a leaner training file consisting of 5000 training and 5000 testing records using the Caret package in r. The substantially reduced size was considered mainly for performance reasons as the algorithm is fairly computationally heavy. We attempted to determine optimal parameter settings by trying to maximize AMS score as well as minimize Out of Bag error on the subset of the training data. The parameter tuning step yielded an optimal level of 7 for the ‘mtry’ variable which denotes the number of predictors that should be considered at each split in the tree (from among the 30 available predictors). A tree depth of 10 terminal nodes was also found to be the best though growing deeper trees would always help learn more from training data while not necessarily overfitting the test data.After utilizing these parameters on the data sets split by the number of jets we achieved a fairly low AMS score of 2.53. We were surprised at these results since this split data required minimal imputation of missing values and it more closely represented a logical view of the data that should be present by the number of jets. On our second attempt with the unsplit training data set of 250000 records we needed to impute a larger amount of data including some variables that had 30 to 70% of their data missing. We decided to use a bagimputed approach which uses a bagged model of the predictors that have data to impute the values for the predictors that don’t. We made an additional tweak to let the random forest learn as deeply as possible without pruning. While there is a notion of overfitting on the training data here it still yielded a better result on the data with a resulting AMS score of 2.9. As such we used the probabilities from this model’s output in the final ensemble. Another reason for using Random Forests was for the interpretability that comes with the variable importance scores. This analysis quantifies the increase in misclassification error if a variable is not included in the model. The table below summarizes the variable importance of the final model (top 20 metrics included). The caret package produces importance scores that have been scaled to be on a 0 to 100 scale for ease of comparison.As can be seen from the table above three out of the top four variables are mass related variables suggesting that this is an important aspect of the data that would help us identify signals in a more efficient way versus the background noise.  There is one more mass related variable included towards the bottom of the table but this was one of the metrics that had 70% of its data imputed likely reducing the amount of variance in the metric. Compared to the other models we built the logistic model was simple to build quick to run and optimize and offered better predictability.  Like with the other algorithms we split the data by jet number and trained three individual models and compared to a logistic model trained on the full dataset.  A first set of models was trained using all variables/features however when reviewing summary outputs of the logistic model in R it became clear that there were a significant number of variables which we could not say were different than zero with any confidence.  We excluded these features from the final models.  The table below shows the variables we were able to confidently say were significant to the final model.  Interestingly a large number of variables appeared to be significant in each of the three split datasets and the full model.  With more time we would want to look further at the excluded variables to see if they do not contribute to the predictability in other models.  The final models were trained using a reduced number of variables/features and individual tests were run on the full dataset and the split datasets.  AMS scores differed significantly between the two methods and the split dataset ultimately provided the best accuracy and thus AMS score on the training data.  While scores compared to the XGBoost model were low we included it in the ensemble model to see if it could provide an incremental boost in score.  The full model ultimately scored 1.20 on the test data while the split data utilizing a threshold that maximized the AMS on the full dataset scored 1.83 on the test data.  This is the ideal model to include in the ensemble model.The models were combined in an ensemble model first with only the logistic regression and XGBoost models and then once again with the random forest model. The optimized weights were as follows:The results for the 2model ensemble are not altogether surprising: it heavily favors the individual model which produces the highest AMS score the XGBoost model. The AMS score of the ensemble was 3.605 when uploaded to the Kaggle private leaderboard which would have garnered place 569. This was our best result.The figure above shows the training AMS score on the yaxis and the model weights on the xaxis for the optimized probability threshold of 0.93 for the signal/noise classification. That means that the model weights represented by a horizontal line on the graph will equal to 1 and the intersection of that horizontal line with the yaxis will show the AMS score of that particular ensemble. The highest AMS score on the training data was achieved with the XGBoost weight at 0.98 and the logit weight at 0.02.The results for the 3model ensemble were more surprising and require deeper investigation to fully understand. The most accurate model the XGBoost model was given a weight of 0 and therefore had no influence on the 3model ensemble results. The random forest dominated the weights sharing 10% of the outcome probability weight with the logistic regression model and maximizing the AMS with a threshold of 0.45. The test AMS on the Kaggle private leaderboard was 2.794 significantly worse than the XGBoost + Logit model indicating that the dominating model the random forest was fit too well to the training data. Since the weights were chosen from the training data it makes sense that the random forest model would dominate. In order to reign in the influence of the random forest a simple voting or average ensemble would be a logical next step to still include the benefits of each of the models while not allowing one to dominate the results. These results could then be added as another predictor variable in the original dataset and fed into another XGBoost model. This is known as stacking and has been shown in other contests to produce even more favorable results.,NA,Signal to Noise: Finding the Higgs Boson
https://nycdatascience.com/blog/student-works/kaggle-higgs-boson-machine-learning-challenge/,51,"This blog encompassesin this case is binary:  either there is a Higgs Signal 3.53929.A descriptionThe data mainly consists ofThe snapshot below shows this information as it is displayed on the website. peculiar aspect of both the training and test data is thatThe data’s description statesOne can analyze the nature and extent of these 999.0 entries by replacing them with ""NA's"" and doing this missing data analysis. We star The plot to the leftin thefigure it is evident thatwhere one or no jet events are undefined. FurthermoretheThere are alsoIn conclusionin terms of whether or not the Higgs mass is defined and correspondingly if the event resulted in  onejet  more than one jet  or no jet production at all.  (2 x3) . We  add two new features to incorporate this information .AMS istheparticularfor the competitionThe red region corresponds to high AMS scores which are linked to low false positive and high true positive rates That is expectedone such model would be identifying just 25 % of Higgs signal correctly keeping the false positive rate to 3%and still have the same AMS score of 4.in the previous plotproduced by a perfect prediction of the training dataTheblue line below indicates AMSevery prediction is said to be aFor each of the data subgroups we calculate the AMS score by assigning either signal or background noisetoall the events in that subgroup and using correct predictions for the rest of the data .The figure above shows that performance on data where the Higgs is not defined has almost no effect on the AMS score if everything is classified as background signal for these subgroups.  This behavior occurs because there is very small signal in the data where the Higgs is not defined.Moreoverthe weight of the signals is low and the background noise has higher weights. These three factors make  identifying everything as background noise is as good as identifying everything correctly.Thusclassifying all events where the Higgs is not defined as backgroundin terms of thethe subgroupsThis is a pA densityWe first look at theThe order here is {210 }. Upper triangle which corresponds to DER (derived) features in all three seems to be correlated .  These are not observations but features engineered by CERN group using particle physics. The lowercontainsExamining the variance explained by theroom for the For examplefifteenWe do this forLet's look at the density plot of the last 9 whitewashed features of subgroup 2. We see they share a few common characteristics .As pointed earlier they  are all angle features for directions of particles and jets.In the case of the 5 phi angle features they have uniform and are identically distributedover the range for both the signal and background. This is true to some extent for eta features also but for phi it is strikingly true. Conceptually it does make sense as the particles and jet would scatter off in all directions whether or not they are signals or background. Thusthe variableswillfollow auniform distribution.The plot below contrasts this uniform distribution aspect of the least influential features to the density plots of the first 9 most influential features.The lastwith respect to which features are least influential in explaining the variance. But this particular Higgs Kaggle competition's success is determinedmaximizing the AMS score. ThusBut we can still use the above insights as a guideapproach.Let’s evaluate the approach outlined above. Using only the data which matters will reduce computation power and time needed.  This model will be more accurate as it will reduce noise.  Moreover less data to deal with means that one can try more computationally expensive models like Neural networks and SVM's and try to get lucky with automatic feature engineering .  To quantify  this benefit we  plot the amount of data used at each modeling iteration.We fit an xgboost decision tree model to our training data using the insights above . I chose xgboost here due to its having  low variance  low bias high speed and more accuracy.  We will follow the ""Third Iteration"" schema  from section 7 .  Let's prepare the training and testing sets by dropping the phi variables and assigning the background noise label to data where Higgs is not defined and splitting the data where Higgs is defined into 3 subgroups.""AUC"" is the metric of choice here as it responds well to misclassification errors.  The optimal number of trees to maximize the AUC score will be found by cross validation. We fit the whole training data to  the optimal number of trees for each dataset and make predictions for the test data to submit to Kaggle. The Private AMS score for this model is  . That's satisfactory for me at the moment considering we didn't tune any hyperparameters except for the number of treesand set the same threshold for all three subgroups. One need to do a grid search to find the three different thresholds.",NA,Kaggle  Higgs Boson Machine Learning Challenge
https://nycdatascience.com/blog/student-works/decoding-god-particle/,51,"The discovery of the Higgs Boson marks one of the greatest days in science – the standard model had been confirmed. For decades scientists have searched for the particle that can explain how things have mass and only recently did ATLAS and CMS experiment begin the building of CERN that lead to this discovery. Following these events CERN put out a kaggle challenge to test the public’s capabilities in understanding and solving their simulated data.The ultimate goal of the Kaggle competition was to find out machine learning classification methods to improve the statistical significance of the Higgs Boson experiment.  The challenge was designed to develop a model that classified events based on if Higgs boson decays into tau tau channel (signal) or background noise. For this kaggle challenge team Quark was tasked with creating a two class classification algorithm to determine whether or not an event’s signal is indicative of a Higgs boson’s. Our algorithm was scored based on a metric called the Approximate Median of Significance (AMS).The AMS score provided above is a localized formula of the Gaussian significance of discovery  which in its basest form can be simplified to the ratio of true positives to false positives. Scientists need at least a significance of 5 to make claim to a new discovery which is equivalent to p  2.87 x 107­­. The Higgs Boson kaggle dataset was used in this analysis. Exploratory data analyses and algorithm development was performed in R. Our main goal was to maximize the AMS score   the higher the score the better our model was able to detect the tau tau decay signal of Higgs boson from known background noise.During the exploratory analysis of data we noticed that the distribution of variable “weights” which only appears in the training set is quite special. When we ranked the weights from large to small in a descending order we were amazed by seeing that the response variables in the training set could be directly inferred from the values of weights. These findings made us curious about the role of weights in the dataset. It turned out to reflect the true nature of simulation from which the “signal” only amount to a very tiny fraction of the “background”. The physicist provided more “signal” observation to train the model otherwise what the machine would learn would be basically the pattern of “background” and would not distinguish “signal” and “background” accurately. Therefore the “signals” typically have lower weight and should be given less emphasis in the model training phase. It seems the weights are a representation of the domain knowledge of the physicist in controlling the price to pay of in the false positive and false negative cases. Based on the data provided the model is scored higher by predicting “background” correctly than predicting “signal” correctly.The sheer amount of missing values also caught our attention. We discovered that seven data fields have missing values up to 70% and three data fields have missing values up to 40%. One data field(DER_mass_MMC) has 15.24% missing values. A closer inspection at the pattern of the missingness led us to discover that all missing features are related to PRI jet num except except DER mass MMC. The technical documentation for this competition justified our finding by explicitly stating that some of the features associated with ""PRI jet num""(recorded as 0 1 2 and 3) are undefined.    The figure above shows the pattern of the missingness (highlighted in red). We noted from the combinations plot (right side) that there were 6 patterns in our dataset  Only one combination was complete and the remaining five had some missing columns.For the observations where PRI_jet_num  0 there were 11 columns with missing values of which 10 columns were completely missing and “DER_mass_MMC"" (The estimated mass of the Higgs boson candidate) occasionally had missing values. Similarly for observations where PRI_jet_num  1 we found a pattern where there were 7 out of 8 columns values were completely missing and  ""DER_mass_MMC"" again had occasionally missing values. For the observation with PRI_jet_num  2 or 3 there were no completely missing columns. The observations all had complete data except for those which likewise observed ""DER_mass_MMC"" occasionally missing. It was obvious the features that were completely missing when associated with values of PRI_jet_num are not missing at random (MAR). While we were able to identify the 6 patterns of missingness in our data we had to ask the question: What is this missingness trying to tell us? Should we be imputing or removing the missing data? Are these columns even necessary? We then investigated the reason that accounts for the missingness of “DER_mass_MMC”. The visualization of both of the training set and test set helps us to be convinced that the division of observation based on “PRI_jet_num” and missing value pattern could be generalized to the test set. We noticed that the proportion of “DER_mass_MMC” missing value are different across the columns This tells us that that the % missingness of “DER_mass_MMC” is also not completely missing at random  if it were we would see equal proportions of missingness across all groups. Interestingly the proportion of missingness in groups PRI_jet_num in 2 and 3 are similar suggesting that the mechanism to which their missingness is derived might be the same. One benefit associated with it was that we had more observations to train a single model for PRI_jet_num 2 and 3. Based on our assumption that PRI_jet_num strongly impacts the nature of missingness and should not be treated as a whole we decided to divide the training and test dataset based on the number of PRI_jet_num values. Given the analysis above the whole dataset was divided into 3 groups:Group 1 – Observations with PRI_jet_num value  0Group 2 – Observations with PRI_jet_num value  1Group 3– Observations with PRI_jet_num value  2 and 3After dividing the data into 3 groups we removed the columns which were completely missing in each group. The documentation provides compelling proof about the reason behind the complete missingness in columns  contextually. It did not make sense for these features to have values; some were specific to multiple jet particles e.g. variables that ended in _jet_jet meant it was a derived calculation between at least two jet particles. As a result it was explicitly stated that for PRI_jet_num values below the required value could not be calculated and as a result undefined. Therefore it is justifiable to remove the columns that were completely missing in each group. After removing all columns that were completely missing in each group the only variable that still had missingness was ""DER_mass_MMC"". The missing data for DER_mass_MMC was unrelated to number of jets due to the topology of the event being too different from the expected topology. As we earlier determined “DER_mass_MMC” is also not missing completely at random (MCAR) and therefore would not be wise to just remove. In order to ensure we were not removing an important feature “DER_mass_MMC"" required imputation..The missing data in “DER_mass_MMC"" for each group was imputed using the Multivariate Imputation by Chained Equations (MICE) function from the MICE package. The function used a Multiple Imputation (MI) approach which is based on repeated simulation; it has been commonly used for complex missing value problems. The function returned a set of complete datasets (by default 5) that is generated from an existing dataset containing missing values. In our case 5 complete datasets were created by imputing values for “DER_mass_MMC” by interpreting missing values from all other variables in the dataset for each group using multivariate linear regression. Standard statistical methods were applied to each of the simulated datasets by this function. The final step in imputation for each group was the average of the 5 datasets that were created by the function.We used different machines learning approaches to build models classifying the tau tau decay signal of the Higgs boson from background noise. The models selected were Logistic Regression Random Forest and Gradient Boosting Machine  the reasoning will be described in the following sections. The models were trained on the training dataset and scored on testing dataset to obtain a final prediction. We then checked the performance of each model by comparing AMS scores evaluated through kaggle submissions.The first model we decided to try to model the events was Logistic Regression. Logistic regression’s main strengths are its quick training and computation as well as clear interpretability. Although being a supervised modeling method it also contains descriptive information regarding the feature selection. Logistic regression is a nonlinear log transformation on the traditional linear regression to the odds ratio. As a result it is a simple and effective method to model classification problems. The assumptions made in linear regression do not transfer over: The results above compare regressions between two models within one group: with and without weights. We wanted to observe the effect of weights. The default threshold value is 0.5 here.The model in the left with weights taken into account has a significantly lower (64% vs 71%) accuracy nonetheless the highest AMS. It’s reasonable because the evaluation is based on AMS with weights rather than just accuracy. As a result of weights not equally assigned for “signal” and “background accuracy could be compromised in order to get a higher AMS score in the model training. We could see the prediction of the model favored the “background” which was consistent with the conclusion we made before.  The model suggest that DER_mass_MMC is not actually important  the most important being DER_mass_transverse_met_lep DER_mass_vis DER_deltar_tau_lep and DER_pt_ratio_lep_tau. However based on the previous analysis DER_mass_MMC is what we expected to be important in determining the response variable. It might be the case that DER_mass_MMC and probabilities for the classification are not linear. We need more proof from treebased method which are a group of nonlinear methods.After submitting and scoring against the test set we got a result of 2.034Random Forest (RF) is an ensemble learning method for classification and regression. By creating a large number of decision trees it collates individual trees’ votes in order to predict the class; Sometimes that may be the mode of the classes (classification) or mean prediction (regression) of the individual trees.We needed to tune the number of randomly selected features (mtry) used in a given decision tree. For classification problems it is recommended in “Applied Predictive Modelling” book by author Max Kuhn and Kjell Johnson to start with an mtry values around the square root of the number of predictors. The book also suggested to start with an ensemble of 1000 trees and to increase that number if performance is not yet close to a plateau to get optimal results. The tuning parameters set for the random forest model for all groups are mentioned in the table below.The final mtry values for our groupings of PRI_jet_num  0  1 > 1 were 5 5 and 4 respectively.",NA,Decoding the God Particle
https://nycdatascience.com/blog/student-works/deciphering-tau-tau-decay-higgs-boson/,51,The theoretical discovery and experimental observation of the tau tau decay of Higgs boson is a milestone in particle physics that “contributes to our understanding of the origin of mass of subatomic particles”. Its importance is acknowledged by the Nobel Prize in Physics of 2013.The Kaggle community also paid huge attention and hosted a data science competition with nearly 1800 teams participating in 2014. The task was to analyze a set of simulated particle collision data containing features characterizing events detected by the Large Hadron Collider at CERN. The objective was to classify events as either a signal indicating the tau tau decay of Higgs boson or a background noise.  Although this completion was finished almost two years ago we were still very interested in the Higgs boson Kaggle dataset and wanted to apply our newly obtained machine learning knowledge to analyze it.   The initial explanatory data analysis helped us identify 11 out of the 30 features have missing values.  For convenience we renamed all the 30 features with serial name from var1 to var30.  As shown in the table below of all the 800000 observations from the training and test set there seems to have three systematic types of missingness: 71% have missing values in var5 var6 var7 var13 va27 var28 and var29(Type I); 40%  have missing values in var24 var25 and var26(Type II); 15% have missing values in var1(Type III).   As shown in the correlation plot below there are three clusters of correlated variables that are of interest: var4 var10 var12 var20 var22 var24 var27 and var30 seem to correlate with each other; var5 var6 var7 and var13 are correlated; var1 va3 and var8 are correlated.   With a further look into the scatterplot with var1 and var3 it seems that there is a strong positive correlation between the two features.Then we checked the distribution of signal (in cyan) and background (in red) with regard to the two features. It looks like that the two labels have similar distribution regarding the two features.The two graphs below show there are strong positive correlation between var24 and var30 and between var27 and var30 respectively.  So why we were interested in analyzing those correlations plots above? It’s because we aimed to utilize those correlation to impute the data. Type III missingness only accounts for var1 and exists in 15% of all observations; Type II missingness shows in 40% of all observations with three features including var24; Type I missingness is available in 70% of all observations and involve 7 features including var27. We would lose much information if we just remove all incomplete features. We used simple linear regression to predict those missing values of the the 11 incomplete features from the 19 complete features such as var3 and var30. Since three of our models were tree based we were not much afraid of increasing collinearity due to the linear regression imputed features.A combination of those three types of missingness could result in any of the six missing patterns as shown in the table of variables with missing values. If missingness itself is information that could be utilized for predicting the signal then we should integrate the missingness for modeling. As such we created three dummy variables (0 for missing 1 for nonmissing) to represent the three missing types. Since they are categorical features their combination could well represent the six missing patterns.  Since this is a binary classification problem logistic regression model was first employed due to its efficient computation capability.  We built three logistic regression models with different features. The first model uses var1 imputed by a timeconsuming Multivariate Imputation by Chained Equations(MICE) method and the 19 complete features. The second model  has all the features from the first model and the other ten features which were imputed by linear regression. The third model has all features in the second model and the three additional dummy variables . AIC and BIC were calculated to evaluate the performance of the three models. As the model complexity increases with more features both AIC and BIC become smaller. It indicates that more features would result in better model performance. The Approximate Median Significance(AMS) score which is used by Kaggle to measure the model accuracy also favors the more complex model with all the 30 features and the 3 dummy variables. In the following modeling we would use all the 33 features. Random Forest was an efficient ensembled supervised learning for classification. It operates by constructing many decision trees for training and outputting the class that is the mode of the classification of the the individual trees. It has two important tuning parameters: number of variables randomly sampled at each split(mtry) and number of trees to grow(ntree) to get the best model performance.  According to rule of thumbs square root of the number of variables (5.74)might be likely to be the best parameter of ‘mtry’ so we searched for the best ‘mtry’ around 5.74. We used 5fold cross validation with different ‘mtry’ values from 1 to 9 and ‘ntree’ values of 100 500 and 1000.As shown in the graph above the highest AMS value appears when ‘mtry’ is 2 and ‘ntree’ is 500. Extreme Gradient Boosting(XGBoost) is very similar to the regular gradient boosting framework but it’s more efficient due to the fact that it can perform parallel computation on a single machine as well as the fact that it uses a greedy algorithm. XGboost supports classification.Before running XGboost we must set three types of parameters: general parameters booster parameters and task parameters.  parameters relates to which booster we are using to do boosting commonly tree or linear model. Whereas the  parameters depend on which booster you have chosen and lastly the the parameters that decides on the learning scenario. In our model we relied on default parameters in the  section of tuning. We focused mainly on tuning our  parameters. XGBoost comes with a wide array of different parameters to tune. We focused mainly on tuning those parameters that are generally considered the most important to tune. The 5 parameters we tuned are the following: eta ntrees maxdepth scale_pos_weight and gamma.Gamma : minimum loss reduction required to make a further partition on a leaf node of the tree. the larger the more conservative the algorithm will be.Max Depth : maximum depth of a tree increase this value will make model more complex / likely to be overfitting.Scale Positive Weight : Control the balance of positive and negative weights useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases)While we did perform cross validation in our analysis it’s important to note that one of the special feature of xgb.train is the capacity to follow the progress of the learning after each round. Because of the way boosting works there is a time when having too many rounds lead to an overfitting. You can see this feature as a cousin of crossvalidation method. XGBoost allows user to run a crossvalidation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. This is unlike GBM where we have to run a gridsearch and only a limited values can be tested. The XGBoost gave us our best results. We obtained an AMS score of 3.67 and placed 197 on the leaderboard. This was in the top 10% of all submissions.Treebased model would also allow us to draw the feature importance plot based on comparison of information gain of all the features. We drew feature importance plot below and it seems variable var1_d is an important feature which ranks number 7 of all 33 features indicating the missingness of var1 might be important in predicting the signal.  Ensemble modeling combining multiple predictions generated by different algorithms would normally provide robust predictions as compared to prediction by a single model which only has one individual rule.  It’s expected that the diversification and independent nature of each model would improve the predictive accuracy of the ensemble model.  From those models discussed above the XgBoost model seems to perform way better than the other three. When doing ensemble it’s reasonable to give a better model with more weight. We rank the model by their AMS score on the test set and assign weight 4 to the best model (XgBoost) 3 to the second best( Random Forest) 2 to the third(GBM) and 1 to the fourth(Logistic Model). All the probability predictions on the test set have their rank. The higher the rank is the higher probability it is signal; the lower the rank is the higher probability it is background. We could utilize the weighted average to ensemble those four predictive results on the test set and then reorder the averaged result by rank. Based on our best model the top 14% in rank are predicted to be signal and the rest are background. For the ensemble we use the same threshold to classify signal and background. The ensemble gives an AMS score of 3.4 which is worse than the XgBoost model which yields 3.6.  ,NA,Deciphering the tau tau decay of Higgs Boson
https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-competition/,51,As a part of exploratory data analysis we employed Principal Component Analysis an unsupervised method to determine whether it is practical to reduce dimensions. The first principal component explained only 23% of variance. Based on the result of scree plot we reduced dimensions to 10 principal components but even then only 75% variance is explained. PCA showed us that it would be difficult to eliminate variables without sacrificing prediction accuracy. Before building the model we subset the training dataset provided on Kaggle into our own subtraining and subtest. Consider an extremely unbalanced ratio of signal and background counts we utilized the sample.split() function in R to ensure that both subtraning and subtest would have same signal/background ratio. In other words we avoided a scenario where subtraining might have 85% background while subtest contained 99% background.The xgboost model yielded a pretty good result with default parameters. We then constructed our own crossvalidation function to grid search the best tuning parameters for high accuracy and AMS score. This submission ranked us to top 100. The rationale behind this performance advancement was that we improved each individual tree by increasing the maximum depth of a tree and eased the overfitting issue by decreasing number of rounds for boosting.,NA,Higgs Boson Signal Detection
https://nycdatascience.com/blog/student-works/linlin_cheng_proj_5/,51,projectprojectsprojectsAn effective algorithm would increase the probability that funds are directed to those who can used them best and need them the most.,NA,"Pump it up, Drill it down: an Analysis of Water Projects in Tanzania"
https://nycdatascience.com/blog/student-works/starbucks_collector_world/,52,The five categories are selfexplanatory except One cannot easily label those mugs with few traders seekers and owners because there are two possible explanations. They might be new products which just have not started trading yet or they could simply be unpopular. ,NA,Starbucks Collectors on Fredorange.com
https://nycdatascience.com/blog/student-works/secret-of-lol/,52,ddict boyfriend at 5:00 and it takes 1 hour to be the restaurant. She tells the function the date time and destination. At 3:20 my function helps her find that her boyfriend still in game so an email will be sent as follows:,NA,Secret to Winning a League of Legends Game
https://nycdatascience.com/blog/student-works/muse-automated-music-playlisting-recommendation-application-using-weighted-clustering/,52,song's title and artist name will then be used to query nd return and then play the top related video.,NA,Muse: a Better Music Recommendation Application
https://nycdatascience.com/blog/student-works/exploring-anatomy-successful-ted-talk/,52,From Pericles to Barack Obama some of the greatest orators of our time have committed themselves to spreading an idea that will not only inform but also inspire.  TED (Technology Entertainment and Design) continues this tradition as a nonprofit that serves as a medium for successful and unique individuals to share their stories passions and innovations. As a global community TED strives to engage people on and off the internet through videos of conference talks. Driven solely by the question “How can we best spread great ideas?” TED has become a wellestablished clearinghouse for knowledge inspiration and wisdom. But what drives viewership? What makes one video more popular or rated more beautiful than another? For this project I sought to explore these questions by extracting data from the TED website:1.Speaker Name2.Speech Transcript3.Views4.Title5.Rating6.Month uploaded7.Duration8.Speaker GenderUsing ScraPy 1874 transcripts were collected ranging in time from 2006 to 2016. You can see the code used in the development of the spider and collection of the data here:Sometimes it may not be the diction tone or length that controls views – in fact it may sometimes be an audiences hidden biases. One of the major things that can be seen from doing a simple numerical exploratory data analysis is the difference in male vs. female speakers. The ratio is almost 2.2 males for every 1 female speaker which may imply that men are given more opportunity than women to speak at the conference. Take a look at the following Rating Proportion graph: What you can see here is that women are more likely to be rated ‘Beautiful’ much more likely to be rated ‘Courageous’ and ‘Inspiring’. Men on the other hand are more likely to be ‘Fascinating’ and ‘Ingenious.’ A Chi squared test shows that at least one of the categories is dependent on another with a significant pvalue of 2.2e16. This leads me to believe that women are generally recruited to speak about social topics such as feminism. Despite the difference in rating proportion Women and men garner about the same amount of views: Diction is also considered a huge factor in determining a speech’s success. For each transcript the top ten words that were used were collated then summarized by frequency. Comparing the frequency tables of words among different quartiles:  It seems that the top ten words are similar across all quartiles: ‘can’ ‘one’ ‘like’ ‘people’ ‘just’ ‘now’ ‘know’ ‘actually’ ‘see’ ‘really’ ‘world’. The rating proportions are visualized in the form of a word cloud – what can be seen here is the same prevalence of words shown above:   Finally to understand if any of the continuous data collected had any effect on views I ran a linear regression against views with the duration of the talk the gender of the speaker and the date it was uploaded. The following are the initial results: The initial QQ plot looks terrible above 1 quartile. I tried to linearize this relationship with a boxcox transformation (lambda  18/99):  The QQ plot looks much better and the result is a significant (pvalue  2.2e16) equation. The significant coefficients are the duration and the date – the latter being much more significant. The equation however only explains 7.3 % of the variance. Date is also correlated with views because the longer a video has been uploaded for the more views it is likely to have. As a result this linear model is inconclusive. With more time further relationships may be investigated with text mining machine learning algorithms such as Naïve Bayes classifiers. What can be noted is the discrepancy with male and female speakers in proportion and numbers. While TED is a great organization that warms our hearts and inspires our thoughts it may have some legwork to do before becoming fully egalitarian.,NA,Exploring the Anatomy of a Successful TED talk
https://nycdatascience.com/blog/student-works/web-scraping/movie-rating-prediction/,52,"How can we tell the greatness of a movie before it is released in cinema? This question puzzled me for a long time since there is no universal way to claim the goodness of movies. Many people rely on critics to gauge the quality of a film while others use their instincts. But it takes the time to obtain a reasonable amount of critics review after a movie is released. And human instinct sometimes is unreliable.Given that thousands of movies were produced each year is there a better way for us to tell the greatness of movie without relying on critics or our own instincts?The tools I used for scraping all 5000+ movies is a Python library called ""scrapy"". Below are some brief steps. The source codes and documentations can be found in github page .Many important movie information were considered and scraped from IMDB website. For example movie title director name cast list genres etc.The scraping process took 2 hours to finish. The scraping of movie posters took a little longer than pure text data. In the end I was able to obtain all needed variables for 5043 movies and 4096 posters. Overall they span across 100 years in 66 countries. There are 2399 unique director names and 30K actors/actresses.The image below shows all the 28 variables that I scraped. Roughly speaking half of the variables is directly related to movies themselves such as title year duration etc. Another half is related to the people who involved in the production of the movies eg director names director facebook popularity movie rating from critics etc.I am especially interested in knowing the answer to this question: Will the number of human faces in movie poster correlate with the movie rating? Movie poster is an important way make public aware of the movie before its release. It is quite common to see faces in movie posters. Below are the the movie posters from 8 movies that are not so great in terms of IMDB rating score (below 5). They tend to have many faces.It should be pointed out that it is unfair to rate movie solely based on the number of human faces in poster because there are great movies whose posters have many faces. For example the poster of the movie ""(500) Days of summer"" has 43 faces all from the same actress. But remember that having large face number (> 10) in poster and simultaneously being a great movie is uncommon based on my findings. Interestingly many posters made my face recognition algorithm fail to work such as:Overall nearly 95% of all the 4096 posters have less than 5 faces. BesidesOut of the 28 variables I am especially interested in know how does the IMDB rating score correlate with other variables. From the 3D grosscountryrating plot below we can see that United States produced the largest amount of movies across the past 100 years (19052015). The sheer amount dwarfs other countries in the number of produced movies. The points at the top corner of the plot denote the movies having the highest gross in the movie history. Many countries produced great movies but still there were quite a few bad movies.Movies having rating larger than 8.0 are listed in the IMDB top 250 and they are truly great movies from many perspective. Movies with rating from 7.0 to 8.0 are probably still good movies. Viewers can gain something from them. Movies with rating from 1 to 5 are sometimes considered as ones that ""sucks"" in one way or the other. One should avoid those movies unless they have to. Life is short. USA and UK are the two countries that produced the most number of movies in the past century including a large amount of bad movies. The median IMDB scores for both USA and UK are however not the highest among all countries. Some developing countries such as Libya Iran Brazil and Afghanistan produced a small amount of movies with high median IMDB scores.In the last century it seems that the number of movies produced annually largely increased since 1960. This is understandable since the development of filming industry goes hand in hand with the development of science and technology. But we should be aware that along with the boom of movie industry since 2000 there are many movies with low IMDB score. The social network is a good way to estimate the popularity of certain phenomena. Therefore it is interesting to know how does the IMDB score correlate with the movie popularity in the social network. From the scatter plot below we can find that overall the movies that have very high facebook likes tend to be the ones that have IMDB scores around 8.0. As we know IMDB scores of higher than 8.0 are considered as the greatest movies in the IMDB top 250 list. It is interesting to see that those greatest movies do not have the highest facebook popularity. I highlighted several movies to illustrate this finding. The movie ""Mad Max"" and ""Batman vs Superman"" both have very high facebook likes but their IMDB scores are slightly above 8.0. The movie ""The Godfather"" is deemed as one of the greatest movies but its facebook popularity is hugely dwarfed by that of the ""Interstellar"".It is plausible to believe that the greatness of a movie is highly affected by its director. How does the movie IMDB scores compare with the director facebook popularity? From the plot below it can be seen that the directors who directed movies of rating higher than 6.0 tend to have more facebook popularity than the ones who directed movies of rating lower than 6.0. And I listed the top four directors who have the most number of facebook popularity (Christopher Nolan David Fincher Martin Scorsese and Quentin Tarantino) along with their four representative movies.Great actors/actresses make a movie great. They are the souls of movies. How does their facebook popularity look like?For a given movie I scraped all the available cast members in the IMDB movie page. After retrieved the number of facebook likes for all cast members I ranked the numbers in descending order and picked the top 3 actors/actresses. This is based on a simple assumption: leading actor/actress tends to have more facebook popularity than supporting actor/actress; and no matter how great a movie is there will be no more than 2 leading actors/actresses. For notation purpose I named the facebook popularity for the top 3 actor/actress as ""actor_1_facebook_likes"" ""actor_2_facebook_likes"" and ""actor_3_facebook_likes"". Note also that the variable ""cast_total_facebook_likes"" is calculated by summing up the facebook popularity of all the available cast members.The assumption indeed matches with the plotted graph below. The top first actor/actress has the most number of facebook popularity while the second and the third actor/actress have much lower popularity. But it can also be shown that high facebook popularity of the leading actor/actress does not mean that a movie is of high rating.The prediction of movie ratings in this article is based on the following assumptions: With those 28 variables available for all scraped movies can we predict movie rating? Before we begin it is necessary to investigate the correlation of those variables.Choosing 15 continuous variables I plotted the correlation matrix below. Note that ""imdb_score"" in the matrix denote the IMDB rating score of a movie. The matrix reveals that:Surprisingly there are some pairwise correlations that are perhaps counterintuitive:The threedimensional PCA plot shown below reveals more information than the correlation matrix. For the 15 continuous variables we can see their relationship with the three principal components in space. The colorful points denotes all the movies. We can see that some variable vectors tend to cluster and point at similar directions meaning that those 15 variables have multicollinearity between some variable pairs. This may lead to problem when we want to fit linear regression model to predict movie rating.Although initially I scraped 28 variables from IMDB website many variables are not applicable to predict movie rating. I will therefore only select several critical variables.Both the correlation matrix and the 3D PCA plot show that multicollinearity exists in the 15 continuous variables. When fitting a multiple linear regression model to predict movie rating we need to further remove some variables to reduce multicollinearity. Therefore I remove the following variables: ""gross"" ""cast_total_facebook_likes"" ""num_critic_for_reviews"" ""num_voted_users"" and ""movie_facebook_likes"". Some variables are not applicable for prediction such as ""num_voted_users"" and ""movie_facebook_likes"" because these numbers will be unavailable before a movie is released.The plot of the fitted multiple linear regression is illustrated below. From the ""Normal QQ"" plot we find that the normality assumption of regression is somewhat violated. Thus I apply the boxcox transformation and refit the model. Although the model became uninterpretable the assumptions of multiple linear regression namely no multicollinearity normality constant variability and independence are wellsatisfied. From the detailed information of the fitted model we find that the model is significant since the pvalue 2.2e16 is very small. The ""title_year"" and ""facenumber_in_poster"" has negative weight. The ""actor_3_facebook_likes"" variable was not included in the model at all meaning that the social network popularity of the third actor in the cast member is not significant to predict the movie rating. This model has multiple Rsquared score of 0.201 meaning that around 20% of the variability can be explained by this model.Random Forest model was fitted to predict movie rating using the following variables:The movie dataset was divided into two parts 80% of the movies were treated as the training set and the rest 20% belonged to the testing set. Up to 4000 trees were generated to fit the random forest. The number of variables tried at each split of the decision tree is 2. The mean of squared residuals is  and the percentage of variable explained is 27.21% better than that of multiple linear regression.From the fitted random forest model the variable importance can be revealed in the graph below. It is interesting to see that duration is the most important variables followed by the budget and the director facebook popularity. Different from the multiple linear regression model above the ""actor_3_facebook_likes"" is considered as an important variable even slightly more important than the ""actor_1_facebook_likes"". Since the fitted Random Forest model explains more variability than that of multiple linear regression I will use the results from Random Forest to explain the insights found so far:",NA,Predict Movie Rating
https://nycdatascience.com/blog/student-works/sentiment-analysis-yelp-user-review-data/,52,Social data provides important realtime insights on consumer opinion – on lifestyle habits brands and preferences. Because these opinions are unsolicited they provide genuine insight into consumer feelings and as such they should be valued. Yelp provides restaurant details including name price rating address and reviews. The ratings given by the users say how good the restaurant is but do you really think that the ratings alone is sufficient to give the correct information? No because people who really hated a restaurant would comment on their experience.  The same goes for athe good experience. SoThus one would expect thatperforming sentiment analysis would give give a better insight about judging ainto the masses’ opinions of restaurants.Web Scraping the yelp.com to scrape the restaurant data. The data I scraped Restaurant data was scraped from Yelp using the python package BeautifulSoup. The data consists of information such as restaurant name rating price number of reviews address and user reviews.  I split the web scraping module into two tasks. The first one is toscraped the  restaurant name rating price number of reviews and address. The second one is toscraped the restaurant name and user reviews. The data sets were then merged. Finally we merge the two datasets. We use BeautifulSoup to scrape the data.My scraping was restricted to the restaurants in a 2 mile radius around Times Square. I scraped the restaurants in and around Times Square. To be precise I scraped the restaurants placed in a 2 mile radius.Most of the restaurantsin the first 20 pages of the yelp data hashave a rating of 4.The distribution shows that the majority of the restaurant reviewsrange from 0 to 1000.Sentiment Analysis was performed using the Natural Language Toolkit. The name of the specific package used is called Vader Sentiment. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rulebased sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. The code for Sentiment analysis is as follows:It works on the word levelbyclassifyingsplitting each word into either positive negative or neutral. I want toWe concentrate on the positive and negative words as neutral words doesn't add value. The plot of the sentiment analysis is as followsA restaurant with a rating of 4 has an equal mixture of negative and positive  words. It’s safe to say that these reviews are mixed So the restaurant has mixed reviews.The algorithm can be combined with the the text mining so that the dish name specified in the reviews can be combined withincorporated into the sentiment analysis algorithm to give an output saying thatsay whether or not a particular dish is associated withhas a positive sentiment or negative sentiment and a overall score can be specified.,NA,Sentiment Analysis Of Yelp User Review Data
https://nycdatascience.com/blog/student-works/secretepipeline-higgs-boson-machine-learning-challenge/,52,Contributed by (Emma) Jielei Zhu Le Wei Shuheng Li Shuye Han and Yunrou Gong. They are currently in the NYC Data Science Academy 12week full time Data Science Bootcamp program taking place from July 5th to September 23rd 2016. This post is based on their fourth project  Machine Learning (due on the 8th week of the program). ,NA,Higgs Boson Machine Learning Challenge
https://nycdatascience.com/blog/student-works/apartment-hunting-manhattan/,53,Manhattan is without a doubt one of the best cities in the world to live in.  The city is rich in history culture art and vibrancy which never ceases to amaze. It is the city that never sleeps and there is always something to do. Most people want to move to this incredible city and are willing to compromise luxury by living in shoebox apartments and paying crazyhigh rent. The rental market in New York is one of the most expensive in the country.  How you chose to live can be as important as WHERE you choose to live. The high volume of demand makes the NYC real estate market very expensive. Looking for an apartment in Manhattan used to be far more stressful.  However with the arrival of numerous rental websites and apps house hunting has never been so easy and convenient. Trulia is one of the many online residential real estate websites that gives access to housing information for people interested in renting apartments in the United States.  It lists information such as price number of bedrooms and bathrooms school rating and crime level. There is a wide range of housing options which people can chose from. The structure of the webpages was in 2 different formats for Apartment Community Complex and Individual Apartments. This project only focused on  Individual apartments which had only one type of bedroom and ranged from onebedroom to sevenbedroom apartments. As you can see in the picture below After developing the code in python I was able to scrape the information for all apartments in  Manhattan by looping through each page. In total I scraped data for 6767 individual apartments which were exported into excel. I did some data munging and cleaning to get the data in the proper structure to do further analysis. The apartments were divided into 10 Manhattan neighborhoods based on postal code information.  In total there were 19 variables for each apartment. After data preparation I built a  using R Shiny which displays the apartments on the interactive map from their geo location. The apartments have been classified into 4 classes that are 1 bedroom 2 bedroom 3 bedroom and more than 3 bedroom to make visualization more interpretable. The users can filter their apartment search based on neighborhood number of bedrooms and bathrooms and rental price. We can get additional information about the apartments by clicking on the points which display pop up boxes with apartment details. Similarly the small bargraph shows the frequencies of apartment types that are currently displayed on the interactive maps. We can also view the database of the apartments by clicking on the “Data” tab.This Shiny app analyzes the data and gives information about total number of apartment types and median prices for each neighborhood in Manhattan.  It gives users the option to select the type of apartment they want to get information about. In the figure below we analyze one and two bedroom apartments.Analyzing the bar graph for the number of apartments in each neighborhood we can see that there are more one bedroom apartments than two bedroom apartments in most of the neighborhoods.  This is particularly so in Chelsea Clinton the Upper East Side and the Upper West Side.  In contrast Central and East Harlem have more two bedroom apartments and the actual number of apartments for rent in these areas is relatively low.Looking at the bargraph of the median prices in each neighborhood we can see that rent prices for both one bedroom and two bedroom apartments are very high in Lower Manhattan and Chelsea Clinton. Two bedroom apartments are particularly expensive in Lower Manhattan.  It is relatively cheaper to live in East Harlem or the Inwood Washington Heights area. This information can be very useful for the users to plan their rental search based on their budget.Every potential renter wants to live in a place that is safe for them and their family. Each apartment has crime level ratings based on the number of crime incidents that occurred nearby the apartments. The crime level rating has been categorized into 4 groups (lowest low high and highest).We are analyzing only the apartments that have the “highest” and “lowest” crime rating. We can see that there were relatively more apartments on the Upper East Side and Upper West Side that had “lowest” crime level. There were very few apartments in this region that had “highest” crime rating which tells us that these neighborhoods are relatively safe.  On the other hand there were many apartments in Greenwich Village Soho that had “highest” crime rating and very few “lowest” crime rating. Therefore we can say that it is safer to live on the upper East Side and Upper West Side areas of Manhattan than Greenwich Village Soho.Each apartment has ratings information for Elementary Middle and High School based on the average rating of all schools nearby.  These ratings come from “GreatSchools.com”. The ratings have been classified into 3 groups: “Above Average” “Average” and “Below Average”. From the elementary schools bar graph we can see that there were lots of apartments in the Upper East Side and Gramercy Park Murray Hill that had school ratings “Above Average”. There were more apartments in Inwood Washington Heights which had “Below Average” ratings. Overall there were more apartments with “Above Average” ratings than “Below Average” ratings in most of the neighborhoods.We can see that there were more neighborhoods that had “average” schools than any other type.  Noteworthy are Gramercy Park and Greenwich Village for their “above average” schools.   While Central Harlem and East Harlem are noteworthy for the lack of any “above average” schools.We can see that there are very few neighborhood that had schools rated as “Above Average”. Most neighborhoods have a higher number of schools rated as “Average” than “Below Average”.  Looking at bargraphs for Gramercy Park Murray Hill Inwood Washington Heights and Central Harlem we can see that there are more schools rated “Below Average” than “Average”. I would like to sum up by stating that this app can be really helpful for the users who are looking to rent apartments in Manhattan. The users can improve their apartment search by filteringapartments according to their needs and playing with the interactive map to gain more insights about apartment rentals. They can also get additional ideas about the price ranges school ratings and crime level for each neighborhood and decide which neighborhood they want to move to. Happy Apartment Hunting !!!,NA,Apartment Hunting in Manhattan
https://nycdatascience.com/blog/student-works/much-christies-making/,53,Like many other firms Christie's went through a period of ups and downs in the late 90’s and again in 2008.  In both cases however Christie’s recovered and has shown relatively steady growth. Aside from classical auctions the company has also extended its business into private transactions and a wider category of art. See Figure 2. for a word cloud of Christie's sales event titles.This project focuses on analyzing the sales data from Christie's Auction house’s using a dataset built from .  Since the dataset is not publicly available this project also presents the webscraping code that I used.As a transaction intermediary with a fine reputation Christie's Auction House slices off a significant amount of money as commission: it charges 25 percent for the first $75000; 20 percent on the next $75001 to $1.5 million and 12 percent for revenue above $1.5 million. Christie's global sales revenue corresponds well with the overall trend of the global economy: when the economy was booming in early 2000s the art business grew steadily.When the subprime crisis occurred in 2008 Christie's sales witnessed a plummet of around 50%.   As stated above it resumed healthy growth subsequently.Despite a drop in the number of events in its main office in London in more recent years Christie's offices in the two London offices remain the busiest venues among the rest in terms of total number of events held. In addition the auction house signed a 30year lease for its Rockefeller office in New York and opened a number of subbranches to accommodate the ever growing demand from the American market.,NA,What is Christie's selling? A web-scraping project
https://nycdatascience.com/blog/student-works/web-scraping/finding-dream-house-data-help/,53,The Portland Oregon metro area has a lot to offer. A 90 minute drive west brings you to the beach. In the opposite direction there's a snowcapped mountain. It has a thriving food and wine scene a burgeoning tech sector and temperate weather. An increasing number of people would like to move to this metro area. Perhaps the greatest concern for these new residents is finding housing. Data can help.Records for house sales on  over the past three years in the neighboring cities of Portland OR and Vancouver WA were scraped by using the Python packages BeautifulSoup and Selenium. Around 20000 sold house records with 15 variables are extracted including zipcode address location selling price days on redfin beds and baths size price per sqft and etc.  The public school ratings were gathered by scraping the website  according to zip code. A screenshot is below.We see that in the Portland metro area the median house price per square foot increased 35% from Aug 2013 to Aug 2016.  However the house transaction volume was increasing during the past three years.The house transaction volume is strongly seasonally dependent  it is highest in summer and lowest in winter. The new school year starts in September so the real estate market is influenced by people looking to move within a desirable school district. Since offers need one or two months to be processed the data shown here is delayed. Agreements for house sales usually happen in the Spring. Therefore this is the hottest season for real estate market.Vancouver WA and Portland OR are the twin cities in the metro area. However which of these is the best place to live?  In general the living cost in Portland is more expensive than Vancouver as shown in the boxplot. The red boxes show the price of each zip code area in Portland and the green boxes show those in Vancouver. In seven out of ten zip code areas in Portland the median price per unit area is above $200/sqft. The median price per unit in all the areas of Vancouver is lower.During the past three years the price growth rate per unit area demonstrates the potential appreciation capability. The top 10 zip codes are in Portland and the bottom 10 zip codes are in Vancouver. In this regard some areas in Portland such as 972029720697220 have gradually increasing house prices. Buying a house in these areas would conceivably be more rewarding in the long run. In comparison the price appreciation in Vancouver is relatively weak.The public school ratings for elementary schools middle schools and high schools were webscraped. The ratings were averaged by zip code since the house data was segmented in this way. The school ratings are positively correlated as shown in the plots.  Especially for elementary and middle school ratings the correlation coefficient is 0.9083869 which is very high. Usually good elementary middle and high schools are bound together and vice versa.As shown in the plots the housing cost in Portland is higher than that in Vancouver for the same education quality. Also as school ratings increase so do house prices. Now you know when and where to narrow your target for house hunting based on price location and school district. What specific features do you care about in a house? The following plot shows the correlation relationships among those numerical features. As previously explained school ratings among elementary middle and high schools are highly correlated. The price house size (sqft) number of bedrooms and number of bathrooms are positively correlated. Larger houses more bedrooms and more bathrooms lead to higher house prices. In general one would expect smaller houses to be cheaper than larger houses. However when one takes into consideration the price per square foot this relationship doesn’t really hold.As observed for the house in one zip code and within a short time period (August 2015 to August 2016)  house price is positively correlated with house size (sqft). However the price per unit square foot shows a complex trend. When the size is less than 2000sqft the price per unit square foot drops. It is constant from 2000 to 5000sqft until it slightly decreases after 5000sqft. Therefore buying a house smaller than 2000sqft means you pay more for each square foot. The combination of house price and house size should be optimized before submitting an offer.,NA,Finding a dream house?  Data help!
https://nycdatascience.com/blog/student-works/web-scraping/glassdoor-web-scraping/,53,Data scientists are tech savvy. We do not like manually browsing through job postings. We would rather our profile and skills be matched with the best available positions. Aren't there dozens of companies providing similar services? Sure but you have to sign into multiple websites share your personal infos set an alert check the company reputation filter out useless recommendations ... Way too manually intensive!,NA,Landing my dream job by scraping Glassdoor.com
https://nycdatascience.com/blog/student-works/web-scraping-olympics-games/,53,"Contributed by Shuo Zhang. She is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between  July 5th  to September 23rd 2016.I really like to watch the summer Olympics.  It’s simply breathtaking to watch the world's best athletes compete in the various sports!  I also love the Olympics because of the plethora of data available. From judging to timing to preliminary rounds to finals to the the various Olympic records there is data for every sport and country at the Olympic Games. Most of it especially more recent data is free and easy to find. The Rio Olympics is almost over but we can be confident of one thing:  the US United Kingdom and China will top the medals table when it's all over. One question we might want to ask is why these countries are so successful. Has the Olympic games achieve gender equality in competitors?  Does age impact the number of medals the athletes can get? Data can help answer these questions.Past summer Olympics data can be found at: .The overall website is organized and structured very well. I wrote a web scraper in Python using the package ""Beautiful Soup"" and extracted the following data for the further analysis：Here's an example of the code used to do the scraping:Analyzing the number of Olympic medals won by geographic region by year reveals the true impact and extent of medal diversification.  For example whilst more countries are winning Olympic medals how many medals are they capturing compared to traditionally strong Olympic nations? Is their success fairly minor or more pronounced?  What are the possible contributing factors to their success in the Olympics? I listed the history of total medals won by the top 13 leading countries.This graph shows that since the first modern Olympic games the landscape of medalwinning nations has markedly changed. Before World War II Olympic success was dominated by the United States and Europe. Afterwards more African and Asian countries begin to participate in the Olympics and the medal standings are marked by the arrival and growth of many regions including Japan South Korea China and Hungary. Here are three factors that affect medal standings revealed by analysis: Medals won in the past can be seen as an indicator of a ""sports culture"".  The United States for example always perform quite well. Sporting prowess is important to them so  many people take part. The United States hosted the 1904 Olympics and won 231 medals compared to 48 at the previous games. The phenomena occurs again and again.  For instance  China hosted the 2008 Olympics and collected 100 medals compared to 63 at the previous Olympics. This is a recognized pattern. Performing in front of a home crowd combined with extra investment in sport  gives the host country a medals boost. Australia won 27 medals in 1992 followed by 41 medals four years later. This was probably due to increased investment in sport in the runup to the 2000 Sydney Games. The UK as another example increased its medal haul from 30 to 47 between 2004 and 2008 prior to hosting the 2012 Games.This graph also illustrates some national indicators such as GDP population GDP growth and life expectancy also possibly play roles in the medalwinning battle of the Olympics: Countries with a high GDP like Germany or the USA can afford to invest in sports facilities and their populations have enough leisure time and money to take part in sports. This may not be the case in poorer countries. A big population means a big talent pool to choose athletes from  in China's case 1.36 billion people.These countries with a high GDP growth  tend to invest more in sport because they value the prestige that sporting success brings. China is a good example.Countries with a high life expectancy have a big healthy pool to choose athletes from such as Japan.T0 take a closer look I extracted the data from the 2012 Olympics  including the medal winning record of all the participating countries  and analyzed the relationship between total winningmedals won by each country and its population GDP GDP growth and life expectancy.This correlation graph demonstrates the total winningmedals is primarily correlated with population and also other variables such as GDP and life expectancy affect the total winningmedals. And the scatterplots illustrates cluster pattern between the five variables. Thus a Kmeans clustering model is applied to find the underlying pattern.KMeans clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted and the algorithm just tries to find patterns in the data. The key of Kmeans clustering is to determine the number of clusters K. I used the average silhouette method to determine K.The graph indicates that the optimal number of clusters is 5. I choose two dimensions that cover the most of data and plot the data to see the clusters.The distribution of the 5 clusters by different variables is illustrated below and we can see cluster 5 has highest average total winningmedals a relatively high average population a relatively high average GDP per capita a relatively low average GDP growth and a relatively long average life expectancy. Cluster 4 has the lowest average total winningmedals a relatively small average population the lowest average GDP per capita the highest average GDP growth and the shortest average life expectancy. Which countries belong to these clusters?To get a clear view of the country distribution I only labeled the top 5 leading countries in the total medals standings in cluster 2 3 4 in the following graph.  We can see the Kmeans clustering well separates the countries to developing and developed countries. While developed countries win the most medals China as a developingcountry catches our eye for its successful performance in the Olympics.By applying the same Kmeans clustering to the data of 2008 Olympics we can see the same pattern in the below graph.I created a breakdown of which countries the data shows will excel in which sports by investigating its past performance in 28 summer Olympics given the total medal values– where a gold medal is worth three points silver two and bronze one.The three graphs shows that United States Russia and Jamaica are likely to dominate the athletics events the United States Australia and China excel in swimming and China the United States and Russia athletes are good at gymnastics.This graph shows that more women participate in the Olympics. My focus is on the popular Olympic sports. What differences remain between the ways that male and female athletes are involved in Olympic competitions? I analyze all of the men’s and women’s events at the London 2012 Olympics to identify gender differences in the structure and rules of the sports and in the opportunities for male and female athletes.There were 132 women’s events and 162 men’s events at the London 2012 Olympics. Of these 57 events on the program are gender exclusive (i.e. there were medal opportunities for men but not for women and vice versa): 42 events are open only to men (25.9% of men’s events); and 15 events are open only to women (11.4% of women’s events). Together these exclusive events constituted 18.9% of the Olympic programs.This graph shows sport distribution of these exclusive events. For example there are more men's events than women's in the wrestling and canoeing sports while there are more women's events than men's in synchronized swimming and rhythmic gymnastics.This graph shows a total of 10903 athletes competed in the 302 medal events – 6068 men and 4835 women so there are 1233 more men than women competing in London. And male athletes compete for 991 medals while female athletes compete for 872 medals. In conclusion the data shows that there is still some way to go to achieve gender equality.I wanted to dig a little deeper so I created density plots of the distribution of athletes’ age by 4 types: all athletes athletes who won gold medals athletes who won silver medals and athletes who won bronze medals.You can see that very young medalists (early teens) and older medalists (late 30sover 40) tend to win fewer medals than medalists within the “sweet spot” of late teensearly 30s.Some sports like equestrianism have older athletes winning medals whereas a sport like gymnastics has a peak agerange of earlytolate teens  and early 20s.We can see which sports produce the most medals.  Athletics is number one followed by swimming rowing and football. So if you’re an aspiring Olympianbut you’re not quite sure what sport to train for you’ll increase your chances of medaling if you choose athletics.If I had more time  I would apply Kmeans clustering to all the data from 19602004 and investigate whether the same pattern exists and if there is difference what is the reason for this difference.",NA,Analysis of  Olympic Games by web scraping
https://nycdatascience.com/blog/student-works/makes-fund-raising-campaign-success/,53,"The website’s front page has a very simple layout with previews of a few campaigns and of categories on the left side of the page. I scraped the first 100 ads from the Medical Memorials Emergencies Volunteer Charity Animals Sports  and Education categories.The first campaign from the snapshot above provides a general template of the data available. It has a customary title monetary goal  current amount creation date number of contributors the creator’s name the creator’s location the campaign’s category under which it was set up  number of likes  number of Facebook shares story section describing the fundraising cause photo  name and time someone donated and comments left by people .Due to time limitations I decided to scrape only the numerical data for this project so the processing and analysis could be completed quickly. I used   for crawling web sites and extracting the data. I started with setting up containers to collect the items.The most time intensive portion of creating the scraping framework wasfinding the relevant xpaths for the data . Once that was done the Scrapy spider collected data from 100 campaigns across eight categories which was stored in a csv file. It took me a while to get a handle of finding xpaths and making scrapy spider but ultimately tweaking   proved to be sufficient for the task in hand.I cleaned and processed the data in Python. The cleaning process mostly involved encoding strings and removing punctuation and numerical abbreviations.  I then created two new features: ""days""  for the number of days the ad has been up since the creation date and percentage of monetary target met. Then i focused on averages of the downloaded and new features across the eight categories.I started with plotting  the distribution for number of days an ad was up for different categories . Unsurprisingly urgent ads such as those for memorial services  emergencies and medical needs were the shortest (a median of 3 days). Personal categories such as sports volunteering and charity were up there for longer time (a median of about 10 days) . To my surprise the animals category also had a low median (about 4 days) Americans love animals !From the perspective of  campaign targets  the medical category seems to have high values  with a few statistical outliers with targets of half a million.  Another stark outlier is in Education where a campaign had a target of one million dollars. I personally would like to know what that person is studying !not influenced by categories.If we look at current funding statuses with respect to the percentage of the target amount garnered there is a uniformity across all categories of about ~ 60% ! I am  impressed that if one sets up a $1000 campaign they can raise on an average at least $600.My next investigation looked at the average number of people who contributed to a campaign and the average number of social media shares per category. Campaigns with urgent needs are more active in this regard while medical needs have more people contributing on average. Memorials havethe highestnumber of Facebook shares.The correlation plot of all the features shows two strong correlations. One seemed a bit trivial in that is the money raised being proportional to the number of contributors. The other one between money raised and the number of Facebook shares is more interesting. It means even if a person contributes or doesn't contribute it always helps to spread the word and pass the ad around on social media.The major focus of this project was to scrape and analyze data from Go Fund Me. I believe that there is more to learn from the data I collected and even more potential insights in the uncollected data. Some of the things I want to explore in the future are:",NA,What makes a crowdfunding campaign a success ?
https://nycdatascience.com/blog/student-works/seed-accelerators-social-media-made-vc-fund-startups/,53,The success of a startup depends on many factors such as the founders funding and theenvironment of the industry in which it is established. Startups never stop searching for achance to improve their probability of success. It’s the same for venture capitalists(VC’s). VC’s work hard to select the best target to invest with  to maximize theirprofit.Navigating the early part of its existence well is crucial toa startup’s success. A good seed accelerator can provide enough mentorship and funding support for startups. Mentorship help founders clearly understand what they want and what they should focus on. This iswhy some successful startups are usually born in the same seed accelerator.After a seed accelerator VC’s play an important role in helping startups to become stronger. However it’s difficult for VC’s to know whether a young startup will succeed or fail. It’s common to use the Discounted Cash Flow method to a public company but this method can’t be applied to a startup. In fact most startups don’t have clear financial records and formal financial reports. Therefore Relative Valuation is a choice for evaluating fastgrowing startups. A critical part in the Relative Valuation for online companies is finding a related company and assessing whether the two or similar or not based on the number of users. We can also explore how startups behave on  social media to indirectly assess its  number of  users.Thisproject focuses on webscraping data from  and . The first contains data about seed accelerators while the latter serves as the source forsocial media data of startups.I first took out the top ten seed accelerators with the most past funding. 'Y Combinator' dominates the feed in this respect. This is partly due to its being older. According to Wikipedia it is the first seed accelerator. Y Combinator’s creation was followed by TechStars (2006) and Seedcamp (2007). The bar chart to the left cements the importance of age when it comes to seed accelerators.The top ten startups from companies with a valuation greater than 1 million dollars are ordered by their total amount of funding. Most of them are very popular today.  All of them are online companies which proves the importance of the number of users in the valuation of startups.The data scraped from twitter contains some interesting insight. Friends_num statuses_num and favourites_num are more correlated with each other than with followers_num but these three variables are less correlated with funding (total amount of funding) than followers_num. This means that followers_num has greater direct influence on how much funding a startup can get. It's reallya reasonable projection since the number followers on Twitter depends on how popular the startup is and people who follow the company’s Twitter account are more likely to be users of itsbusiness. However the other three variables is not direct indexes of how popular the business of the startup is because a startup can write as many statuses as possible on Twitter even though it has only a few followers.The analyses above serve as a guide on how to apply multiple linear regression to this problem. The initial form of the model lies below.However assumptions such as multicollinearity need to be checked before building an effective regression model. It is also possible that the variance explained by the model might be small due to the variables not having enough predictive power.,NA,Seed Accelerators and Social Media: What made VCs Fund These Startups?
https://nycdatascience.com/blog/student-works/tracking-global-lpg-trade/,54,95% of the world's goods are transported via the ocean including a variety of cargo and essential energy resources. Liquid petroleum gas commonly known as LPG or referred to as propane is just one of these energy resources. LPG is mixture of flammable gases often used for heating cooling appliances and cars in developing countries who lack the energy infrastructure for more common types of fuel. As these developing countries grow the volume of LPG transported has followed to meet these developing countries increasing energy demands. The volume of LPG transported worldwide has increased drastically over the past 3 years and the order book for new vessels will account for 35% of all vessels on the water when completed by 2019. These trends substantiate the importance of this growing commodity type and this application was develop to better map global LPG transportation at a country and vessel level in order to assess the transformative patterns that have occurred over the past few years and dramatically visualize this data which has existed mainly in only private excel sheets. Furthermore this project lays the foundation for a dashboard that could not only visualizes historical data as it does in its current state but could merge this import/export data with automated production and ship position data to predict future over and undersupply of LPG in various regions (see next steps section for more) and give ship owners an edge positioning their open vessels in this increasingly competitive market. Probably the most noticeable trend in the last five years in global LPG transportation is the the dramatic change in which countries export the most LPG. The Far East region including primarily China Japan South Korea and India have remained consistent as the largest importers of LPG year after year with an increase in Indian imports and a decrease in Japanese imports over the last few years. Although the volume of imported LPG has increased among these countries the countries have remained fairly consistent. The same however cannot be said about countries which export LPG. Over the last five years the greatest export countries of LPG have shifted dramatically. Five years ago countries within the Arabian gulf region including Qatar UAE and Saudi Arabia dominated the global exports while now the U.S. has become the single largest exporter of LPG (up from nearly zero exports in 2011). For those in the industry this trend is common knowledge but again this known trend has probably not been visualized in such a dramatic mapped fashion or through  flow charts that adjust dynamically over time. The real power of this application exists not in   but through the potential it has if  further data sets are compared against this preexisting data. If I were to get access to APIs that automatically feed in the real time data for the preexisting data sources the current positions of all global VLGCs with their ballast or laden (empty or full) statuses worldwide and LPG  inventory and production data at a country level it would not be very difficult to assess present and future over or under supply of LPG globally. Comparing these data resources against the flow data the application already presents would allow a user to assess the number of VLGCs in each region or country compared to the inventories and production levels for that country both historically and forward looking. Predicting future over or under supply of VLGC and inventories has amazing potential because ship owners could then make more accurate speculations on where to send their open vessels.  ,NA,Visualizing Global LPG Trade
https://nycdatascience.com/blog/student-works/how-expensive-are-the/,54,Every year people come to New York City from all over the world to pursue careers chase dreams and seek success. They might even want to buy a home. In that case these questions may come to mind: What are the prices of the properties in NYC? What kind of property will I be able to buy? What is the average price in my neighborhood?  This shiny project seeks to provide answers to these questions.The data comes from a real estate company website City Realty and covers sales history from 2003 onward. The shiny app focuses solely on the year 2016.  The original data contains the following variables:For preprocessing the geocode() function turned addresses into coordinates to facilitate plotting on a map. Sometimes this process produced errors such as putting a NYC property in Australia.  To solve this the area of note was restricted to NYC. The addition of a variable called Area was created which assigns boroughs to each property.  Finally the Price.FT2 and Price variables were transformed from strings to numbers and renamed  Cost.FT2 and Cost.   The default map where all the sales are plotted appears first in the Interactive Map panel.On the side bar you can choose which neighborhood you want to have a look at the price range and the property type(2b2b eg.). Let's say you chose Brooklyn the map will give all the location of the sales and number of sales in each area.If you are particularly interested in one of the properties you can simply zoom in and click on that property. A popup will appear and show you the address and a website linkto this property.In the Explore data session I calculated the average price per square feet for each Area so that you can see the price trend over time.Of course you can compare prices in multiple Areas as well.Thank you for reading this blog post. Please feel free to give any comments!,NA,How expensive are the real estate properties in NYC?
https://nycdatascience.com/blog/student-works/thinking-starting-business-check-gem-first/,54,"As one of the most dynamic markets in the world my homeland China is now undergoing a huge economic transformation.  A friend once told me that almost everyone born after 95s’ in China is now thinking about starting his own business. Putting aside the credibility of this claim his wordsindeed incited my interest in investigating the entrepreneurship environment in China. In searching for the answer I came across Global Entrepreneurship Monitor (“GEM”). They've been investigating this topic for more than 15 years! Looking at their public datasets I initiated the idea of visualizing it hoping this App will aid the future businessmen across the world when they do their initial researches about entrepreneurship.For example in this screenshot of the app I selected the tab ""Global Overview"" and then ""Perceived Opportunities"" and the App shows a corresponding map with darker color representing higher perceived opportunities. If you click on the map the country name and specific statistic will pop out. (Note that countries colored grey are those ones outside the scope of  GEM 2015 survey)Ok. Now you have an overall idea about where are the hottest zones in terms of starting a new business but you may now start to wonder can I really do this in my country? Will the government support me? Can I find a place to build the factory? How will people think about me if I really quit my job and start a small business?From a GEM point of view these factors are called Entrepreneurial Framework Conditions (EFCs). They are the conditions in an Economy that would either enhance or hinder the cultivation of new businesses. I've put them all on a radar chart you can choose whatever countries of your interest to plot. By observing the shape of the polygons you can clearly see which countries' are advantaged and which are disadvantaged with respect to entrepreneurship.Rank rank rank. It's not a valid infographic if it does not contain ranks. So here I provide the choices for you to create your own rank! Simply slide to choose the top n's you want to show and choose 2 ratios of your interest you'll get 2 colorful bar charts showing the ranks that you want to examine.To create this App I used 2 datasets downloaded from GEM they are the National Expert Survey(NES) 2015 and Adult Population Survey(APS) 2015 for convenience of the users I embedded the two data tables inside the App.  You can scroll search any data you want and if you are really interested you can always go to .",NA,Thinking about Starting Your Own Business? Check GEM First!
https://nycdatascience.com/blog/student-works/popular-phones-apps-china/,55,"Contributed by Shuo Zhang. She is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between  July 5th  to September 23rd 2016. The shiny website: China is the 4th largest country in the world with a population of 1 401 586 000 (and still counting). There are 980.6 million smartphone users in the country as of today. China is as great as their Walls and is considered a giant when it comes to mass production of goods electronics furniture toys and now it is dominating the advanced world of smartphones. You can easily spot a ‘China Phone’ with the huge display screen vivid display colors HD camera features and the plastic (most likely) outer case.Apps are revolutionizing the way we travel and nowhere is this more true than China. Downloading these apps before you get to China can make everything a whole lot easier as it alleviates problems with the language and can help you get your bearings.When you travel to China and you want to buy a phone the first question that comes to mind is “what are other people like me buying”. There are so many phone brands making it difficult to pick one. Supposing that you finally make a choice and you are ready to install apps the second thought is “what are the most popular and useful apps?” From the phone manufacturers' perspective who will win the phone battle and occupy the China market? Who are their target customers? How can they improve sales volume and increase market share? From another point of view when you walk into a store and you use your app to make a payment how much information does the store manager receive about who you are (i.e. gender and age) based on the phone and app you are using? The shiny app described in this blog can help answer these questions. I hope you enjoy your tour of exploration.TalkingData China's largest thirdparty mobile data platform releases a weekly report detailing every android phone and app used in China (i.e. excludes the iPhone). Phones with the Android system dominate the Chinese smartphone market representing 89.9% of the total number of phones sold in China according to research from iiMedia. The dataset contains a wide variety of variables about the location gender and age group of the users involved. We'll focus on those variables for the shiny. The data schema can be represented in the following chart:From this main dataset I created two separate datasets. One is for device_id related to phone brand users' gender age and group and location ; the other is for device_id plus app_id related to phone brand users' gender age group  location  app categories whether apps are installed or not  and whether apps are active or not. For data analysis please refer to the link for R codes: From this shiny you will get answers to the following questions:China’s phone market is dominated by Xiaomi and Huawei. But Samsung Vivo and OPPO also catch our eyes for their good performance. Also I list the top 4 popular phones in 2015 provided by IDC. By comparison  Samsung's dramatic increase in 2016 is most evident and Samsung now ranks third in the market.Furthermore the shiny app provides a way to identify the trendiest phones in the top 3 brands.We can take a look at the possible reasonswhy the market is preoccupied with Xiaomi Huawei and Samsung.First let’s look at the Xiaomi Redmi Note2.This smartphone has broken all the selling records in China. It sold over 800000 units within 12 hours. It is one of the few pocketfriendly phones in China and with its 13MP camera and a powerful Octacore MediaTek chip. It also offers a userfriendly interface and a lively display .As Huawei’s flagship phone the Ascend Mate 7 makes the enthusiasts silent with its display size of 6 inches advanced processor 13MP of a rear camera and its long battery life.Samsung S6 and S6 Edge offer a better performance than the previous generations of the Samsung Sseries. It has a powerful Exynos 7420 processor built on a 14nm process which makes it the most advanced among all the other smartphones offered in the local tech market including the latest Android 5.0.1 Lollipop.Xiaomi and Huawei are popular local smartphone brands in China. Yet different cities  in China may have a preference toward different smartphone brands. To get a clear view of their geographical distribution I separate the map to two parts.Consumers in different tier cities have different concerns with respect to smartphones when purchasing new sets. The graph shows a distribution of the smartphone users by gender. Men have more smartphones than women in China with 70% of men owning these devices and 30% of women on average in May 2016. Additionally different phone brands have different distributions of female and male users. Huawei and Samsung seem to have an advantage in attracting males while OPPO and Vivo are more attractive to females. This is not surprising.It suggeststhat females prefer fashion phones such as OPPO and Vivo while males like functional phones such as Huawei and Samsung. Xiaomi is the king here (of course Apple is excluded).Let’s take the top 5 phone brands for example. To get a clear view of the age distribution I categorize the ages into  6  groups: below 27 (the post90s) 2736 (the post80s) 3746 (the post70s) 4756 (the post60s) above 56 (the post50s).First let's investigate the general distribution of each brand.The youngest users are between 16 and 18 years old  the oldest users are between 65 and 82 years old and the users' average age is between 27 and 32 depending on the phone brand.Secondly let's dig deeper to the age distribution by different age category.Differences exist in the purchase of smartphones by age. The post90s consumers are more likely to buy Xiaomi Huawei and Vivo. However consumers in the post80s post70s post60s and post50s are more inclined to buy Xiaomi Huawei and Samsung. All graphs reveal the same information that the young generation prefer to buy the fashion and more affordable phones such as Vivo and OPPO while senior people are more in favor of Huawei and Samsung. Again Xiaomi is the dominant.The further analysis of age group can be performed by the following code:  The graph shows the rank of  the top 10 most popular app categories in terms of installed apps. In May 2016 industry tag is the firstmost popular category  the secondmost downloaded app category is property industry 2.0. Also property industry 1.0 and services 1 hold places in the apps market.Let's take industry tag and video for example.Types of installed apps in smartphones have a significant discrepancy in different cities. For industry tag Shanghai Shenzhen Beijing Sichuan and Henan occupy the largest market share. While for video Shenzhen Anhui Shanghai Sichuan and Beijing have the largest market share.Let's take Xiaomi Huawei and Vivo for example.Different phone brands have different distributions of APP categories. Industry tag and property industry 2.0 seem to have an advantage in attracting Xiaomi and Vivo phone users while property industry 1.0 and services 1 are more attractive to Huawei phone users.The graph shows a distribution of the app categories by gender. Men download more apps than women in China with 75% of men installing these apps and 25% of women by average in May 2016 and different phone brand shares a similar distribution.Let’s take the top 5 app categories for example.The youngest users are between 16 and 17 years old  the oldest users are between 68 and 80 years old and the users' average age is between 28 and 32 dependent on each app category. While many people think of apps as things that younger people use there are older people who download and use apps. After all there are all kinds of senior citizens who are very techsavvy and are using computers and mobile devices every day.How about the preferences of different age groups?Differences exist in the downloading of apps by category and age group.  The post90s and post80s consumers are more likely to install  services 1 and video. However consumers in the post70s post60s and post50s are more inclined to download services 1 and p2p net load. Again industry tag and property industry 2.0 are the dominant.The further analysis of age group distribution can be performed by the following code:
Surprisingly there are more inactive downloaded app categories than there are active ones which means most people install a lot of apps but they actually do not use them frequently.In the shiny you can customize your own word cloud by selecting your gender and age.For example I am female and 30 years old.From the graph I find out the top 5 bestselling phones are Xiaomi Huawei OPPO  Samsung and Vivo and the top 4 mostdownloaded apps are industry tag property industry 1.0 property industry 2.0 and services 1.I think you can find your own answers now:If I learn more machine learning skills  I could b",NA,What are the most popular smartphones and apps in China?
https://nycdatascience.com/blog/student-works/live-safely-los-angeles/,55,In this interactive interface users are free to check crime and collision types during different time periods and different seasons. The temperature in Los Angeles is really stable all the year round so a year is only divided into two seasons hot and warm according to the historical record. Hot season is from May to October (6 months) and warm season consists of other months (6 months).In addition I also divided a day into seven periods for deeper exploration:  • Early morning: 4am  8am • morning: 8am  11am • Noon: 11am  1pm • afternoon: 1pm  5pm • evening: 5pm  8 pm • Night: 8pm  11am • Midnight 11pm  4am. Generally speaking 'Hot' season (May to Oct.) has slightly more crime and collision than 'Warm' season (Nov. to Apr.) in all periods of a day. As to crime and collision types 'Traffic' related issues are the major part and burglary and theft related issues also worth notice.,NA,"WHERE, WHEN, WHAT – How to live safely in Los Angeles?"
https://nycdatascience.com/blog/student-works/h1-b-visa-explorer/,55,Unlike Apu who  didn't want to go home after finishing education and had to live in US illegally for US not having an H1B visa program at that time  the author wanted to explore  the chances of an application getting approved  how long it takes for an application to be processed  which profession  job title  employer has high volume for applications and the success rate for applications.The US H1B visa is a nonimmigrant visa that allows US companies to employ foreign workers in specialty occupations that require theoretical or technical expertise in specialized fields such as in architecture engineering mathematics science and medicine. The job must meet certain criteria to qualify as a specialty occupation. Under this visa a US company can employ a foreign worker for up to six years. Individuals are not able to apply for an H1B visa to allow them to work in the US. The employer must make the petition for the employee. H1B visas are subject to annual numerical limits. Current immigration law allows for a total of 85000 new H1B visas to be made available each year. This number includes 65000 new H1B visas issued for overseas workers in professional or specialty occupation positions and an additional 20000 visas available for those with an advanced degree from a US academic institution.For details you can follow the link below:The case disclosure file covers VISA determinations issued between October 1 2015 through June 30 2016.Link to the data is below.The web app's basic layout is designed with a principle of being simplistic intuitive and easy to use. It provides three types of data exploration: “Temporal” “Statewise” and “Top of the Bunch”.  The Temporal analysis explores thequestion of how long it takes for applications to be processed as well as how many applications are submitted on a daily basis.   and time series of applications submitted and processed . Statewise analysis allows the user a to choose a number of different visualizations to dig intostatewise data.  Top of the bunch answers which states  jobs   job titles and employers have the most  applications and highest salaries.In a typical work week two thousands applications are submitted and processed. The data shows apretty big spike in the month of March for both submitted and processed applications. As it turns out April 1st is the last day to submit and process applications for year 2016 H1B visa cap.Most applications seem to be processed within a week's time frame as is evident from the plot with the mode being on day 6.The app allows different visualizations for the data . Below is a way to look at total certified applications on a pie chart. The number of applications approved and denied are proportional to applications submitted so this pie chart for applications approved follows a similar trend as the map where California and Texas had the most applications submitted.Approval percentage for applications is quite high with a minimum of 88 % to as much as 98 % in a few states. It indicates that once an application gets selected via lottery it has a very high chance of being approved in all states.The app lets the user explore up to top 20 results of by  states occupations job titles and  employers in terms of number of applications being approved for H1B visa and salary ( Average salary for top states) . Below are some interesting features .Bar chart below shows California  Texas  New Jersey and Illinois have the most applications approved .IT and consultancy companies from India are the top beneficiaries of the the program. Google and Microsoft are among the US companies that stand out.Job titles are related to occupation category so the same result of software professionals and financial analysts getting most approved applications still shows here.The app also lets users explore top states occupations  job titles and employers in term of salary .  Though number of applications are dominated by software professionals .  Top salary occupation and job titles are ones belonging to medical field .If one is interested in knowing which specific employer offers the highest salaries here is a graph.Surprisingly North Dakota  Washington and Maine are the top three states in terms of average salary.,NA,H-1B VISA Explorer Shiny App
https://nycdatascience.com/blog/r/shiny-tracking-migration-patterns-eastern-europe/,55,Over the last 8 years more than 1.7 million migrants reached Southern Europe either through Turkey or crossing the Mediterranean Sea. At the same time another 2.8 millions registered Syrian refugees are currently located in Turkey. ,NA,Tracking migration patterns through Eastern and Southern Europe with Shiny
https://nycdatascience.com/blog/student-works/monopoly-underdog-whats-flavor-soccer/,55,In the last season of the major European soccer leagues Barcelona Paris SaintGerman Bayern Munich and Juventus won the title in their own league. The big surprise came from Leicester City an underdog that was crowned the champion of England. Some would argue that the success of Leicester City well reflects the best part of soccer  its unpredictability. Others appreciate a dominating power like Barcelona which has brought modern soccer to a new level with a delicate balance of individual talents teamwork and control. If you like watching soccer what’s your flavor? In other words do you prefer watching a more predictable match with one dominating side or a less predictable match that is highly competitive? You might have your own preference. This blog introduces an interactive Shiny app that demonstrates the predictability of European soccer matches on three levels: league season and team.From Kaggle’s recently published European Soccer Database I used 8 European soccer leagues’ match data from seasons 2008/2009 to 2015/2016. Those leagues are England Premier League France Ligue 1 Germany Bundesliga Italy Serie A Netherlands Eredivisie Portugal Primeira Liga Scotland Premier League and Spain La Liga. Variables that are used to examine match predictability include match id league name country name season home team name away team name and betting decimal odds given by 10 bookmakers.So how does one measure game predictability?  When the referee blows the full time whistle there are only three outcomes: home win(HW) draw(D) or away win(AW). Each outcome might be accompanied by a range of emotions by the spectator from disappointment to anger elation to surprise.  Only the bookmakers tasked with objectively considering the odds are watching with equanimity.  The bookmakers predict the probabilities of all possible outcomes of a match and convert them to betting odds. These odds indeed reflect the objective chance for any of the match outcomes at a particular moment before the end of a match. For instance if a bookmaker gives a match decimal odds of 1.5 3.5 and 7.0 for HWD and AW the outcome of home win draw and away win will have a predictive probability of 0.67 0.29 and 0.15 respectively.Based on Information Theory Shannon’s Entropy(H) can be used to measure the uncertainty of an event’s outcome. Given the probabilities of all three outcomes of a match H can be calculated as H(P1*log2P1 + P2*log2P2 + P3*log2P3). H reaches its maximum when all three outcomes have equal probability where H(3*(1/3)*log2(1/3))≈1.58. This would indicate that a match is extremely difficult to predict since all three outcomes are equally possible. When the probability of one outcome is very close to 1 let’s say a bookmaker is almost 100% sure Barcelona will win a match H is approximate to 0. It would suggest a very predictable match. Therefore converting a bookmaker’s betting odds to entropy allows us to measure the uncertainty of a soccer match. For a soccer match with three possible outcomes the entropy has a range in [01.58] with 0 indicating an extremely predictable match and 1.58 indicating an extremely unpredictable match.Since one bookmaker tends to attract other bookmakers to advertise similar odds for a certain match the deviation of odds among bookmakers is pretty small. However when adding up the probabilities of the three outcomes of a match converted from the betting odds the sum is usually between 101% and 125% which is surprisingly not 100%. The portion that exceeds 100% is known as “overround” which represents the bookmaker’s expected profit. In an ideal situation if a bookmaker accepts $120 in bets at his own quoted odds he will pay out only $100(including returned stakes) no matter what the actual outcome of the match. Different bookmakers might set different overrounds. The true predictive probabilities of a match from bookmakers can be derived by normalizing the overround such that P’1  P1 /(P1 + P2 + P3) P’2  P2 /(P1 + P2 + P3) P’3  P3 /(P1 + P2 + P3). The true probabilities are used to calculate the match entropy. The predictability used for analysis equals the average entropy from multiple bookmakers. This metric is supposed to hold more information about the match predictability than any single entropy from one bookmaker.Below is the main interface of the Shiny app. The predictability measured by entropy is shown in the map below. The 8 countries with respective leagues are colored based on the degree of predictability as shown in the tab of “Soccer League Map”. The lower predictability has a darker color and the higher predictability has a lighter color. It appears that France Ligue 1 is the least predictable among the 8 leagues during the 2015/16 season and Spain La Liga is the most predictable.Moving to the next tab of “Trend by Season” you can check the change of predictability since the 2008/2009 season for each league. As shown below it has become more and more difficult to predict the England Premier League since 2013/2014. The past 2015/2016 season is the least predictable season. Indeed if you’re not familiar with the English league let me remind you that there was a dark horse named Leicester City that then beat multiple big names including Manchester City Chelsea and Liverpool.  Leicester City went on to win the title with a 10 points’ lead on the final scoreboard.With a further look into the 2015/2016 English league in the “League by Season” tab Leicester City has an average entropy of 1.51 in the full season making it the least predictable team in the league.Now let us look at the Spanish league shown below the average predictability has maintained a relatively high level since 2011/2012. In other words the Spanish league is relatively less competitive. We know that Barcelona has dominated the league in recent years and bookmakers tend to be more optimistic towards their win. Therefore the matches involving Barcelona were relatively easy to predict. Paris SaintGerman and Bayern Munich were also dominating their own league respectively. However those two leagues especially French league were much more competitive than the Spanish league. Why is that?By comparing the Spanish and French league in the two graphs below the Spanish league actually has two very predictable teams including Barcelona and Real Madrid. Their advantages have strengthened steadily since 2008/2009.  In the French league Paris has shown an obvious lead and has been more predictable since 2012/2013.  Other teams' predictability are relatively low and distributed in a narrow range. One interesting observation here is that it’s oligopoly instead of monopoly that might level up the predictability of a league. That makes sense in that when two or more super teams are in a league there will be more matches with a dominating team and thus be more predictable.If you check the table below and rank all Spanish teams by predictability it’s easy to see that Barcelona and Real Madrid are dominating the league with a large margin of predictability.After a brief introduction to this Shiny app you might have found some interesting insights regarding the predictability of European soccer leagues in the past few years. Oligopoly appears to make more league matches less competitive as compared to monopoly. After a season one underdog might be enough to surprise the whole league. Given the data some interesting “next step” questions might be:Now I guess you might want to play with this app. Please go ahead and .,NA,"Monopoly or Underdog,  What’s your flavor for soccer?"
https://nycdatascience.com/blog/student-works/united-states-fury-road/,56,Driving fatalities have clearly decreased in the past 10 years with the overall numbers decreasing around the time of the financial recession and continuing to stay low even into the recovery. It's unclear exactly why this decrease happened however I imagine it was a combination of safer cars combined with lower levels of driving. Drunk driving distracted driving and speeding all seemed to follow a similar trend suggesting that the rate of these dangerous driving hasn't decreased significantly in the past ten years.Finally I included a simple tool to allow you to compare states directly based on any rate that you wish to see. In the above example you can see that my home state of Connecticut has about one fifth of the driving fatality rate that Wyoming has. ,NA,United States: Fury Road?
https://nycdatascience.com/blog/student-works/visualizing-global-terrorism-trends/,56,It wasn’t the sound a firecracker would make. However this is an artifact (and a limitation) of the data.  What is captured here is a trend in improved data collection in this increasingly digitized age.  ,NA,Visualizing Global Terrorism Trends
https://nycdatascience.com/blog/student-works/world-population-explorer/,56,"The world population was estimated at 7441490775 as of 9am on August 6th 2016 according to Worldometers.7441490775 is far from a small number.  I cannot resist doing some thought experiments.Assume the average height of human (including men women and children) on Earth is slightly more than 1 meter and each person stands on the shoulders of another one to build a ladder sticking to the blue sky. Given the circumference of Earth at the equator which is about 24874 milesthe ladder of height 7441490775 meters (4623928 miles) can wrap around the Earth’s equator 186 times! If we collectively stack ourselves towards Mars then the ladder height is 3.3% of the mean distance from Earth to Mars (140 million miles).If each of us stands straight within a square of 1 square yard in area one besides another to form a massive infantry square the entire world population then occupies 2402 square miles which is equivalent to 105 Manhattan Islands or 7.9 New York City or 21% of the entire Hawaii Islands.In computer science terminology if each person maps to one byte of memory of your laptop then the entire population consumes around 7.44 GB of memory.Isn't this a huge number? Don't forget as you take 3 minutes to think through these facts in your head hundreds of babies have been born.As we analyze the past 60 years of human history we can see that different regions of the world have shown distinct trajectories of population growth. I am especially interested in investigating these growth trends and answering the following questions:The Shiny app can be found here: Github code: If the world were comprised of only 100 people in 2015 there would be 60 people in Asia 15 in Africa 11 in Europe 14 in America (9 in South America and 5 in North America) as shown below. The infograph below was generated using .75% of the earth is covered by ocean and only a portion of the remaining 25% of all lands are habitable for humans to live and flourish. Overall the distribution of the population in the world is largely affected by various factors including the territorial landscape geographic environment and regional economic level etc. Our mother planet superimposed with population across all 232 countries/regions is shown below.All men are created equal but not all regions where men live develop equally:The global population reached 4 billion in 1974 5 billion in 1987 6 billion in 1999 and 7 billion in 2012. Note that there is no exact day or month the world’s population exceeded those milestones since the worldwide population census was usually conducted every 5 years. It is worthwhile to know that United Nations estimates that the world population will reach 8 billion by 2024 and will possibly reach 9 billion by 2037.Asia is the most populous continent and the Asian population is equivalent to more than half of the world population. We are especially interested in looking at eastern and southern Asia because China and India are the world’s two most populated countries. They together contributed around 37% of the world’s population in 2015.China and Japan are two of the most populous regions in eastern Asia but from the visualization China shows larger growth than that of Japan.Overall the curve of China is increasing over the last 60 years since 1950. But when looking closely into the curve we may find the increase is not at the same speed.(Graph courtesy of worldometers [3])It is not hard to notice that there were three subtle ""valleys"" along the population curve of China between1960 and 1990. From the Yearly Population Growth Rate plot we see three major drops in population growth rate around 1959 1972 and 1988 caused by Great Leap Forward Cultural Revolution and the onechild policy respectively.As developed countries Japan and Korea do not have a large population base and there is no sign of large increases in either country.India has the largest population in southern Asia and the second largest population in the world. As described in previous sections Pakistan and Bangladesh rank 6th and 8th in world's population in 2015.In Northern America the USA has the largest population and Canada is the second runner. The population of Canada has a constant speed of growth whereas the population of US had subtle ""valleys"" over the last 60 years indicating an unstable growth rate.
Three years namely 1955 1985 and 2015 were investigated here to illustrate how the top 10 countries evolve in population.In 1955 China ranked 1st and accounted for 20% of world population. The Russian Federation ranked 4th in the leaderboard and accounted for 4.05% of the world population.In 1985 Indonesia surpassed the Russian Federation and ranked 4th in the ladder. Germany UK and Italy were all squeezed out by Bangladesh Pakistan and Nigeria.In 2015 Mexico kicked in leaving Japan out of the top 10. China India and US remained the top 3. Surprisingly the population of Russian only increased by 481150 people from 1985 to 2005. But digging into Russian history in the past 20 years we know the root cause: dissolution of the Soviet Union.Concern about overpopulation has lasted for decades. Researchers have developed many models to predict the future world population. Debates about overpopulation still echo in both academic communities and among common people.The good news is that globally the population growth rate has been steadily declining from its peak of 2.19% in 1963. Nevertheless growth remains high in Latin America the Middle East and SubSaharan Africa [4] and no one can exactly foresee what the future population will be.(Graph courtesy of [4])Based on the visualization of this Shiny project the following conclusions can be drawn:",NA,7.4 Billion Earthlings on One Planet: Trends of the Past 60 Years
https://nycdatascience.com/blog/student-works/r-shiny/past-future-energy-sources-electricity-generation-usa/,56,There is a lot of talk about the amount of energy US imports while scarce attention is paid to how much it produces.  Not only did  the US generate 4 trillion kilowatt hours in 2015 but in term of global warming the US is producing an increasing amount of renewable energy. About 67% of the electricity generated in the US is from fossil fuels (coal natural gas and petroleum). The fossil fuels are nonrenewable sources which are limited in supply and will one day be depleted. Burning fossil fuels creates carbon dioxide the number one greenhouse gas contributing to global warming. The remaining 23% of electricity is generated from renewable energy sources. Sun wind and water are perfect renewable energy sources depending on where you are. They are nonpolluting renewable and efficient. With the development of technology the energy conversion efficiency of solar photovoltaic is increasing dramatically and the cost for generating every kilowatthours is decreasing.This app displays the energy sources of electricity generation over years and by state.  When you click “Plot_General” from the dashboard sidebar you see  the plots of electricity information over the past 10 years. The left plot shows the net electricity generation and the right plot shows its annual growth rate. The parameters that can be chosen in the list of total electricity generation are coal natural gas nuclear renewable and others.When you click “Map_General” the left map shows the rank of states in the use of different resources for generating electricity. The right pie chart shows the percentage of  each energy source used. (In the “Plot_General” and “Map_General” renewable energy sources are combined into one variable.)The second group of buttons are “Plot_Renewable” and “Map_Renewable” which provide specific information by type of renewable energy. This information allows the shiny app user to compare the production of different renewable energy sources nationwide and how this has changed over time.The left “Map_renewable” plots show the rank of states in the use of renewable resources and the right pie chart shows the percentage of  each reneable energy source used. Just as plants are built so they are retired over time.  The “Plot_Units” shows the count of retired plans as well as the addition of new electricity plants projected to 2023.  This reveals the future focus of sources of electricity production.What do we find by using the App?During the past 10 years the net total electricity generation was constant which is stable at 4 trillion kilowatthours.  The usage of coal has been decreasing and the usage of natural gas has increased. At the end of 2015 natural gas exceeded coal as the number one source for electricity generation.  The electricity generated from renewable sources  has continued to increase every year.Among renewable sources  hydroelectricity ranks number one and then wind and solar photovoltaic.  The growth rate of solar photovoltaic is impressive. Even though the rate of growth  slowed in 2015 it is still an impressive ~50%.Looking at each state separately reveals some interesting findings.  Texas ranks number one for electricity generation in both coal and natural gas.  53% of electricity is generated from natural gas and 19% is from coal.  16% of electricity is generated from renewable sources 94% of which is from wind power.In Washington state 82% of electricity is supplied by renewable energy with hydroelectricity the leader at 90%. Washington state illustrates how natural resourcesare able to supply more than 70% of human’s needs in certain natural environments.With its large population and high tech culture California demonstrates another mode of using energy resources for electricity generation. California ranks number one in using solar energy for electricity. About 25% of total renewable electricity is from solar photovoltaic.From 2016 to 2023 there will be 112 natural gas units 36 coal units and 31 petroleum units retired in U.S.. The planned additions are 402 solar photovoltaic units 384 natural gas units and 169 wind power units. In the near future natural gas will dominate as a source for electricity. However the share of renewable energy keeps raising.  For some states like Washington state renewable energy already can supply above 70% of its electricity needs. The portion of solar energy will dramatically increase. The reliance of humans on fossil fuel will continue to decrease.Interested in this App? Please click the link!https://jingyuzhang.shinyapps.io/Jingyu_ShinyEnergy/,NA,The Past and Future of Energy Sources for Electricity Generation in USA
https://nycdatascience.com/blog/student-works/consumerism-original-sin-global-warming/,56,Global warming is happening at least 95% of the scientists believe so. And the results are quite severe: the sea level is rising animals are dying and we are witnessing more extreme weather conditions in recent decades than ever. This blog walks through some of the evidence for the existence of global warming a possible cause and a call to action.  Figure 1 plots the anomalies of world average temperature relative to the Jan 1951Dec 1980 average.Despite of the variations in late 18c the deviations away from zero has a clear trend increasing since the Second Industrial Revolution.On the global level Figure 2 captures snapshots of North America and Antarctica for the daytime temperature in the week of June 17 2016. Areas that are hotter than the average temperatures for the same week or month from 20012010 are marked in red while areas that are colder are colored in blue. In the north where it is summer in June there is a higher degree of concentration of red dots showing that the summer is hotter while the opposite happens in the south where the winter is colder.What is the cause of such changes? An immediate answer goes to the accumulation of  that tends to absorb and emit radiation in the atmosphere. But since GHG has always existed on the planet earth there must be an external reason for this the series of changes. And that reason most likely is us humans.As conquerors and consumers of the material world humans emit an excessive amount of GHG to generate power for direct consumption and to produce goods and services.  There are two ways to quantify the emission of GHG: CO2 equivalent by Production per capita and CO2 equivalent by Consumption per capita; the two terms represent different metrics to quantify the impact of GHG on the atmosphere.The first method measures the per capita level of total carbon emission during the production process within a geographic border. It is the most commonly used measurement. However measuring emission based on production ignores the fact that in many cases producers and consumers are geographically spread out in a highly globalized world.Therefore by reallocating the amount of carbon emission during the production process to the final users of the goods and services the second method provides better measurement as well as additional insights on the subject of Consumerism and Global Warming.And as a matter of fact we do see a clearer positive relationship between Carbon Emission (per capita) and National Income (GDP per capita a proxy for consumption) if we select CO2 based on final demand rather than CO2 based on production. (See Figure 3 and Figure 4)Similarly as the economy grows for countries around the globe over time we can also observe a higher rate of emission per capita as brown countries (high emission per capita countries) become more brown and green countries (low emission per capita countries) become a darker green.It is interesting to point out that the relation becomes less clear if we switch to the first method of Carbon Emission measurement.Finally if you are still reading this blog post and wonder if there is anything that you can do to help slow down the anthropogenic tragedy from occurring so fast on a global level the truth is: Yes You Can.Consumption is a necessary driving force of economic growth and social development the purpose of this article is not to argue for zero consumption.  Rather unnecessary purchase behaviors as well as the habit of wasting can be improved. In addition the pursuit of efficiency is highly advised whether it be the purchase of vehicles with best Fuel Economies or the usage of public transportation overall. At last you can always be the one to spread the word!,NA,"Consumerism, the Original Sin of Global Warming?"
https://nycdatascience.com/blog/student-works/what-do-americans-do-for-fun/,57,Working in the marketing industry I've always had consistent interest in observing people’s lifestyle: How do they use media? How do they spend money? And of course how do they spend their time? With my deep curiosity about this particular topic and passion for data analysis I discovered ATUS the American Time Use Survey.  is sponsored by the Bureau of Labor Statistics and is conducted by the U.S. Census Bureau. The major purpose of it is to develop nationally representative estimates of how people spend their time. ATUS respondents are selected from the Current Population Survey(CPS) samples and they are interviewed only one time about how they spent their time on the previous day where they were and whom they were with.After data cleaning the raw dataset came down to 1118532 observations of 33 variables.Speaking of Time Use the first thing I looked at was the trend along atimeline.  The bar chart below shows the top 10 activities over a five year period from 20112015. I initially hypothesized that certain healthy activities like working out would show a gain in popularity year after year but unfortunately the answer was NO.In this chart I summarized and ranked the total duration time of each independent activity and divided it by the unique respondent number of that year. This gives us. We can see of course the number one activity was sleeping. One interesting finding is that on average At first glance I could not believe my eyes and went back to check the code.What I foundis that the survey blends weekdays and weekends and does notexclude the unemployed. But anyway this finding is valid enough to indicate the irreplaceable status TV and movies havein American culture and the impact on people’s life.That’s partly why we have the word “couch potato” which isn’t a favorable description of a person at all.  To discover more in this realm I decided to drill down into recreational activities specifically and find out what are the other things Americans do for fun.Using the same method I listed the top 10 recreational activities that Americans spend most time on. Note that a missing bar means that particular activity appears in the top 10 list at least once but is replaced by another activity in that year. The pattern stays consistent and . But I was even more surprised to see that no sports activities made it into the top ten as I tend to think of Americans as active sports lovers.  The data told me that it's just my stereotype and it seems most Americans prefer a sedentary lifestyle.Next I do acomparison between people with different backgrounds and locations: The first thing I need to do is to .I found that together the top 10 recreational activities possessed nearly all of the time people spend on leisure so it is fair enough to look at only the top 10s since they are representative enough.I divided them into 2 categories: Beneficial & Non Beneficial.are:  are: This is for sure a disputable classification but there is barely any standard criteria for this so I made the classifications based on my judgement.  Applying the definition above I calculated a new variable which is:I call it  and .The graph above clearly shows the relationship between Region Family Income and Beneficial Fun Time Ratio.  There are several interesting conclusions we can draw:On the map above The  is very interesting and sort of . I was supposing with sunshine and ocean Californians and Floridians would have a very high ratio.  On Youtube videos is looks like they surf and bike all day long.  I even went back to check my code and data manipulation process but unfortunately in the world of data visualization counterintuitive things happen all the time. The only reason I can think of is that maybe the geeks in California spend too much time playing games.I also want to examine the possible relationship between lifestyle and education level. Thus I plotted the graph below and I also added a new continuous variable personal weekly income as another factor.The first thing everybody will notice is that people who did not finish college have obviously less income than those who did.  But that's not the purpose of this graph.  but that never happens. It seems that some doctors can possibly be couch potatoes and some people with lower degrees can maintain a healthy lifestyle too! So my conclusion here is Given the dataset I can also explore the relationship between different job titles industries and the Beneficial Fun Time Rate. I also need to drill down to each state's micro data to explain the map I plotted previously. In all this is such a fun data exploration journey and if you read till here I would like to thank you for your patience and you've probably been sitting for too long now you may need to get up and go for a walk!,NA,What Do Americans Do for Fun?
https://nycdatascience.com/blog/r/12868/,57," variety of servicesFor every year sinceSince 2013 YELP have been organizing the ""Yelp Dataset Challenge""Possible research directions such asThe data is available on 's website across descriptionincludes a concise look at the data’s contents.This maiden experience of working with json data in R was difficult. However with some research the data’s loading and transformation was eventually complete. I first explored the distribution of business ratings. MostA similar pattern occurs when the microscope is turned onhe average number of reviews for different ratingsOn an average businesses with ratings between 3.54.5 starshave more reviews.I suspect that this is due to a positive feedback loop. People will look for highly rated business on YELP to visit and then themselves leave high ratings themselves Going deeper in the data a cursory glance at the state level revealed that Arizona has theAt the citylevel took the crown for most businesses in Yelp. This is unsurprisingly given the city’s many casinos  hotels and restaurants.The Las Vegas effect is perhaps responsible for Nevada’s havingthe highest number of reviews even though Arizona has more Yelp business listings. Again Ithink this is a knockon effect of  its the service industry .When it comes to the businesses themselves the field is dominated by restaurantsThe distribution of the various restaurantsubcategories is more uniform.",NA,Round 7 Of The Yelp Dataset Challenge
https://nycdatascience.com/blog/student-works/data-visualization-lending-club-issued-loans/,57,Lending Club (“LC”) is the world’s largest peer to peer online platform. It reduces the cost of lending and borrowing for individuals with advanced data analytics.  Peer to peer companies’ function is matching people who have money with people who want to borrow money. As a leader in the peer to peer lending industry LC completed its initial public offering in December 2014 which definitely improved public trust in this fast growing company.This analysis is based on data published on the website of Lending Club company. (You can find it here.)  The dataset contains 1021286 observation and 111 variables collected from 2007 to 2016Q1. Forty variables were selected for this analysis.  Four trends were investigated: growth of issued loans both in terms of dollars and volume geographical distribution of loans purposes for loans and interest rate changes over time.Lending Club got launched during 2007 but its business grew significantly since the start of 2012. From 2014 to 2016Q1 monthly loan amount and volume have been increasing drastically but the increasing trend is unstable in the short term. Additionally the increasing paths of the loan amounts and volume are similar. We can still expect a high growth rate in the following year based on past growth and the performance of first quarter in 2016.In order to learn more about the growth we fit a basic trend line to figure out the average changes in loan amounts. Above line chart shows average loan amount increased at a constant rate from 2007 to 2012 but it grew with a descending rate from 2013 to 2014. The average loan amounts in 2015 and 2016Q1 roughly remained unchanged.From a geographical perspective California Texas New York Florida and Illinois have the largest dollar amount and volume of loans.Peer to peer lending focuses on individual lending and borrowing so it’s important to understand why consumers decide to borrow money.Unsurprisingly debt consolidation is the most common reason for borrowing. The greatest advantage of peer to peer lending is the low cost so loans issued by LC usually charge lower interest rates compared with money provided by traditional banks. Most consumers choose to consolidate debt to enjoy lower borrowing costs.The color in the above tree map stands for different average amounts of loans for different purposes. Debt consolidation credit card house and small business are the most popular reasons for borrowing.Two loan terms were explored 36 months and 60 months.  (Note in the plot below the analysis begins in 2010 because LC did not introduce 60month loans until 2009.)  Interest rates vs. loan terms vs. loan grade are represented in the graph below.According to the risk theory longer loans own higher risk as well as higher interest rate than shorter ones. However we cannot obtain the corresponding conclusion from above graph. Interest rate distributions with different terms have no significant difference.However it’s really interesting that interest rates of different grades behave in a significant trend through years. From 2010 to 2015 interest rates for loans with grade D E F G obviously increased. We still see there are larger disparities of interest rates among grades possibly due in part to LC’s credit policy updates. This change is a great symbol that LC has become more and more proficient in the evaluation of loans’ risk.In this project we have a brief and clear introduction to the growth of LC’s main business. It’s obvious that LC is currently a fastgrowing but the volume and amount of loans are erratic.  This is surprising given that we expect public companies to be more stable.  The drastic fluctuation of LC’s stock price also proves the conclusion. However LC’s business model still brings a great advantage over traditional banks. LC is improving its risk and credit evaluation technology and trying to extend its market from individuals to businesses. From the latest growth data we still believe LC issued loans will continue to grow rapidly.Geographical distribution of issued loans may be influenced by several factors such as state financial standing state culture LC’s advertising strategies etc.  We need more data for the deeper exploration so combining current dataset with other datasets is a good choice.FICO is widely accepted by traditional banks as a credit index but LC claims its algorithm considers more than 2000 variables in credit evolution and risk management. There may be some interesting correlation between grade and factors such as homeownership number of inquires debt to income ratio etc. We can explore this more with more advanced data analytics.,NA,Data Visualization of Lending Club Issued Loans
https://nycdatascience.com/blog/student-works/new-york-city/,57,The New York City Taxi & Limousine Commission has released staggeringly detailed historical data covering over 1.1 billion individual taxi trips in the city from January 2009 through June 2015. Taken as a whole the detailed triplevel data is more than just a vast list of taxi pickup and drop off coordinates. It specifies some other useful information about the number of passengers pick up times location and revenue.I chose this project to understand the dynamics of the yellow taxi industry better.  What kind of trips are made in cabs? Where do those trips occur? Does the number of passengers using the taxi follow any pattern? What are the predominant costs and locations of taxi trips and what are the implications of these findings?The primary goal of this analysis is to find useful insights to help the yellow taxi cab drivers  work smart not work hard.The dataset includes trip records from all trips completed in yellow taxis from in NYC from January to June in 2015. Records include fields capturing pickup and dropoff dates/times pickup and dropoff locations trip distances itemized fares rate types payment types and driverreported passenger counts.The data has 77080575 rows and 11 columnsThe pick up location for taxi is scattered all through Manhattan. The intensity is high all around the cityThe plot shows that Thursday Friday and Saturday are the days where the business is at peak and Monday is the dullest. The surprising element to notice is that Wednesday has more customers than Sunday.Total Amount of Revenue Generated is directly proportional to Total Number of Passengers.               The number of customers increases steadily from the start of the day and reaches its highest level at the end of the day. There is a slight increase in the passenger count at noon. This is seen in all weekdays and may be due to people going out for lunch. What makes Friday unusual is that there is continuing increase in taxi rides through the night  most likely because Friday is a night when people go out.The day starts with high passenger count and decreases as the time passes. It reaches the lowest point at 6 am and then increases linearly till 3 pm. After 3 pm there is no notable difference in the passenger count.  It looks like most people end their partying and are at home by 6am.The number of passengers decreases from the start of the day till 6 am and increases from 6 am till 6 pm. Then there is a continuous decrease till the end of the day. Perhaps New Yorker’s don’t go out on Sundays because they are resting after the heavy weekend partying; or maybe they are just preparing for the upcoming week.The Number of Passengers travelling increases continuously all day and reaches a peak from 4 pm to 6pm and decreases as the day comes to end.  Likely this  is because the people who work are most likely to complete their work between 4pm and 6pm after which they tend to stay at home.  Afterall it is a “school night”.  ,NA,Exploratory Analysis Of New York City Yellow Taxi Data
https://nycdatascience.com/blog/student-works/exploring-response-biomarkers-clinical-trial/,57,Biomarkers are widely used in clinical research and their presence as primary endpoints in clinical trials is now accepted almost without question. What is a biomarker? According to FDA a biomarker is a characteristics that is measured as an indicator of normal biological processes pathogenic processes or responses to an exposure or intervention including therapeutic interventions. Molecular histologic radiographic or physiologic characteristics are types of biomarkers. A familiar example is the use of blood glucose levels as a biomarker to measure the effectiveness of a diabetes medication.This study looks at the way in which biomarkers were used in one study to uncover which biomarkers change as a result of medication and which biomarkers can be used to predict adverse effects.  In a Phase I clinical trial 39 healthy volunteers enrolled in a study and were asked to take a pill daily. Measurements of 20 different biomarkers were collected before the subjects started taking the pill (week 0) and on a weekly basis thereafter (from week 1 to 8). The values of some biomarkers might change as a result of the treatment and thus reflect the effectiveness of the pill. 12 volunteers had an adverse event while on study. Adverse event is any unfavorable and unintended sign symptom or disease temporarily associated with the use of the pill whether or not considered related to the pill.  The change of some biomarkers might show certain patterns before the event and therefore help to predict the occurrence of the later event. This prediction could be useful in deciding whether and when a volunteer should stop taking pills during the course of treatment. To sum up two questions that are of interest: 1) What biomarkers are changing as a result of the treatment? 2) What biomarkers might work as indicator of the adverse event?When comparing measurements of all biomarkers during the treatment (week18) with the corresponding measurements before taking the pills (week0) in the line chart below it seems that the values of biomarker M1 M2 M3 and M4 generally increase during the course of treatment.   There appears to have positive linear dependence between values of M1 M2 M3 M4 and length of treatment. The rest of the 20 biomarkers instead show no significant difference between weeks. As such M1 M2 M3 and M4 might have shown response to the treatment.To further check the change of M1 M2 M3 and M4 the standard deviation of those biomarker values were plotted in the line chart below. There is large standard deviation for M1 M2 M3 and M4 which would potentially make the between week difference of those biomarkers not significant. However those measurements could be further normalized by taking the ratio of Marker Value during treatment (Week  18) and Marker Value as control (Week0). There are a few Marker Values at week0 equal to 0. As such both the numerator and denominator of the normalized marker value is added by 1 and the new normalized metric named Fold isFold  (Marker.Value(Week110) +1) / (Marker.Value(Week0) +1)The line chart of Fold vs Week is shown below. As compared to the chart of Marker Value vs Week above the standard deviation for M1 and M2 have become much smaller when tracking the Fold change during the treatment. However M3 and M4 still have relatively large standard deviations which might be caused by some large outliers. Based on the Fold change during the treatment M1 and M2 might work as better biomarkers in response to the treatment since they have less variance between volunteers.To study which ones of the M1 M2 M3 and M4 biomarkers’ changes are COnrelated to adverse events the volunteers were grouped by whether they had an adverse event or not during the course of treatment. 12 of them who had an event were grouped together and labelled with Event1 and the left 27 were grouped with Event0. The two groups were compared in the line chart below.In the chart for M1 the two groups have relatively large deviation at Week6. A large deviation between groups is also shown in the chart for M3. Among the 12 volunteers who had an adverse event 7 of them had the event at Week 7 which is one week after the large deviance occurred for M1 and M3. In the chart for M4 the largest gap between the two line charts is at Week 3 which is one week before 4 volunteers experienced an adverse event. There is 1 volunteer who had the event at Week 6. In chart for M3 the two groups also had large differences at Week 5 which is one week before the event occurred for that volunteer. As such the between group difference one week before the adverse event might be a potential predictor for the upcoming event.Since the adverse effect occurred at Week4 6 7 the boxandwhisker plots below drill down and further analyze the between group difference one week before the event. The distribution of Marker Fold for M1 at Week 3 and 6 and that for M3 at Week 3 5 and 6 appear to be different between groups. Since the sample size from group with event and group without event are not equal Welch’s two sample ttest is applied to examine those differences. However none of the 12 pairwise differences demonstrated in the chart below are statistically significant at alpha0.05 level. It’s worth noting that 12 of those 24 distributions shown here have some outliers beyond the reach of whiskers. For instance in the group without event at Week 6 for M1 there are three outliers above the upper whisker of the box.  The sample size for each group is between 12 and 27.  Therefore the influence of even one outlier to the group mean might be significant.  If those outliers are removed the sample mean from this group will be smaller and its difference with the sample mean of group with event will get bigger. Then the two sample ttest might have to accept the hypothesis of difference of mean between groups. M1M2M3M4 might work as biomarkers that respond to the treatment.M1 and M2 might be used as predictors for the adverse event.There are some outliers that need to be further understood.,NA,Exploring Response of Biomarkers in a Clinical Trial
https://nycdatascience.com/blog/data-science-news-and-sharing/restaurant-in-manhattan/,57,containing the results of restaurant inspections that are conducted once a year. The whole data set contains 450781 historical inspection records of 24765 restaurants. Every year each restaurant is assigned a score and grade according to that score which reflects the health and hygiene of that restaurant.Since 75% restaurants are graded AThe center of A restaurants is located nearby Times Square and extends to cover the upper east side of Manhattan and the Columbia University areaThe C group heatmap is greatly different from the previous two. The center is further south and there is a separate group that covers northwest Manhattan.From three heatmaps above we can draw two conclusions:  that we’d better go midtown to north if we are seeking a great restaurant even though that area is basically full of restaurants from every group; and people in Harlem are suffering from food sanitary problems.Indeed while the distribution matters so do changes in grades. The scatterplot below was generated based on a comparison between “overall grade” and latest grade from each restaurant. This captures the trend of how restaurants’ sanitation condition changes over time. Overall grade was then compared to most recent grade.The red dots indicate the restaurant grade is getting worse while theAs the boss of a restaurant you should think twice before every business decision. With the result of this analysis the runner of a restaurant can also take the grade into consideration. For example if the owner is running a restaurant in Harlem the grade would not trouble him or her due to the average poor sanitation condition.   The restaurant owner could save the money rather than trying to improve his grade.Want to have a healthy dinner? Look at the map before looking at the menu.,NA,Analysis of Hygiene of The Restaurant in Manhattan
https://nycdatascience.com/blog/student-works/bitcoin-legs-visual-analysis/,57,"Bitcoin has been in the news a lot recently.  As a digital currency it has been the subject of investigations relating to legality of goods and services for which it is used as payment.  As a technology it has been under scrutiny regarding the security of the currency and the initial application of blockchain technology which is of interest to industry beyond just digital currencies.  Despite the excess of speculation about its usefulness as a currency and perhaps because of the associated hype the value of the currency has been hugely volatile.  This post will provide a basic visual analysis of the effect of the volatility of the currency on the adoption of the currency as a means of transacting for goods and services.Most of the necessary data for this visual inspection of relationships is available via Quandl.com and is provided by Blockchain.info a site via which one can view all of the transactions in the blockchain or central ledger in which all bitcoin transactions are recorded.  From Quandl and Blockchain.info I chose to look at three data sets:: An estimate of the number of unique addresses used in bitcoin transactions daily.  Blockchain.info chose to remove activity from the top 100 most active addresses as activity from these addresses most likely should be excluded for reasons beyond the scope of this post.: An estimate of the total USD value of transactions with the same top 100 addresses removed.  As best I can tell this is also separate from exchangebased transactions in bitcoin (the exchange of bitcoin for other currencies).: An estimate of the number of unique transactions in bitcoin on a daily basis. Blockchain.info adjusts the total transactions for “change” that is returned to the original payer from the receiver of payment which is present in most transactions.Blockchain.info also provides a daily value for bitcoin vs various other currencies. However for purposes of this study I wanted to find bar data (open highlowclose). This can be useful in analyzing the true volatility of an instrument.  I was able to get this data from bitcoincharts.com and chose to use data from the exchange bitstamp which has had decent trading volumes and a longenough history from which to construct bar charts.A simple visual analysis of the time series plot confirms that the currency is highly volatile.  From its humble beginnings the value of bitcoin spiked well over $1000 retreated back below $300 and is currently worth around $650 (at the time of this analysis).The candle chart for the period from the end of 2013 to early 2014 provides more evidence of the volatility of bitcoin.  The large ranges on single days and the pattern of largemove days following other largemove days highlights the volatility.A distribution of daily returns also provides visual evidence of the volatility.  The histogram appears fairly normal but has a standard deviation of 4.9% and has large outliers reaching and exceeding 20% daily moves on occasion.  As a comparison the standard deviation of returns for the USD/GBP pair is 0.56% in the same period with just a single outlier value in excess of 5% (8% around the recent ""Brexit"" vote).Before considering the effect of volatility one can view the adoption of bitcoin as a function of time and see that the three metrics unique addresses transactions and total transaction volume have been steadily increasing.  The three charts below of these three metrics confirm the acceptance of bitcoin as a means to pay for goods and services.A visual comparison of the adoption of bitcoin and the volatility of the currency may provide insight into whether users are affected by the volatility.  A first comparison of the weighted daily price vs the number of transactions daily provides little evidence of a relationship.  The shading by date indicates that there are clusters of points separated by date.  This chart doesn’t truly reflect what we wish to see which is the relationship between price volatility (a derivative of price) and usage rather than the simple nonderivative price vs use.This second chart is a comparison of the daily returns and the value of transactions.  By splitting the daily returns into four equalsized buckets one is able to do a quick visual comparison of the value of exchanged bitcoins during days with large moves on the up and down side vs the value of transactions during less volatile days in the middle two buckets.  While not providing any statistical evidence this visual allows us to see that the value of transactions seems unaffected by the daily return.  Coloring by year also suggests that there is a consistent disregard for price volatility across years.Finally the chart of the daily price range vs the number of transactions provides no clear evidence of a relationship either.  With a simple coloring by year one may be able to deduce that the price range and transaction counts are independent and that the number of transactions may simply be a function of time.  Each year it appears there are more transactions.  This was confirmed by the earlier charts.This analysis provided no evidence that there is a relationship between the volatility of bitcoin and the adoption of bitcoin as a means to pay for goods and services.  The most likely conclusion from this analysis is that adoption of bitcoin is a function of time and that price and volatility are completely independent variables.  Further analysis might provide statistical evidence of this or may provide evidence of a relationship between other derivatives of price and the adoption of bitcoin.  In the absence of further analysis that may provide evidence of a relationship it is clear that the number of users and the number and value of transactions has been increasing steadily and will likely continue.",NA,Bitcoin - Does It Have Legs?  A Visual Analysis
https://nycdatascience.com/blog/student-works/analysis-forest-fire-predictors-montesinho-natural-park/,57,Montesinho is a beautiful protected area located in the municipalities of Vinhais and Bragança northeastern Portugal. Sections of the southern slopes of the Serra da Coroa (Sierra de la Culebra) fall within the park.It is home to many different kinds of animals. Its biodiversity includes the Iberian wolf roe deer wild boar Iberian lynx common genet red fox and European otter.What a disaster it would be if there were a forest fire!Today I am going to analyze the Forest Fire Predictors In Montesinho Natural Park. The forest fire data concerns burned areas of the forests in Montesinho Natural park due to forest fires. It was collected from January 2000 to December 2003 . It contains 517 instances13 variables (1 dependent variable 4 discrete attributes and 8 continuous attributes). These are the variables:How large can a forest fire be? We did a summary on the burned down area data and categorized the area as 'small' if the area is under first quantile as 'median' if the area is between first and third quantile as 'large' if it if above the third quantile. So the summary is the following:summary(mydata_new$area)Min. 1st Qu.  MedianMean 3rd Qu.Max.0.092.146.37   24.60   15.42 1091.00Categorize area by the above information:(02.14)  size of area  ‘small’(2.1415.42)  size of area  ‘median’(15.421091.00) size of area  ‘large’We did a boxplot of each category and found out that there are some outliers. After taking out the outliers we zoom in and do a box plot for category median and small.Second we come to the question: which variables are the most significant? In order to achieve this goal I did some statistical analysis. It turns out that thefit  glm(log(area+1)~FFMC+DMC+DC+ISI+temp+RH+wind+raindatamydata_newfamilyGamma())> summary(fit)Call:glm(formula  log(area + 1) ~ FFMC + DMC + DC + ISI + temp +RH + wind + rain family  Gamma() data  mydata_new)Deviance Residuals:Min 1Q Median 3Q Max2.11417 0.52704 0.08414 0.28686 1.39097Coefficients:Estimate Std. Error t value Pr(>|t|)(Intercept) 5.405e01 6.776e01 0.798 0.426FFMC 2.129e03 7.666e03 0.278 0.781DMC 6.427e04 3.960e04 1.623 0.106DC 8.486e05 1.036e04 0.819 0.413ISI 1.043e02 6.706e03 1.556 0.121temp 7.194e04 4.458e03 0.161 0.872RH 1.994e03 1.420e03 1.405 0.161wind 1.043e02 1.004e02 1.039 0.300rain 1.099e02 4.172e02 0.263 0.792Based on the above analysis DMC ISIRH are the three most significant variables having critical values of 0.1060.121 and 0.161 respectively.RH:What can we suggest to tourists and to the fire department? Summer and Fall are the seasons when there are the most tourists thus we suggest tourists be more careful when using fire (camping BBQ etc).The fire department should look closer at measurements of RH ISI DMC variables and prepare accordingly.One of the drawbacks of this data set is that it does not record human activity which I believe does have a huge impact on the occurrence of forest fires.What times do forest fires happen the most?Indeed fall and summer are the most dangerous times due to high temperature and low relative humidity. Most importantly people usually go camping and hiking in this two seasons which brings potential danger.Next steps:I will keep on improving this project. Thank you very much for reading this post. Please feel free to give any advice!,NA,Analysis Of Forest Fire Predictors In Montesinho Natural Park
https://nycdatascience.com/blog/r/p2p-loan-data-analysis-using-lending-club-data/,58,LendingClub Corp LC is the first and largest online PeertoPeer (“P2P”) platform to facilitate lending and borrowing of unsecured loans ranging from $1000 to $35000. Aiming at providing lower cost transaction fees than other financial intermediaries LendingClub hit the highest IPO in the tech sector in 2014.The dataset covers an extensive amount of information on the borrower's side that was originally available to lenders when they made investment choices.  By further segmenting the loan dataset into finished cases and current outstanding loans this project breaks down the composition of the default cases and examines the correlation among indicators. In the end the goal is to provide investors and borrowers as well as LendingClub additional insights regarding investment opportunities and contingent loan collection advice. (Please note that for the purpose of the visualization effects and simplicity of diagrams this project recoded some of the items with little or no observations.)We can almost always regard interest rates charged upon loan insurance  as a  form of cost that borrowers have to incur and the number of approved cases as an indicator of demand. By rough eye balling the two time series plot of average interest rate and number of approved loans over time corresponds quite closely with each other.  Exceptions are the plummet of interest rates in late 2007 thanks to VC fund injection in the figure above and fluctuations for the number of Approved Cases around 2015 in the figure below (because of the managerial scandals)..Therefore it comes as no surprise that a scatter plot of interest rates and number of approved cases for the time period presents a positive relationship as all else being equal increasing demand drives up the prices.This section briefly discusses two of the indicators as an example of the richness of the dataset: Home Ownership Types and the borrower's rating grade.As can be inferred from Figure 4 the stack of counts under the account 'Fully Paid' is much higher than the ones under 'Default'. Thus fortunately for the LendingClub investors most of them were able to receive their funds with preallocated interest rate. We can also infer from the histogram that there are relatively more applications with mortgage and rental places than those who own their own place.As can be seen from the graph above there is no relationship between the type of Home Ownership and default rate. (However a closer examination of the ratio of default by types of homeownership the probability of default for the past observations are almost identical.). What this means is that there is an equal chance for applicants with different housing types to default.  Rating grade on the other hand has a more direct relationship to default.  The probability of default increases stepwise as we move down the rating grade of borrowers.Since interest rates are calculated based on the profile of an applicant interest rate plots are good indications of the quality of the application pool.  As can be seen above average observed interest rates differ by month year and geography. The lowest average interest rate occurred in July and November and highest occurred in June. Applicants from Idaho and Iowa and Maine experienced relatively much lower rates on average than the ones from Indiana and Tennessee. Interestingly the shade of color for average default rate by state reflects pretty much the opposite of the one for interest rate. And by plotting them together in a scatter plot with LM curve there is a clear positive relation quite comparable to the relationship of increasing risk premium to compensate risk.Last but not the least to demonstrate the predictive power of the dataset this section presents an application of logistic regression to estimate the expected loss using the segmented data on loans whose status are listed as 'Current'.The expected loss is defined by the following equation:where the expected loss for state i is the summation of each probability of default times the  payment gap defined as the difference between total amount of the loan and the amount already paid at a specific point in time.The probability of default is obtained by matrix transformation based on the parameters estimated from a training set with variables as annual income funded amount home ownership borrower's grade and the amount of the installment.  The logit probability cut off is set at 0.7 for visualization effects. The results based on the model assumptionsshow that the states of California Texas New York and Florida are the ones with heaviest risk of large losses whereas the midwest states present a much more optimistic loan payment expectation.The project uses visualization to analyze LendingClub’s loan applicants and extends to an application of logit regression for future loss estimation. I find that the trait of applicants usually exhibit quite different default probabilities especially the probability of default for rating grades goes up stepwise with lower ratings. In addition  average interest rates differs quite a lot across states and time and serve as a good indicator of the application pool of the borrowers. Lastly the expected loss for the outstanding loans at time being is relatively much higher in California Texas New York and Florida that more resources should be allotted to  loan recollection and screening for new applications in these states.,NA,Loan Data Analysis and Visualization using Lending Club Data
https://nycdatascience.com/blog/student-works/impact-hba1c-test-diabetes-treatment-1999-2008-2/,58,Diabetes mellitus is a chronic disorder associated with disturbances in carbohydrate fat and protein metabolism and characterized by hyperglycemia. It is one of the most prevalent diseases affecting approximately 24 million individuals in the United States.The motivation of this study is to use data science to help focus treatment development on more effective forms of treatment. In this work I use the Health Facts database (Cerner Corporation Kansas City MO) a national data warehouse that collects comprehensive clinical records across hospitals throughout the United States. [1]  The data is an extract representing 10 years (1999–2008) of clinical care at 130 hospitals and integrated delivery networks throughout the United States. The final data includes 69051 patients’ distinct visits with 50 variables.  The plot below shows that patient numbers dramatically increase with age and reaches a peak in the range of 8090 years old. Within the diabetic population the highest count is Caucasian then African American Hispanic and Asian respectively. Because the data is collected in the USA the result strongly depends on the specific demographics of this country.  In our dataset each observation corresponds to a unique patient diagnosed with diabetes. However not every patient had diabetes as a primary diagnosis.  In many cases other diseases were more dominant in terms of symptoms and so even people with diabetes might have a different primary diagnosis.  The plot shows the breakdown by primary diagnosis of patients who also have diabetes.  Circulatory disease ranks first  followed by respiratory disease neoplasms and digestive conditions. Diabetes as a primary diagnosis ranks 5th.  Since different diseases receive different medications I categorized the data by primary diagnosis.A subset of the original 69051 diabetics was chosen for further study.  The selection was based on including only those patients whose initial diagnosis was performed using the HBA1c test as this is test is unique in being able to identify how well the disease is being controlled.  12000 subjects all diagnosed with HBA1c remained in this study for further investigation.These 12000 subjects were further broken out into three groups: those whose diabetes is  under control (“normal”) and two groups whose diabetes is not under control.  In one instance (“high without change”) the doctor did not change medication despite the high occurrence of diabetic symptoms and in the third group (“high with change”) the doctor changed medication in response to the diabetic symptoms.The objective of this study is to understand the relation of HbA1c test result and the medication numbers.The distinct medication number with HbA1C test  is analyzed in box plot. With primary diagnosis of diabetes the median medication numbers are lower  than that of circulatory by HbA1c test result respectively. Among the top ranked primary diagnosis such as circulatory respiratory neoplasms and digestive patients with high HbA1C test result and medical changed received more distinct medications compared with those with normal test result. Oppositely those with high test result but no medical change received less distinct medications.  This information combined with readmitted ratio back to hospital proves the treatment results according to HbA1C test.4One of the features to demonstrate an effective treatment is the readmission rate of patients back to the hospital within 30 days. This bar plots show the relation of HbA1c test and readmitted ratio by primary diagnoses.Circulatory patients overall have higher readmission rates among all the patients. The diabetic patients with high test results show reduced readmitted rates only after being treated.  The respiratory injury and musculoskeletal patients with normal HbA1c test result show a relatively lower possibility of going back to the hospital compared those with high test result. We assume the patients with high test results are more serious than those with normal results. For the patients with primary diagnosis of diabetes it appears that treatment reduces the rate at which they return to the hospital.  It is the opposite for  circulatory respiratory digestive diabetes musculoskeletal and genitourinary diagnoses.  In these cases patients appear to do better if there is no change in their medication.  For neoplasms and injury patients with high test result the changed medication reflects a lower readmitted ratio than the previous medication. The readmitted ratio by medical specialty is plotted in the bar chart.  Patients with hematology/oncology oncology  and vascular surgery ranked among the highest readmitted rates.  This suggests that these diabetic patients are very likely to return to the hospital in short time. On the bottom of the chart the patients with obstetrics/gynecology and otolaryngology issues show very low readmission rates.This study looked at diabetic patients from 1999 to 2008. First we found that the diabetic population is increasing with age.  The rates vary for different ethnic populations from Caucasian (the highest) to  African American Hispanic and then Asian respectively. Second we found that many patients with diabetes have other primary diagnoses such as circulatory respiratory neoplasms digestive  injury musculoskeletal and genitourinary in sequence by population. Third compared with normal test results when HbA1c high test with medication change the number of medications increased. However for those high test result without medication change the number of medications is decreased. Fourth we looked at the HbA1c test impact readmitted ratio. We found strong evidence of when there is a need to change medication based on the diagnosis.  Fifth the diabetic patients  with different primary diagnoses show different readmission rates.I believe that for the data from 1999 to 2008 people realize how important of HbA1c test is for the diabetic patient’s treatment. Further data analysis for diabetic patients from 2008 to present with large amount of HbA1c test is ongoing.References:,NA,Impact of HbA1c test on diabetes treatment from 1999 to 2008
https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-case-studies/,59,With the 42 variables we currently have we were able to reduce the dimension down to the 15 synthetic variables.Next step is to reconstruct the given variables in terms of newly created Principle Components and train logistic regression on the newly constructed variable dimensions.  Using 5 folds crossvalidation and repeated 5 times on the sample data.Note that the metric to optimize is accuracy in this model.  We didn't tune the model in terms of AMS. The AMS submission on the final test data is about 1.61.  Takeaways from this mode:Third model we trained was gradient boosting model with 500 tree  learning rate of 0.1 inter depth of 10.  We performed a 2 fold crossvalidation twice on the entire training data.  After doing some research we make a cutoff on the probability prediction and call the upper 14% of events as signal.ptimize this threshold to the AMS. used a testing grid. After running several tests around the best cutoff is the top 14% (0.86).Neural networks have been widely applied to different areas of industries including stock market prediction credit rating fraud detection property appraisal and medical diagnosis. For the Higgs boson data where the actual Higgs boson events are much rarer than the other background events. This shares the same character as the data shown in fraud detection where the multilayer neural networks were successfully applied. With the limited computational resources instead we only consider one layer of neurons and use neural number as the tuning variables to learn about our data. For the prediction accuracy we use 20 principle components from the scree plot of PCA analysis. We split our training data into 80 percent for machine learning and 20 percent for test set validation. We observe the single layer possess 80 percent training prediction precision almost comparable to the result from test set without overfitting. The prediction accuracies are almost unchanged when the number of neurons are above 4. For the AMS measure the test set prediction (around 1.2) is much worse than the training set result 2.3.To dramatically improving the learning an adaptive boosting may be applied to the single layer neural network to boost the weight of the importance for the rare events. The outcome can be beneficial when one face with limited computational resources.Why we care about Higgs bosons? Bosonic particles in nature such as photons and phonons are all massless. The interesting aspect of bosons are due to the exchange symmetry which leads to the enhancement of probability for observing another bosonic particle when a boson particle exists a priori. The Higgs bosons are special due to its bosonic nature with the mass originated from Higgs mechanism from the standard model. The Higgs mechanism provides an explanation on the origin for fundamental particles with mass.,NA,Higgs Boson Kaggle Case Studies
https://nycdatascience.com/blog/student-works/get-got-d/,59,Education is a key factor for achieving longterm economic growth. Determinants of students’ performance have been the subject of ongoing debate among educators academics and policy makers.This study focuses on secondary education in Portugal.  During the last decades the Portuguese education level has improved. In the secondary schools the core classes of Mathematics and Portuguese (the native language) is the most important since they provide fundamental knowledge for the success in the remaining school subjects (e.g. physics or history). The data of student performance in Mathematics and Portuguese holds valuable information and can be used to improve decision making by parents and schools and to optimize student success. Modeling student performance is an important tool for both educators parents and students.  It can help us better understand this phenomenon and ultimately improve it.This data set provides information about student achievement in two Portuguese secondary schools. The data attributes include student grades demographic social and school related features.  It was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). The target is to investigate the contributing factors associated with G3 (the final year grade).The raw data contains 382 observations and 53 variables. The target variables are G3.x (the final year grade of Math) and G3.y (the final year grade in Portuguese). The contributing factors will be presented in 3 categories: schoolrelated (i.e. school extra education support) studentrelated (i.e. past course performance age study time desire to pursue higher education) and familyrelated (i.e. parents' status quality of family relationship parents' education and job). I analyzed most variables and listed the top contributing factors in the following illustration.The boxplots of final year grade distribution show the difference in student performance by school. In this graph we can see that for the GP school the median final year grade for Math is C and the median final year grade for Portuguese is B while for the GP school the median final year grade for Math is D and the median final year grade for Portuguese is C. We can conclude that the GP school has better student performance. In the following analysis I will separate the plots based on school.The next question we want to assess is whether G1 and G2 will significantly influence G3. Let's take math performance for example.There are a couple interesting facts that show up in these graphs. First we notice the data trend can be categorized into two relationships: the cluster with 0 grade (students who dropped a course) and a strong correlation between G3 and  G2 G1 (students who did not drop a course ). So the analysis is divided to two parts based on the trend.The figure shows a linear relationship between the current grade and the past grade which means the better you did in the first and second grade the higher final year grade you would get.Upon further inspection of the data it becomes obvious that the group with 0 grade most likely belongs to students who dropped the course. There are a couple of interesting facts that show up on the previous graph. First it has G1 and/or G2 grades but final grades of 0. Second there are no G1s of 0 but there are G2s with 0 value.The graph shows the fact that past class failure plays a role in current student performance and we can summarize that successful students tend to have a history of success.From this graph we can conclude that the age of the students also plays a factor in the final year grade. The older the student is the lower the final year grade he is likely to achieve.In terms of study motivation the student with a desire to pursue higher education has a higher probability of achieving success.The graph shows an association between study time and the final year grade; the  successful students tend to spend more time on coursework.It is hard to conclude a relationship between number of school absences and final year grade. To get a better understanding of the plot I grouped the number of school absences into 4 categories: 09 1019 2029 30+.The boxplots at the left of the graph indicate that students with working mothers tend to have better course performance than those with homestaying mothers. Also upon further investigation of mother job types the boxplots at the right of the graph  demonstrate that students whose mothers have a higher education level are most likely to achieve success in their courses. Furthermore plotting mother's job with education allows us to understand how the job distribution varies among education levels. Here we see the working mother has a greater portion of higher education level and especially the mother who works as a teacher has the most advanced  education degree on average. In conclusion the student who has a working and welleducated mother tends to be more successful.From the graph we can see that the first consideration is the quality of school course and the second is whether the school is close to home.I have addressed the data visualization of secondary student grades of two core classes (Mathematics and Portuguese) by using past school grades (first and second periods) demographic and school related data. In conclusion the student achievement is highly affected by previous performances. Also there are other relevant factors that contribute tostudent performance such as: school related demographic (e.g. student’s age study time desire to pursue higher education parent’s job and education). The conclusion is summarized:,NA,Why did she get an A while I got a D?
https://nycdatascience.com/blog/student-works/7-sins-in-nyc/,59,When Vito Corleone the head of the Corleone crime family in the movie “The Godfather” was shot on the street of New York by hitmen I was shocked.I was shocked not just because I was so immersed in the movie but also due to one sentence echoing in my mind: “no one is an island”.Uncertainty is everywhere even for the mafia boss not to mention millions of ordinary New Yorkers.Safety is one of the most fundamental needs for people. As one of the most populous urban agglomerations in the world New York City is heaven for many but perhaps hell for few especially those who were unfortunately affected by the seven “sins”:Each week NYPD publishes  containing detailed weekly statistics of crime complaints on 7 felonies. For example for the one report during 7/4 to 7/10 of 2016 there were 1888 total crime complaints in NYC: 6 murder 35 rape 304 robbery 444 felony assault 202 burglary 765 grand larceny 132 grand larceny of vehicles.1888 is not a small number although the total complaints decreased 5.51% as of 2015. By simple math we know that there were on average 11.24 felony incidents per hour or 1 felony incident per 6 minutes in NYC.This project investigates 7 sins a.k.a felonies which occurred in NYC in the past 10 years (20062015). It focus on answering the following simple yet important questions:The  dataset:According to the NYPD Incident Level Data Footnotes:The first point indicates that the number of actual incidents is larger than that in the dataset. Since we know nothing about which types of offenses are typically associated together in incidents of multiple offenses we can make no assumptions. The second point affects the accuracy of incident locations.  Nevertheless at the scale of borough or city level the inaccuracy in longitude and latitude will not have a major impact on the overall distribution of incidents. Quick exploration using R revealed that although the years in the dataset span from 1919 to 2015 over 95% of all incidents occurred after 2005. I thus mainly focus on the year from 2006 to 2015. This 10year period covers 1.1 million incidents. First let us take a look at the overall trend of 7 felonies in NYC in the last 10 years.Grand larceny is the most frequent offense of all 7 felonies.  The number of incidents is almost twice that of the second most frequent one.Three felonies are declining: robbery burglary and auto theft. I cannot help but link this to the widely used technology in camera surveillance. Wrongdoers know their big faces will instantly show up in NYPD screens once they risk themselves.Murder and rape have stayed at the same level across 10 years.The number of felony assaults is on a slightly increasing trend.To sum up it is safe to conclude that NYC is getting safer. NYC’s seasons are defined as follows: Late winter and early spring tend to have the smallest number of incidents for almost all 7 felonies with February having a particularly low felony incidence.  These can be considered as the safest seasons. This is understandable. During those months it can become very chilly windy and snowy. Who would want to go out in such weather?Summer and early fall tend to have the largest number of incidents for almost all 7 felonies. Summer months in NYC are usually hot and humid and temperatures may remain high at night.  This can make certain people ornery. Friday is the least safe day in the week. This insight is easily perceived from the histograms. On Friday burglary grand larceny larceny of motor vehicle and robbery occur more frequently than on other days. Maybe people tend to feel very relaxed on Friday after one week’s work perhaps therefore not being as vigilant as they otherwise might be. This could give wrongdoers great opportunities to break into houses steal property such as cars or commit robberies on the streets. As for the weekend the number of incidents for burglary grand larceny auto theft and robbery declines. If you think that people are at home playing with their kids enjoying family time watching favorite TV shows or preparing for their next week’s work then maybe there is less of an opportunity for wrongdoers to sneak into their homes.  On the other hand weekend is less safe in terms of felony assault rape and murder. Home violences bad family relationships and unkindly words may all related to an unhappy or disastrous weekend. So maybe family time is not equally great for everyone!Knowing which hours are safe or unsafe for certain offenses is vital for New Yorkers since hour is a “tangible” and controllable unit. One can choose to be at one place at a certain hour or not.It strikes me that even with just simple density and histogram graphs without any complex machine learning models we can still distill many insights from history.It is easy to see on a clock when each of the deadly sins peak in terms of frequency.  You can almost map the life of a felon and only  few hours in a day are really safe e.g. 5am is a safe time to be alive.We should also keep an eye on where felonies occur.  From the histogram above it can be seen that Manhattan has the most number of grand larcenies. This is somehow not surprising. Perhaps Wall Street and most financial companies are located there and wrongdoers can get their hands dirty easily. Brooklyn is the second runner and Staten island has the least. Despite Manhattan being the winner when it comes to grand larceny Brooklyn in fact appears to be the most dangerous borough.  It ranks first on the of incidents for 6 out of 7 felonies.  In contrast Staten Island ranks last.The density map below depicts a visualization of crime in all 5 boroughs. It turns out that each borough has its own distinct pattern of hot locations.New Yorkers may rely solely on NYPD to solve those problems. But if each New Yorker is aware of the time/space patterns identified in this report s/he can take proper action and things may be different.  NYC is getting safer and safer. But we should not be satisfied with this. .  I believe more work can be done including but not limited to:,NA,7 Sins in NYC
https://nycdatascience.com/blog/student-works/higgs-boson-machine-learning-challenge-3/,60,The DataMissingnessElasticnet RegressionMachine Learning AlgorithmFull Model vs Reduced ModelPrediction ComparisonConclusionImprovements,NA,Higgs Boson Machine Learning Challenge
https://nycdatascience.com/blog/student-works/11652/,60,With the 42 variables we currently have we were able to reduce the dimension down to the 15 synthetic variables.Next step is to reconstruct the given variables in terms of newly created Principle Components and train logistic regression on the newly constructed variable dimensions.  Using 5 folds crossvalidation and repeated 5 times on the sample data.Note that the metric to optimize is accuracy in this model.  We didn't tune the model in terms of AMS. The AMS submission on the final test data is about 1.61.  Takeaways from this mode:Third model we trained was gradient boosting model with 500 tree  learning rate of 0.1 inter depth of 10.  We performed a 2 fold crossvalidation twice on the entire training data.  After doing some research we make a cutoff on the probability prediction and call the upper 14% of events as signal.ptimize this threshold to the AMS. used a testing grid. After running several tests around the best cutoff is the top 14% (0.86).Neural networks have been widely applied to different areas of industries including stock market prediction credit rating fraud detection property appraisal and medical diagnosis. For the Higgs boson data where the actual Higgs boson events are much rarer than the other background events. This shares the same character as the data shown in fraud detection where the multilayer neural networks were successfully applied. With the limited computational resources instead we only consider one layer of neurons and use neural number as the tuning variables to learn about our data. For the prediction accuracy we use 20 principle components from the scree plot of PCA analysis. We split our training data into 80 percent for machine learning and 20 percent for test set validation. We observe the single layer possess 80 percent training prediction precision almost comparable to the result from test set without overfitting. The prediction accuracies are almost unchanged when the number of neurons are above 4. For the AMS measure the test set prediction (around 1.2) is much worse than the training set result 2.3.To dramatically improving the learning an adaptive boosting may be applied to the single layer neural network to boost the weight of the importance for the rare events. The outcome can be beneficial when one face with limited computational resources.Why we care about Higgs bosons? Bosonic particles in nature such as photons and phonons are all massless. The interesting aspect of bosons are due to the exchange symmetry which leads to the enhancement of probability for observing another bosonic particle when a boson particle exists a priori. The Higgs bosons are special due to its bosonic nature with the mass originated from Higgs mechanism from the standard model. The Higgs mechanism provides an explanation on the origin for fundamental particles with mass.,NA,Higgs Boson Kaggle Case Study
https://nycdatascience.com/blog/student-works/predicting-demand-historical-sales-data-grupo-bimbo-kaggle-competition/,60,regular gradient boosting. Xgboost can do cross validation regression classification and ranking. Since it can also give feature importance it is a very good model to use in competitions such as Kaggle.,NA,Predicting demand from historical sales data- Grupo Bimbo Kaggle Competition
https://nycdatascience.com/blog/student-works/predicting-new-users-first-travel-destination-airbnb-capstone-project/,60,Our ultimate project goal was to complete the task assigned by the  That task was to predict the country of a new user’s first destination out of 12 possible outcomes for the destination country: United States France Canada Great Britain Spain Italy Portugal Netherlands Germany Australia other or no destination found (NDF).  you can find a Shiny app that gives basic information on the possible destination countries. The code for this project can be found .The competition allowed up to 5 predictions for each new user in the test dataset. For example: United States France Australia Italy and Portugal. The metric used to grade these predictions was  (NDCG) which measures the performance of a recommendation system based on the relevance of the recommended entries. It varies from 0.0 to 1.0 with 1.0 representing the ideal ranking of the entities. This metric is commonly used in information retrieval and to evaluate the performance of web search engines.The Airbnb Kaggle dataset consisted of: With respect to our workflow we created the engineered features using Random Forests and Gradient Boosting Machines. We then decided to train and run predictive models using XGBoost and AdaBoost. We ran XGBoost models on the unstacked data. We ran XGBoost models and AdaBoost models on the stacked data.UnstackedIn both the unstacked and stacked models both missing age/gender are some of the most important features to prediction. This is consistent with our exploratory data analysis which appears to show associations between both (i) missing age/gender and a country of first destination and (ii) missing age/gender and NDF (the latter of which we assume contributed more to the models). Counts and sum_sec_elapsed were also found to be important and are from the users' sessions data.As a team we:Steps to improve our predictions:,NA,Predicting a New User's First Travel Destination on AirBnB (Capstone Project)
https://nycdatascience.com/blog/student-works/forecasting-the-higgs-boson-signal/,61,"The Higgs Boson is a type of unstable subatomic particle that breaks down very quickly.  Scientist studies the decay of the collision and works backward. To assist scientist in differentiating the background noise from the signal we offer some machine learning algorithms to better predict the Higgs Boson.Like all data science projects we began with some exploratory analysis of the variables. We first used a correlation plot to inspect the different relationships going on between variables. As indicated by the dark blue and dark red points there seems to be a high correlation among many of the variables. We noticed the PRI_jet variables for example have a lot of blue dots in relation to the variables DER_deltaeta_jet_jet DER_mass_jet_jet and DER_prodeta_jet_jet. This is likely since according to the documentation for the challenge  those DER variables are derived quantities computed from the PRI or primitive quantities measured from the particles.Next we wanted to zoom in on the correlation plot and see some scatterplots of select variables.Here we looked at scatterplots between PRI_jet_leading_pt with the three variables PRI_jet_leading_eta PRI_jet_leading_phi and PRI_jet_all_pt. The orange circles represent events that were classified as signal while the blue circles were those classified as background. We noticed some variables seemed to have a linear relationship such as PRI_jet_leading_pt and PRI_jet_all_pt while others did not have any obvious form.Looking at scatter plots amongst select DER variables we saw a linear relationship to still be present. Between DER_deltaeta_jet_jet and DER_prodeta_jet_jet for example there was a negative linear relationship.Finally we had a look at some scatter plots between a DER variable and a few PRI variables. We found it interesting to see the plot of a DER variable and its related PRI variable from which it was calculated from. For instance the variable DER_deltaeta_jet_jet and PRI_jet_subleading_eta had a somewhat vshape as it is derived from the absolute of the difference between that primitive variable and another. 
Classification tree models are great for descriptive purposes.  They produce relatively easy regions to trace the process of the model. Although they tend to have a lower prediction accuracy.
Our first basic tree without pruning produced an accuracy of 72% with 3
For a more robust model with greater prediction we attempted a Random Forest.  A Random Forest is the average of a collection of trees resulting in an ensemble with greater predictions. Although unlike a single tree you lose descriptive abilities.  We ran the model with all the variables on the righthand side is the variable importance plot returning an AMS score of 1.781. Impressive just changing the model type the AMS increased by over 0.5 points.We then decided to try a model with
only the top 14 variables along with two columns created to focus on the missing values for DER_mass_transverse_met_lep and DER_deltaeta_jet_jet.  We found these columns to individually reduced the errors at most and together worked great. Resulting in an AMS
score of 2.795 with an accuracy of 83.5Finally turning to a more predictivebased model we attempted using the support vector machine algorithm. Using the scatterplots earlier and seeing as how there was much overlap between signal and background we chose to use a radial kernel as the problem did not look linearly separable. The pros and cons of using this algorithm are as follows:ProsConsDue to the model being computationally costly with the size of the dataset we considered tweaking different parameters in order to get it running without consuming too much time. We chose to train it on 1% of the data and used 5fold crossvalidation to find the best estimates for cost and gamma. Next we used the results from the random forest model and reduced the data set to the 14 most significant variables. Finally with trial and error we found the best threshold for the model to be 0.7. Using all these led to an accuracy of 0.8056 and an AMS score of 2.72036. Considering the number of models we ran for the Higgs boson challenge we recommend the random forest model since it has the greatest balance of predictability and interpretability. The random forest model gave us the highest AMS score of 2.795 and was able to be trained on all the data. We noted that in terms of AMS score the support vector machine model was second however we choose to recommend the random forest model as the latter is less complex to train has less parameters to tweak and is not computationally inefficient as the former. In addition should the main purpose be for description we recommend the basic classification tree with some pruning.",NA,Forecasting the Higgs Boson Signal
https://nycdatascience.com/blog/student-works/higgs-boson-machine-learning-challenge-2/,61,The DataMissingnessElasticnet RegressionMachine Learning AlgorithmFull Model vs Reduced ModelPrediction ComparisonConclusionImprovements,NA,Higgs Boson Machine Learning Challenge
https://nycdatascience.com/blog/student-works/sneak-peek-panama-papers-offshore-companies-data-leak/,62, company is not liable to taxation in its country of incorporation it will be taxed in the country where it carries on its business.  The two main purpose of having the offshore companies are first to have tax benefit and the other is to hide the final beneficial owners from this structure.  Again the purpose of this App is to unveil this offshore world.  There are many complicated structures within itself and governed by different jurisdiction laws. ,NA,Sneak Peek of Panama Papers (Offshore Companies) Data Leak
https://nycdatascience.com/blog/student-works/new-york-city-street-trees-in-shiny/,63,     After deploying the Shiny app it became clear that missing values for species was due to the tree condition (#6 and 7) in which they were categorized as Stumps or Empty Tree Pits.  This makes sense as it is difficult to identify a tree if it is missing or just a stump. ,NA,New York City Street Trees in Shiny
https://nycdatascience.com/blog/student-works/a-comparison-of-supervised-learning-algorithm/,66,Which supervised learning algorithm is the best? For people who just start their machine learning journey this question always comes to their mind.,NA,A Comparison of Supervised Learning Algorithm
https://nycdatascience.com/blog/student-works/visualizing-artificial-satellite-data/,67,For my data visualization project I looked at artificial satellite data from the . It concerns the 1305 operational satellites orbiting Earth as of the 31st of August 2015. While there have been fascinating visualizations of space debris done by Google Earth and others the UCS data contain satellite origin locations and ownership information useful for analyzing the overall industry.The data is collected from multiple sources. First the Convention on Registration of Objects Launched into Outer Space requires that countries report satellite information to the United Nations Office of Outer Space Affairs. Even so some countries neglect to report launches of covert satellites. However other countries act as watchdogs reporting when they detect a foreign satellite launch. Amateur astronomers also fulfill this watchdog role.Next I examined the launching state of these satellites. While 25% of satellite origin countries are labeled “NR” (Not Reported) many of these are new satellites that are not yet labeled. These values are therefore missing at random (MAR) with respect to time and do not severely undermine the analysis. Using  and  I grouped the most active satellite origin countries and plot them on the world map:Source: Source: The majority of geosynchronous orbiting satellites are ± 1 km of the equilibrium distance of 35786 km. Even though the UCS dataset claims not to give enough information to find a single satellite’s exact location it does contain the longitudes of geosynchronous satellites. Since geosynchronous satellites orbit above the equator at the same rate as the Earth’s rotation I was able to plot the locations of geosynchronous satellites based on their longitudes. Positive longitudes correspond to “degrees east” and negative longitudes correspond to “degrees west”.,NA,Space Oddities – Where do Satellites Come From?
https://nycdatascience.com/blog/student-works/using-python-and-k-means-to-find-the-colors-in-movie-posters/,68,To answer this question we need analysis movie posters of different movies. First of all we need to build a training dataset of movie posters. So I used the  Image search engine for this. And the part of images I got looks like this:To extract the color of images we need to represent them in an easier and numerical way: Converting one image to a matrix of RGB pixels. For example for a 200x200 px image we convert it to a series of 40000 RGB pixels.To maintain the size of the dataset I resized the images down to 200x200.Then I used kmeans clustering to do image segmentation separate the pixels into different clusters based on their colors. Here is an example of image segmentation and compression from Bishop’s book I used python to do the kmeans analysis the code is shown as below:I tried k3 k5 and k10 since lots of the movie posters used black letters and frames k3 and k5 didn’t capture the main colors of posters. I chose k10 to do kmeans to all these 112 images. Finally I got 1120 colors of 112 movie postersPart of the result shown as blow:,NA,Using web scraper and K-means to find the colors in movie posters
https://nycdatascience.com/blog/student-works/capstone/predicting-steps-wearable-activity-tracking-device/,68,Nowadays there are wide selections and different price options for wearable activity tracking devices or “wearables”. Most of these tracking devices harness a 3axis accelerometer to understand the user’s motions. By analyzing acceleration data the trackers provide information about frequency duration intensity and patterns of movement to determine a multitude of health metrics such as steps taken distance traveled calories burned and sleep quality. Another advantage of owning this device for data and fitness enthusiasts is the ability to log their food activities and weight over time with the possibility to set daily and weekly goals for themselves for steps calories burned and distance walked. One of the most popular wearable tracking devices is Fitbit. Like most wearable activity tracking devices  there is the option to synchronize your data to your user account via Fitbit Connect. In this work we consider Steps as our health and fitness goal that we wish to predict.Fitbit users can extract their data by logging into their Fitbit Connect account as shown in Fig.1 below where data for one year of activities and sleep could be extracted in CSV format.Fig. 1 : Data Extraction for Fitbit user’s webpageAfter gathering a one year activities and sleep data an exploratory data analysis is achieved using Python in order to determine the assess the relationship or determine the patterns between different type of measurements from May 8 2015 to May 7 2016. For each day the extracted data has the following features (columns of our datasets):First we take a look to the distribution of our raw data as shown in Fig.2 below.Based on the raw data from Fig.2 we can see the following:In this dataset we distinguish 3 types of health or fitness goals: Steps Calories burned Hours of Sleep and Sleep efficiency. To assess the sleep quality for a specific day we define Sleep efficiency as the percentage of Minutes of Sleep over Length Of Rest In Minutes.In the following section these goals are represented over days of the week months and grouped by work days (from Monday to Friday) and weekends (Saturday and Sunday).Most of the global recommendations on physical activity for health are targeted toward meeting certain daily goals. Fig.3 below shows the daily achieved goals.Although there are no global recommendations on physical activity for health it is interesting to observe the evolution of goals parameters by month. We can see that October was the least active month with few hours of sleep despite having the highest sleep efficiency. From Fig.4 we can also observe that the Fitbit user is more active during Spring and Summer months with higher hours of sleep but low sleep efficiency.A new feature is created to group days by workdays (from Monday to Friday) and Weekends (Saturday and Sunday). Fig.5 shows the achieved goals parameters. During the workdays the Fitbit users improve all the health goals comparing to weekends.In the following section the relationship between several features in the dataset is observed. Along with the feature that distinguishes between workdays and weekends other features are added to the datasets such as: Yesterday sleep hours and Yesterday's sleep efficiency in order to assess the effects of a previous night sleep on the goals. The following features: Days Months and Weekdays have been encoded.In this work we consider Steps as our health and fitness goal that we wish to predict.First we look at the trend shared by predictors i.e the features that will be used to predict Steps. We compute the correlation matrix shown in Fig.6.From Fig.6 we can observe some strong correlation between some sleep predictors.  Distance is strongly correlated to Floors and both are intercorrelated to Minutes very active.Fig.8 shows the relationship between Steps and other predictors. We can see that there is a strong linear relationship between Steps and the predictors. Distance according to Fitbit is calculated by multiplying your walking steps by your walking stride length.In order to predict the Steps we split our data into train (75% of the dataset) and test (25% of the dataset). The train data has the following predictors:We fit different machine learning algorithms on our train dataset and make corresponding predictions on our test dataset. First we assess variable importance using linear regression and test p values for each predictor. Results are shown in Table.IFrom Table II simple linear regression seems to be well suited for the nature of this prediction problem as it has a lower value of RMSE in this case we have an error of 3437.32 steps. No improvement is observed using regularization such as Lasso or Ridge.Considering the nature of our data we have some multicollinearity and also a strong linear relationship between some features and our response variable Steps. Multicollinearity is important in regression analysis as it may cause unstable estimated coefficients and a loss of the model interpretability as it depends on the data used to train our model. In order to have a constrained model we introduce regularization of the linear regression using Ridge and Lasso methods to predict the Steps based predominantly on the most important features. Regularization in this case shrinks the least important coefficients (Ridge) or can eliminate them completely (Lasso) in order to improve generalization of the model. For this model regularization doesn't improve interpretability or the predictive power of the model.As for the Random Forest Regressor it fails in predicting Steps especially for small values as observed in Fig.9. In the exploratory analysis the mean daily Steps is between 8000 and 12000. The poor results of Random Forest Regressor are due to the nature of the algorithm being a nonlinear algorithm. In our case our data is very linear and thus we need a lot of branches per tree to get a good approximation.In this work the relationship between the measured Fitbit parameters was explored. We choose to predict the number of steps based on multiple features. Despite an apparent multicollinearity simple linear regression seems to give better results predicting the number of steps.,NA,Predicting Steps from Wearable activity tracking device
https://nycdatascience.com/blog/student-works/using-r-shiny-app-to-visualize-gender-pay-gap-in-the-united-states/,68,Use the App below to explore changes of income and gender pay gap over the past 30 years and see how different factors influence the pay gap.,NA,Using R Shiny App to visualize gender pay gap in the United States
https://nycdatascience.com/blog/student-works/interactive-exploration-of-nba-lineup-data/,69,My  explored the relationship between the performance of NBA lineups (5man combinations of players) and the 5 players who made up those lineups. A full breakdown of how I put together this dataset can be found at the link above. However this initial investigation really created more questions than answers.,NA,Interactive Exploration of NBA Lineup Data
https://nycdatascience.com/blog/student-works/walmart-and-random-forest/,70,In the recent Walmart Kaggle competition I used a Random Forest classifier to solve a market basket problem.  A market basket model is built on the idea there exists relationships between items purchased together.  For example a person purchasing a new toothbrush is more likely to also purchase toothpaste than motor oil in the same shopping.  Retailers use these market basket relationships in the design of their stores for ease of use and also to increase sales.  In this specific problem Walmart has broken up their shopping trips into 38 unique 'TripType'.  They give you a training data set with over 600000 rows and a test data set of the same size.  The training data set contained the 'TripType' and it was your job to predict the 'TripType' of the test set.  The 'TripTypes' were numeric dummy variables and contain no explicit information.  Each row in the data sets contained features about one item purchased.  Participants were given a 'Upc' code a 'Weekday' 'DepartmentDescription' 'ScanCount' and 'VisitNumber'.  You are also given a 'FinelineNumber' which was used as internal market basket classifier for an item.  Unlike a Upc number which was unique to an item the 'FinelineNumber' was shared by items commonly purchased together.  Toothpaste and toothbrushes might share a 'FinelineNumber'.  Participants were not allowed to use any external data in this contest.  There was plenty of information to extract from the data already given.  In the Walmart Kaggle forum the person at the top of the leader board made a post stating that the secret to success in this contest would hinge on feature engineering and not so much on complicated ensemble methods.  Based on that advice I leapt directly into generating new features.I looked at the columns in the data set and thought about what information was there but not explicitly stated.  Feature engineering is also an example of where domain knowledge is important.  I thought about shopping trips and what are the things that make trips different.  Trips are different in their composition and quantity of items.  Through some very simple arithmetic operations one would be able to generate other features that improves the granularity of the dataset.  They tell you that any item that has a negative 'Scan Count' is a returned item.  Could that be an important feature?  Because you have the 'Scan Count' and the 'VisitNumber' you are able to calculate both the total number of items purchased and also the percentage of each trip that belongs to a specific item.  Once you have the total number of items in each trip you are also able to generate features to show the percentage of each trip that belongs to a 'FinelineNumber' and a 'DepartmentDescription'.  It also should be noted that there were some missing data in both the training and test set.  I replaced the NaNs with a proxy value not found in the data set.  If you try deleting the rows based on the logic that they comprise such a small percentage of the data set you will run into problems with your test submission Kaggle expects the submission to have certain dimensions.   My code for this feature engineering is shown below.Once you have your new features it's time to run some models.  I chose Random Forest due to it's ability to handle the large amounts of categorical features contained in the data set.  It also avoids the issue of overfitting the data based on the way that the trees are built.  Each tree is built on a random subset of features that reduces the correlation between trees.  There is a beautiful library for Python called Scikitlearn that will have you building random forests in no time.  I split my training set in a 80/20 split saving the 20% as a validation to get an estimate of the test error rate.  I then built my model on the entire training set and then made a prediction with my test set.  I ran Random Forest a number of times as I engineered different features and you can see the results of my Kaggle submissions below.  Kaggle uses logloss as a measure of accuracy it can be thought of as the amount of entropy attached to a prediction.It should be noted that the features that I mentioned above were not the only features that I tried and you can see from my Kaggle results not every feature adds to the predictive power of your model.  Once I had features that I was happy with it was time to tune Random Forest.  While just making submissions based on each new feature that improved my estimate of the test error was a reasonable process for feature engineering it wouldn't work for parameter tuning.  There are way too many possible parameter options to use such an inefficient process.  The best way to tune your parameters is to use a grid search with cross validation.  In this process you create a matrix of all ranges of values for your parameters.  You then run Random Forest on all of these possible parameter values and record the estimated accuracy.  This will give you the best combination of parameter values.  You used cross validation because multiple splits will give you a better estimate of the test error than one split alone.  I used code directly from the Scikitlearn documentation for this process.  This method gave me the optimized parameters that provided my best score.  I'm continuing to work on the Walmart problem but with two advancements.I'm trying to see if XGBoost gives me better results than Random Forest (not yet) and I'm also moving my work on to an AWS instance to improve speed.  If you want to see what I did in its entirety please check out my Github link at the top of the page.,NA,Walmart and Random Forest
https://nycdatascience.com/blog/student-works/project-1-exploratory-visualizations-of-yelp-academic-dataset-draft/,70,The dataset visualizations and result outputs in this post are NOT representative for any types of overall business users reviews in Yelp,NA,Project 1: Exploratory visualizations of Yelp academic dataset
https://nycdatascience.com/blog/student-works/m3-how-to-fund-your-startup/,70,"| All well and good but what remains largely unaddressed is the question of funding. The general process is known  line up your investors hone your pitch and be prepared for rejection. But that first crucial step: how do you find wouldbe investors? There are resources available but it often still feels like a stab in the dark. Perhaps we can do better.This is M3  a tool meant to aid in exactly this process. What does it do?These are firms that have funded startups like yours in the past and will fund similar ones in the future. In short  we're a shortcut for finding the right venture capital firm. How exactly this fingerprint is generated and used will be discussed in more detail below.Lots of infrastructure. This was a challenging app for us to get up and running in just under two weeks  at times we were much closer to being fullstack engineers rather than data scientists!An aside: Shakespeare scholars often discuss issues of authorship and authenticity in Shakespeare's oeuvre. Example: the scene involving the Greek goddess Hecate in MacBeth is often attributed to fellow English playwright Thomas Middleton. There are historical arguments to be made about its role as song and dance interlude but there are also stylistic and syntactic arguments. It just doesn't read quite like the rest of Shakespeare.If you'll permit a slight stretch these scholars are doing what we're doing. Semantic fingerprinting is a technique that maps bodies of text into comparable analyzable ""fingerprints""  a fingerprint being simply a large sparse vector otherwise known as a Sparsely Distributed Representation (SDR). Once you have an SDR for two bodies of text whether they be Shakespeare or company descriptions comparisons become simultaneously insightful and simple.The underlying theory of the semantic fingerprinting technique used here comes courtesy of Cortical.io an AI company led by researcher and author Jeff Hawkins. Cortical.io is seeking to create a new kind of artificial intelligence by taking cues from the most powerful intelligence engine we know of  namely the human brain. In brief Hawkins and researchers at Cortical.io believe that the human neocortex (the largest and most evolutionarilyrecent area of the brain) is underpinned by one universal learning algorithm. The stands in opposition to a more compartmentalized understanding of intelligence in the brain (this area corresponds to language here music here math) but modern research supports the idea. A 2009 article from Scientific American discusses technology allowing   a strong example of the brain’s ability to adapt by employing a common learning algorithm across all senses and experiences.A full explanation of Hawkin’s theory is beyond the scope of this blog post  interested readers are enthusiastically directed towards his excellent 2004 book  which has remained relevant despite a full decade of progress in AI. Two shorter but more complex reads are available on the  and a white paper on .As outlined in the mathematical properties paper above two vectorized corpora are related to each other in a simple way. The core of it is overlapping bits  take the union of two SDRs and the more bits they share the more closely related they are. It’s not quite so simple of course  some common distance metrics employed are:",NA,M3 - How to fund your startup
https://nycdatascience.com/blog/student-works/on-visualizing-hollywood-boxoffice-revenue/,70,My goal was to analyze the accuracy of news headlines relating to Hollywood; including but not limited to the changes in domestic versus overseas BoxOffice revenue and the marketability of different genres overseas. Specifically I focused on articles that had little or no visualizations but drew clear conclusions based on general trends.,NA,On Visualizing Hollywood BoxOffice Revenue
https://nycdatascience.com/blog/student-works/on-the-rise-of-data-science-startups/,71,"Our project is centralized around the development of an open source workbench that is focused on providing data scientists with automated tools for exploratory analysis and model selection.  The full stack design is made in R a statistical programming language. Before getting into the lowlevel details let's take a step back and think about the trending term ""Data Science.”It seems that everywhere you turn these days there’s someone starting a “Data Science” company.  Are you a PhD Dropout from Berkeley? Start a Data Science company. Are you a programmer that knows how to use MongoDB? Start a Data Science company.  Did you study english at Yale? That’s right  Data Science.  The good people cbinsights.com made a chart about Venture Investment in AI over the last 5 years:What gives and why now?There’s been a lot of attention on data science platforms and workbenches that attempt to improve the data scientist’s workflow or allow non datascientists to perform data science through an immersive user interface.  We’re going to show you how to build your very own open source machine learning workbench in R.  Please steal it. Everyone has seen some form of the below chart with Computer Processing power rising exponentially.  While the fast CPU has driven many innovations the inexpensive CPU is not the critical factor in Machine Learning.It’s not just CPU’s that are dropping in price but .  Distributed Machine Learning algorithms depend as much on memory network speed and to a smaller degree hard disk speed as much as they rely on CPU speed.  It’s the aggregation of multiple exponential trends that is democratizing access.  Many people think despite these price drops that tools like AWS are still “too expensive.”  This couldn’t be further from the truth I want to explore the AWS pricing.Here’s an AWS pricing list as of 11/20/15.  The critical factor here is the $0.126 Per Hour Pricing.Assuming that you live in the NorthEastern corridor California or the midwest and assuming that your computer draws at least 1 kWh renting server space from amazon is less expensive than just paying for computer electricity in your home state.  I live in NYC pay my own electricity and I was able to save money by moving my computation demand onto AWS.With AWS you can get a server with up to 244 GB of main memory and up to 40 CPUs; no more limitations by hardware and computation time for your Rbased analyses.  While hardware price reductions are nice we will see that Machine Learning software prices have collapsed even further. Open Source machine learning libraries has been a revolution for the machine learning community.  The once obscure and specialized topics of machine learning and statistical learning can now be leveraged by a much larger demographic globally. Machine learning workbenches have been implemented in academia and industry.  The following is a short list of many popular platforms. ($140000 for the first year) ($51000 per user) ($58500 per user)Dataiku DSS & etc (~$10000 per user per year)You get the idea.The worst part about the SAS/IBM/H2o prices is that much of the software that runs these machine learning libraries is open source to begin with.  These companies have a business model of taking freely available open source tools building a GUI on top of the system and charging tens of thousands of dollars per year for support.Weka’s software design is centralized around java. Dataiku DSS uses primarily Java Python and a kernel design compatible with multiple languages including R.  H2o is Java and R.  IBM’s software is built on SPSS an older programming language that was . Most of these expensive products have proprietary model formats and data cleaning requirements making interoperability and portability of code a near impossibility.  Python libraries for Machine Learning (Free)  Java libraries for Machine Learning (Free) Open Source Machine Learning (Free) Open Source Machine Learning (Free)R Python Spark Hadoop Caret (Free)Model Class Package Caret Function SyntaxldaMASS predict(obj) (no options needed)glm stats predict(obj type  ""response"")gbmgbm predict(obj type  ""response"" n.trees)mademda predict(obj type  ""posterior"")rpart rpart predict(obj type  ""prob"")Weka RWeka predict(obj type  ""probability"")LogitBoost caTools predict(obj type  ""raw"" nIter)Shiny is not the only new tool for computer visualizations but is a fully functional web app development package that can streamline R code directly into an interactive frame without the need to know know javascript or html.  The Shiny package is compatible with many other interfaces including Google Viz Tableau matplotlib bokeh (and a ton of others).  With R and Shiny you can setup a webserver and provide visualization tools to your BI teams in realtime.  Did we mention this tool is free?   We’re broke students and our classroom is at WeWork where we get free coffee and beer.  Shiny and R fit right in.Let’s now delve into the app we made.  As we mentioned before we wanted to make an opensource application in theme with the growth of data science startups.  Using a combination of Shiny caret and other great open source tools we made a fairly workable platform that can perform basic data analysis preprocessing modeling and validation.  We spent only three days developing this app and there will be surely many bugs and glitches with the code.  Keep in mind that our main intention was to create a functional prototype to showcase a small fraction of creative possibilities available to us through the open source community.  We used R Studio as the main IDE for our app.  R console works fine as well.  This blog will give a general overview of our development process.  The code is available here if you wish to play around with it and learn more about our full stack.To begin we created two blank r files: ui.r and server.r within a new project directory or folder.  You can name this folder anything.  For the ui.r file you will need to install and load the following packages below.In the server.r file you will need to install and load the following packages as well. In order to make a visually appealing and straightforward interface design we implemented the shiny dashboard package.  This package had a very straightforward documentation that can be found here.  The shiny package comes with a large pool of high quality icons css themes and other bootstrap quality elements.  I recommend following the  and then cross reference your learning with our code to get the most out of this blog post.  The server.r code was tricker than the UI for a couple of reasons reactive shiny features is a must for creating an interactive shiny app.  The main shiny blog does a fantastic job explaining dynamic and reactive scripting in R so I will leave the explanation to them in .  Essentially reactive functions in shiny means that you are creating smaller “pseudofunctions” that automatically receives user input when interacting with features such as a checkbox or slider.  We wanted reactive functionality to allow the users to customize their tuning parameters for the modeling part of the app.  The second part is the analysis feature.  The analysis features contains three subfeatures: preprocessing feature graphing and modeling.  For the preprocessing subfeature we kept the options minimal and included cross validation IPA and ICA options that can be activated through shiny widgets.  The feature graphing is a simple graphing application that produces a lattice plot of all data features in the data set.  For the modeling subfeature we made four algorithms available for the user to choose and tune: KNN logit boost gradient boosting method and neural networks.  All of the modeling algorithms were from the caret package and is a fantastic package to familiarize yourself with if you want to pursue more machine learning application in R.   The demo video above provides a visual overview on how to operate the shiny application.  Feel free to reach out to us if you have any questions or comments about the code.  Again this is completely open source so please take it and play around with it!",NA,On The Rise of Data Science Startups
https://nycdatascience.com/blog/student-works/diabetes-obesity-and-income/,71,This was my first data science project associated with the bootcamp.  The goal of this project was a visualization and explorations of New York state diabetic incidence data.  I wanted to look at data sets where there existed previously established relationships.  I chose a public health issue of diabetic rates and obesity it has been shown that there is a strong link between these two issues.  I thought it would be of interest to see how this relationship played out in New York state.  The data used were part of the 1250 freely available data sets on the  website an amazing resource for public health issues.  Four different sets were utilized: an income data set; a diabetic rate set; a diabetic mortality rate and an obesity rate data set(all from 2009).  All of these data sets provided granularity at the  county level.  The inclusion of the last three data sets are very straight forward.  Income data was included in this analysis for two reasons first there is a long established relationship between income and diabetic rates.  Secondly New York is the only state where the poorest county(Bronx) is directly adjacent to the most affluent county(Manhattan/New York County).  I was curious to see what relationships existed alongside this economic disparity.  New York state contains fiftytwo counties so I wanted to find a way to represent this data that made comparisons between counties easy and also retained spacial relations.   Choropleth maps were ideal for this task.  These maps use color intensity to represent aggregated data within a geographic region either intensive or extensive.  Luckily for me the R programming language had several libraries to aid in this mapping choroplethR had the shallowest learning curve.  Rather than just bury the reader in plots I will present only the most interesting plots in the post.  But here is a link to a Shiny application that I created that will allow you to explore with all the data.  Before my data could be mapped with choroplethR library it required some manipulation.  Thankfully the data was complete if anything there was more data than I required.  Several of the data sets contained information years beyond 2009 my year of interest.  Some data sets included aggregations of regions that required exclusion.  Lastly all of the data sets contained formating that required adjusting so they joined with the FIPs dataset required by choroplethR.  Here is the code for the for my loading and data munging.After the data sets had been joined to the FIPs county data set they could be visualized with choroplethR.  The code for the plots is listed here.The data was also plotted in scatterplots to better visualize some of the nonspacial relationships within the data sets.  The code for the scatterplots is listed below.Conclusion:The data substantiated the already established relationships between income obesity and diabetes.  Counties with the highest average incomes had the lowest incidence of diabetes.  The link between obesity and diabetes was also shown.  Interestingly counties with high incomes and high obesity rates had low rates of diabetes.  The link between income and diabetes may be as strong as the relationship between diabetes and obesity.  I haven't included my Shiny app code for the sake of space put it can be found on my Github here.,NA,"Diabetes, Obesity, and Income"
https://nycdatascience.com/blog/student-works/the-great-escape-health-wealth-and-the-origins-of-inequality/,71,: Build a companion interactive presentation for a book : Build an interactive presentation with embedded  apps:   and  winner of the 2015 Nobel Prize in economics is a Professor of Economics and International Affairs in the Woodrow Wilson School of Public and International Affairs and the Economics Department at Princeton University.In  Angus Deaton–one of the foremost experts on economic development and on poverty–tells the remarkable story of how beginning 250 years ago some parts of the world experienced sustained progress opening up gaps and setting the stage for today's disproportionately unequal world.,NA,"The Great Escape: Health, Wealth, and the Origins of Inequality"
https://nycdatascience.com/blog/student-works/lending-club-investment-simulator/,72,We build 2 mains tools to explore and run simulations on the data provided quarterly by Lending Club.,NA,Lending Club investment simulator
https://nycdatascience.com/blog/data-science-news-and-sharing/hadoop-3/,73,"The Hadoop system now hosted at Apache Software Foundation (hadoop.apache.org) consists of about a dozen toplevel projects.It began in 2006 as an effort to create an opensource implementation of MapReduce Google's system for exploiting parallelism in their large data centers.HadoopMapReduce is still at the center of the Hadoop ""ecosystem"" but compared to the Hadoop of 2006 today's Hadoop is more general more efficient and easier to use for a variety of applications.Among the systems in the ecosystem are Hive which provides an SQLlike database querying facility for massive distributed data; Pig which uses its own language to simplify the construction of MapReduce ""pipelines;"" and Spark which leverages the Hadoop ecosystem to provide MapReducelike parallelism at greatly reduce cost.Today Hadoop is used by countless companies and universities either in their own datacenters or on commercial systems like Amazon's EC2 or Google's GCP.Financial companies use Hadoop to analyze stock purchases; drug companies use it to explore drug interactions computationally; scientists at NASA use it to process massive streams of data from satellites.Its popularity comes from its simplicity and from the growing suite of tools supporting not only basic parallel processing but also applications like data mining and data visualization.Increasingly Python and R are being deployed within the Hadoop parallelism framework; the simplicity of using dynamic languages and the efficiency of parallel computers makes this a great solution for processing huge data sets.",NA,What is Hadoop?
https://nycdatascience.com/blog/community/dataiku-partnership/,74,says  (founder of NYC Data Science Academy). During our 12week fulltime Data Science Bootcamp students analyze an immense amount of data. Dataiku’s software  will now be used to enhance the students' experience. Their expert data scientists will also be on hand to guest lecture courses and help guide projects for students.This initiative complements Dataiku’s vision of a world where all businesses whatever industry or size can produce their own datadriven strategies by converting raw data into business impacting forecasts. And supports our goal to deliver the most relevant and highest quality content to our students. We're very excited about the partnership between NYC Data Science Academy and Dataiku. It's another great step towards bringing the efficient practice of advanced analytics techniques to aspiring data scientists. We expect that their feedback will be invaluable as we continue to develop the best possible platform for collaborative data science.If you're a prospective data scientist looking to enhance your quantitative and coding skills and you're an independent selfdirected data geek then apply now to the  and  to get started immediately.You will learn the following technical skills:This is just the next step for NYC Data Science Academy and our team of to help prepare tomorrow's new crop of data scientists.You can follow the project on Twitter using Twitter:  HashTag: Our upcoming 12Week Data Science bootcamp starts on January 11th 2016.  to secure a spot in our winter cohort!,NA,NYC Data Science Academy Proud to Announce Partnership with Dataiku
https://nycdatascience.com/blog/student-works/team-oriented-grasp-and-lift-eeg-kaggle-competition/,74,Joseph Russo– – “” “”’’’  ±±Given that deciphering signals to characterize brain activity requires expertise in signal processing we searched the Kaggle forum to gain a better understanding for preprocessing and analyzing this type of data. M“”After experimenting with other models (see below: Earlier Models) we chose 'deep learning' as our final model as it had the best AUC (0.91 on the test set). The initial incentive for choosing deep learning is simply because we were analyzing neural timeseries data and it would be fun to learn. We are heavily indebted to Tim Hochberg and the Python NNet script he shared on the site.At the core of deep learning is a hierarchical framework of linked (stacked) neural networks. To be deep there has to be more than one of the neural net layers (stages) between the input and output layers. Figure 3 presents what a neural network could be for a subset of our data (subject 1 CSPpreprocessed data created using R packages neuralnet caret and e1071). On the far left are the inputs in this case the 4 CSP features in the middle are 2 layers with 16 and 8 nodes and on the far right the 6 events as outputs. Please compare this to the illustration (Figure 4) from  made by Michael Nielsen of a deep learning model with convolution and pooling layers. One core concept of deep learning is to build complex functions from simple ones and that each layer provides a nonlinear function or feature transformationthat enables complex feature generation. This modular hierarchy of combining multiple levels of functions leads to a hierarchy of feature abstraction at each layer. A striking example is provided by deep networks from the imageNet model [Ref 1 Figure 2]. In this case when observing the output of the sequential layers the feature representation is indeed hierarchical: pixel > edge > textonor texture unit > motif > part > object [Ref 2].Another core concept is that the network should be able to build its own features or representation of the data relying on the incoming data itself and not be handcrafted by individuals for each source of data. For example scaleinvariant feature transform (SIFT) features are handcrafted lowlevel features versus having the network learn similar features such as edgedetectors. This idea stems from the ability of different parts of the animal brain to be able to learn from different stimuli e.g. the auditory cortex learns to see. This suggests that there may be just one learning algorithm used by the brain [Ref 3].Deep neural networks have become the benchmark for performance and are commercially used by numerous companies including Microsoft Google and Facebook [Ref 2]. These deep networks scale to large data sets with millions of examples that require optimization over billions of parameters.However training a deep neural network to achieve performance is not trivial as there are many variables to consider [Ref 3]:Architecture of the networkLoss function (regression: squared error classification: crossentropy)Optimization of nonconvex functions Unlike convex problems nonconvex functions have no guarantee for global minimization Initialization of the networkSupervised finetuning (backpropagation)Our first model trained on the raw data (all 32 channels) downsampled by 10 and run on each individual person for series 1 through 8. The performance on the test data (series 9 and 10) was an AUC of 0.89. We then thought to use the preprocessed data (normalized Butterworth filtered CSP features) downsampled by 10 and the performance was an AUC of 0.53. We therefore went back to the original data and trained on all data for each person (deep learning does better with more data). The AUC increased to 0.91. However running this took about 6 hours on a MacBook pro with 16GB memory.We now wanted to run the deep learning model in PySpark. Since SparkMllib does not have a deep learning algorithm we hypothesized that the easiest thing to do would be to load all into an RDD group by person (there are 12) and run the Python deep learning script on each subject in parallel. In other words an 'embarrassingly parallel' problem.As a first step we set a Spark context that would use all 4 available cores and 10GB of memory:We then pulled in a csv to create an RDD to be grouped by subject: As we couldn't check whether our grouping had gone as expected or not we tried a different approach. We grouped outside of PySpark and then pulled in as a RDD directly with key/value pairs:Creating the new object data_ls which is a list of lists having subject in sublist with subject's data. In other words a key/value mapping as list that could then be pulled in to Spark as an RDD having 12 partitions:Now the next step will be to push the partitioned RDD through the deep learning. This requires changing the deep learning Python script to read input as a key/value data frame rather than pulling in csv's. As a comparison to logistic regression we decided to also run a support vector machine model (svm) on the CSP features. Logistic regression is usually a better choice when the data is very noisy. In this case the data were not particularly noisy and the separation hyperplanes may not be linear.As a first step overlapping classes were removed. The entire training sample data set over all subjects and series has 17985754 data points (time frames). 478939 of those were not uniquely classified (the data point fell within 2 events); this 2.6% was removed from the entire training sample.Using crossvalidation with 3 folds several parameters of the svm model were tested using the grid search function in the scikit learn package in Python. Quadratic and cubic separating hyperplanes were tested as well as the radial bias function (rbf) kernel. C which shrinks the error to zero as it increases was set at 1 and 10. Gamma was tested at 0.001 and 0.0001 for the rbf kernel. In order to run the model on a laptop (RAM  16GB SSD  1TB) a subset of the first subject was used (280000 out of data points).In order to run this faster we decided to use Spark Mllib. However the package currently only supports binary classification.When we embarked on this final project our hope was to use the tools that we learned during the Data Science Bootcamp from beginning to end. We are thrilled to report that we accomplished that goal! A critical ingredient to our success was teamwork. At the outset we structured our project for optimal transparency. We agreed upon strategies for analyzing the data visualizations preprocessing and machine learning. We developed a project workflow to manage discrete tasks and assign ownership. We created team GitHub Dropbox and Slack accounts to share code documents images and most importantly updates. We met daily to share progress reports problem solving and changes to workflow. We used both Python and R to accomplish our tasks. We relied on each other to help debug code and research other solutions. In the end we were proud of the way we worked creatively independently and as part of a team and how we inspired each other to deliver. Go team!“” 2 Deep Learning: The Theoretician's Nightmare or Paradise? (LeCun NYU August 2012)https://www.youtube.com/watch?vHrMU1GgyxL8&spfreload13 Bay Area Vision Meeting: Unsupervised Feature Learning and Deep Learning (Andy Ng)https://www.youtube.com/watch?vZmNOAtZIgIk,NA,Kaggle Competition project: Grasp-and-Lift EEG
https://nycdatascience.com/blog/student-works/find-twitter-influencers-through-social-network-analysis/,74,’ll probably find’s’s ’s InfluenceFlow Scoreainly focused on identifying and classifying superfans and d,NA,Targeting Twitter Influencers Through Social Network Analysis
https://nycdatascience.com/blog/data-science-news-and-sharing/learn-from-master-owen-zhang-1-on-kaggle-com/,74, Owen explains how he utilized “ ’s solution was written entirely in R. Owen used’s’s Interview below.,NA,Learn from the Kaggle Master: Owen Zhang Discusses Winning Solution
https://nycdatascience.com/blog/student-works/event-hunting-in-nyc/,75,Fridays and Saturdays have the most events (191 and 192 respectively) although Monday albeit being the leasteventy isn’t too shabby with 105 options. This is New York City after all.,NA,Become a Master Event-Hunter (and Coolest Kid on the Block) at the Click of a Cursor: My NYC Event Finder R-Shiny app
https://nycdatascience.com/blog/meetup/meetup-scikit-learn-workshop-by-andreas-mueller/,77,Andreas currently is a Research Scientist at the NYU Center for Data Science a research group dedicating to open source software for data science. Previously he worked as a Machine Learning Scientist at Amazon focusing on computer vision and forecasting problems.,NA,Meetup: Scikit-Learn Workshop by Andreas Mueller
https://nycdatascience.com/blog/student-works/speed-cameras-revenue-or-public-safety-well-get-you-up-to-speed-in-a-flash/,78,g  plot(ggmap(nycmap)+geom_point(aes(xlon ylat size  log(count) color  'red') data Cam)+theme(axis.text.xelement_blank() axis.text.yelement_blank() axis.title.xelement_blank()axis.title.yelement_blank()legend.position'none')+stat_density2d(aes(xLon yLat fill  ..level.. alpha..level..) geom'polygon' dataAccidents.lastyear))+scale_fill_gradient(limitsc(1:60))ggplot(xaes(xDateyAccidents))+geom_line()+geom_point()+ylab('Pedestrian Vehicle Accidents') +theme_bw()+xlab('')camandtixsum  function(x){ y  data.frame() for (i in 1:length(x)){ y[i1]  length(grep(x[i] Cam$Address)) y[i2]  sum(Cam$count[grep(x[i] Cam$Address)]) } y  cbind(y x) return(y)}byBorough,NA,Speed Cameras: Revenue or Public Safety? We'll get you up to speed in a Flash!
https://nycdatascience.com/blog/meetup/featured-talk-1-kaggle-data-scientist-owen-zhang/,78,Featured Talk: #1 Kaggle Data Scientist Owen ZhangThank you very much for coming and thanks Vivian for the generous introduction. I’m sorry to say this but it sounds much more impressive than it actually is. I want to tell you a little bit of background. I used to work in the insurance industry. I worked several years in Travelers and several years at AIG. So one thing that I learned from my corporate career is that it’s very very important to manage your expectations. We didn’t do a very good job so I’m trying right now. So I’m going to give about one hour of talk. We’ll cover a few topics and I’ll cover a few topics including my personal take on how to do well in Kaggle competitions. If you have any questions please wait until the end. We have about a half hour of open Q&A session so everybody will have a chance to ask questions.Today I’m going to cover these few topics. First I’m going to give a brief introduction on how data science competitions are structured at least how Kaggle does it. I’m going to cover a few pointers on how I do it: philosophy strategy and also techniques. Then I will have two example competitions so two that did I relatively well. I just want to use that example to show what I actually do and to show that each competition is quite different. The last I want to cover is the things that we learn from a competition are they useful in the real world or not. I do want to answer that question at the end because actually I get asked that question quite often.How is a data science competition structured? At least how is a supervised learning competition structured? Usually the host will have all of the data and then they will split the data into three pieces. There’s the training piece which we can all download and we have both the predictors and the target. And then there is the public leaderboard piece. We havetraining for all three pieces the difference is what we know about the target. So we know all the targets exactly for training. Then we get a summary metric on the target for the public leaderboard.After you build a model on the training data you apply the model to the public leaderboard data and make predictions. Then you upload the predictions. Because the host has [the third piece on] the server they have all the target variables they compare your predictions with the actuals to see how well your prediction predicts it. If I tell you summary score that’s where the public leaderboard is. Throughout the competition it will last a few months you will get constant feedback of how well your model performs on that piece of data compared to all the others. So you can get some form of idea if you’re doing the right thing or not. So at the end of the competition what really determines the outcome is the private leaderboard. So you’ll have some sort of idea if you’re doing the right thing or not. But at the end of the competition what really determines the outcome is actually the outcome is actually the private leaderboard ranking. Which you only get to see once. That’s after the deadline passes then they will switch over from the public to the private. It depends on the competition. It’s more often than not [the public and private leaderboards] are the same. Sometimes the public leaderboard and private leaderboard are very different. So there’s quite a bit of uncertainty involved.Why is the competition structured that way? Because this forces us to do what the predictive model is supposed to do: that is to predict what we haven't seen before. So the private leaderboard matters most because we have absolutely no knowledge about the target variables in the private leaderboard data. That is how our model would be used in the real world. So whenever we build a supervised learning model the purpose is to use the model to predict something we haven't known yet. So if we know something already there is no reason to predict it we can just take a look at it. So it is very important to evaluate the model on data that the model does not have knowledge of.So a little philosophy: I am actually quite fond of philosophy because when it comes time to predict something you have no knowledge of there is usually nothing to go by so you have to fall back on philosophy.So I'm sure that most of you have certain data science background and we all know that one of the biggest pitfalls in building a model is overfitting. Overfitting means that you have a training data set and you build a model on the training data set you can throw a lot of factors at it and try all kind of different things. You can build a model that fits the training data set very well however the model may not generalize. So you can have a model that does exceptionally on the training data set but if you build a model that predicts very well on things you already know but it doesn't predict well on things you don't then that would be a model that is overfitting.In this case in a lot of Kaggle competitions overfitting is the biggest risk. So what I learned through the years is that there are many ways to overfit. The easiest way to overfit I think we all know is to have too many parameters in your model. So if you have one hundred data points in your data set and you have one million predictors in there you can probably predict it exactly because you have so many degrees of freedom. But that is only the most common way of overfitting. There are actually many other ways.One very subtle way of overfitting is that you keep building different models and you test each model against the answer which is your public leader board and then you you see we have a model works well you keep it and whenever it doesn't work well then you can drop it. You can keep doing this. Actually in a lot of situations you can keep doing this and you will find that your model's performance on the public leaderboard just keeps growing. And there's no end to it. But the reality is that you are just overfitting the public leaderboard. I think that happens quite often.In statistics (ok I'm not a statistician so let me know if there is a statistician in the audience) but in statistics we always use a five percent significance test. So if something is given a null hypothesis you have less than a five percent chance of observing what you have observed then you have to say that this is significant. But we run multiple comparisons very very often when doing models its better if you try very hard. You randomly try one hundred things then a few of them might turn out to be statistically significant but that is because you tried so many things so here the philosophy is This is the is the secrets slide. These are the secrets. The longer version is on the left and the shorter is on the right. The slides will be posted on Vivian's website or on Slideshare so there's no need to take notes on the slides' content.The short version is that (1) you need to be disciplined. The temptation to overfit public leaderboard is very strong. Sometimes psychologically I cannot control myself just to overfit the public leaderboard because that makes me feel good. We are all human. We need to be disciplined so we don't look embarrassed after the private leaderboard comes out and we had been doing so well on the public leaderboard then it flips and you say 'ahhh' you can't take pride in yourself anymore. It doesn't look good.The next is to work hard. The Kaggle system rewards effort. You never lose anything by participating. So you can rank absolutely last in a contest and you still relatively speaking do better than not participating. Participating more gets you better results. Even within competition trying to work hard gets you better. We will talk on that how to properly work hard later.The next one is learn from everyone. This also is a little bit against human nature. We all have our own pride right? Occasionally I would think ok this time I just try to do everything myself I don’t want to look how other people do it. There is a little bit of like hubris in there. But on the other hand people other than you always know better than you individually right? Because there so many of them there is only one you so it’s pretty hard to compete against everybody. I always feel that really learning from others that you can really improve. There is really no pride in doing everything by yourself otherwise. Actually I’m not sure how many people can really invent all the mathematics that learned in elementary school. And I think that takes more than one lifetime to invent all that. So there is really no “oh I learned this myself.”The next one is luck. So we are predicting very noisy data and things we haven’t seen yet. And there is really no absolute to differentiator between noise and the signal. There is none. So the only way to really differentiate them is that you have more data. Which if data science competitions there is none. You have everyone’s data there is a signal to noise ration that’s pretty determined. So there is always nonzero luck. So if you participate in something and you don’t do very well in a particular competition that’s probably just bad luck but if you do well that’s all your effort. []I always get reminded before I go to give a presentation to make sure that I bring something that’s concrete that people can take away. The rest of the presentation is something that’s more concrete. But I personally feel it’s how you approach the problem. The philosophy and strategy are more important than specific techniques. So I don’t want to say just learn all the techniques and you’ll do well. So a lot of times it’s not about what you do there are a lot of specific things you know is necessary to build a good model. The key there is actually how to allocate your effort. We all have a time budget. There is a deadline and then we have to do our day job and feed our kids if you have kids. Turns out there is limited number of hours per day. It’s actually worth it to think about how much you want to allocate to doing feature engineering how much time you want to allocate into trying different forms of models and how much time you want to allocate into hyperparameter tuning. So actually I do think about that consciously.Now to talk about some technical tricks. Gradient Boosting Machines. I use GBM on everything that I can use GBM on. There are things that you can’t GBM. One example would be very large data. If you have 100 million observations you probably don’t want to write GBM. But for anything that can use GBM GBM. I probably overuse GBM. So why is GBM so good? GBM automatically captures to a very large degree nonlinear transformations and subtle and deep interactions in your features. GBM also treats missing values gracefully.I use the R implementation of GBM that is what I work with so often. There are at least four very good GBM implementation today. Two years ago there was only R that was good. But nowadays you can use either the R package you can use scikitlearn package its very good. And if you want go parallel there is H2o package and xgboost. They all are very good implementations. Each has their own quirks. But they are all able to do nonlinear transformation interactions. Those two things are what people spend most of their time on when we’re building linear models or generalized linear models. In the days when we had logistic regression or when we had only Poisson regression in the industry in the insurance like we were. Whenever we are building models we are not really  models because we always run the  models like Gamma regression. So what we are doing is looking at all the outliers all the transformations turning it to Ushapes Wshaped hockey stick shaped whatever shaped. If you use GBM it will take care of this for you automatically thats why I use it so much. If you haven’t tried it please do try it.GBM like all the other modern machine learning algorithms has a few tuning parameters. When your data set is small you can just do a grid search. But actually GBM has three separate pretty orthogonal tuning parameters. If you want to do grid search you actually do need a pretty big range. I usually use some ruleofthumb kind of tuning so it’s mostly for saving time. So if you do a very smart grid search you can probably do better than the ruleofthumb. But rule of thumb is very time saving.The first set of tuning parameters is how many trees you want to build and the learning rate. They are reciprocal to each other. A Higher learning rate will get you to use fewer trees. I want to save time right? Usually I just target 5001000 and trees and tune the learning rate. The next one is just a common tuning parameter for decision trees (GBM is based on decision trees). The number of observations in the leaf node. You can look at the data and get feel of how many observation you’ll need to get mean estimate. So if your data is very noisy then you make more and if the data is stable you probably need less. Interaction depth is a very interesting for the R version of the GBM. The R version of interaction depth describe how many splits it does; so it’s not interaction depth as in the  depth. If you have interaction depth of 11 that means you have above 10 leaf notes. It doesn’t mean that you will have a 1000 leaf nodes. Don’t be afraid to use 10; I use 10 very often.There is one thing that GBM does not doactually there is one thing that all the treebased algorithms do not do well which is dealing with higher cardinality categorical features. So if you have feature which has a categorical variable that has many many levels throwing them into any tree based is basically a bad idea because the tree will either run super slowly if you are encoding all of them or just overuse that feature because it’s overfitted. So for high cardinality featuresif you want to use them in the treeyou need to somehow encode them as numerical features.There are few different ways of encoding. High cardinality features are very common. We often see things like zip code or in medical data you see diagnosis code (there are tens of thousands of ICD9’s) or text features. One approach for encoding them is to build ridge regression. Starting a logistic regression is very simple you can add regression based on your categorical features and then use prediction itself that will be numerical as an input to your GBM. That’s what people usually call stacking. Basically you build a stack of different models–subsequent stages of models will use the previous stages of models’ output as their input. As I described if you have text features or numerical features you get all the text features into your ridge regression model then you make a prediction. Then you put all your predictions side by side with all of your raw numerical features and fit this in a GBM. And this usually works pretty well. If you haven’t tried it you can replace these by a categorical features. And for a lot of Kaggle competitions you just do this and you reach the top 25%. So people that run bottom half of kaggle probably aren’t using GBM. [] Because it is quite easy to rank in the top half. There are many beginners that really get into this on the bottom.The one particular risk here is that if you use the same data to build the first stage model and then use the same data again on Model 2 very often the prediction from Model 1 will be overfitting in Model 2 because you already used the targeted variable in Prediction 1. Prediction 1 has a leakage in there. Basically it has leaked the information from the actual target. Whenever you put the Prediction 1 like that with numerical features Prediction 1 will be given too much weight.The more you overfit Model 1 the more weight will be given to Prediction 1. So it can be quite bad. What you want to do here is use different data for Model 1 and Model 2. So you can split your data in half. Use half of the data for Model 1 and half for Model 2. This will give you a better model than using all the data for Model 1 and all the data for Model 2. You can swap them you can split the data half for A half for B using all of the data in this way. This will usually give good results. If you’re not trying to win competitions i.e. for practical purposes I feel that this kind of model is already good enough for a lot of practical applications.You can take this one step further if you have smaller data where even half and half might be limiting your data too much you can do something like crossvalidation–you can split your data tenfold and do 10 different Model 1’s each one using nine to predict the other ones. You can then concatenate the models together so that each record will have an outofsample prediction from Model 1. Then you can avoid the overfitting problem in Model 2. So that’s how you deal with categorical features.Once you can deal with the categorical features GBM is good for 95% of general predictive modeling cases. You can make a GBM model a little bit better (i.e. for marginally boosting Kaggle ranking it may or may not be worth it for you).The reason is GBM can only approximate interactions and nonlinear transformations. So when GBM is approximating the transformations or interactions it can’t differentiate noise versus signal so inevitably you pick up noise. So if you know there are strong interactions it’s better that you explicitly code them into the GBM. GBM’s feature engineering is just a greedy space search so that it can’t really find very specific transformations. So for example a lot of times when we build a sales forecasting model the best sales forecaster is actually the previous time period’s sales. If you want a GBM to automatically discover that I think actually that’s pretty difficult so you should code that definitelyThe second most often used tool by me is linear models like generalized linear models or regularized generalized linear models. I used to use glmnet very often it’s also a very popular R package. So from a methodology perspective glmnet is actually all the generalized models are kind of opposite of a GBM. So all the generalized linear models are global models assuming everything is linear and all the tree models are local models assuming everything is staircase shaped. So they compliment each other very well.So linear models become quite important when your data is very very small or when your data is very very big for different reasons. When your data is very very small you just don’t have enough signal to support a nonlinear relationships for interactions. It’s not that there isn’t; there always are nonlinear relationships in interactions. But if you don’t have enough data to detect them it’s better to stick with a simpler one so that’s when the linear model works well. On the other end of the spectrum and you really have billions and billions of observations only linear models are fast enough for you so everything else will never finish in our lifetime so that’s why go back to linear models when the data is very large. So the linear models when you do a model blend (GBM and glmnet complement each other well) you get a very nice boost in performance. The downside of glmnet or anything similar to this kind of model is that it requires a lot of work. All the things GBM does for me automatically I have to deal with them myself–I have to deal with all the missing values all the outliers all the transformations and interactions. It really takes a lot of time.And the last thing I want to cover is regularization. My personal bias is this: In this day and age if you are building linear models without any regularization you must be really special. Always regularize. It’s required. There are two very popular regularization approaches one is L1 one is L2 so basically L1 give you sparse models L2 just makes everything’s parameter a little bit smaller. The sparsity assumption is a very good assumption. The book  describes why that is the case. If your problem is sparse in nature assuming sparsity will get you a much better model than not assuming it. If your problem is  sparse in nature then assuming it will not hurt that much. So the lesson is always assume sparsity. Unless its a problem you  is not sparse.So there are actually problems that we know is not sparse like text mining or if you’re doing zip code. If you think ‘what is sparse?’ like there are three zip codes that are special they have some kind of parameters and everything else is the same. That’s the same thing in text mining if you assume that 500 words is very special and all the other 500 thousand are not special. So in that case when you’re doing text mining or doing high cardinality categoricals that’s where you  assume sparsity. But other than that it’s simple assume sparsity.First I admit that I’m not a very good text miner if that’s a word. I Google things and I read whatever other people write and try to follow their examples. My approach to text mining is the simpler stuff usually works better. As far as we have come in Machine Learning the Ngram and Naive Bayesbased approaches actually work surprisingly well and a lot of times actually it’s quite hard to do better. Here on text mining my view is make sure you get the basics. The basics such as Ngrams and also the trivial features such as how long the text is how many words are in the text and if there is any punctuation those kinds of features are actually very important.The last one is: a lot of problems that are heavy on text are not necessarily driven by text. This year’s KDD Cup is an interesting example. The KDD Cup it’s a problem it’s actually sponsored by a New Yorkbased company. They have a website where you can donate to your local school’s teacher’s projects. So if you’re a teacher in an elementary school or a high school you can post credits say “I”m doing this reading club with my students I need 100 dollars to buy this carpet.” People really host those! If you sympathize with that effort you will say I’ll give you five dollars I’ll give you ten people can add together doner will chose. They setup the competition to see which project is more likely to get attention from people and more likely to be funded.Teachers will put up essays and summaries of their projects. So it’s very nicely written—some people write very nice essays not everybody. All text data are provided also with the cost and type of project. Whenever you have unstructured data the text always is way bigger than the structured data so that I always feel that I’m obligated to use the text a lot. But it turns out that usually some other features are much more important. For example projects that cost less are much more likely to be funded. That doesn’t need the essay to tell you. If you ask people for one thousand dollars for a laptop no essay will save you. So it’s much easier to ask people for math books. That’s what the data tells us.If you do a competition on Kaggle then you will learn this. It’s almost never that a single model will win a competition. People always build a variety of models and blend them. Blending is just a fancy word for a weighted average of models. You can do a little bit fancier than weighted average but not that much. I usually just use weighted average. My rationalization for why blending works is because all of our models are wrong. As George Box said “All models are wrong but some are useful.” I would like to think all my models are wrong but  useful. “It’s very hard to make predictions especially about the future.”When we study regression analysis or something similar in school we always start or models with Independent Identical Distribution (IID) Gaussian errors. In my entire life I have  seen a real data set with that distribution. It never happens. I don’t know where it should happen but it never happens. So our assumptions are never right. So whatever you assume unless you’re doing a simulated data set everything is wrong. But the hope is that our models are wrong in different ways so that when you average them their wrongness kind of cancels each other’s out.There are a few things to keep in mind. One is that simpler is usually better when things are uncertain. If you are not sure just add Model 1 Model 2 Model 3 and divide by three. That’s usually better than taking the parameters unless you have a lot of data. The next one is a very easy and useable strategy which is to intentionally overfit the public leaderboard. And if you do it carefully it might give you a nice boost. So what you do with this you build many different models and you submit them to the public leaderboard individually and then based on their score you keep the better ones and then average them and that’s your blend. So based on the public leaderboard that might give you a nice boost. This model will work better than any individual model submitted. So either that will work better on the private leaderboard or not depend on how much data you have on the public leaderboard. If you have a lot of data on the public leaderboard this will help you because when you’re doing this you are actually implicitly using the public leaderboard data as your training data. So you actually have the advantage over people who don’t do this. A real advantage if the public leaderboard data is large enough. But if it’s  large enough you will do terribly bad.One thing that’s very useful when doing blending is that you are not going after the strongest individual model. You always want a model that works but you want a model that’s different. Sometimes I will intentionally build a weak model. They help a lot when you add them to a strong model. So the key is the diversity—I’m sure HR people will be very happy to hear this we need diversity in model building. This is true it’s actually scientifically proven it’s good.There are many ways to build diverse models. You can use different tools. You use different model structures. You can use different subsets of features. You can use different subsampled observations. You can build a model so it’s weighted. You can build both weighted and unweighted numbers. Usually the unweighted one will work worse. But if you can take 90% of the weighted one and a bit of the unweighted one it will work better than just the weighted. But here is try and build those models more or less blindly; try not to look at the answer. You know the public leaderboard is always there and you always want to test them. Try not to use that as a guide to filter out the models unless you are confident that the dataset is large enough. It’s a total judgement call based on the noise and the size of the data.Let me try to address this question objectively although it’s not possible—obviously I like competitions. But sometimes people ask me “other than being a fun game is there anything really useful from competitions?” First of all let’s acknowledge that it’s a fun game. The fun game itself is entertaining so we shouldn’t dismiss a fun game as something not useful. But beyond that there are two ways of looking at this.The model in a competition only covers an actually very small portion of the necessary work to make the data science very useful in this work. So to make the model really useful we need at least another three pieces. One is that we need to select the right problem to solve. So if you find a problem that’s interesting only abstractly but there is no realworld application there is no immediate value. The next one is we need to have good data. The model is certainly a garbage in garbage out problem. If you don’t have good data you cannot expect to have good models. The third one is we need to make sure the models are used the right way. A lot of the time it’s possible to build very good models but then they are implemented wrong. It happened to me in the past; you have a good pricing model in insurance and when they swap the parameters like x1 parameter goes to x2. And then you figure out this doesn’t work and then say. “oh they swapped it.” And I never asked why they swapped it but they do. With all these three plus the right generalizable model and then you’ll have the right solution. So the fun part of building a model in Kaggle is only a small part we all need to keep that in mind in reality.But aside from that a competition helps us in very many ways. So I have two ways it helps the sponsor. Iif you have a company either a startup company or enterprise. Building a computation for your own data set is very helpful. There are two ways this is very helpful. One is to measure to a degree the signal versus noise in the data so if you put up reasonable prize money needs you can request 99% valid signal and that will be squeezed out by other people on Kaggle. Kaggle people are quite good at that. The second way is [finding out] if there are any flaws in your data so a lot of times it is because of the data collection process issues. You may have predictors that are not real predictors. Such as if a particular field is missing you always have a yes or no as an answer. So if you have anything like that the Kaggle crowd will find it for you so you can go back to fix it. The model is not useful if you have that problem but at least you can fix it.But as a participant I learned two major things on Kaggle one is that I have to build a generalizable mode—just predicting what I know is useless. That discipline is a hard discipline to learn. The next one is to fully realize day to day that there are other approaches and there are other people with better ideas. So this always keeps me on my toes and learning new things. Otherwise it’s really easy for me to sit in my corner and think “Oh I do really nice modelling work.”So let me just give two examples of two competitions I did okay in. One is Amazon’s user access competition. This is one of the most popular competitions. It used to be the most popular competition but recently the Higgs competition has more participants. This one has about 1700 teams participating. I got second place on this one. This is one of the most interesting experience for me. I was the no. 1 in the public leaderboard. But this time I didn’t try to overfit the public leaderboard. Sometimes I do but this time I didn’t. I still lost somehow to another team. Never figured out why.The [challenge was] to use anonymized features to predict if employee access request would be granted or denied. If you work you know for a large company this is quite often; if I need access that folder someone needs to say ‘yes’ or ‘no.’ And all the features are categorical: resource IDs manager IDs user IDs and department IDs. Many features have many many levels. But I wanted to use GBM right. This is how I converted all the categorical data into numerical. There are two ways I used to encode categorical features into numerical. One is how many that levels appear in the data. You can do this for all your categorical features and all the interactions for categorical features. And another one is using the average response—the average y for that level as a predictor. So here you have to do something slightly more complex than doing a straight average because a straight average will lead to overfitting on certain levels.Beyond those encodings the final model is a linear combination of three different kinds of trees plus the glmnet and other features plus two subset features base trees. So this is a blend that I tuned manually. At that time I did not fully understand the online learners like a Vowpal Wabbit. So I didn’t use it. In retrospect if I had used them I would have derived a better model because they have a very different algorithm. This competition had a particular requirement that everybody had to publish their code. So my code is up there on the Github. Do  evaluate my software engineer skills when you read the code.So here is something that is very easy to do actually for encoding categorical feature by doing the mean response. This is a very simple data set we have a categorical feature the UserID And for the level A1 we have six observations. Four of them are in the training data and two of them are in the test data. For the training data you have the response variable then 0 1 1 0 and in the test data you wouldn’t have the response variable. So here I show how to encode this into a numerical. So what you do it to calculate for the training data the average response for everything  that observation. So for the first one it’s 0. For this particular observation there are 3 other observations in the same level there’s number 2 3 4. And there’s two out of three (that’s why it’s 0.667). The second one it also has 3 other observations but it’s 1 0 0 (so it’s 0.333). Do not use itself. If you use it itself then you will be overfitting the same data.Sometimes it also helps to add random noise on to your training set data. It helps you smooth over very frequentlyoccurring values. For example if you do have this you will see that [these numbers can be thrown] into GBM GBM goes nuts because it treats them as special values. So if you add small noise on top of that it actually makes it a little more real from a data perspective. You do not need any such special treatment for the testing data. Testing data is a straight average of the response value for that level for the training. So two out of four (that’s 0.5). So the basic thing I need to do to use categorical features in GBM. This is much easier to do compared to building a separate ridge regression and I do this very very often. That’s Amazon. The amazon competition is a very simple data set. Mostly do feature engineerings on anonymized categorical features. And the response is 1 or 0.So the Allstate competition is unique in a different way in that it has very structured target variables. So it has seven correlated targets so you have A B C D E F G which represent the options people choose when people buy personal auto insurance policies from Allstate. So you can choose like you want comprehensive coverage etc. So this turned out to be a very difficult competition for two reasons. Actually a lot of people hate this competition a lot. One [reason is that] the evaluation criteria is all or nothing. So you have to predict all seven correctly to get a point. If you predict anything wrong it’s zero. So basically partial answers get no credit for this one. We had a long debate on the Kaggle forum but the Kaggle people didn’t budge. I heard this story even Allstate was upset with us so even the sponsor wasn’t happy with us.So the [challenge] is to predict the final purchase option based on earlier transactions. But whichever option they choose for their last known transaction is actually a very good predictor for the final purchase option. So the model is that if you just use the trivial “last quoted” as the predictor no change. It’s right about 53.269%. You really need a lot of decimals to see the difference that’s why they’re decimals. And then I got the third place in the competition—I was able to improve that by 0.444% and then another one’s solution improved that by 0.03%. I’m proud to say that this is statistically significantly better. I don’t know if in business it is significantly better but statistically speaking it’s significant.The challenge to this is actually mostly on the targeted variable side so that A B C D E F G are not independent of each other actually every one of them can be predicted very well. More than 90% accurate. But if you put them together they have actually very interesting structures. And then the two challenges are to capture the correlation and not lose the baseline.So I build chained models for the dependency. So I first build a freestanding F model then assuming you know F you build a G model etc. And you can actually change the order of models to put your free models first and your independent models later. And this works quite well. This is a little bit more theoretically appealing because it’s a very systematic model. So the next one is so as not to lose the baseline we build twostage models. We build one model to decide: either use the baseline or the prediction. And then you want to use your prediction then use your prediction otherwise stick with the baseline. So at least for the dummy cases you do as well as the baseline and for where you can find improvement you improve it. So I think that’s how you can improve upon very strong baseline.To finish there are kind of a few trivially useful pointers. If you really want to learn how to build generalized models participate in the competition. Just watching it doesn’t count. Form my own perspective the more frustrated I am doing it the more I learn after. You really get a good learning experience that way. If you’re just observing as a spectator you really don’t get into the problems. They also post links to papers to books. I need to admit that I have no PhD my education was in engineering related but neither in statistics or computer science. So I learned a lot just reading books and Kaggle forums. One book that really helped was Elements of Statistical Learning. It’s highly recommended. It’s like the bible of machine learning. It’s also freely available—legitimately—on the internet. Thank you.,NA,Featured Talk: #1 Kaggle Data Scientist Owen Zhang
https://nycdatascience.com/blog/meetup/overview-of-scikit-learn-machine-learning-in-python/,78,Regression:Classification(Discriminant Analysis):Classification(Tree based model):Classification(the others):Classification(Nearest Neighbors) :Classification(Naive Bayes):Unsupervised Learning:Feature Selection:CrossValidation:Model Selection:,NA,Overview of Scikit-Learn (Machine Learning in Python)
https://nycdatascience.com/blog/student-works/steps-toward-recreating-the-facebook-ipo-plot/,78,"big white blob appeared in the center of the map. Some of the outer edges of the blob vaguely resembled the continents but it was clear that I had too much data to get interesting results just by drawing lines.""",NA,Steps toward recreating The Facebook IPO plot
https://nycdatascience.com/blog/meetup/achieve-real-time-results-with-big-data-through-in-memory-computing/,79,"Speaker Bio:
Nikita Ivanov is founder and CTO of GridGain Systems a leading inmemory computing platform. He has over 20 years of experience in software application development building HPC and middleware platforms contributing to the efforts of other startups and notable companies including Adaptec Visa and BEA Systems. Nikita was one of the pioneers in using Java technology for serverside middleware development. Hi is an active member of Java middleware community and contributor to the Java specification.",NA,Achieve Real-time Results with Big Data through In-Memory Computing
https://nycdatascience.com/blog/student-works/minority-and-non-minority-business-creation-in-nyc-2005-2013/,80,H0: Medians of MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita are equivalent.H1: Medians of MBE  Minority cap and NonMBE  NonMinority cap are NOT equivalent.H0: MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita could be representative of the same set of data.H1: MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita could NOT be representative of the same set of data.,NA,"Minority and Non-Minority Business Creation in NYC, 2005-2013"
https://nycdatascience.com/blog/meetup/tableau-workshop-iv-beginner-level-maximize-your-visualization-effect/,80, What language is Tableau written in? According to their own site (and Wikipedia..) it’s a new language they created called VizQL – combination of SQL and MDX for dealing with both querying and visualizing. Their conference paper: ­,NA,"Tableau Workshop IV: Beginner Level, Maximize Your Visualization Effect"
https://nycdatascience.com/blog/meetup/hack-session-for-nytimes-dialect-map-visualization-sponsored-by-oreilly-strata-2/,80,"Speaker:
Vivian Zhang is CoFounder & CTO of Supstat Inc a Statistical Consulting firm of top Data Scientists. Vivian is a data scientist who has been devoted to the analytics industry and the development and use of data technologies for several years. You can follow her on Twitter at @ @ @ or #nycdatasciPreviously:",NA,"Hack Session for NYTimes Dialect Map Visualization, Sponsored by OReilly Strata"
https://nycdatascience.com/blog/meetup/github-workshop-ii-common-git-workflow-given-by-github-employee/,81,covered the common Git workflow including  publishing a project on GitHub and how to collaborate on an existing repository.,NA,Github Workshop II: Common Git Workflow (given by Github employee)
https://nycdatascience.com/blog/student-works/learning-r-analyzing-the-english-premier-league-iii/,81,. This part is going to concern with the fun part of analysis as in all the data has been cleaned and we're ready to look at team performance. You can see all code examples and the sample data on my  GitHub page.daplySeasonratiosdaply.(Season Visiting)rlerleeach cbindeachGoalsOppGoalscolwiseyGoalsOppGoalsxSeason,NA,Learning R: Analyzing the English Premier League (III)
https://nycdatascience.com/blog/meetup/r-workshop-xix-mixture-topics-of-r-scrap-web-pages-make-treemaps-predict-rain/,82,"Laila showed her analyses focused on the restaurant sanitation in New York City. Several main tools were implemented in her project including craigslist pet maps R google fusion table pet finder and shiny application.
",NA,"R workshop XIX: Mixture topics of R-scrap web pages, make treemaps, predict rain"
https://nycdatascience.com/blog/student-works/where-are-my-single-friends-making-the-ok-cupid-algorithm/,82,Contributed by Harrion Alder. Harrison took R002 class with Vivian Zhang(Data Science by R Intensive beginner level) in MarApr 2014 and did great in class.,NA,Where are my single friends? Making the ‘OK Cupid’ Algorithm
https://nycdatascience.com/blog/meetup/hadoop-workshop-ii-run-map-reduce-jobs-on-your-amazon-cloud-cluster/,83,In Hadoop workshop I and II I will walk you through the steps to configure a Hadoop cluster on Amazon EC2 and run two simple mapreduce jobs on the cluster.,NA,Hadoop Workshop II: Run Map Reduce Jobs on your Amazon cloud cluster
https://nycdatascience.com/blog/meetup/authors-for-open-data-and-technology-ii-being-a-data-skeptic-doing-data-sci/,83,This talk focused on the current scope and dangers of data science. The speaker also discussed her two new books.,NA,Authors for Open Data and Technology II: Being A Data Skeptic & Doing Data Sci
https://nycdatascience.com/blog/meetup/excel-workshop-ii-warm-up-session-for-excel-intermediate-level-weekend-class/,83,"Please have an installed version of Excel ready to go.
Macs: The sample files will work with MACS although the screen interface may vary from the ones displayed.",NA,Excel Workshop II: Warm Up Session for Excel Intermediate Level Weekend Class
https://nycdatascience.com/blog/meetup/node-js-workshop-i-get-started-2/,84,Have node installed on your computer. Highly recommend using a Mac and using brew to install node however  has instructions to get up and running on any platform. If you can run 'node' and 'npm' at your command line you should be all set. ,NA,Node.js Workshop I: Get Started
https://nycdatascience.com/blog/meetup/r-workshop-x-interactive-documents-from-r/,84, Ramnath Vaidyanathan Advisory Data Scientist our rCharts/Slidfy/Shiny Instructor. Dr. Vaidyanathan is an Assistant Professor of Operations Management at McGill University.  He holds a Ph.D. in Operations Management from the Wharton School of UP and worked as a Business Analyst at McKinsey & Company before advising SupStat. He has great passion for R and has developed two R packages  and  both aiming at simplifying the creation and sharing of interactive webbased content with R.,NA,R Workshop X: Interactive Documents from R
https://nycdatascience.com/blog/meetup/r-workshop-iii-data-visualization-with-ggplot2-second-time/,84,Vivian introduced 7 basic concepts in ggplot2(mapping scalegeometic statistic coordinatelayer facet) and go over techniques to draw point barhistogramline tile and map drawing.Vivian Zhang is cofounder and CTO of a statistical consulting firmSupStat Inc. She got double Master degress in Computer Science and Statistics. She focus on business analytics and big data technologies and is a dataaholics visualization evangelist and programmer. She worked for several wellknown research institutes in the past five years and published a few papers with top scholars. The most recent publication is about effective statistical method design to reducing HIV test cost and is accepted by Journal of the American Statistical Association(JASA). Download Rstudio and ggplot2 R package.,NA,R workshop III: Data Visualization with ggplot2 (Second Time)
https://nycdatascience.com/blog/meetup/what-i-learned-from-100000-open-data-across-100-open-data-portals/,85,"He also talked a bit about brainstorming and six thinking hats. Then people did a couple of exercises.  Choose an open data catalog. Diagram how a person could manually download all of the datasets. Then change the labels in the diagram so that it describes a computer program that downloads the datasets.
 Select a guideline from one of these lists and brainstorm ways of testing it. ",NA,"What I Learned From 100,000 Open Data Across 100 Open Data Portals"
https://nycdatascience.com/blog/meetup/d3-js-workshop-iii-intro-to-binding-data-to-the-dom/,85,"Adam Pearce is a Data Interaction Developer for . He posts data visualizations on his  and frequently answers questions about D3 on .D3 is built on top of CSS HTML and Javascript. If you're not familiar with them check out . Before coming make sure to download these  and to have Chrome + .The creator of D3 has a several excellent tutorials that cover similar ground:


",NA,D3.js Workshop III: Intro to Binding Data to the DOM
https://nycdatascience.com/blog/meetup/policy-in-practice-series-councilwoman-brewer/,86,Another useful resource for what NYC is doing on technology and where there is work left to do can be found here: .,NA,Policy in Practice Series: Councilwoman Brewer
https://nycdatascience.com/blog/meetup/python-workshop-ii-pandas-for-data-analysis-no-python-basic/,86," We dived into using Pandas to do some analysis and visualization directly.
 We broke into groups to give everyone a chance to apply the material from the talk.Bring your favorite editor and python shell. Install pandas with 'pip install pandas'.",NA,Python Workshop II: Pandas for Data Analysis (no Python basic)
https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-4th-time/,86,This was a repeated session of August 29th July 8th and July 29th “R workshop I: R basic”. You could find useful information from comments at those events.Chris Whong Socrata(Vendor of NYC Open Data Portal). Individual hack time people could make sure all the R codes run on their own PC before they go.,NA,R Workshop I: R Basic (4th time)
https://nycdatascience.com/blog/meetup/visualizing-docgraph-in-gephi/,87,Janos demonstrated how to extract a subset of doctors that are practicing in New York City and how he combined DocGraph with a modified version of the federal NPI database. He also talked about some of the challenges and pitfalls of working with DocGraph and other healthcare datasets.,NA,Visualizing DocGraph in Gephi
https://nycdatascience.com/blog/meetup/kaggle-talk-series-top-0-2-kaggler-on-amazon-employee-access-challenge/,87,Introduced our solution to the Amazon Employee Access Challenge.The Kaggle competition The Hewlett Foundation: Short Answer Scoring and the winners' solutions.,NA,Kaggle Talk Series: Top 0.2% Kaggler on Amazon Employee Access Challenge
https://nycdatascience.com/blog/meetup/nyc-open-data-portal-one-mobile-app-using-the-data/,87,Andrew introduced us to NYC OpenData () and NYC Developer Portal (). He also touched on NYC Open Data licensing problem. Kevin shared his experience using NYC open data and developing mobile app. He addressed some technical challenges he faced when building the app which were iPhone programmingspecific. His deck is .,NA,NYC Open Data Portal + One Mobile App Using the Data
https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-2nd-time/,87,Founder President and our R and Hadoop Instructor. Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. She obtained expertise on data analysis and data management using various statistical analytical tools and programming languages. She cofounded SupStat founded NYC Data Science Academy and is an organizer of . Prior to taking entrepreneurial steps she worked as a Senior Financial Analyst at Memorial SloanKettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received her continued education in Statistics and Computer Science from Brown University Stanford University and Double Master Degrees from Stony Brook University and San Jose State University. She likes to portray herself as a dataholic visualization evangelist and programmer.,NA,R Workshop I: R Basic (2nd time)
https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-3rd-time/,87,"sponsored us their office space for this ""R workshop I: R basic"" event.Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. She obtained expertise on data analysis and data management using various statistical analytical tools and programming languages. She cofounded SupStat founded NYC Data Science Academy and is an organizer of . Prior to taking entrepreneurial steps she worked as a Senior Financial Analyst at Memorial SloanKettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received her continued education in Statistics and Computer Science from Brown University Stanford University and Double Master Degrees from Stony Brook University and San Jose State University. She likes to portray herself as a dataholic visualization evangelist and programmer. Vivian explained what R is what R can do how R compare to other tools you might already know and use. And She explained some R codes.
 Individual hack time people could make sure all the R codes run on their own PC before they go. to install Rstudio before you come. You can download Rstudio from 
 to get yourself a free github account so you can get some R sources right away. sign up at",NA,R Workshop I: R Basic (3rd time)
