title,page,link,post.x,shares.x,post.y,shares.y
Predicting House Prices with Machine Learning Algorithms,1,https://nycdatascience.com/blog/student-works/housing-price-prediction-using-advanced-regression-analysis/,Intuitively which of the four houses in the picture do you think is the most expensive?Most people will say the blue one on the right because it is the biggest and the newest. However you might have a different answer after reading this blog post and discover a more precise approach to predicting prices. In this blog post we discuss how we use machine learning techniques to predict house prices.  The dataset can be found on. The dataset is divided into the training and test datasets. In total there are about 2600 rows and 79 columns which contain descriptive information on different houses (e.g. number of bedrooms square feet of the first floor etc.). The training dataset contains the actual house prices while the test dataset doesn’t. The house prices are rightskewed with a mean and a median around $200000. Most houses are in the range of 100k to 250k; the high end is around 550k to 750k with a sparse distribution.Most of the variables in the dataset (51 out of 79) are categorical.  They include things like the neighborhood of the house the overall quality the house style etc. The most predictive variables for the sale price are the quality variables. For example the overall quality turns out to be the strongest predictor for the sale price. Quality on particular aspect of the house like the pool quality the garage quality and the basement quality also show high correlation with the sale price.The numeric variables in the dataset are mostly the area of the house including the firstfloor area pool area number of bedrooms garage area etc. Most of the variables show a correlation with the sale price.One challenge of this dataset is the missing data. For missing data such as pool quality and pool area  where a missing value means there is no pool in this house  we replace the missing value with 0 for numeric variables and “None” for categorical variables. However for missing data that are missing at random we use other variables to impute the value. Dealing with a large number of dirty features is always a challenge. This section focuses on the feature engineering (creating and dropping variables) and feature transformation (dummifying variables removing skewness etc.) tasks. GarageYrBlt (year the garage was built) and YrBlt (year the house was built) had a very strong positive correlation of 0.83. In fact more than 75.8% of these values were exactly the same. Hence we decided to drop GarageYrBlt since it had many missing values which could be compensated by YrBlt.Because we have to work with so many variables we introduced the use of regularization techniques to address the issue of multicollinearity found in our correlation matrix and the possibility of overfitting using the multiple linear regression model. We address that in the exploratory data analysis section. The great thing about regularization is that it reduces the model complexity as it automatically does the feature selection for you. All the regularization models penalize for extra features.Regularization models include (Lasso Ridge and Elastic Net). The lasso model will set coefficients to zero while the ridge model will minimize the coefficients making some of them very close to zero. Elastic net is a hybrid of both the lasso and ridge model. It groups correlated variables together and if one of the variables in the group is a strong predictor then it will include the entire group into the model. The next step is to tune the hyperparameters of each model through the use of crossvalidation. We choose alpha  .0005 for the Lasso model and alpha  2.8 for the Ridge model. We choose alpha  .0005 and L1_Ratio  0.9 for Elastic Net. Because Elastic Net with a L1_Ratio of 0.9 is very similar to the Lasso model which has a default L1_Ratio of 1 we do not depict it here.Positive coefficients for Sale Price: Above Grade Living Area Overall Condition and the Neighborhoods (Stone Bridge North Ridge and Crawford).Negative coefficients for Sale Price: MS Zoning Neighborhood Edwards and Above Ground Kitchen.Positive coefficients for Sale Price: General Living Area Roofing Material (Wood Shingle) Overall Condition.Negative coefficients for Sale Price: The General Zoning requirements Proximity to Main Road or Railroad and the Pool Quality being in Good condition.The two graphs below show how accurate our model prediction is for the sales price vs the actual price. Dots closer to or on the red line show how accurate the model prediction was. There are some outliers that we should investigate as future work on the model. Gradient Boosting Regressor was one of our best performing algorithms. We first trained gradient boosting machine using the entire set of features (baseline model). We performed crossvalidation with parameter tuning using GridSearchCV function from scikitlearn package for Python. Our best model parameters were: learning rate of 0.05 2000 estimators and max depth of 3.  We created a relative importance chart to visualize feature importance in gradient boosting. Feature importance scores indicate how useful each feature is in the construction of the boosted decision tree. Above Grade Living Area Square Feet Kitchen Quality Total Square Feet of Basement Area and Size of Garage in Car Capacity were among most valuable features.  We then attempted to improve our baseline model performance by reducing the feature dimensionality. High dimensional data can be sparse or spread out which makes it harder for certain algorithms to train effective models. In general predictive algorithms benefit from optimal nonredundant subset of features that improve the rate of training as well as enhance interpretability and generalization. We managed our machine learning workflows with scikitlearn Pipelines. Scikitlearn Pipeline class allows us to apply a series of data transformations followed by the application of an estimator. We built several pipelines each with different estimator (e.g. Gradient Boosting Regressor Linear Regression etc.) For Gradient Boosting Machine our pipeline included:After we completed feature engineering we had over 200 features and about 1500 rows in our training set. We decided to keep 150 principal components after examining cumulative percentage of variance chart. 150 components accounted for over 85% of variance of our data. Variance measures how spread out the dataset is. Not all tweaks improve results. After we implemented PCA our cross validation scores did not improve. In fact our scores deteriorated (cross validation score declined to 0.87 from 0.91 for baseline model). We believe that reducing dimensions caused loss of some important information. PCA not only removed random noise in our data but also some valuable inputs. For Multivariate Linear Regression our pipeline included:Using PCA with Multivariate Linear Regression did not produce good results as well. Our cross validation score decreased as compared to a baseline model (training Multivariate Linear Regression using the entire set of features).  Using single isolated models gives us a decent result. But usually all real life problems do not have a direct linear or nonlinear relationship with the target variable that can be captured alone by a single model. An ensemble of conservative and aggressive linear and nonlinear models best describes the housing price prediction problem. To begin with we tried a simple ensemble model of XGBoost (nonlinear) and ENet (linear) with a 5050 weightage. Next following the standard stacking approach we stacked different models to see if we could do better. Our stacked model consisted of the linear ENet model a conservative random forest of short depth a fullygrown aggressive random forest a conservative gradient boosting of short depth and finally a fullygrown aggressive gradient boosting model. The performance has been recorded below:The below correlation heatmap shows our predicted sale prices for some of the models used. We can see that Elastic Net Lasso and Ridge were very similar in nature whilst Ensembling and Stacking was also very similar. The one standalone model with distinctly different results was the XGBoost one.,NA, Usually it makes sense to delete features that are highly correlated. In our analysis we found out that It sometimes makes sense to engineer new features that can help increase the model’s performance. We created the following two new features:XG Boost model was our best performing model while multivariate linear regression was our worst performing model. The results were similar among Ridge Lasso and Elastic Net models.,NA
Spilling the Beans on Trade Coffee,1,https://nycdatascience.com/blog/student-works/spilling-the-beans-on-trade-coffee/, curated collection of seasonal freshly roastedtoorder coffees. I was curious what this new homedelivery service had to offer me and thought perhaps I could further investigate.470 products down to 452.An NLP analysis on the product descriptions and flavor profiles gave a picture on what products one might find on Trade Coffee. the NLTK package was used to process the words from all the product descriptions and flavor profiles. In order to get an informative word cloud I added words such as 'coffee' 'flavor' 'taste' 'varietal' into the dictionary of stop words that remove redundant and noninformative words from the description content.,NA,The National Coffee Association National Coffee Drinking Trends (NCDT) report is the coffee industry’s leading survey of US coffee consumer behavior and attitudes. According to the NCDT more Americans are drinking a daily cup of coffee than they have been for the past six year. Up 2 percent from last year 64 % Americans are drinking their daily cup of coffee. Since 2001 the NCA began tracking US consumption of specialty coffee which is defined by the NCA as “coffee drunk hot or iced that is brewed from premium whole bean or ground varieties. This includes espresso based beverages iced/frozen blended coffee cold brew and iced coffee infused with nitrogen.”There clearly has been a consistent growth in the specialty coffee market.The US specialty coffee consumer drinks an average of 2.97 cups off coffee per day. The US market share of specialty coffee has grown significantly over the last 7 years from 40% in 2010 to 59% at present.Americans consume the most coffee in the world and spend an average of $3.16 for a cup of coffee. The average American spends $1110 on coffee annually. When taking into account the price one has to pay for a cup of specialty coffee the number is even bigger. For instance one can find America's most expensive cup of coffee in Brooklyn NY which goes for $18 a cup.As a coffee consumer myself I am always on the lookout on ways that I can save money and yet still enjoy a great cup of single origin coffee without breaking the bank.As an alternative to buying from specialty coffee shops home brewing has allowed me to save hundreds of dollars and indulge in some of the best cups coffee I have put my hands on. As I was searching for my next cup of coffee I came across Trade Coffee  a website that allows consumers to purchase direct from different roasters across the country and get them freshly roasted on their doorstep.The Trade Coffee website boasted aAs I began my inquiry these were some of the questions I had in mind:The web scraped that I needed to build had to scrape the main product Trade Coffee page and the individual product pages. I initially built a Scrapy spider to parse through the web site but ran into some issues such as missing data hidden elements. As a result I transitioned to Selenium to scrape the elements on the pages that I was interested in (product name roaster description price weight etc.). Some of the more detailed product information had to be put under one 'basket' as these elements did not have unique Xpaths.Data cleaning was an important step in making the data easy to work with and manageable. Duplicate entries were removed. Empty strings were removed or filled with a marker if it was necessary in the analyses downstream. Price and weight data entries were converted into numeric vectors. Unnecessary punctuation and other artifacts were removed.  This narrowed down the number of products to be analyzed : Trade Coffee offers beans in different package volumes and it was necessary to convert these into similar units. In order to normalize price data I decided to use the price per cup data as a measure that can be compared across all products. Following my own brew recipe: I set 20g as the amount of coffee beans used per 300mL of coffee. I used this to calculate the number of cups I could get from a single bag of coffee. Most products on Trade Coffee yields 17 cups of coffee.Trade Coffee partners with 51 roasters from 39 cities offering 14 profiles of Coffee which can be either one of 261 Single Origins or 209 Blends. Offering the consumer a great variety of options to choose from. The average price per bag is $18.92 and price per cup is $1.05Diving deep into the data I wanted to look at patterns and relationships specifically if there was a significant price different between Single Origins and Blends. As I expected Single Origin coffees were priced higher on average than the Blends. The same trend follows when price per cup data is looked at. There were several single origin coffees that were far from the median and mean price per cup of coffee. The highest price per cup of coffee was at $6.00 a cup or $85.00 per bag . The average for a single origin coffee is $1.16 per cup and $17.40 per bag. The average for a blend coffee is $0.92 per cup or $ 15.49 per bag. These numbers show that it is much cheaper to brew a cup of specialty coffee at home than going into a coffee shop.Trade coffee offers 14 different profiles of coffee. Below is a look into the breakdown of their products into these categories.Below are word clouds of the flavors describing the different product on Trade Coffee. One is using the short and more simple flavor notes and the second was created with the more descriptive and verbose roaster notes and descriptors.Trade Coffee has a wide variety of coffees for a all types of home brewers. Consumers have the option to purchase budget budget friendly coffees as well as special limited and seasonal lots and roasts. The findings of this venture reaffirms that homebrewing is a costeffective way of enjoying specialty coffee in the comforts of one's home without making a big dent on the wallet.It is important to note that Trade Coffee regularly changes their inventory depending on the roster of roasters on the site and most especially the seasonality and availability of certain coffees throughout the year. It would be interesting to see trends in their product offerings as well as how often these inventory changes occur.I would like to take a closer look into the specific details of products such as varietal origin subregion in addition to the price points and shipping costs.,NA
Variation in Hospital Charges and Medicare Payments for Inpatient Procedures in the United States,1,https://nycdatascience.com/blog/student-works/variation-in-hospital-charges-and-medicare-payments-for-inpatient-procedures-in-the-united-states/,U.S. healthcare costs have been on the rise over the past several years outpacing the growth of the economy overall. The Centers for Medicare and Medicaid Services (CMS) estimates that American healthcare spending increased by 4.6% in 2017 to reach $3.5 trillion. The increases in medical care are  driven by the Medicaid expansion the private insurance market as well as the aging population. Consequently h,NA,ealthcare spending is projected to continue to grow at an even faster pace over the next decade. According to a  report prepared by the CMS healthcare spending in the U.S. is projected to grow on average by 5.5% per year from 2017 through 2026 to reach $5.7 trillion by 2026. Healthcare spending is projected to reach 19.7% of GDP by 2026 up from 17.9% in 2016.In the current state of continued uncertainty around the future of the Affordable Care Act and healthcare reform financing policymakers are becoming more interested in the role of prices in driving healthcare spending.Hospitals are usually billing for more than what they expect to receive for services provided. Actual payments differ from billed charges because Medicare and Medicaid set reimbursement rates that they will pay for procedures while commercial payers negotiate rates with hospitals.Historically prices paid by private health insurers have remained highly confidential in the country.In 2013 in an effort to increase price transparency the CMS began providing Medicare claims and payment data for most common inpatient diagnoses at over 3300 hospitals. Each year the data has continued to show dramatically different prices for the same procedures at different hospitals. We have analyzed the CMS data for fiscal year 2015 to measure inpatient charge and payment variability. Our analysis has shown that wide variations in hospital charges and payments for the same services persist.The dataset is owned by the CMS and is part of the inpatient utilization and payment public use file. The dataset provides information on utilization payment (total payment and Medicare payment) and hospitalspecific charges for over 3000 U.S. hospitals that receive Medicare Inpatient Prospective Payment System (IPPS) payments. The dataset includes about 201k rows and 12 columns. The data is organized by hospital and Medicare Severity Diagnosis Related Group (MSDRG).“Average Charges” refers to what the provider bills to Medicare.“Average Total Payments” refers to what Medicare actually pays to the provider as well as copayment and deductible amounts that the beneficiary is responsible for.The Medicare data app is an interactive dashboard that visualizes hospital charge and payment variability across the U.S.Average hospital charges and payments differ across the U.S. with many procedures costing more in some states than others. We have ranked states by average Medicare payments average total payments as well as by average covered charges. States with the highest hospital charges are California New Jersey Pennsylvania Colorado Nevada Florida and Alaska while states with the highest average Medicare payments are Maryland California Alaska New York Washington Massachusetts and Hawaii.Our analysis has shown that among the most common Medicare hospital discharge diagnoses in 2015 were septicemia major joint replacement heart failure and shock kidney & urinary tract infections as well as simple pneumonia & pleurisy.Among the most expensive Medicare inpatient discharge diagnoses in 2015 were heart transplant or implant of heart assist system ECMO liver transplant allogeneic bone marrow transplant extensive burns or full thickness burns as well as lung transplant.Our analysis of the CMS data has shown that wide variations in hospital charges and Medicare payments for the same services persist. For instance average Medicare payments for heart transplant or implant of heart assist system ranged from about $240k to about $430k in California and from about $120k to $180k in Florida.Major joint replacement surgery is another example of variation in hospital charges. Average Medicare payments for major joint replacement or reattachment of lower extremity surgery ranged from about $15k to about $48k in California and from about $12k to $26k in Texas.In 2015 on average hospitals charged 5.2 times what Medicare reimbursed them for the same procedure (standard deviation was 2.75) with a range of 0.09 to 42.For example the median charge to reimbursement ratio for heart transplant or implant of heart assist system surgery was about 4.4x with a range of about 3.3x to 7.5x in California. For the same procedure the median charge to reimbursement ratio in New York was approximately 3.5x with a range of about 2.5x to about 3.6x.The difference between charges and reimbursement rates remains largely unexplained. It would be interesting to conduct further research to identify hospital characteristics associated with higher charged rates compared to actual Medicare reimbursed rates.It looks like there is a strong positive correlation between what hospitals charge and what they are paid by Medicare. According to our analysis correlation between average covered charges and average Medicare payments is 0.83.Email:,NA
Will the NBA Player's salary contribute to the team’s Win?,1,https://nycdatascience.com/blog/student-works/will-the-nba-players-salary-contribute-to-the-teams-win/,"Which team will win the championship this season? Who will win the MVP?   Many sports stars surprise people with huge amounts of contracts.
However So are their salaries contributing to the team's victory?
My topic is to learn about the relationship between NBA games (WL) and salary using Web Scrap.  
Modern basketball is evolving right now.  Because of the unique nature of basketball we can not simply say that the salary contributes to the team win.  There are also many invisible indicators such as mental and leadership for victory. It is difficult to relate salary and victory but I think it is also fun to watch sports.",NA," Ok. Let's check Team Record
The bar graph represents a team win. Houston on the left means the highest win  Phoenix the lowest winner.
The red dotted line means Salary Cap.As you can see the high salary of the team doesn't lead to the highest victory.
GSW / CLE / OKC etc. invested heavily in the championship and relatively higher HOU / TOR than their investment. IND had a high multiplier with low salary. This is also a remarkable point.This page is Scatter Plot.
The xaxis indicates the team wins and the yaxis indicates a player's Salary
This graph shows the salary distribution of the overall team players. A relatively lowefficiency player that can't contribute to a team win is also noticeable. But I also know that basketball is a team sport so I can not claim victory only for those players.The key player with a high playing time is a lot of money. On the other hand a lot of money players are forced to play a lot. Therefore A player who receives a lot of money has a lot to do with team wins.Let's look at the correlation between team win and salary very simply.
W means the number of wins when the player participated in the game. If you look at the relationship you can't confirm that the player with the highest salary contributes to the team victory.So how can you confirm that you have contributed to the team win?
There are a lot of Stats in Basketball and it's hard to check them one at a time.
In a similar concept the indicator WAR is officially used in baseball but the index I use today is Win share.
WS is an indicator of how much it has contributed to the team's victory with various stats entering the element and the formula on the right is one of them. We add the attack index and the defense index to derive a comprehensive win share.So let's take a look at WS Top 10 for the season.
First place is James Harden of Houston and as you can see by numbers and salary he is the NBA superstar.
In addition He was MVP last Season. It is the players that contributed a lot to the team victory as there are a lot of salaried people. And the high salaried people are ranked in the highest rank.Let's look at the relationship between Win Share and Salary.
As you can see the salary and WS trends are the same but there is often a Low Efficiency Player.
On the other hand with a small salary you can look for players who contribute significantly to your team.Let's look at the graph. Yokochi shows a remarkable contribution to the team even with a small salary even if you look at the graph. And one person who stands out is Spencer Dinwiddie. In fact he is not known but he is a player with enough talent to be a skill champion in NBA AllStar.
As you can see from the record impressive players were selected.Let's enjoy the NBA 1819!Thanks",NA
Forecasting Cryptocurrency Price Trends,2,https://nycdatascience.com/blog/student-works/forecasting-cryptocurrencies-price-trends/,"A cryptocurrency's price is mainly influenced by security problems of the blockchain technology new policies of governments (for regulation or boosting) and public opinion from news and forums. The plot below describes that the price trends of Bitcoin (the first cryptocurrency and the one with largest market capitalization) and the frequencies of key words from news. You can see the period from late 2017 to early 2018 when the price of Bitcoin steeply went up and suddenly collapsed.  The cryptocurrencies are increasingly adopted as a means of payment in real life. Thus each government has been considering various regulations in different ways.  In case of US government was basically in the position to regulate cryptocurrencies within the framework of existing financial regulations. As several large asset managers consider investments in the cryptocurrency market the government is trying to tighten regulations for financial supervision. In case of Japan cryptocurrency transactions and exchanges were prevalent during early days of cryptocurrency. So the government has created policies and refined them since 2014. Similarly Singapore government also has refined policies about cryptocurrency. The government defined the cryptocurrency as “Good purchased product for purchasing goods”. And they created specific tax policy imposing on transactions of cryptocurrencies since 2014. On the other hand China and South Korea have considered several regulations such as prohibition of exchange of cryptocurrency and some of the regulations have been implemented. Of course the complete prohibition was impossible.
Currently the market capitalization of cryptocurrency is 199 billion dollars. (for comparison the 2017 US Defense Budget was 590 billion dollars)
 The cryptocurrency has not only negative side but also has positive side. On the positive side it can be an alternative to the existing financial system. In case of XRP (as known as Ripple) They entered remittance business by taking advantage of the fact that there is no commission fee for oversea transactions. On the negative side the cryptocurrency still has security problems. One of the largest exchange Bithumb was hacked and 31 million dollars were stolen. Bithumb is an exchange ranked 6 in the world.Then the centers (mean) of each of the 3 clusters were computed and their relative distances were evaluated by cosine function: 1 cos(𝞱) taking values in [0 2]. Here we can see from the heated map that each of the 3 clusters is perfectly collinear with itself (distance  0) cluster 1 and 2 are mostly anticollinear (antiproportional) while cluster 0 is mixed. Then we labeled 3 clusters as D (down) M (mixed) and U (up) based on the corresponding position of the cluster center then computed the possibilities of D/M/U of the next day based on today’s label. We can conclude that for each case the prediction of M is always the majority case.Then we continue to use PCA to visualize the division of the 3 clusters as shown above including the compositions (weights) of the first and second principle components (PC) of each of the 8 features (cryptocurrencies). The scatter plot shows how the ~3000 points (days) in colors are determined by the 1st and 2nd PC. The boundary is not parallel to either axes indicating both PC’s are import.We considered two different architectures for the LSTM network. The first one considered the entire Bitcoin price history as a long chain: the LSTM network would remember all the intermediate states. The second one only considered rolling windows of a fixed size: the LSTM network would start over with clean states for each window. The graphic representations of the architectures could be found below:For interested readers our codes and notebooks could be found .",NA,"From Those features show some interesting trends comparing with Dow 30 which shows more distinct market movement clusters based on the different industry background while cryptocurrencies are mostly lead by BTC. And if we look closer to their background XRP price is mainly used for remittance and BCN is based on anonymous exchange. The differences among price movements among cryptocurrencies are related to their origins too.The details of both PC’s are shown in bar plots. In both cases BCN has the largest weight while other features are in similar scale with either same or opposite sign. It is consistent with previous hierarchical results that BCN has distinct market movement from most of the rest and ends up with a special indicator. However the leading factor BTC only shows as the least weighted features in both of the PC’s. It is probably because of the fact that BTC has much longer history than the others do. Therefore has less daily log return fluctuation than those that were just created for the time period selected for the analysis.We chose Bitcoin price to do time series analysis because it has longest history and is the ""bellwether"" of cryptocurrency market. We firstly applied classical (linear) time serise model which requires stationary time serise with constant statistical properties (mean variance etc...) to make better predictions. Therefore we transformed the original data into log and log difference to examine the stationary property by the pvalue of the DickeyFuller test. The figure below shows the results for Bitcoin price in monthly average.The hidden periodicity inside the original and transformed data were decomposed by STL (Seasonal and Trend decomposition using Loess method). The third row shows that there is strong yearly trend inside the Bitcoin price movements. The original data shows highest Dickey Fuller's test. However further transformations did not continue to reduce the pvalue under 0.05 to reject the null hypothesis. Therefore we let the integrate part in ARIMA (autoregressive integrated moving average) model to do the differencing & integrating for us on the logtransformed data.We also examined the ACF and PACF to see how many days' lag would impact current Bitcoin price. The ACF plot in the left below shows that there is still strong correlation between current price and the one about 30 months ago and that there are some periodic pattern among those days that have negative correlations. Since longterm effect can hide and incorporated into recent correlations the PACF in the right reveals the true correlation: There are suprisingly huge correlation between current price and those several years ago. To include the yearly trend and the longterm influence of the time series we incorporated the ""seasonal"" factor into our ARIMA model known as ""SARIMAX"". Therefore besides the fundamental () for the ARIMA part there is also a set of ( 12) for the seasonal part (here ""12"" corresponds to the yearly trend shown in the STL plot since the data used here are monthly average). After grid search we found the optimized parameter of our SARIMAX model: (1 1 2) × (0 1 1 12). The four plots below analyze the residual after applying the chosen parameters. There is no long or shortterm trend remaining in the ACF. However the histogram plot (upper right) shows that the residual is not perfectly normally distributed: it has a long right tail. The plot below shows the comparison between the real and predicted Bitcoin price. The log prices in the middle panel is predicted by the SARIMA model the upper one is the prices transformed back by exponential and the bottom one is log return the signs of which indicate the increase/decrease of the price. The blue shadows indicates the 95% probability band which literally says that any price is possible for the actual price in the upper panel in the next few months. However besides the absolute price values we are also interested in how the model predicts the price increase/decrease. The overall accuracy of the predicted signs of the monthly return is 0.58 with more details included in the table.The main shortage in (S)ARIMA model is that its predictions are only based on the price of a certain cryptocurrency itself. However in reality there are many outside factors that can have huge impacts on the Bitcoin price such as stock indices market volatility and metal prices. The news media explosure and people's reaction can also influence/reflect in the cryptocurrency prices. VARMAX model can incorporate different outside factors to improve the predictions. Meanwhile since Bitcoin is the leading factor among most of the other cryptocurrencies using Bitcoin itself as an inner factor to help predicting the prices of other cryptocurrencies may also be a useful method.
We applied the VARMAX model to predict Litecoin (LTC) and Bytecoin (BCN) prices with & without using Bitcoin (BTC) as inner factor. The outside factors we use are: BTC volume S&P 500 Nikkei 225 Stoxx Europe 600 DXSQ VIX Volatility Index and the prices of gold and silver. Since VARMAX model does not include the integrate part we directly used weekly log return as the time series data.The comparison between the real and predicted weekly log return values for both LTC and BCN are shown in the plots. The overall accuracy of the predicted signs of weekly log return are shown in the left table. Despite the fact that LTC has very similar market movement while BCN doesn't both of their prediction accuracy are improved after using BTC as an inner factor. However the improvement is marginal. One possible reason is that we took log return for all the vectors. But for metal prices which do not have significant fluctuations they may not need log transformations. Therefore both of our VARMAX model and inputs need further careful optimizations.Finally we tried to use recurrent neural network long shortterm memory (LSTM) network specifically to model the price movement of Bitcoin. We made use of both numerical data (Bitcoin price volume international stock index prices commodity prices interest rates and CDS volatility index) and text data (news articles scraped from  and ). For the numerical data we took log return to make the scales uniform. For the text data we used a pretrained module to embed the sentences into 128 dimensional vectors. Then we trained a neural network using the sentence vectors to predict price movement and we extracted the last hidden layer (a 16 dimensional vector) and add it to the numerical data to form the input of the LSTM network.
However our models did not provide meaningful predictions on test set : the models tended to fit the training set very well and failed to generalize to unseen data.",NA
Machine Learning Project: Ames Housing Dataset,3,https://nycdatascience.com/blog/student-works/machine-learning/machine-learning-project-ames-housing-dataset/,The Ames Housing Dataset was introduced by Professor Dean De Cock in 2011 as an alternative to the Boston Housing Dataset (Harrison and Rubinfeld 1978). It contains 2919 observations of housing sales in Ames Iowa between 2006 and 2010. There are 23 nominal 23 ordinal 14 discrete and 20 continuous features describing each house’s size quality area age and other miscellaneous attributes. For this project our objective was to apply machine learning techniques to predict the sale price of houses based on their features.In order to get a better understanding of what we were working with we started off with some exploratory data analysis. We quickly found that overall material and quality (OverallQual discrete) and above ground square footage (GrLivArea continuous) had the strongest relationship with sale price. Using a widget (shown below) we then examined the remaining features to get a sense of which were significant and how we would be able to feed them into linear and treebased machine learning algorithms. 53 of the original features were kept in some fashion.  For categorical features the majority were handled using onehot encoding with minority classes below a certain number of observations being excluded. Certain discrete features were changed to binaries if we found their presence to be more impactful than their frequency (eg. Number of fireplaces → Is there a fireplace?). Given the range of some of the continuous features in the data we found it useful to apply log transformations where appropriate like each house’s lot size (LotArea). Lastly there were some special cases like the selfexplanatory YearBuilt feature (figure below).  We determined there to be no meaningful relationship with sale price for values prior to 1950 so we made that the minimum value. After this initial round of preprocessing and deciding to remove two outliers (with square footage >4000sqft. and sale price <$200k) we were well on our way to making our first set of models!This problem lends itself well to linear regression. In fact we can draw a simple regression line between above grade square feet and sale price that explains 54% of variance in sale price! This model produces a crossvalidation error of 0.273 in terms of Root Mean Squared Logarithmic Error (RMSLE).As a quick side note we choose to use RMSLE for model evaluation in order to match the scoring metric of the Kaggle competition. RMSLE ‘standardizes’ prediction errors between cheap and expensive houses so that we are not incentivized to build a model that predicts better (on a percentage basis) for expensive homes than cheaper homes. Practically it also makes our crossvalidation results a more accurate indicator for Kaggle scores.While we can obviously do better than a onevariable model the simplistic case highlights an issue that we will need to account for in linear regression. Inspecting the residual plot we can see a classic case of ‘fanning’ residuals. This violates one of the key assumptions of linear models  that the error term be identically distributed.The underlying issue is nonnormal distributions of both sale price and above grade square feet.Applying the boxcox transformation to both variables results in much more normal distributions and a regression on the transformed variables produces a much betterbehaved error plot. Surprisingly this improvement is not associated with a reduction of model error in the case of simple linear regression. However box cox transformation does result in a massive reduction in error when employed for multiple linear regression models.Of course we can start to improve our linear predictions by incorporating the influence of additional explanatory variables. There are strong (and obvious) relationships between some of the explanatory variables such as above grade square feet and above grade rooms and lot area and lot frontage. Regularization techniques will be critical for controlling for this multicollinearity.Indeed elasticnet regularization reduces crossvalidation RMSLE from 0.251 to 0.118. Elastic net ridge and lasso all performed equally well in crossvalidation.The variable coefficients from elastic net confirmed our insights from exploratory data analysis. House size and quality seem to be the most important variables for determining sale price.Going beyond linear regression we next tried fitting our data to treebased models. The simple decision tree below with a maximum depth of 3 gives an idea of the features on which our models could split and the breakdown of how our data might be divided:We initially feed our data to a straightforward decision tree regressor from the ScikitLearn Python package. The feature importance plot is shown below:While these last two are almost trivial examples they help us get a sense of the features that might be important for this class of models as well as visualize the importance of the hyperparameters we can tune for our treebased learners. So far we see that the features we hypothesized to be important from our exploratory data analysis and feature engineering are in fact significant. We do not spend much time on the untuned decision tree model even though it resulted in a 0.141 RMSLE training score and a crossvalidation RMSLE score of 0.181; these scores are better than we expected and they most likely are the result of our extensive feature engineering. Nevertheless we move on to a random forest model where we can tune hyperparameters for the number of trees the maximum tree depth the maximum number of features considered at a split the minimum number of samples required to make a split and the minimum number of samples required at a node. The feature importance plot for our random forest is shown below:The tuned random forest resulted in a training RMSLE of 0.122 and a crossvalidation RMSLE of 0.131. This score is much better than the single decision tree in large part because a random forest reduces the variance (overfitting) one sees when working with only one tree. However there is a higher bias with a random forest than with a decision tree. This is because only part of the training data is used to train the model (bootstrapping) so naturally higher bias occurs in each tree. Additionally the random forest algorithm limits the number of features on which the data can be split at each node which in turn means the number of variables with which the data can be explained is limited inducing higher bias. In an attempt to lower the bias we next look to a gradient boosted treebased model; the plots below show the difference between the tuned and untuned feature importance for this model.Even with the untuned model the crossvalidation RMSLE is 0.116 (training RMSLE 0.037) which is already better than the random forest. This is because the bias of the model is reduced because of boosting features. This model tries to reduce the error in predictions by for example focusing on poor predictions and trying to model them better in the next iteration and hence it reduces bias (underfitting). One can see the importance of tuning hyperparameters though when looking at the important features from the untuned model. There are some potentially collinear features such as LotArea vs. LotFrontage and GarageYrBuilt vs. YearBuilt that show up while some features shown to be important in our previous models such as OverallQual and OverallCond are absent. Yet by sequentially tuning the number of trees the max depth the min samples for a split the min samples for a leaf and then increasing the number of trees while simultaneously decreasing the learning rate we see less “redundant” features and more relevant features (OverallCond OverallQual Functional) with high importance for our model. The tuned model is the best individual model we trained with a training RMSLE of 0.082 and a crossvalidation RMSLE of 0.112. Another visualization of the difference between the tuned and untuned models is shown below.,NA, Despite being pleased with the performance of our standalone models we thought it would interesting to use this opportunity to explore some ensembling methods with the hope being that by combining several strong standalone models we could produce a metamodel that is a better overall predictor.Up to this point our highest performing models as judged by the Kaggle Public Leaderboard (PL) were an ElasticNet regression model (PL RMSLE 0.121) a tuned gradient boosting machine (PL RMSLE 0.122) and a tuned random forest model (PL RMSLE 0.145).  Our initial approach was to average the predictions from each of our three top models and attribute equal weight to each prediction.  Interestingly our score did not improve.  However we did see an improvement once we dropped the weakest link (random forest) and attributed equal weight to the ElasticNet and the gradient boosting machine.  This resulted in our strongest model up to this point (PL RMSLE 0.118).  We attribute this increase in performance to the increased diversity of our ensemble that results from dropping the 2nd treebased model.In our final model we decided we would explore stacking.  We opted to include each of our three top standalone models (ElasticNet Gradient Boosting Machine and Random Forest) as base learners and opted for a linear regression as our metamodel.As judged by the Public Leaderboard this resulted in our top model!  The RMSLE was 0.117 which as of the time of this publication was in the top 15% of submissions.Our largest takeaway from working with the Ames Housing Dataset was the value in careful thoughtful feature engineering.  We attribute the strong performance of our model to the time we put into this phase.  If we were to continue working with this dataset we would explore both the applicability and effectiveness of principal component analysis and multiple correspondence analysis in reducing dimensionality.  It would be interesting as well to explore the effectiveness of different strategic tuning parameters in our stacked models.,NA
"Predicting House Prices in Ames, Iowa Using Machine Learning",3,https://nycdatascience.com/blog/student-works/predicting-house-prices-in-ames-iowa-using-machine-learning/,"There are several factors involved with the sale price of a house. Some things that come to mind are size number of bedrooms year built neighborhood proximity to grocery store etc. In the Kaggle Competition: “House Prices: Advanced Regression Techniques” created by Dr. Dean De Cock our team explored a multitude of variables to further understand what influences the value of a house and how we can use these variables to predict the sales price given specific characteristics. 
The goals for this project were:The first thing after getting this dataset is to explore the data. This dataset contains all kinds of data type: numeric ordinal categorical and binomial. The entire data set (training + test) has 34 variables with missing data 13965 values out of 115340 values (12%). Some of the variables such as ‘PoolQC’ ‘MscFeature’ ‘Alley’ and ‘Fence’ have more than 80% data missing. The imputation of those columns could be a huge challenge without any leads. The natural way is to find how the original data set says about those variables. The data description has specific definitions for many of the ‘NA’s in the data set for example ‘NA’s in ‘Alley’ column simply indicates that there is no alley for the house and ‘GarageArea’ would be ‘NA’ too if there is no garage in the house. Therefore by looking at the data description 15 categorical and 10 numerical features can be imputed by either ‘None’ or number 0 depending on the original data type of the feature.After this round we only have 3425  9 columns left to be imputed. If we look at the numbers of the missing data among those features 8 of them are only missing less than 4 values which can be imputed by the most frequent values in their corresponding columns. Now the only feature left here is ‘LotFrontage’ the linear feet of street connected to property. Since there are 486 (17%) data missing a cautious way to handle it is to use kNN to help determine the values with the consideration of all other features including ‘Neighborhood’ and ‘LandContour’ that may have bigger influence on the street around the house. However in order to use kNN we have to wait until all other categorical features to be properly converted (labeled) with numbers.There are 79 features (36 numerical + 43 categorical). We examined each of them and divided them into 4 cases: A B C and D to handle them differently. Case A indicate 33 numeric features that do not need any further transformation. They are mainly area in square feet or quality levels that are already in ordinal numbers. Case B however are 3 numerical features that require closer look. By definition ‘MSSubClass’ is using integer numbers to indicate certain house types. Therefore they are actually categorical values. We converted this feature to categorical > numerical by sorting out their corresponding average ‘SalePrice’. ‘SalePrice’ does not have monotonic dependence on ‘MoSold’ or ‘YrSold’ either. So we also converted them in a similar way as ‘MSSubClass’.Case C consists 25 categorical features that are evaluating qualities or conditions. They can be intuitively labeled by ordinal integer numbers. Some of them such as LandSlope’ ‘LandContour’ and ‘Functional’ are also showing specific trends regarding certain house conditions. So they were also labeled based on their direct meanings.Case D includes the last 18 features with categorical values that are not showing obvious dependence with ‘SalePrice’ by the first look. For example it is hard to tell how the ‘RoofMat1’ ‘GarageType’ ‘SaleType’ or ‘Neighborhood’ would influence the house price. Instead of ‘onehotlabeling’ or ranking them based on value frequency we again use their corresponding average ‘SalePrice’ to sort and give them integer labels as we did to Case B.With all the other 78 columns filled and properly converted to numerical values it is time to revisit the last column with missing data. We chose to use Manhattan distance to impute ‘LotFrontage’ to reflect more subtle details from other data points and to reduce the influence from larger distance dominating the imputation results. kNeighbor  5 was determined by the R^2 resulted by a series tests of k numbers.
Three linear models were tested and used to predicted on the training data. While Ridge Regression tend to keep all the 79 features Lasso Regression dropped 24 features as unimportant. After the grid search Elastic Net Regression consisted 91% of Lasso. In all cases the ‘OverallQual’ ‘OverallCond’ and ‘GrLivArea’ are the three features that show most import positive influences on the ‘SalePrice’.Besides linear models two tree models Gradient Boosting and Random Forest as well as a nonlinear model Kernel Ridge Regression were also tested and used for house price prediction. The corresponding training error and Kaggle Public Board error are shown below. The training and PB errors show that overall our Gradient Boosting and Kernel Ridge models are making the best prediction linear models show slightly larger errors and that the Random Forest model produces largest errors. Therefore we chose Gradient Boosting (GBoost) Elastic Net (ENet) and Kernel Ridge (KRR) Regression for further model stacking.Before running into model stacking we examined the actual comparison between the predicted and true ‘SalePrice’ values along the y  x line for our three best models. ENet and KRR show similar data distribution features despite the fact that ENet has bigger errors at high price region mainly because they are both using l2squared error in the loss function. One of the main sources of our linear models is rooted in the multicollinearity: We didn’t combine strongly correlated living/basement area features. Comparing with the other two GBoost did better overall predictions but it still made several predictions that are far deviated from the y  x line. Therefore further model stacking is needed to help adjust the differences between models to give more accurate results. A simple averaging between the predicted ‘SalePrice’ among three models has already reduce the training and PB error significantly.Our stacking approach followed simple “greedy manner”. Since we only have three starting models there are only three choices for the metamodel when picking two as bases. Our results shows that the metamodel of ENet trained by stacking GBoost & KRR gave the lowest errors which are lower than the simple averaging results. For the model ensembling we again apply the greedy forward approach: picking up the best model linear Lasso Regression among other candidates. With a simple ensembling formula of 3 quarters of metamodel + 1 quarter of Lasso our final RMSLE PB error was further lowered down to 0.11718 among the top 600 results. Since the main source of our linear models came from multicollinearity our future improvement in the data preprocessing part will be including:Meanwhile for the modeling part:",NA, For more information welcome to visit our group  to find more details. We also thank Kaggle Kernel authors for useful  and for the dataset.,NA
Predicting House Prices with Machine Learning Models and Algorithms,4,https://nycdatascience.com/blog/student-works/housing-prices-ml/,The general process can be visualized below:Fourteen of the factor variables (mainly the ones associated with quality and condition) had levels that needed to be assigned. After that we recombined all the groups as training and testing sets.After feature engineering we had 50 numeric variables and 36 categorical variables.We applied onehot encoding methods to the categorical featuresWe also checked skewness. For example the QQ plot below shows that sale prices are not normally distributed. We calculated that there is a right skew upward to 1.879. To correct for this we took the log of SalePrice. By consulting the QQ plot below we found that the transformed sales price is normally distributed. After correcting for the SalePrice variable we repeated this process for all remaining numeric variables. We performed log transformation on any variable with a skew larger than 0.8 (normalizing the data with the perProcess function).,NA,The goal of this project was to predict real estate market values in Ames Iowa. We set out to model housing prices based off a widerange of features categorized in a dataset from kaggle.com. We employed a variety of machine learning models and algorithms to manipulate train and fit data for predictive analysis.to “binarize” the category and include it as a feature to train the model.,NA
Global Data Scientist market demand analysis,5,https://nycdatascience.com/blog/student-works/web-scraping/global-data-scientist-market-demand-analysis/,"As for the number of job openings New York is the city with the highest number of postings with over 1400 for the past month. NYC is followed by San Francisco and Seattle in the USA London in Europe Bengaluru in India and Singapore.Regarding the title demand almost 40 percent of the job openings are interested in bachelor's degrees almost 30 percent in master's degrees and about 25 percent in PhDs. USA is the only country that has a much higher demand orf bachelor's than for master's. India and Canada have high demand for bachelor's/master's. Interestingly European countries have higher demand for master's than bachelor's and PhDs seems to be most valued in USA UK Singapore and Canada.What we want to know next is whether the variability between countries and titles correlates to differences in salaries. For analyzing the salary data an important limitation was the low number of postings that reported salary (~10%). The salary data shown in these graphs corresponds to the countries of the top 25 cities (per number of postings).  This salary data is the result of averaging the upper limit of the posted salary range and the lower limit of the posted salary range per location and normalizing to annual pay and US $ currency. As expected USA has the highest paying jobs and the highest paying cities within it are: New York San Francisco and Seattle.PhDs earn about $10K more than people with a master's degree who in turn earn about $2K more than those with only a  bachelor's degree.However if we normalize for cost of living and rent UK Singapore and Canada catch up with the US. This trend is also reflected in the cities chart in which London Singapore and Toronto stand now almost at the same level as the highest paying cities in the USA. For normalizing for cost of living and rent the average salary was divided by the Cost of Living Plus Rent for Country Index 2018 ().
However if we divide the data between USA (Panel A) and nonUSA countries (Panel B) we can see that while in the US a master’s degree is related to a higher pay in the NonUSA countries it is not the case probably due to the fact that most nonUSA countries demand master's degrees and the number of postings that require a bachelor's is very reduced.Finally let’s have a look at the top hiring companies. Considering all the job postings Amazon Microsoft and Facebook stand at the top.If we look at the three top companies in the top 10 cities we find: JUUL Labs Autodesk and Uber for San Francisco and NYU Langone Health Weill Cornell Medicine and JP Morgan Chase for NY.Summing up considering that I acquired a solid understanding of Python R SQL and Machine Learning at NYC Data Science Academy bootcamp and these skills are required in all countries and that I have a PhD which is most valued in USA UK Singapore and Canada and within these countries the highest paying cities are: New York San Francisco Seattle Toronto London and Singapore you’ll probably find me in one of these cities hopefully not too many months from now!",NA, Data Scientist has been named the best job in America according to  for three consecutive years. While we know that data scientist are in demand in most countries a rigorous comparative analysis of the number of positions and salaries of data scientists around the world is needed. In order to figure out the best places for data science career opportunities we want to answer the following questions:To answer these questions I used Scrapy to scrape the  website for all the Data Scientist jobs posted in the last 30 days in 58 countries and 1144 cities resulting in a 50Mb database of  over 14000 job postings.Scraping Indeed for international data presented several challenges. The main ones I had to work around are the following:This dataset provided important insights about Data Scientist job market around the world.The first question I asked was whether the skill requirements vary depending on the location. As we can see in the following graph Python is the dominant language closely followed by machine learning.The second and third most wanted languages are R and SQL.Now when we compare the skill demand by city in the top 25 cities based on number of postings while we see the clear dominance of python followed by R and SQL we can also observe interesting differences between the cities. For example Singapore’s interest in R is almost as high as in Python. New York and San Francisco postings call for the three languages Python R and SQL. Whereas San Diego shares the interest in R and Python it shows  low interest in SQL. Redmond has a very high demand for AWS and C probably related to the fact that one of its largest employers is Amazon with Microsoft competing for cloud talent.Lastly a word cloud based on all the job postings confirms the absolute general interest in machine learning.Strikingly after normalizing for cost of living and rent the differences in pay between master's and bachelor's are lost.Panel APanel BThe Spider code can be accessed in this .The graphs were done with .The data was scraped from the .Image credit: .,NA
Scraping Zillow,5,https://nycdatascience.com/blog/student-works/scraping_zillow/,Data Collection:Cleaning:Quick EDA:First I wondered what is the difference between Zestimate and sale price whether that is the price a house was sold at or what the current price listing is. Below shows the Zestimate minus the sold price and as we can see houses  in the designated zip code generally sell for less than their listed Zestimate. Looking Forward:,NA,"House prices are constantly changing and if you are in the market for a house this can be a scary realization!  How are you an everyday person supposed to keep up with the growing complexity that is the price valuations of houses?  Luckily for you www.Zillow.com exists.The objective of this project was to assist in both the buying process and selling process of a house by examining houses within a given zip code and producing an analysis of similar houses.In the end the way I collected and formatted the equivalent data from each house was by using a python dictionary to collect all possible fields such as ""Beds: "" ""Baths: "" and ""Heating: "" and creating scrapy fields out of each one.  The lasting issue from the collection process was that many of the dictionary fields collected corresponded to similar data bytes for example: ""Elementary school: North main"" and ""Grade school: North main"".  These were manually assessed and condensed into groupings of similar values.I took in a deep breathe thinking the hard part was over and that my data was going to come out nice and clean... boy was I wrong.  Once I had a look at the resulting data frame I found multiple inconsistencies throughout the data.  Luckily for me many of the errors were repeated errors certain sections of the columns needing to be shifted left or right depending on what data was in the left most offending column.Why is this?  My assumptions would be:Next I wondered was there an effect on the sale price of a house based on the length of stay on Zillow or the Number of views.After making these graphs I realized they can be misleading in that:Lastly I took a look at creating the similar houses data.  This portion is only the beginning of the end goal so it is pretty rudimentary.   I did a simple grouping of houses by number of beds and baths and added a interval (+/2) to each.As we can see from the graph as well as saw earlier in the post houses generally sell for less than their Zestimate.  On the right are examples of future grouping variables as well as some summary statistics for each one.I hope to continue to develop this project so that one day one will be able to very clearly see and compare the housing prices for similar houses based upon their own requirements.  Thus allowing them to either budget for their new ""dream home"" or appropriately understand what they will reasonably be getting if they list their house at a certain value.",NA
How healthy is your neighborhood?,6,https://nycdatascience.com/blog/r/visualizing-local-health-data-across-500-us-cities/,Within an American city the health of its citizens can vary from one block to the next. I used a dataset from the  to visualize this difference between neighborhoods across five hundred cities in the United States. This project analyzed data from the annual core questions from the  and used these in combination with census data to calculate  within neighborhoods of American cities.The project covers five hundred American cities which involved around 100 million citizens or about . The  within this dataset were split into three categories: unhealthy behaviors preventative measures and health outcomes. For each measure the dataset included crude prevalence with 95% confidence intervals. At the citywide level the ageadjusted prevalence was also given in which the prevalence that would have existed in a standard population’s distribution of old and young people was calculated. See the full list of measures in the table below:I decided to visualize this dataset at three scales: statewide citywide and within cities. On the statewide tab a user could compare two health measures across all the states within the U.S. and see the correlation between those two measures. This would allow a user to compare for example unhealthy behaviors with health outcomes. In the example below I chose “Sleeping less than 7 hours among adults aged >18 Years” and “Mental health not good for >14 days among adults aged >18 Years” which has a Pearson correlation coefficient of 0.83.,NA, On the citywide tab a user can compare one health measure’s prevalence in all the cities within as many states as they’d like with the overall prevalence in the U.S. In the example below I have chosen “No leisuretime physical activity among adults aged >18 Years” and compared two states New York and Colorado. The horizontal black line represents the overall prevalence in the U.S.As one might expect most cities in New York state have a higher prevalence of “no leisuretime physical activity” than the U.S. as a whole while cities in Colorado fall below the U.S. average. This makes sense considering all the outdoor activities that are a common part of the culture within Colorado state.On the last tab I created an interactive map that made use of the most important part of this dataset: the small area estimates for regions within cities. The circle markers on this map have a radius proportional to the population of the census tract area where they are placed and a color red orange yellow or green corresponding to quartiles of the U.S. overall within that measure.One can zoom in to one city clicking on a circle to get more information like the exact prevalence the confidence interval and the population:This interactive map makes use of the most innovative part of the dataset: the small area estimates that scale down to the size of census tracts. This would allow policy makers and city officials to determine where improved health services may be needed on the level of very specific neighborhoods.I think the ideal implementation of this visualization would have involved a correlation between two measures inside an interactive map somehow combining the first and third tabs. This would allow users to see exactly how certain unhealthy behaviors or preventative measures might correlate with health outcomes to determine which preventative measures or health programs might be best used and exactly where.Link to code:,NA
Sentiment Analysis and Data Exploration of Yelp Reviews,6,https://nycdatascience.com/blog/student-works/sentiment-analysis-and-data-exploration-of-yelp-reviews/,":  Restaurants are constantly getting feedback.  is one channel but many restaurants document verbal feedback gather feedback from questionnaires or even from other review sites. When faced with a huge amount of text restaurants need a way to objectively interpret their reviews. It is very difficult for business owners to go through list of reviews and distill  relevant information from it. Also it does not provide advanced analytics to business owners to grow their business and improve their services. My solution will focus on providing advance analytics from Yelp data to help existing business owners future business owners and users.The objective is to design a system that uses existing Yelp data to provide insightful analytics and help both existing and future  business owners make important decisions about launching or expanding their business. It provides opportunity to business owners to improve their services and users to choose the best business from the available options. By building a way to relate textbased feedback to a star rating we can help these restaurants understand how they would rate on Yelp. This post is about using the web scraped online reviews from Yelp to have a better understanding of the industry. These review analysis can help the customers what to expect from a particular restaurant and also business can look into the different opinions and work on the aspects that could possibly lead to a low rating.  :I divide this project into 2 segments based on different objectives.I started with recognizing the pattern of 50 pages and implemented the same in my algorithm to set those as the starting URLs in a list comprehension. Most of the time it wouldn’t scrape beyond the second page. The whole process of web scraping (Yelp website) was quite a challenge but rewarding.While scraping the Pricing and Rating information I ran into scenarios where the Pricing was not listed for a few restaurants which resulted in some missing values. These missing values are imputed by using the high frequency value for pricing.As the data got bigger the categories also increased for each variable. In order to visualize the values we had to restrict the data to limit to top 15 places in the city with the most restaurants.Most of the attributes were obtained in an easy manner. However Pricing posed some challenges as some restaurants were missing this information. Also the class information for Location was not consistent across the restaurants causing us to handle these as exceptions.I got started by looking at the places in NYC which tend to be expensive. Below is the barplot of these attributes. As we see West Village and Greenwich Village has the high volume of expensive restaurants.Here is a plot of pricing and Rating of these restaurants. Surprisingly restaurants which are more reasonably price do seem to be having the highest rating. Does that mean that more and more customers visit the places that are more affordable causing them to have good rating? That is one possible cause.Now let’s see where are these highly rated restaurant which are also affordable are  located.The Lower East Side is the place to be. This place has the top most rating (a rating of 5.0) followed by DUMBO. The Majority of the restaurants in the financial district seem to have a very poor rating.These visualizations will enable existing and future business owners to decide to what extent  they need to consider location in their plans. This would help them identify good locations in which tor open their first restaurant or branches of their existing business .Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text. Sentiment is often framed as a binary distinction (positive vs. negative) but it can also be a more finegrained like identifying the specific emotion an author is expressing (like fear joy or anger).In our analysis I applied  both forms of sentiment analysis i.e. the traditional and the NRC.There are many ways to do sentiment analysis though most approaches use the same general idea:For example ""sick"" is a word that can have positive or negative sentiment depending on what it's used to refer to. If you're discussing a pet store that sells a lot of sick animals the sentiment is probably negative. On the other hand if you're talking about a skateboarding instructor who taught you how to do a lot of sick flips the sentiment is probably very positive.Since we are only interested in 4 highly reviewed restaurant we treated these separately. The 4 restaurants are Eataly Morimoto Tao Uptown and  Ilili. Scraping them individually also posed some challenges due to the limits beyond certain pages. The reviews are sorted by low ratings and these pages were scraped to analyze the sentiment behind these.The reviews from these 4 restaurants are then imported into R and combined into a single file.The review contents is then cleaned to remove any punctuations junk character white spaces and new lines. The stop words like ‘I’’ me’ ‘we’ etc. were also removed as they wouldn’t add to our sentiment analysis.I generated a word cloud of the 100 most frequently appearing words across all restaurant review textto get an idea of the vocabulary people tend to use in their reviews. I noticed a lot of the bigger words seemed neutral but we do see words like “time dinner disappointing wait”etc. It was also interesting to see words such as “manager “crowded” and “price” appear large in this cloud as it gave us an initial indication of things that might matter a lot to reviewers.:Future entrepreneurs always to consider where and what kind of business one should open to gain maximum profit. Using the above proposed solution business owners will be able to determine what kind of business is more profitable sustainable and attract more users in a particular city or area. In addition they will also be able to determine what type of restaurants (e.g. Mexican Chinese etc.) price range and location are more favorable to succeed in particular city.",NA," try:Price  review.find('span' attrs{'class': 'businessattribute pricerange'}).textexcept:Price  """"try:location  review.find('span' attrs{'class': 'neighborhoodstrlist'}).text.strip()except:location  """"Are these expensive restaurants really that good?? Hmm…may be not…Below is the bar plot of the sentiment analysis:Link for the GitHub Code:https://github.com/KiranmayiR/YelpWebScraping ",NA
Interactive Restaurant Discovery with Shiny,7,https://nycdatascience.com/blog/r/interactive-restaurant-discovery-with-shiny/,Welcome to the world of restaurants that we have yet to discover. There is so much to venture off too that what we know so far about food has barely begun to scratch the surface. Thankfully due to Zomato. A online database that helps inform us of all restaurants around the globe. Its like yelp but international. They are not as popular in the states but to those who are interested get a sense of wonder of what has left to be discovered. With Zomato I was able to construct a shiny app displaying the various types restaurants out there.,NA, Overall this dataset has provided an numerous amounts of data to play with to understand how much we have yet to discover with its tantalizing dishes. It would seem that it is still lacking a hidden bulk of its discovery has still yet to be obtain. In due time I believe that more and more people will definitely help report more restaurants and review them in its entirety. For now we will just have to wait...The purpose of this project is to visually see the various types of restaurants using an interactive map in R. With the help of the shiny dashboard I was able to implement the use of R and leaflet to create the visual analysis of the restaurants around the world.,NA
Visualization of Gun Violence Incidents in US,7,https://nycdatascience.com/blog/student-works/r-shiny/visualization-of-gun-violence-incidents-in-us/,US is famous for its second amendment: the right to bear arms.  As a side effect the amount of gun violence incidents are also very high. For my shiny project I chose to work with  and try to glimpse into some statistics behind these incidents. My shiny app can be found  and the source code is available on my github .The data was downloaded from the . It contains roughly 240k incidents ranging from 1/1/2013 to 3/31/2018. The record includes rich information about each incident such as date place information about participants information about guns involved incident characteristics and so on. I first cleaned the data with the following criterion in mind: I would like the information to be as complete and accurate as possible. So I filtered out those observations with missing or inconsistent information. I ended up with a dataset encompassing roughly 180k incidents.The dashboard contains three tabs on the sidebar: an interactive map to show the distribution of incidents on the map a collection of interactive charts to show aggregated statistics and a table to give further information about each incident. There are also two filters on the sidebar for the users to use to select the information they would like to display: either by number of victims or by characteristics of incidents.The interactive map displays cluster points of the incidents happened between the time frame of users' choice. And the popup contains brief information of the incidents alongside with a URL directed to the page from the gun violence archive's website where further information can be found. For example the map below shows the distribution of all incidents with drug involvement in the dataset and highlights a particular incident that happened in North Platte. ( The map is defaulted to focus on the mainland. though users can drag around to see more on Alaska and Hawaii.),NA, The interactive charts contain numbers by states numbers by years quarters and months.For example the bar plot below shows the number of incidents which result in at least 4 victims ordered by number decreasingly.Here are two charts showing number of incidents which involve children: the first one shows a trend of increasing numbers during 2014 to 2017 and the second one shows distribution among different months.Finally the datatable shows more information about the incidents. Note how many different characteristics are assigned to some incidents.For example the table below shows first 10 incidents which contain political violence in the description of characteristics. ,NA
Examining Historical NFL Gambling Data,8,https://nycdatascience.com/blog/student-works/r-visualization/examining-historical-nfl-gambling-data/,"This application is primarily interested in examining the disparity between the realized results of NFL games and the predictions made by the Sports Books in Las Vegas. The visualization tools attempt to isolate a small handful of variables to identify trends and predict error.Bet on the underdog to cover the spread! Don't only look towards the most recent Super Bowl Champion Philadelphia Eagles who were underdogs in all 3 of their playoff games. Take a look at the first chart on the spreads page showing that nearly 400 more games saw the underdog cover the spread than the favorite. The Error Variable (i.e. residuals) is expected to average 0 as we examine more games because we assume ""Vegas knows"" but instead a simple ttest shows that the average Error is 0.35 points and is statistically significant.Now let’s take a look at spreads from the 2014 season. If there ever was a year of the underdog that's it.This application uses a dataset of every regular season and postseason NFL game since the 1979 season through the Super Bowl in 2018 (the culmination of the 2017 season). In addition to descriptive data of the NFL games and their results the data includes what Las Vegas sports books offered as spreads (including the favorite team of course) as well as the over/under line.In both the spreads and O/U analysis there is an important variable called . Error is measured in points and can be positive or negative. In the case of the spreads positive Error means the favorite covered the spread while negative Error means the underdog covered the spread. In the case of the Over/Under positive means the teams combined to score more than the over/under line while negative means the teams scored less than the over/under line. Error is essentially another term for residuals. We would expect error to be 0 but the data proves that to not always be the case. We can also examine absolute error  measuring by how much Vegas missed the mark.The NFL is the highestgrossing sports league in the world. In 2017 the NFL generated approximately $14 billion USD. In July of 2018  that 29 of the NFL's 32 teams are included in their list of the 50 most valuable sports franchises in the world. The American Gaming Association a casino lobbying group projects that Americans bet $4.76 billion on Super Bowl LII in 2018 with more than 97% of that figure represented by illegally placed bets. Some of those types of bets may become legal in the near future though. In May of 2018 the U.S. Supreme Court lifted the federal ban on sports gambling. States will gradually legalize sports gambling over the next few years and the industry could very well evolve to become unrecognizable from what it is today.Two common aspects of the game to place bets on are the winner of the game and the final score of the game. Since not every matchup between two opponents is an even match this can be offset by the spread. The  is essentially a number of points handicapped to the perceived underdog so as to make the bet of who is going to win the game one that can be made with (nearly) even money. The  bet is simpler: it's a figure that represents the total score of the two teams combined.",NA,We can examine the 2014 season by looking at the trend over each week of the season. The below chart is filtered to include only data from the 2014 season (showing the mean error for each week).We can see there was only one week in which the average point differential between every game's spread and the realized results was in the direction of the favorite. It should be noted that that week is week 20 a.k.a. the conference championship games of which there are only 2 games to draw data from (compared to the first 17 weeks of the schedule of which there are at least 13 games per week.) I'll add that 1 of those 2 games saw the favorite blow out the underdog by a wide margin but it also saw that same unnamed favorite illegally deflate their footballs.Regardless of how guilty of cheating the Patriots are it is clear that the 2014 season is an outlier in this spreads data. If we omit data from the 2014 season we can see that Vegas has gotten better at forming their lines over the years by seeing a negative slope in the simple linear regression.Bettors beware! Vegas has gotten more accurate with their Over/Under lines over the years.The linear regression clearly has a negative slope meaning that with each additional year Vegas is reducing (on average) the error in their O/U lines. This time we didn't omit any data to draw this conclusion.The data is also hard to find any obvious trends in.While we can see that the under has been correct more times than the over a ttest of the error shows the mean to be +0.65 and statistically significant.The under bet has been right more times than the over but on average games will score a little bit over the offered line.You may notice the histogram is a bit rightskewed meaning that the mean (symbolized by the vertical line) is to the right of the peak. There is a reason for this. In any given game the teams can not combine to score less than 0 meaning that there is a limit to the negative values. There is not however a limit to positive value (how much more they can combine to score when compared to the offered line).What about looking at situational betting opportunities?When the offered line is low (37 or less) the over tends to hit. Is the inverse situation true?Yes it is. If you're facing an extreme over/under line betting against its extreme bias will win more times than not. For reference the average line in all games since 1979 is about 41.7 and the average line this most recent season (2017) is 44.1.,NA
Visualizing Wine Reviews,8,https://nycdatascience.com/blog/student-works/visualizing-wine-reviews/,The wines reviewed originated from 42 different countries and ranged in price from $4 to $3300. Reviews were written by at least 20 different professional wine tasters (some anonymous) and included a rating of the wine on a 100point scale. Only wines with a rating of 80 or higher are reviewed and included in the database. The rating scale used by Wine Enthusiast magazine is provided below. A “Classic” rating is extremely rare  in fact only 115 wines among the 110000 reviewed received a rating of 98 or higher.One of the first questions you may have when shopping for a wine and see its rating is to ask whether the wine will be good enough for the occasion or if it’s better to spend more money and get a better wine. To better visualize the relationship between price and rating I’ve plotted the rating against the wine price for 5000 wines sampled from the data set. It’s no surprise that yes higherpriced wines do tend to have higher ratings. However the extremely highpriced wines graphed on a linear scale make it difficult to see the relationship for the majority of the wines reviewed so I’ve also plotted the price on a logarithmic scale which shows a much more direct relationship between price and wine rating. The good news for (frugal) wine lovers is that the spread in the data for many of the wines in the $10 to $100 range reveals that there are still many wines with “Excellent” ratings of 90 and above within reach. Using the linear regression line in the Rating vs. log(Price) graph allows us to determine that wines plotted several points above the line are better values compared to others in its price range or rating category.Next I was interested in finding out if the ratings and prices for different varietals varied by country and if there was a significant difference in price and rating. These bar charts which compared the average rating and median price for several varietals from the 5 countries with the most wines reviewed showed some surprising insights as well. For example Bordeaux Redblends from Portugal on average were more highlyrated than the other four countries and had a much lower significant median selling price. On the other hand Spanish Rieslings were rated lower than the other four countries shown but sold for a similar price. Comparing favorite wine varietals in this manner enables consumers to find better deals on higherrated wines and encourage them be more adventurous in trying different wines.Finally I also put together a few tools to help explore the full database of wines reviewed and find the best values on wines specified by varietal and price range. The user can also choose a desired rating category and search for the wines Idetermined were the “Best Values” from my wine rating vs. price analysis. Another tool allows the user to see the most popular wine varietal from different countries around the world and each of the U.S. states that produce wines. This may be useful for travelers wishing to find the best type of wine to drink when visiting or to buy as a memorable souvenir.Cheers!Data source: Data analysis and interactive charts were developed with R and Shiny and can be found .,NA, Have you ever been wine shopping and wondered if the ratings actually mean anything? Do only highpriced wines get good reviews? This analysis attempts to demystify some of the confusion behind these ratings by examining a wide range of wines reviewed by a popular wine publication and showing ways to choose a wine based on other factors such as country of origin and varietal.I obtained a data set of reviews for over 110000 different wines published by  magazine between 1999 and 2017. ,NA
A Closer Look at Tree Boosting Methods,9,https://nycdatascience.com/blog/student-works/capstone/a-closer-look-at-tree-boosting-methods/,"In the book  the authors stated that “Boosting is one of the most powerful learning ideas introduced in the last twenty years.”  This statement was so captivating that we decided to do our capstone project on boosting and learn about the AdaBoost Gradient Boosting (GB) and XGBoost methods.  This capstone project is unlike others in the NYDSA Bootcamp because it is researchoriented in nature.  What we hoped to achieve from this was the ability to locate and understand relevant and reliable  literature and be able to understand them. This is important because there are so many sources of information nowadays ranging from blogs to online discussions with varying degrees of reliability.  Furthermore modern learning models such as neural networks are becoming increasingly complex. So it may not hurt to gain some technical exposure. Lastly we hoped to learn how to explain technical information to a nontechnical audience in our final presentation and in this blog.Boosting is a learning approach that was initially developed over twenty years ago for classification problems.  The publication years for the models we consider were 1996 for AdaBoost (Freund and Schapire) 2001 for GB (Friedman) and 2016 for XGBoost (Chen and Guestrin). Boosting turns out to be a general purpose method that can use .   But what is amazing as we shall see is the basic idea behind boosting with AdaBoost which was to combine a sequence of socalled “weak” learners to create a much better prediction model.  This was an innovative idea when it was first introduced in the scientific literature in 1996.  XGBoost appears to be the most popular choice as it is used in many .  We set out to find the answers to the following questions: Why is XGBoost so popular?  How is it different from AdaBoost and GB?  What is boosting?  Is boosting used exclusively for decision and classification trees? How do we use these boosting models? In this blog post we will provide answers to some of these questions.Our blog is organized as follows:  First we illustrate the fundamentals of boosting using AdaBoost.M1 (Freund and Schapire 1997) in a binary classification task.  Second we describe the basic ideas behind GB and XGBoost.  The background knowledge for these two models are somewhat steep so we will focus on an “in a nutshell” type of discussion. Third we discuss model parameters.  We will not provide a stepbystep recipe but outline a general strategy based on what we learned from our research.  Finally we present some results from an analysis of a  from Kaggle. Our primary aim in the data analysis is to compare the performance of the three boosting models. The main idea behind boosting in AdaBoost is to combine “weak” classifiers to achieve a much better classification performance.  A “weak” classifier is defined as a learning algorithm whose error rate is only slightly better than random guessing. In binary classification the “weak” classifier is better than tossing a fair coin.  A model example is a two terminal node decision tree called a “stump”. In AdaBoost a weak classifier is sequentially applied to a dataset.  In each iteration the data is modified according to whether observations are correctly classified or not.  Observations that are incorrectly classified are assigned higher weights than in the previous classification whereas those that are correctly classified have their weights reduced.  This allows previously misclassified observations to have a higher chance of being classified correctly in the next iteration. Initially the observations are assigned weights of 1/N where N is the number of observations in the training set.    Suppose that the binary outcomes are {1 1} and that there are M sequentially produced weak classifiers by AdaBoost.  A final prediction can then be obtained by applying the sign function (sign(x)  1 if x<0 0 if x 0 and 1 if x>0) on weighted predictions of the M weak classifiers.   These weights are determined by the boosting algorithm and are distinct from the weights that are used to modify the data. Higher weights are assigned to the more accurate classifiers which results in a weighted majority vote.  The AdaBoost algorithm process just described is illustrated in the figure below.  This illustration was adapted from a toy example in  by Freund and Schapire.  We suppose that there are M3 weak classifiers. After a classifier is applied to the data the misclassification error and classifier weights are calculated.  From these two items the weights for modifying the data can be calculated and consequently applied to the data. Note that in the figure the data that are misclassified and hence upweighted are slightly enlarged.  The correctly classified data remain the same size in the figure although these should really have reduced weights and sizes. At each iteration the weights sum up to one. The sign function applied to the weighted sum of the weak classifiers produces a classifier that is more complex and captures a good decision boundary for classifying the data.  We encourage you to determine how this final classifier was obtained by working out the arithmetic inside the sign function in the second figure for each data point.Surprisingly a combination of stumps produces a very good classifier.  ESL provides a simulated data example (10.2) which we reproduce below using codes from the .  Random samples from independent standard Gaussian variables are generated to produce 10 features. The classes are determined based on whether the sum of the squares of the 10 features is larger than 9.34 the median of the chisquare random variable with 10 degrees of freedom.  Recall that the sum of the squares of p independent standard normal random variables is chisquare with p degrees of freedom.  The simulated data has 2000 training observations with approximately 1000 cases in each class and 10000 test observations. We train and test a stump a ninenode decision tree and a boosted classifier of 400 decision tree stumps.   The figure below presents a visualization of the simulated data points using the first two principal components.  We can see roughly two sets of data points  one set inside (dark color) and another set outside (yellow color) the cloud of points. The figure below presents the classifier error rate versus the number of boosting iterations (or weak classifiers) of the three classifiers.  The green horizontal line on the top corresponds to an error rate of approximately 46% when using only a single stump as classifier. A decision tree with a depth of 9 has an error rate of approximately 31% (orange horizontal line).  When boosting stumps (blue lines) the error rates dramatically decrease with the number of iterations achieving approximately zero training error at around 250 iterations. Note that beyond this we expect our model to become more complex and hence overfit.  Note also that the test error appears to continue decreasing which suggests that there is no overfitting. This surprising result was empirically observed for boosting and it contradicts the conventional biasvariance tradeoff. But this is not always the case and there are examples of boosting leading to overfitting.AdaBoost has been actively studied for several years after it first appeared in the literature.  It was not until after a few years later that it was discovered to fit an exponential loss function under a general framework called forward stagewise additive modeling (FSAM).  We shall omit the discussion about FSAM and describe GB and XGBoost only in the general sense. A boosted tree model is a function that is formed by a sum of tree models.  Basically you initialize a tree and fit it to your data.  You then keep adding more trees to it until you obtain a more accurate model.  Each added tree is supposed to lead to an improvement of the previous treeaccumulating model that is a better fit in the sense of minimizing the loss function.  You may recognize this idea of accumulation that leads to an improvement from a numerical optimization procedure called gradient descent used in neural networks.  In gradient descent you initialize the model parameters and move in “steps” that should lead you to the minimum of the loss function. The accumulated “steps” will give you the parameters that yield losses that are closer to the minimum of the loss function.  However whereas in gradient descent we are searching within the parameter space in gradient boosting we are searching within a “function space.” Thus GB can be regarded as gradient descent in the function space (ESL; Nielsen 2016).   The “step” that we just described consists of a gradient which determines the direction of maximal descent.  In GB a tree is fit to the negative gradient using least squares. This will yield an optimal tree partition.  Then the weights or values assigned to the partitions are determined via an optimization procedure. Thus for GB optimization involves finding an optimal tree partition for the fitted tree and then finding the optimal weights for each partition separately.  XGBoost also approximates the negative gradient but the gradient is weighted by the hessian or second order partial derivative of the loss function and the fit is performed using weighted (by the hessian) least squares (Nielsen 2016). What this ultimately means for XGBoost is that the optimization for the tree partitions and the weights are done simultaneously instead of separately as in GB.  This may be a more efficient optimization procedure and could lead to better estimates. We also suspect that this leads to faster computation.Whew! That was plenty of technical stuff.  But we can actually derive a simple result from this.  For regression problems that use the squared error loss function the negative gradient is simply the ordinary residual.  Thus in GB the boosted tree model is obtained by fitting a tree on the residual from the tree model in the previous iteration.  In other words for squared error loss in regression GB combines trees that are fit sequentially on the residuals. GB is a general model because it can accomodate any loss function.  When using an exponential loss for binary classification GB will yield AdaBoost.  Note that in Python GB uses deviance as the default loss function. However these two loss functions are actually very similar (ESL); so we should expect AdaBoost and GB to yield very similar results.   We briefly discuss selected model parameters in this section.  We will not attempt to provide a specific prescription of how to tune these parameters.  Rather we summarize suggestions or recommendations that we found in the literature and recommend a “to start with” strategy.  The selected parameters are shown in the table below.  They are categorized into three. The first three are the main parameters and they are common to all three models.  The remaining parameters are grouped according to their purposes to induce randomness (in yellow) and provide regularization (green).Note that AdaBoost can be implemented in Python using the AdaBoostClassifier which includes the three main parameters.  However AdaBoost can also be implemented using the GradientBoostingClassifier with an exponential loss as the loss function (the default is deviance).  Using this implementation allows AdaBoost to also utilize the additional parameters subsample and max_features. Max_depth controls the depth of the tree.  The deeper the tree is the more complexity is introduced and this will not only increase the variance of the estimator but also decrease the computational speed of the algorithm.  Note that adding just one level doubles the complexity of the tree. A recommendation by ESL is to use tree depths of between 4 and 8 inclusive in practice. The learning_rate controls the contribution of each tree that is added to the ensemble.  It can be regarded as controlling the learning rate of the boosting procedure. N_estimators is the number of boosting iterations (or the number of trees in the boosted model) and it should be set depending on the value for the learning_rate.  Low learning rates typically need more iterations and viceversa. Note that more iterations translates to slower computational speed. ESL states that the best strategy might be to set learning_rate low (<0.1) and then choose n_estimators by early stopping.When tuning we found it reasonable to start first with these three main model parameters.  We also found it helpful to create a plot of the training and test error rates (for classification) versus the number of iterations (n_estimators).  The following plot presents an example showing the results from the simulated data we saw earlier with the boosted stumps for AdaBoost and learning_rate1.    The figure also includes the error rates in red when using a tree with max_depth3 instead of a stump.  For this more complex tree model the training error decreases much faster but the test error is worse than the stump’s.  It appears that the stump will yield the better estimator for the simulated data. This should come as no surprise because the data was created in an additive fashion with no interaction.  In general stumps capture additive main effects while deeper trees can capture more complex interactions. We found it useful to plot the learning curves above instead of doing a grid search where we select only a few values for n_estimators e.g. 100 200 400 and 1000.  A problem with this approach is that the learning curves are not always continually decreasing. In the figure above it seems that there is a minimum test error rate when n_estimators  50 which will be missed by our grid search example.  The parameters that induce randomness are subsample max_features (in GB) or colsample_bylevel (in XGBoost) and colsample_bytree.  When tuning these parameters only a portion of the data will be used for training as described below. This strategy allows the model to generalize better. To subsample is to use a portion of the observations or examples for training e.g. 30%.  This could produce more accurate models and can also reduce the computing time by roughly the same fraction as the subsample.  Note however that based on simulations a subsample can be harmful when learning_rate  1 (no shrinkage) (ESL).Max_features and colsample_bylevel is the same strategy used in random forests.  Tuning these parameters entails using only a portion of the features or predictors when constructing each tree level in an iteration.  Colsample_bytree means that you use only a portion of the features or predictors when constructing a tree in each iteration. Thus when using both colsample_bylevel and colsample_bytree the portion of features or predictors when constructing each tree level can be considerably reduced which may improve computational speed.The last two parameters reg_alpha and reg_lambda perform L1 and L2 regularization of the leaf weights respectively.  Recall that the weights are the function values assigned to the tree partitions. These parameters shrink the leaf weights in an effort to combat variance.  When tuning these other parameters you may want to first study the effects of each parameter on the learning curves.  For example you can study what happens to the learning curves when subsampling by 0.3 0.5 and 1.  Then you can study what happens when you vary colsample_bylevel.  By doing this you will hopefully gain some insights as to how the learning curve changes when tuning these parameters.  From there we hope that you can then formulate a grid search or other strategies to find the optimal model parameters.  We explore the performance of the three boosting methods using a  from Kaggle.  The task is to build a model that’s capable of classifying comments as toxic obscene insult severe toxic identity hate or threat.  The dataset contains 159571 comments from Wikipedia’s talk page edits.  The following figure presents some useful information about the data.The left side of the figure shows that this classification task is not a multiclass but a multilabel problem.  In a multiclass problem each example or observation is assigned to only one class or label. In a multilabel problem each example or observation can have more than one classes.  For example in the third row of the table above there are 3800 comments that can be classified as toxic insult and obscene at the same time. The right side of the figure presents the dataset as unbalanced in terms of the labels frequency.  Whereas toxic obscene and insult are at least 5% frequent the remaining labels are at most 1% frequent. The figure above presents the workflow we adopted.  For multilabel modeling we identified at least three approaches in the literature: one versus rest chain classifier and power set.  For our analysis we chose to implement one vs rest. This strategy treats each label as an unique target and converts the task into binary classification. As there are six labels the final model will be a collection of six binary classifiers. For natural language processing (NLP) we implemented stemming and TfIdf.  Stemming is a process that chops off the ends of the words but keeps its basic meaning.  TfIdf is a numerical statistic that is intended to reflect how important a word is to a text.We compare AdaBoost GB and XGBoost in terms of test accuracy and computation time.  We present only results for the toxic label in the following table. The performances of AdaBoost and XGBoost appear to get better with increasing model ""complexity."" By complexity here we mean deeper trees (larger values of max_depth) or more iterations (larger values of n_estimators).  The performance of GB appears to get worse with increasing ""complexity."" The table below summarizes the best test accuracies from the previous table and the computation time for all the calculations in the previous table.  The test accuracies are almost the same for all three models but they differ considerably in computational time. The computational time for XGBoost is approximately onethird that of either ADaBoost or GB.We also compared the three models on three other datasets: the simulated data from ESL spam email and  from Kaggle.  For the Evergreen dataset we excluded all observations with at least one missing feature for the time being which reduced the sample size by about a half.  Note that our aim is to compare the performances of the three models and not necessarily to obtain the best predictive performance. Note also that the best performance in Kaggle for this dataset was a test accuracy of around 89%.The results for the simulated and spam email data were very similar across all models.  We found differences only for the Evergreen data where there was increasing prediction accuracy from AdaBoost to GB and to XGBoost.  The improved performance in XGBoost resulted from tuning the regularization parameters reg_alpha and reg_lambda and using a tree of max_depth15.  In this capstone project we learned about the basic ideas that led to the development of the earliest boosting model AdaBoost and how well it performs.  We also attempted to describe the general ideas behind GB and XGBoost and learned that GB uses a first order numerical approximation while XGBoost uses a second order approximation.  We suspect that this optimization procedure by XGBoost could potentially lead to better results and may be responsible for its efficient computation. We also described the model parameters and learned that XGBoost has more tuning parameters than GB which makes it a more flexible model.  However this also makes tuning the parameters a more difficult process.  We outlined some strategies for parameter tuning in this blog post but this is an area where we need to do more research on. Although we have not seen clear predictive advantages of XGBoost compared to GB on the datasets that we considered we suspect that there may be other datasets where XGBoost could potentially beat GB.  For example there may be a dataset with many features in which the model parameter colsample_bytree can be used to sample the features and possibly lead to better predictive performance. We have already seen how tuning reg_alpha and reg_lambda can lead to better performance for the Evergreen dataset but we would like to see results on other datasets to confirm this advantage. For the toxic comments dataset we may consider exploring other multilabel approaches such as chain classifier and power set and also other NLP techniques that might help improve model performance such as lemmatization.",NA,"

Our primary references are the following:The first reference includes an entire chapter on boosting but does not include XGBoost.  The discussion on GB is technical and statistical and requires some knowledge of basis functions and additive models which are discussed in other parts of the book.  The second reference is a very well written master's thesis.  It is quite thorough in its discussion of trees and tree boosting and it makes a comparison of GB and XGBoost.  It also goes on further discussion on the advantages of XGBoost.  This is one of the best references we found on tree boosting; it is a very good supplement to the original XGBoost paper by .  The third listed reference describes the basic ideas of ADaBoost.  It was the basis of our discussion of boosting and AdaBoost. ",NA
Credit Card Approval Analysis,9,https://nycdatascience.com/blog/student-works/credit-card-approval-analysis/,":  The decision of approving a credit card or loan is majorly dependent on the personal and financial background of  the applicant. Factors like age gender income employment statuscredit history and other attributes all carry weight in the approval decision. Credit analysis involves the measure to investigate the probability of a thirdparty to pay back the loan to the bank on time and predict its default characteristic. Analysis focus on recognizing assessing and reducing the financial or other risks that could lead to loss involved in the transaction. There are two basic risks: one is a business loss that results from not approving the good candidate and the other is the financial loss that results from by approving the candidate who is at bad risk. It is very important to manage credit risk and handle challenges efficiently for credit decision as it can have adverse effects on credit management. Therefore evaluation of credit approval is significant before jumping to any granting decision.: Algorithms that are used to decide the outcome of credit application vary from one provider to another and across sectors and geographies. However there are high degrees of similarities in the attributes used to generate those algorithms. In this project I have collected data from the Credit Approval dataset available in the archives of machine learning repository of University of California Irvine(UCI) ()The main objective of developing a Credit Card Approval Shiny App is to show the impact of different fields like Gender Age Income Number of years employed etc on the approval for a Credit Card. This app have some static graphs(which include histograms scatter plots box plots etc) and some interactive plots that will help user to select the fields of interest.The primary objective of this analysis is to implement the data mining techniques on a credit approval dataset. Risks can be identified while lendingdatabased conclusions can  about probability of repayment can be derived and recommendations can be put forward.The Credit Approval dataset consists of 690 rows  representing 690 individuals applying for a credit card and 16 variables in total. The first 15 variables represent various attributes of the individual like fender age marital status years employed etc. The 16th variable is the one of interest: credit approved(or just approved). It contains the outcome of the application either positive(represented by “+”) meaning approved or negative (represented by ““) meaning rejected. This dataset is a multi variate dataset having continuous nominal and categorical data along with missing values.Below is the structure of the dataset:str(Credit_Approval)Classes ‘tbl_df’ ‘tbl’ and 'data.frame':690 obs. of  16 variables: $ Male          : chr ""b"" ""a"" ""a"" ""b"" ... $ Age           : chr ""30.83"" ""58.67"" ""24.50"" ""27.83"" ... $ Debt          : num 0 4.46 0.5 1.54 5.62 ... $ Married       : chr ""u"" ""u"" ""u"" ""u"" ... $ BankCustomer  : chr ""g"" ""g"" ""g"" ""g"" ... $ EducationLevel: chr  ""w"" ""q"" ""q"" ""w"" ... $ Ethnicity     : chr ""v"" ""h"" ""h"" ""v"" ... $ YearsEmployed : num  1.25 3.04 1.5 3.75 1.71 ... $ PriorDefault  : chr ""t"" ""t"" ""t"" ""t"" ... $ Employed      : chr ""t"" ""t"" ""f"" ""t"" ... $ CreditScore   : num 1 6 0 5 0 0 0 0 0 0 ... $ DriversLicense: chr  ""f"" ""f"" ""f"" ""t"" ... $ Citizen       : chr ""g"" ""g"" ""g"" ""g"" ... $ ZipCode       : chr ""00202"" ""00043"" ""00280"" ""00100"" ... $ Income        : num 0 560 824 3 0 ... $ Approved      : chr ""+"" ""+"" ""+"" ""+"" ...And some stats for all these fieldsBelow is a quick overview of the missing values in the dataset:These initial plots showed that all variables have distributions that are skewed to the right indicating that the data is not welldistributed about the mean. In order to reduce the skewlog transformations were applied and then plotted again.Below are the plots of the discrete variables that appear to influence whether a credit application is approved.As expected Prior Default and employment status appear to have the most significant effect on the approval. Persons with prior default are rejected more than 90% of the time and those who not employed are rejected 70% of the time.Let’s see if the education level has any effect:From the graph  we see that people with education level ‘x’ have an 85% chance of approval ascompared to ‘ff’ who are rejected 85% of the time.As we see a high credit score resulted in approval 90% of the time and applicants with higher income have a higher than average approval rate.Finally I did a pairwise comparison of all the fields using a scatter plot you can see below:These plots do seem to have a scaling problem. One reason for this could be the presence of outliers. The range of the values is high causing the regression line to adjust for these outliers. For now we will not be working on handling these. But from the plot we can see that Years Employed has the highest linear correlation with the Approved field.From this initial analysis we are able to conclude  that the most significant factors in determining the outcome of a credit application are Employment Income Credit Score and Prior Default.Based on these insights we can work on building some predictive models. They can be used by analysts in financial sector and be incorporated to automate the credit approval process. These results can also serve as a source of information for the consumers.Modern credit analyses employ many additional variables like the criminal records of applicants their health information net balance between monthly income and expenses. A dataset with these variables could be acquired. It’s also possible to add complementary variables to the dataset. This will make the credit simulations more   similar to what is done by the banks before a credit is approved.The shiny application is available on this link: And the code is available on GitHub location below:",NA,Preprocessing of the data includes data cleaning data integration data transformation  data reduction missing values imputation among other tasks. Below are some of the data transformations that were done to the Credit Approval dataset before we apply any EDA techniques.The missing values are found to exist in attributes Age GenderMarital Status Bank Customer. Education level Ethnicity and Zip Code which we filled by NAs. Out of these Age is a continuous variable. There are different methods to impute missing value ranging from deleting the observations deleting the attribute if of no importance zero them out or plug the mean/median/mode value from all the values.Here we imputed the values by using the median value for Numerical fields. For remaining attributes with categorical values the missing values are imputed using the frequency count of the observations. The Class group with highest frequency was used.To start with the distribution of 5 continuous variables Age Debt Credit Score Income and Years employed was observed to get a sense of the nature of the dataset.Among the continuous variables Income and Credit Score seem to also have significant effect on the outcome of the credit application.,NA
Maximizing value from your car sale in India,10,https://nycdatascience.com/blog/student-works/maximizing-value-from-your-car-sale-in-india/,As a car fan I’ve been curious about the used car market in India. The economy and culture of this country have formed a special used car market with high volume and speed of trades. It would be interesting to get some insights of the used car market with web scraping.The website scraped is https://www.olx.in/cars/ . Olx Group is a commercial webbased service that supplies vehicle reports to individuals and businesses on used cars and light trucks for the Indian consumers. This project focuses on visualizing the used cars market in India. There data of more than 100000 used cars ads was scraped using scrapy. The scraped information contains body type color engine mileage model price drive type model year model name and make of each sample.For car fans car dealers used car holders and used car buyers this project provides answers to the following questions:1. What are the best brands to buy in terms of resale price mileage and km driven.2. How can a price of a car appreciate over the years.3. How can a special type of number plate add a significant value to a car.So this is how a used car ad on the website looks:I tried a few web scraping techniques and the best output came out with scrapy in R the data had more than 100000 used cars ads. Olx is a marketplace where buyers meet sellers and can upload their car data.So what are the features a car buyer looks for while purchasing a used car?For most used car buyers the mileage of a car and the year in respect to the price are few of the first basic features a buyer looks for therefore to have a successful sale the seller needs to provide the right data. I gathered these 3 main features and came out with a cluster analysis graph to visualize these features.After these aspects are fulfilled  a buyer sees what is the brand value of the used car?By just adding a special number plate to their car.These are two examples of cars from the year 19601990. The original price of these cars ranged from Rs.15005000 after 20 years the price of the car range from Rs.150000  250000. This shows an appreciation of  almost 10000% which is much expensive than some of the new cars available in the market.A common number plate in India consist of a 10 digit alphanumeric plate but these 5 alphanumeric plates make them extremely rare and precious. The common number plate in India as shown on the left is 'MH63CN3731' and the special number plate on the right is 'DEV2'.Both the cars are up for sale on olx and are from the same year and model but there is a difference of Rs.800000. The original on road price of the car is Rs. 1300000 and the used car with a special number plate on the right above is almost Rs. 700000/10000$ more from the price of a  new car. I think this example would explain how special number plates can add so much value to the car.,NA, These questions can be fulfilled by reviewing the number of cars sold by a brand make per year and what is the depreciation percentage of each car brand/make. A bar plot analysis can explain:,NA
Scraping Used Items on Craigslist.org with Scrapy,10,https://nycdatascience.com/blog/student-works/web-scraping/scraping-used-items-on-craigslist-org-with-scrapy/,As the old adage goes ‘One man’s trash is another man’s treasure.’  This was my first introduction to Scrapy. My goal was to scrape used items from Craigslist.org and perform some basic EDA on the data. I expanded this project into more detail in my final project. You can check out if you’re interested .Using ScrapyUsing Pandas Matplotlib and SeabornUsing wordcloud,NA,I set out to answer what type of items were available where the items were located and how many items there were for each popular location. I also wanted to find out what the price distribution was for the items in the popular locations.This spider scraped the first page of the used items for sale section. This dataset a small sample of what items were actually available on Craigslist in the area. I decided to go this route because I knew that Craigslist blocks IP addresses if they notice unusual request activity they deem inappropriate. I scraped the entire used items for sale section in my  but that required using the paid Crawlera service from Scrapinghub.com. Because this was my first exploration into web scraping I focused on the fundamentals of creating a Scrapy spider and performing EDA on the data.I was able to resolve the questions I set out to answer using Pandas to clean and analyze the data as well as Matplotlib and Seaborn to visualize the data. I created subsets of items to group highpriced motor vehicles together as well as popular locations so I could gain insight into the price distribution of subsets.I decided to add a simple word cloud visualization as well for another helpful visualization of available items in the area.This samplesized dataset was too small for a confident understanding of the price distributions for all used items in the area. But I was able to determine that $1000 would be enough to buy the majority of items available that weren’t motor vehicles.This was my first introduction to data wrangling and EDA with Python and I enjoyed the learning process. You can view my GitHub repo here: .,NA
Simple Interactive Visualization of '16-17 NBA Stats With Shiny,10,https://nycdatascience.com/blog/student-works/simple-interactive-visualization-of-16-17-nba-stats-with-shiny/,Before I set up my Shiny App I wanted to answer the following questions using dplyr:You can find the remainder of this answer in my GitHub repo.You can find the remainder of this answer in my GitHub repo.You can find the remainder of this answer in my GitHub repo.,NA,Fantasy sports is a multibilliondollar industry. Most people play in leagues as an effective way to stay connected to friends coworkers and family through friendly competition. Most of us don't have enough time in our lives to stay current with all the updates and latest news about the sports we have interest in. To reconcile this challenge I practiced my skills I developed in R to build a Shiny App that compiles indepth stats on NBA players.I set out to answer which players had the most values in multiple categories. I wanted to discover some of the midtier players in the ’16’17 season that stood out in certain statistical categories to keep on my radar for next season.To gather the data I needed I first had to search the web and figure out what would would be the most efficient way to extract the stats. There are many websites that display NBA stats but most of them don’t make it simple to download to a .csv or .txt file. I discovered  which had the relevant stats in a simple raw downloadable .txt file. The .txt files still needed a fair amount of data munging to tidy up for usage within the Shiny App.I also extracted advanced stats from  which included further statistics for the guard forward and center positions in the NBA. Since the advanced stats were in tables already the most efficient way to extract the data was to use Pandas readhtml method. Some of the tables extracted with Pandas were missing the column names so I used BeautifulSoup to extract them and combine them with the Pandas dataframe. I then exported it as a .csv file.I created the app to display multiple data tables. A search for whatever keyword the user finds applicable (ie player name team name position stats category) is simple. The app also displays an interactive visualization of the correlation between team PER (player efficiency rating) and team winning percentage.There didn't appear to be a clear correlation between team PER and team winning percentage.A few players when doing my analysis appeared to offer solid value adds in later rounds of a draft; these players excelled at one or a few important categories but are generally overlooked because of their total minutes per game since they play behind more wellknown star players.The analysis helped me to narrow my focus on specific players so I can pay closer attention to what happens to their career next season. Sometimes a trade to a new team can increase a midtier players value if they have already shown promise playing behind another star player.This was a smallscale project that completed in the offseason. I only analyzed data from ’16’17 season. Gleaning insights from the data of the previous season can be helpful but many more intangible variables off the court or field are necessary to come to a confident decision about adding a player to my fantasy team. It is also helpful to glean insights quickly and efficiently while the season is taking place. I may figure out how to extract and import data in a more efficient process as a next step in the project.You can view the project here:  and my code on GitHub here: . It was interesting to learn R and Shiny but I have currently have no intention to use them again in the short term. You can view other interactive data visualizations I've created with Tableau here: . ,NA
Supervised Learning With Kaggle Titanic Dataset,10,https://nycdatascience.com/blog/student-works/machine-learning/supervised-learning-with-kaggle-titanic-dataset/,Kaggle.com offers an introduction to supervised learning with the Titanic Dataset.  The purpose of the competition is to build an accurate classification model that predicts whether a passenger would survive the Titanic crash.  This is a helpful exercise to reinforce the fundamentals of Machine Learning. There are plenty of resources available to assist in filling in the page and deepen understanding of the fundamentals.  I had no experience in programming or advanced mathematics before starting the bootcamp so I wanted to stay focused on the basics. This competition was the most sensible for my needs.I created a Jupyter notebook that is split into two distinct parts.  The first is an overview of fundamental and important concepts of machine learning and the second is the application of those concepts on the Titanic dataset.  As I began the process I set out to answer the following questions: The data came from the Kaggle website. It was split into a training and testing csv files. The dataset was structured with the following features:I performed some exploratory data analysis to get a feel for which features appeared to have a significant effect on survival rate and the number of missing values in the dataset.I then filled in the missing values and engineered some new features to make the dataset more machinereadable.  This was in preparation for applying multiple algorithms within the SciKitLearn library.,NA,I used a random forest classifier to decrease the dimensionality of the features in an attempt to distill the dataset down to only features that had a significant correlation with survival outcome. Feature selection is useful because it reduces redundancy of the data overfitting and it also speeds up the training process.Next I applied crossvalidation to get a more robust idea of how well each algorithm might do on the testing data.Finally I evaluated the models using a confusion matrix to determine how many false positives and false negatives were predicted by each model. I also visualized the ROC curve and Area Under the Curve to determine the performance of each classification model.The features that had a significant impact on survival rate were age fare and sex. The gradient boost model achieved the best results on my test dataset and received the best score on my submissions to Kaggle.This is an evolving Jupyter notebook that I will continue to refine as I practice machine learning into the future. I wanted to create something that I could share with others also starting out on their journey into the world of machine learning. I also wanted something I can refer to when I need a refresh on a concept and also as a reference point to mark my progress. My goal is to see if I can score in the top 20% of this competition within the next year.You can view the project on my GitHub here: .,NA
Using Machine Learning to Build a Predictive Model for House Prices,10,https://nycdatascience.com/blog/student-works/using-machine-learning-to-build-a-predictive-model-for-house-prices/,The data for this project was provided by  and was presplit into a training and testing set. Both data sets are near identical with 1460 observations in the training and 1459 observations in the test set and 79 varying housing features to predict the overall sale price. Our first thought in predicting the sale price was that the location would have a heavy impact. The sale price per neighborhood is plotted below. We found that the extremes on both ends were highly important variables.In moving forward with a linear model the first assumption to test is normality in the data. In plotting the distribution of the target variable sales price we discovered that it was positively skewed with a skewness of 1.88. After applying a log transformation to allow for higher predictive power the skewness dropped to 0.12 as shown in the picture below.Afterwards we investigated the skewness across all numeric features to identify which variables would also need transforming to allow for a better fitting linear model.  Any variable with a skewness above 0.75 was log transformed.The final aspect of our data cleanup and processing involved dealing with the missing values and the categorical features (e.g. which neighborhood the house belonged to or kitchen quality). The only numerical features that had significant amounts of missingness were the year that the garage was built and the lot frontage. Both were imputed with the median value for the respective variable. This was because the difference from a garage built in the year 1980 to the imputed year zero could cause significant issues with a linear model; the reasoning for lot frontage stemmed from the fact that if the house had a measurable square footage it did not make sense to impute the missing values with zero. All of the other missing values were imputed with zero as whatever was left we could gather was not a valid feature of the house such as missing values for the pool or fence quality/square footage. The remaining categorical features were ‘dummified’ or one hot encoded to obtain numerical values in order to (initially) give the model we were developing as much information as possible.Two families of predictive models were considered to solve the problem. The way we evaluated the models was by first splitting our training dataset into an 80/20% split where 80% of the data was used to train the model. After training the model the untouched 20% of the data was used to evaluate the predictions directly assessing the model’s performance. The RMSE was evaluated between the predictions and actual values of sale price from the 20%. The first family linear models achieved high performance with Lasso feature selection technique and minimal feature engineering that addressed the skewness of predictors and dependent variable. Other attempts to improve the performance using Ridge regression produced identical results however we observed a slightly higher RMSE for the Ridge regression model over the Lasso regression model. The treebased models such as Random Forest Extreme Gradient Boosting and XGboost produced inferior Rsquared and were discarded from further analysis. Treebased models would require subjective feature engineering and extensive hyperparametertuning to work well. With this dataset of the original 79 features 268 features were then generated after dummifying. This many features would make it extremely difficult to tune the number of features considered per split hyperparameter with treebased models. Also the number of training observations is relatively low (fewer than 1500) which further hinders treemodels’ performance. Finally the interpretabilityis lost for advanced treebased models which also makes the linear solution preferable. One of the main benefits gained by selecting a linear regression model is for its interpretability.  First let’s examine the home features that were most important to the model. Note that the larger the magnitude of the variable’s coefficient in the model the more influence it has on the sale price of the home.Not surprisingly total aboveground living area was the most influential valueadd variable in the data set.  Another feature which showed up numerous times (as both a valueadd and as a detractor to home value) is the neighborhood.  Tracking this back to the earlier boxplot (and recalling the mantra “location location location”) we see that our model again not surprisingly has indicated that location is a very important factor in determining sale price.  It is worth cautioning against directly extracting a dollar value for each of these potential changes in a home.Since many of these features are on a log scale (done earlier to reduce skew) and normalized (given mean zero and adjusted for the variance) the marginal value increases in a home’s price based on these coefficients requires a calculation but thankfully the model does exactly that.There are many next steps one could take to attempt to achieve better results.  A sensible starting point would be to go after improving the results on the subsection of homes the algorithm performed worst on.  Below we plot our models predicted price home vs actual price home (this is of course on the training set as we never knew the home prices for the test set).We can see that the model performed consistently worse for more expensive homes.  This implies that the effect of these variables on home price is not inherently linear and as such we need to augment the data (e.g. feature engineering) or model in order deal with these homes.  We could also augment the data set with a general “market” indicator (something like the S&P 500 Index as a rough proxy for the value of assets).,NA,The purpose of  was to build a model to predict housing prices in Ames Iowa based on a given data set with features such as total living area above ground neighborhood number of bathrooms etc. We began our work by familiarizing ourselves with the dataset addressing missing values and performing exploratory data analysis and visualization. We then implemented several machine learning techniques including simple linear regression lasso and ridge regularization and treebased methods including random forest and XGBoost. We evaluated our models by calculating the root mean squared error (RSME) between the predicted values and the observed sales price to see which model performed the best. Constructing a linear regression model with lasso regularization yielded the best result with a root mean squared logarithmic error (RMSLE) value of 0.12116 as calculated by Kaggle upon submission. This result represented the top 18th percentile at the time of submission.,NA
Building a Worldloppet Recommender App,11,https://nycdatascience.com/blog/student-works/r-shiny/worldloppet-race-reccomender/,It can be less than ideal to travel across the globe and participate in a race that is above (or below) your skill level.After all…Matching racers with host countries can be a serendipitous and enjoyable international experience. So I designed and constructed a recommendation engine to match an individual's preferences to FIS Worldloppet races. Then  to help skiers find a race for the 2018/19 season. Read on to learn more!My first objective was to extract data on the Worldloppet races.  so I extracted the past 5 years of race results data using . This was done anonymously as racer names were not extracted  only the race time and skier nationality. In total over 600000 race results were gathered.Several features could be engineered from this data despite scraping only a few variables. For example I used regular expressions to identify the host country from the race name. This could be compared to the racer’s nationality to get an idea of how many skiers were tourists for a given race or how much a race has grown in popularity. Given the racer’s finish time and the race distance extracted from that page a race speed can be calculated. A limitation of this calculation is that it does not account for why these speeds differ. For example paces could be slower because the course is particularly challenging with major elevation gains. Conversely “faster” races could just be flatter. Nevertheless there appears to be a distribution of median race speed by host country:I envisioned the following scenario: given my own personal preferences for race distance race speed and how many racers I wanted to ski with which Worldloppet race would be a good fit for me? This problem can be solved using a KNN classification approach. I could compare my preferences to those which have already been recorded within the hundreds of thousands of observations (race distance race speed number of racers) from the scraping portion of this project. The next step requires finding consensus of which race (by host country) was a good fit by finding the most similar datapoints to my preferences. The number of neighbors I selected for the model was determined using a grid search and 10fold cross validation. The subset of classic skiers was extraordinarily accurate using this model with a minuscule misclassification rate of 0.001 to identify the host country. Classifying skate skiers represented a more challenging problem as the misclassification rate for the host country was (0.21). This less accurate match suggests that there are more skate skiing results with similar race distances/speeds than the classic races.Coming to this realization was rewarding but I wanted to share it with the broader XC skiing community. So I reimplemented the tuned KNN model in R and deployed it as a webapp using R shiny. . The app asks for your preferred XC style race distance preferred race speed and number of skiers you'd like to race with. Then using the KNN model described above a host country will be recommended. Popups on the host country markers have a link to that race's website. The Worldloppet series is a fantastic way to experience the best XC skiing that the world has to offer. I built a KNNbased recommendation engine for Worldloppet races and deployed it as a web app.I gathered data to build this model by scraping race results from the Worldloppet website and performed feature engineering. I hope that this app will make new suggestions for skiers and will enrich their XC skiing experience.,NA,"Every ski season 20 nations showcase the best cross country (XC) skiing that the world has to offer. Tourists from all over the planet participate in this global series. As the organizers of this series highlight! However one potential entry barrier is the uncertainty of ""race fit."" While Worldloppet is for everyone elite skiers can be in attendance. One area for additional development of the model is to factor in the difficulty of a race. What is the elevation of the race? What is the total elevation gain? Do these characteristics affect race speed? Another consideration is how tourists are defined. My EDA defined tourists in a binary fashion. It would be interesting to consider a weighted value based on distance traveled. In other words which races draw tourists from which countries? These insights and the ones derived from building the app could help Worldloppet personalize marketing strategies to participants by suggesting new races tailored to their preferences.",NA
Observations from TED Talks,11,https://nycdatascience.com/blog/student-works/observations-from-ted-talks/,TED is a notforprofit organization devoted to spreading ideas via short talks. Please see ted.com for more details. The dataset used in this blog was obtained from  The blog analyzes all the talks published from 2006 to 2017. An associate shiny web app for this blog can be found . This blog and the web app aim to (1) understand people's overall engagement with talks and (2) provide recommendations to potential viewers by providing popular talks and relevant ratings. Figures 1 2 and 3 provide insight into people’s level of engagement as demonstrated by the number of views comments and the ratio of comments per ‘1000views’. [Note: the ratios are typically less than 1% for a social media metrics like comments per views. For example a 0.5% comments to views ratio can be considered good for a youtube video (http://tubularinsights.com/3metricsyoutubesuccess/) this ratio translated into 5 comments per 1000 views. Therefore for the analyses in this blog the number of comments are divided by ‘1000 views’. ]  Viewers describe the talks as mostly inspiring informative fascinating persuasive and beautiful. Please see the full list of descriptions in Figure 4. For each talk every one of the 14 descriptions was selected by 0 or more viewers. The box plot shows the votes for all descriptions for all talks. For example: the median for the rating 'inspiring' for a talk was 220 meaning on average 220 people rated a talk as 'inspiring' and half of the talks were rated 'inspiring' by more than 220 people and half of the talks were rated “inspiring” by less than 220 people.   Ken Robinson’s “Do schools kill creativity” has the highest views 47 Million. 24924 people found the talk to be inspiring. The word cloud shows how viewers rated it.Summary:TED talks continue to be well received but the level of viewers' engagement as demonstrated by their comments has steeply gone down in recent years. Analyzing the youtube statistics for these talks can provide additional insight into the questions probed in this blog.,NA,Figure 4.Figure 5. Top three talks for each rating descriptions.Figure 6. Figure 7. The figure shows the greatest percentage of ratings relative to the total number of views for a specific description. Although some of these talks were viewed much lesser times these were best received regarding rankings by viewers. For example 'Building a park in the sky' by Robert Hammond was rated beautiful by the highest % of viewers.,NA
One Disney to rule them all?,11,https://nycdatascience.com/blog/student-works/one-disney-to-rule-them-all/,Disney's acquisitions over the years reinvigorated the company’s force in the film industry. As you can see in the highlighted table below nine out of the 15 highest grossing movies are from Disney. The question is: are they really above average compared to other production companies or those films are just outliers? Do Pixar Marvel and Lucasfilm have a considerable impact on its outcome?Even though many production companies were scraped I selected the top six companies which produced 505 movies for analysis :Doing some basic analysis I discovered missing values on some of the features. The movies that didn't have any box office information were removed from the dataset leaving us with 452 movies. The ones that didn't have the worldwide gross could be implied using the USA gross since the correlation between those variables is around 0.93. Building a simple linear regression model derived the worldwide gross from the USA gross. This model was used to predict 63 missing values.I also created a new variable called net worldwide income to show the difference between the gross and budget amounts.Trying to understand how Disney managed to beat the record of total worldwide gross in 2016 I analyzed their movies over the years considering their subdivisions. From the bubble graphs below we see that many of the Pixar Marvel and Star Wars movies have greatly positively influenced Disney revenue. The size of the bubble shows the difference between the gross and budget to show which had the biggest net return. The link to the dashboard is at the end of the post and can shows interactively what each bubble represents and additional info.In the year 2016 alone we can see that every subdivision from Disney had released a major film. Under Marvel it released and. Pixar released . From Lucasfilm we had Disney Animation had released two additional movies: andMost of the production companies have divisions and subsidiaries. That could be a problem in how they are represented. For some movies IMDB didn't include the parent company in the list of producers. To make up for that Wikipedia can be scraped to gather the parent information of each subdivision for more accurate results.It’s also possible to apply analysis to the distribution of the films over the year and try to extract some insights from there. For example see how each production company makes its yearly planning.Disney has been leading the box office war against other major production companies and it will probably continue to. The indications for 2018 are good for Disney. Two Marvel movies are already in the top 10 box office list ( and ). The movie reached 1 billion dollars in record time (10th day).  (Lucasfilm) is anticipated to open to recordbreaking numbers over Memorial Day. Pixar is releasing the second  movie in summer. And  (Disney Animation) is due at at the end of the year. With such a lineup it’s possible that Disney will beat its own record this year.,NA,To answer those questions I decided to scrap the IMDB website to gather information from movies from 2010 to 2017. For each movie I saved the title year budget worldwide gross USA gross opening weekend gross and genres. I used Scrapy (Python Web Crawling Framework) to achieve that task.The boxplot below shows the distribution of all movies per production company. We can clearly see that Disney has a higher average than the others. Based on the large interquartile range Disney has also more variability than the other production companies. The scatter points on the side of each boxplot indicate that the distributions are rightskewed. For that reason I had to use the BoxCox transformation to perform a hypothesis test. Analysing the result of the test I can conclude that Disney has a statistically significant difference in the average gross than the other big companies.Examining the companies per year we can see that Columbia Pictures had made most movies in the beginning of years analyzed; however later on Warner Bros. and Universal Pictures were alternating the leadership. Even though Disney doesn't have the highest number of films it is the one that has the highest total worldwide gross amount.,NA
Web Scraping Industry Salaries from Glassdoor,11,https://nycdatascience.com/blog/student-works/web-scraping-industry-salaries-from-glassdoor/,My interest in scraping glassdoor is due to the fact that I recently graduated from college. I was interested in seeing industry salaries for popular cities along with their associated ratings.The questions I asked:Using Scrapy my plan was to look up every job posting on Glassdoor.  After viewing each posting I would be able to retrieve information such as the job title the company’s name the estimated salary for the posting rating and industry the job was in.One issue that occurred was my attempt to retrieve information about the company. Even though it was listed in the source code of the website the code that Scrapy looks at to gather information my program was unable to scrape it properly. When attempting to debug my code looking for a solution I found that even though the information on the company was in the source code the website uses JavaScript something that Scrapy does not recognize to get the information from another link.After learning this I searched through the network files that was being transmitted in hopes of finding the source of the JavaScript. I ended up finding a promising file titled “” which contained a request URL. That URL which was the same for every posting except for a string of numbers which was the company ID contained all the information needed for that company.Once I found this separate page which hosted the missing data I was able to scrape the company information successfully.Another problem I encountered was a strange error where my code would break whenever it reached the 31st page of Glassdoor. As I investigated the problem I learned that Glassdoor only provides the first 30 pages for its viewers. Even though it states “30 out of 5277 pages” there is no 31st page (or any page past it)In order to gather enough data I decided to look up 16 different cities across the US and find all the job postings located in those areas. I also discovered a way for my spider to go to the next website programmatically by decoding what each part of the URL meant.After overcoming these two obstacles I was able to scrape all the necessary information from Glassdoor.,NA,The data collection seemed like it was going to be relatively straightforward but there were a few unforeseen problems that I encountered.After scraping for and preprocessing my data I was finally able to visualize the questions I first asked with a further investigation into the results.In this chart I summed the number of job postings in each industry. Looking at the top 3 job posted industries we have Business Services Information Technology and Finance. If we had more data it would be interesting to look at how each industry evolved over the years. For example it would be interesting to look into information technology and see when its inception to mainstream culture occurred.In this graph I averaged all the industry salaries along with the estimated salary low and estimated salary high to provide myself with the highest paying industry. The top five highest mean salary from highest to lowest is Information Technology Finance Manufacturing Construction Repair and Business Services. Again we see Information Technology and Business Services in the top portion of our industry salary.In this graph I was interested in seeing which industry had the most touted rating. It's interesting to see Construction Repair and Real Estate as the first and third highest rated industry. To dig into the data we've just visualized I found out there's a reason as to why those industries are so highly rated.According to  TINYpulse surveys revealed the top three issues standing in the way of happy employees were:In the construction industry a variety of organizations offer coaching and career mentorship to students. Once they’re ready for the workforce the industry has a long history of providing new workers with apprenticeships so they can learn the skills required to move on to tackle more challenging work.After looking at some metrics involving industry I was interested in looking at some states. To begin my search I counted all the number of job postings each popular state offered.California Texas and New York have the largest amount of jobs posted on Glassdoor.Aggregating each states average salary we see that California New York and Illinois have the highest averages.We see that Massachusetts has the highest mean rating followed by Califonia.Now that we've gathered information on industries I was interested in diving into the salaries job postings and ratings per city. The graph below represents cities that are popular amongst the younger workforce.As you can see New York Chicago Los Angeles Houston San Fran... have a large number of job postings. Now to dive a little further I took a look at the average salary.We see here that San Fran San Jose New York and Austin have a high average salary. It's assumed that San Fran New York and San Jose would have a higher average salary as we saw Business Services and Information Technology are the two highest paying industries. For those cities listed before they promote those specific jobs which will indicate the high salary pay. For the city of Austin I dug a little deeper as to whether there's a problem with my dataset. As it turns out Austin is a growing digital city which means there are many job postings for information technology jobs.Coming to a conclusion I was interested to see if their is a linear regression with rating and salary. We see here that there is no relation between salary and rating.,NA
Mapping Real Estate Sales in New York City,12,https://nycdatascience.com/blog/student-works/r-shiny/mapping-real-estate-sales-in-new-york-city/,"The most recent version of this app is accessible online  while its source code may be found .(It follows that its area will be  times the size of . Arguably this may foster a distorted perspective of the underlying price ratio and for that reason a later version of this app may well alter this. On the other hand the substantial difference in relative sizes makes it easy to pick out variations at a glance whereas smaller variations in size would be harder to see without zooming in on smaller areas of the map.)*All references to ""square footage"" here denote  meaning total floor space of a property.",NA,"This app is a visualization tool for examining real estate sales in New York City drawing on data published by the City of New York available . The data which cover the years 20032015 include a range of variables including price size category of building location and date of sale.The app plots sales from this data on a map to facilitate exploring the connections in particular between time location and price. The following screenshot displays a number of its features:As can be seen the main panel of the app consists of a street map of New York City (sourced from the  project) with dots plotted on it in varying sizes. Each of these dots represents a single property sale with blue dots corresponding to residential properties and red ones to nonresidential properties. (Note that ""property"" here is not synonymous with ""building"": many property sales are of single condominium units in large apartment buildings.)The relative size of these dots is set by the second dropdown menu on the lefthand sidebar labeled . By default they are set to a constant size (relative to each other) which is a good format to give an overview of the overall volume and concentrations of sales. Alternatively however their sizes can be set to vary in proportion with some property of the sale. In the illustration above for instance they are scaled by  with larger dots corresponding to greater prices. They can be scaled instead by  or by . In each case the scaling is linear with respect to the dots' diameters: for instance if the scaling is to price then if property  sells for three times as much as property  its dot will have a diameter three times the size of 's. Clicking on a particular dot brings up a popup bubble displaying salient data points about the sale it represents: address date price and square footage. (Note that all of these data points are displayed when one clicks on a dot regardless of whether they are reflected in the dots' scaling.) Clicking again will banish this bubble.The map may be zoomed in or out via the + and  buttons in its top left corner. The area shown can be changed by clicking the map and dragging it as desired. The adjustable 'slider' beneath the map displays the range dates for which sales are plotted. By default this range is set to thirty days beginning with the date of the first sale in the loaded dataset but it can be adjusted by moving the sliding buttons to the endpoints of the date range one is interested in. Alternatively one may click on the slider to the right or left of the selected range which keeps the range at the existing size while 'jumping' it to the point clicked. Finally clicking the play button at the bottom right of the slider causes the selected range to automatically progress from left to right until it has reached the end of the dates available. While this is progressing the button may be clicked again to pause it.At present this app displays only a small fraction of the data available; specifically it displays sales that took place in the Bronx in 2015. (For this reason the top dropdown menu in the left sidebar labeled  offers only Hobson's choice.) However as I update this app I intend to make the rest of the data accessible the same way as well as adding ways to further explore the data. This blog post too shall be updated so check back soon!",NA
NYC restaurants reviews and inspection scores,12,https://nycdatascience.com/blog/student-works/nyc-restaurants-reviews-and-inspection-scores/,If you ever pass outside a restaurant in New York City you’ll notice a prominently displayed letter grade. Since July 2010 the Health Department has required restaurants to post letter grades showing sanitary inspection results. An A grade attests to top marks for health and safety so you can feel secure about eating there. But you don’t necessarily know that you will enjoy the food and experience courteous service. To find that out you’d refer to the restaurant reviews. For this project I looked at a simple data analysis and visualization of the NYC restaurants reviews and inspection scores data to find out if there is any correlation between the two. The data will also show which types of cuisines and which NYC locations tend to attract more ratings.Nowadays business reviews ratings and grades are the decision making for any business to measure for their quality popularity and future success. For restaurants business ratings hygienic and cleanliness are essential. The restaurant ratings and location information used in this project come from Yelp’s API. The inspection data was downloaded from NYC open data . I merge yelp restaurants review data and inspection data and remove NA rows which doesn’t haveThe data shows that an A is the most commonly assigned inspection grade for restaurants of all types in all locations. I plotted various bar plots to visualized the inspection scores and ratings based on borough and cuisine type.With respect to location this borough bar plot shows that MaAs for cuisine types the cuisines plots shows first 15 restaurants with highest number of counts for based on cuisine.  This indicates that the American cuisine has highest number of A grade compared to other. This indicate that american restaurants are focus more on hygienic and cleanliness compare to others type of restaurants. The review plot indicates that most  restaurants do achieve the top rating of 4 stars. Again Manhattan has the highest number of restaurants with ratings four stars while Staten Island has lowest numbers of restaurants with high ratings. It also shows that almost all borough have a low number of  2 star restaurants. Moreover cuisine reviews plot indicates that American cuisine tend to have the highest rating compared to other cuisines. The reasons could be more American restaurants under this category then others. The scatter plots shows theThe cluster map of NYC restaurants helps visualize locations and  to filter the restaurants based cuisine types. The color mark of the point indicates the ratings and includes  descriptions of the featured restaurants. The heat map show the density of the restaurants based on borough selection or cuisine selection. It indicate which area has a greater number of restaurants. This could be helpful for business people to make informed decisions about where to  open new restaurants based on the types of restaurants already in place.Finally this app can be useful for people to filter the data base on borough cuisine  ratings  and inspection grade.  The people want to go to eat with specific criteria can filters the restaurants and visit their favorite restaurants based on top marks for both ratings and inspection grades. The shiny app link is .,NA,  A popular site for reviews Yelp offers many individual ratings for restaurants. The New York City Department of Health and Mental Hygiene (DOHMH) conducts unannounced restaurant inspections annually. They check if the food handling food temperature personal hygiene of workers and vermin control of the restaurants are in  compliance with hygienic standards.. The scoring and grading process can be found.either inspection score or reviews. I also reassigned the inspection score in the grades A B and C category as this measure is widely used and label on restaurants. There were other scores primarily P or Z or some version of grade pending which we are ignoring in our analysis here. Restaurants with a score between 0 and 13 points earn an A those with 14 to 27 points receive a B and those with 28 or more a C.nhattan has highest number of restaurants with all grades compared to others. This is obvious as it has highest number of restaurants in general.  Staten Island has lowest number of restaurants with grades A B and C among all.relationship between inspection score and rating. It indicates that there is no direct clear correlation between two variables. It is fairly common for a  restaurant with a C grade inspection score to achieve a 45 star ratings in a review. Also it is possible to find a number of A grade ratings for restaurants that only have 12 stars.  This could be because so long as food is tasty people will rate the restaurant well because they do not pay very much attentions to cleanliness and hygienic issues. The scatter plots also show that though some  restaurants maintain a very high level of cleanliness and hygienic food conditions they fail to get good ratings which could be due to bad service or less than tasty food . We can do further analysis on both side of  restaurants by analyzing review comments and try to find why some restaurants have good reviews but low inspection score and viceversa. This require further data about reviews comments and further analysis using NLP.,NA
Transactional Cost on High Frequency Trading,12,https://nycdatascience.com/blog/student-works/transactional-cost-on-high-freq-trading/,The orders in the file are generated by different Alpha Engines. Since the final executed prices are given in the data we can directly calculate the PnL for each order by combing corresponding midprices. After grouping order PnL for each engine we may get some insights into the  between AEs in  costs.,NA,"Special thanks to my great teammate Qiang Ji.This research explores two execution approaches i.e  and . In short Market Taking (MT) method allows us to send market order and aggress market immediately with the latest quotes. While Opportunistic Market Making (OMM) method is more risky which will send limit orders and wait under the certain time limit for a more beneficial price to fill the order. More specifically OMM can also be divided into 2 types according to how to set the limit order price: OMMMid will set the midprice as limit price; OMMSide will set a limit price on your side. Undoubtedly different execution methods should have different characteristics.The 3 execution methodologies can be summarized as:It needs to be mentioned that in this report we define **execution PnL** for each order as:

where ExecutedPrice is the price where an order is finally filled; MidPrice is the latest midprice when entering the market and Buy is a signal {11}.Our team also implement some rules for the stop loss and the time out.The code will be on my In this part we will backtest 3 approaches with SL0.00035 and TTE10 and report some statistics in the recapitulating table. In addition to required statistics we also computed **Hitting Ratios**(proportion of positive PnLs) 95VaR and standard deviation of PnL for each method in order to provide a more comprehensive view of the execution performance.By comparing total PnLs of the 3 execution methods we find that:OMMSide > OMMMid > MT
OMMSide has the highest PnL as the result of successfully taking opportunistic executions. Another interesting thing we found is that all the 3 execution methods have a dramatic decline in PnL around 2 AM Jan 11st. At that time market quote happened to have an unexpected widen spread.Since the original numbers of PnL are too small we rescale them by multiplying 1000. As shown in the table PnL of OMMSide is the highest and also volatile. PnL of MT is the lowest but also less risky. As compensation the execution difficulty is also higher if you are seeking higher average PnL. OMMSide tended to have the highest execution time and also be more likely to touch SL or TTE.
We set the TTE to be 10 s constant we enumerate different SL to test its influence on PnL median as well as Execution Time median.differenceexecutionFrom the perspective of execution PnL we find MAR performed a better opportunistic method in execution and got a positive average PnL. On the contrary SOM performed worst since it ordered many times but ended up with lowest average PnL. It seems more times you trade more likely you will suffer a large transaction cost. That is to say average PnL is negatively correlated with trade times.Another common OMM execution method in practice is called ""Fill or Kill"". This approach also sends limit orders. But unlike previous OMM strategies: ""Fill or Kill"" would wait a certain time to see whether limit order could be filled. If time is out and order is not filled then cancel (""kill"") the order. Obviously implementing this strategy all the executed orders should have positive PnL while there also would be some canceled orders leaving NA in the PnL column. We believe it is worthwhile to dig into such a strategy and compare it with the others.",NA
How to prepare for a data science interview,13,https://nycdatascience.com/blog/data-science-news-and-sharing/how-to-prepare-for-a-data-science-interview/,Founded in 2013 the NYC Data Science Academy offers the highest quality in data science and data engineering training. Their toprated and comprehensive curriculum has been developed by industry pioneers using experience from consulting corporate and individual training and is approved and licensed by the NYS Department of Education. The program delivers a combination of lectures and realworld data challenges to its students and is designed specifically around the skills employers are seeking including R Python Hadoop Spark and much more. By the end of the program students complete at least four realworld data science projects to showcase their knowledge to prospective employers. Students also participate in presentations and job interview training to ensure they are prepared for top data science positions in prestigious organizations. For more information visit .,NA,You’ve spent months studying data science now it’s time to find a job in the industry. Fortunately companies all over the world are looking to hire data scientists  and fast. According to  machine learning engineer and data scientist positions grew 9.8x and 6.5x respectively in the past five years. Candidates who are trained will have a bevy of vacancies to choose from when ready to seek out a new position in the field.Though there are many open data science positions the interview process can be rigorous. The technical tests are tough but hiring managers still look for soft skills when hiring candidates as well. The LinkedIn report found that soft skills still matter even for highly technical positions. HR still screens for adaptability leadership and the ability to learn from others when hiring for data scientists.For anyone ready for a data science career or ready to switch jobs here is how to prepare for this interview process. Though not every company is the same this overview provides the general guidelines for what to expect when seeking a data science position.The first stage is a phone call as with any job. The HR manager will call the candidate to screen out anyone that is clearly not a fit. Often this first phone call is not necessarily about technical abilities but rather to see if the candidate can carry on a conversation that he or she understands the businesses and that the potential employee did some research about the company. The HR person taking this call wants to hear your interest in the company as well so ask insightful questions but don’t worry about getting too technical as you’re not on the phone with the chief data officer.The second stage is likely a call with a team member. At a large firm the data scientists might be split up by specialty so the call will be with a midlevel team member with your skill set. At a small firm however data scientists are sparse so you may end up on a call with the CDO sooner rather than later.This call will evaluate both technical and soft skills so be prepared to take on overthephone coding challenges. The interviewer(s) will ask about past experience and will want you to walk them through projects. Furthermore your mathematical statistical coding and analysis skills will be put to the test with pseudoproblems proposed and wellthoughtout answers expected. Interviewers will want to hear your process how you work through problems what additional questions you ask and whether or not you can get the correct answer. Furthermore the interviewer will want to listen to how you deal with a semistressful situation.An option stage that some companies take is an online test; they may send the test after the first or second stage. At times businesses will want to weed out candidates seeking those who can complete the test while letting go of the ones who do not have the technical ability to do so. Usually the tests are timed so companies can discover the most compatible candidates.The next stage is usually onsite. Especially if the candidate is not local the interview might be an allday experience. The onsite interview will likely include a meeting with the head of HR or head of hiring the CDO and a team and/or team members. An additional technical challenge or test could also be administered this day or after the interviews are done.On the technical side the interviewers will throw different scenarios at you and watch you complete the problems. They might also ask you to explain different concepts to a business leader to see how you can take data and translate it for practical use. Those soft skills are fundamental so that the interviewer can see how you’ll fit on the team.Preparation is crucial for data science interviews. If you cannot find interview questions already listed on sites like Glassdoor (for example here are ) review potential technical questions for each level on other  sites to help understand what might be asked. If you worked with a recruiter ask them about the structure of the interview or any preparation/tips they can give provide.To really prepare try spending time on . The site allows users to practice skills in SQL Python R Hadoop and Spark. Candidates can review the over 1000 coding challenges theory problems and case studies to be fully prepared for any type of data science interview.Practice any skills you know you’ll be tested on so that when you perform tasks in front of interviewers your processes are smooth. While there will be a time when you have to sit and think through a problem make sure it’s not one you should know how to do. You will have an idea about which skills the company will test based on the position.Participate in mock interviews to runthrough how you’ll answer basic questions or even how you’ll pause to think about an answer. Some of the technical questions might cause you to stop and think but you want to sound confident and not shaky. If possible talk through scenarios with other data scientists to rehearse questions for the interview.Formulate questions to ask each person during each interview. It’s important the interviewers know you are an enthusiastic and interested candidate that is eager to learn more. Lastly follow up with a thank you note to show your dedication to landing the position. Following these tips and knowing what to expect can help you get a new or first job in data science.,NA
NYC Real Estate Market Visualization,13,https://nycdatascience.com/blog/student-works/nyc-real-estate-market-visualization/,Perhaps you’re curious about the types when I set out visualize the NYC real estate marketcontains sales records dating back to 2003 that are updated a number of the neighborhood features including For frame of reference elevator apartments buildings are the majority of buildings types in Manhattanfor that are its proximity and many office buildingsthe New York City,NA,Did you ever wonder where the most popular neighborhoods are in New York City? Or which neighborhoods are the most expensive? of properties people are buying and their median price? I had these questions in mind The data is found from the NYC Department of Finance Rolling Sales Data.NYC Department of Finance Rolling Sales Data monthly. For this particular project I used sales records from the time period of March 2017 to February 2018. The variables I focused on were Boroughs Neighborhoods Building Types Sales Prices and Sales Date. I also cleaned out all the zero sales prices due to transfer of ownership. In addition I cleaned out tax class 3 and tax class 4 properties that are mostly made up of office buildings gas stations factories etc.For detailed coding and data cleaning process. See my GitHub repository by following this link:https://github.com/phoebezzz/NYCPropertySalesShinyNow that we have our data set with residential property sales for the past 12 month let us discover what it looks like!Here is the link to my Shiny App. Feel free to explore for yourself in any neighborhood or building types that interests you:Mapping out the entire year worth of transactions amount by neighborhood reveals the answer to our first question.Close to 5000 properties were sold during the past 12 months which is 5 times more than Midtown the location in second place with a transaction amount of 1015. That indicates that the Upper East Side is the most sought after in Manhattan by far. This can be due to several reasons from fancy restaurants shopping experiences on Madison Avenue and the proximity to many cultural museums and beautiful Central Park.But what kind of apartments are buyers trying to buy? Let's take a look at the pie chart for sales breakdown by building types:When is the peak season for NYC real estate market? The bar graph will give you a trend analysis through out the year. We can see thatThis makes sense since summer time is the best time for most buyers to go out and explore options without the worry of the cold or possibly the commitment to their children’s school activities.What about price?Possible reasons for  to the action Madison Square Park and 5 Ave. While SoHo is located at the lower side of Manhattan filled with cute boutiques highend shopping and restaurants. Tribeca is slowly turning from its old industrial buildings to residential lofts that are located alongside the Hudson River with many trendy boutiques and restaurants.Why the change? Take a look at the pie chart we find our answer.Now we have take a closer look into  real estate market. What is happening in the other 4 boroughs? I created a heat map for the transactions for all 5 boroughs. The size of the bubbles are according to the sales price.We can see thatnot surprisinglymost of the high sale price transaction still took place in Manhattan especially around Central Park and downtown. But some neighborhoods in other boroughs are on the rise. As in Williamsburg in Brooklyn and Long Island City/Flushing in Queens.  Large sales are also taking place in Bronx.You can check all the detail records and filter by boroughs and sales price range on the data table on the third tab.There is still so much that can be done with this data set and a lot more to find. In the future I will like to enhance the apps to explore additional aspect including:,NA
Machine Learning Techniques for Predicting House Prices,14,https://nycdatascience.com/blog/student-works/machine-learning-techniques-for-predicting-house-prices/,"By Weijie Deng Han Wang Chima Okwuoha and Shiva PanickerFor decades the American Dream has been defined by the pursuit of home ownership and in 2017 Zillow reported that the median value of U.S. homes hit a record high of $200000. Housing prices are determined by a slew of factors; some quantifiable and some not. Being able to isolate the best factors for predicting housing prices has a clear use for realestate investors as well as individuals looking to move into a new home. In this project we used compared and stacked multiple machine learning algorithms to predict housing prices in Ames Iowa. The data set included 80 variables each that have a varying impact on the final housing price. The variables included details about the lot/land location age basement roof garage kitchen room/bathroom utilities appearance and external features (e.g. pools porches etc.).The data frame provided was relatively clean which allowed us to focus on making the variables compatible with our models.After looking at the Sale Price feature we saw that it did not follow a normal distribution so we decided to transform it to better represent a normal distribution. The transformation we used was a logarithmic transformation of the form log(price + 1). After transforming the sale price we noticed several numeric features were not normally distributed. This was problematic as certain linear models are based on the assumption that predictors are normally distributed and violating these assumptions could negatively affect the linear model thus making predictions inaccurate. To fix this we decided to skew those with degrees of skewness larger than 0.75. We filtered the following features to be skewed which were '1stFlrSF' '2ndFlrSF' '3SsnPorch' 'BsmtFinSF1' 'BsmtFinSF2'  'BsmtHalfBath' 'BsmtUnfSF' 'EnclosedPorch' 'GrLivArea' ‘'KitchenAbvGr' 'LotArea' 'LotFrontage' 'LowQualFinSF' 'MasVnrArea'  'MiscVal' 'OpenPorchSF' 'PoolArea' 'ScreenPorch' 'TotRmsAbvGrd' 'TotalBsmtSF' 'WoodDeckSF'. We use the same logarithmic transformation log(x + 1) to make the features normally distributed.Now we had to figure out a way to deal with the ordinal features and categorical features.For the ordinal features we read in the descriptive text to extract those features so that we could turn them into numeric features. We fed them into a dictionary and applied them to columns.For categorical features we had tried both onehot encoding and label encoding. In linear models and random forest dummification had better performance than label encoding. So we use onehot encoding to handle all the categorical features. Onehot encoding will turn all the items in one predictor into a new column in which 1 means True and 0 means False and delete one item to make it obey the linear model assumption.We used cross validation to find the best parameters for a  gradient boosting regression. These parameters were learning_rate  0.04 max_features'sqrt'n_estimators500 max_depth4subsample0.7min_samples_split  10min_samples_leaf 2.The following graph is of the top 30 variable importance. The top two most important variables are ""Fence"" and ""Full Bathroom."" However there is a limitation that when using dummified data to fit tree models they will usually be misleading and have incorrect answers.Although the RMSE of gradient boosting is 0.118 the RMSE on Kaggle is 0.166. This model has a very serious overfitting problem. We considered the problem should result from the tree model.",NA, The graph below shows the features that have a strong linear correlation to each other. Linear features follow the line of best fit such as in the case of case of overall quality predicting sale price. For those colored in red we can delete one of those two having high correlations. Through this graph we can easily tell which features have the highest correlation with each other.The decision to implement a support vector regression was based on the fact that our data was very sparse and was of high dimensionality. Support vector regressions actually perform well in high dimensional scenarios mainly because in higher dimensions it's easier to construct a hyperplane which splits the data nicely.To implement the SVR we used a builtin SVR function in the sklearn module (from sklearn.svm import SVR). SVR is a computationally demanding model with nonlinear kernels being more demanding than the linear kernel so in the interest of time we chose to go with the linear kernel for our support vector regression. We set our epsilon to be 0.1 and applied a grid search crossvalidation to select the best performing model.To our surprise SVR had the lowest performing test score out of all the models with an RMSE of 0.14602. At first we thought this was due to a poor choice of kernel and epsilonwe thought a nonlinear kernel would perform better than the linear kernel we chose to go with. Ultimately though the low score was mainly attributed to size of our training set. While support vector regressions perform well on high dimensional data sets they also require a large number of samples to train on. With our data being very sparse after encoding and having a dimension of approximately 1500x250 the support vector regression did not perform as well as the other models we used.a. Data cleaning is arguably the most important and most time consuming taskb. Give a hypothesis of which simple model may work best on the given datac. Implement the simple modeld. UNDERSTAND the model and why it gave the output it dide. Update hypothesis and repeat (2),NA
Manhattan Traffic Collision Insight,14,https://nycdatascience.com/blog/student-works/r-visualization/manhattan-traffic-collision-insight/,I used the NYC Open Data Portal to obtain the NYPD Motor Vehicle Collision data. and analysis the of the 15205 crashes that occurred in Manhattan in 2016.I correlated that with weather data obtained from Kaggle to find the weather conditions when a crash happened. The geographic information system (GIS) data of VMT was obtained from NYSDOT.Vehicle Miles Traveled(VMT) data is defined as a measurement of miles traveled by vehicles within a specified region for a specified time period. It is a traditional index to represent traffic volume. In this project I want to visualize the crash data in Manhattan and help the people who are interested in the vehicle accidents gain insight into the space and time distribution. For every crash I record the type of the vehicle the cause of the crash and the weather condition when that crash occurred as the option combination. One can select any combination of options to take a look at one aspect of the data set.With over 15000 data points it is not practical or even necessary to visualize them all in the map.. I divided Manhattan area by 281 districts and use the red circle to represent the crash density in each district.I set three modules. The origin data” shows  the number of crashes happened in the district visualized by the radius of a red circle..The second module is called “normalized by number.”. For some option combination the total number of such kind of crashes are very small and the red circle would be invisible in the map. In that module the radius of the circle is the number of the crashes in the district given by the options selected divided by the total number of crashes with that options which represent the relative crash density.We may also consider the traffic volume. The high frequency of car crashes in some area may be due in part to high traffic volume. In the view of a driver one may be more concerned about the crash rate per vehicle. To show the crash density normalized by traffic flow I made the third option called ‘Normalized by VMT.’ In the third module we can find that the red circle in central park is very large. Although there are fewer crashes there the driver should be extremely careful because the crash rate per vehicle is high.A shortcoming of the hot map is that it doesn’t show the extract location of every crashes. In the heat map one can locate the accident at street level. The time distribution is also very important. I group the data by hours and analyze the time distribution of different crash causes weather condition and vehicle type. For most conditions the number of crashes start rising in the morning and reaches a peak around 16:00. There are some type crashes eg. caused by driving under the influence and the like which  have a different time distribution and reaches the peak at night.,NA,Vehicle collisions have increased in the last decade. Currently road crashes are ranked as the ninth leading cause of death in the world.Thanks to  computerized data the detail of every single crash is easily open to public. I try to analyze the crash data in Manhattan in 2016 and shed light on the cause of the vehicle accidents. Feel free to play around my  to check out some cool graphs. All the codes are available on my .,NA
"Predicting Residential Home Prices in Ames, IA",14,https://nycdatascience.com/blog/student-works/predicting-residential-home-prices-in-ames-ia/,The data for this project came from a presplit evenly into a training set and test set with 1460 observations and 79 features. The first model that we used was Ridge Regression.  Ridge Regression minimizes the sum of the RSS and the penalty which is the sum of squared coefficient estimates.  By finding the value for lambda that minimizes this sum the model aims to shrink the group of coefficients in our model which prevents the model from overfitting and allows for more robust predictions. After tuning the model we found that the optimal value for alpha(lambda) was 9.48 which resulted in an RMSE of 0.1207. Although Ridge Regression is able to shrink coefficients such that they asymptotically approach zero it cannot set coefficients exactly to zero like the next model.The second model that we used was Lasso Regression. Lasso Regression is similar to Ridge Regression because they both strive to minimize the sum of the RSS and a shrinkage penalty. The difference between the two models is that the Lasso Regression model uses a different shrinkage penalty.  Lasso Regression uses the  penalty whereas Ridge Regression uses the  penalty mentioned above.  After tuning our model we found our alpha(lambda) to be 0.00048413 and the model’s RMSE to be 0.1195.  A key assumption in Lasso and Ridge Regression is that the target data is linear. We chose to incorporate the next two models into our metamodels because they do not make this assumption.The third model that we used was Random Forest Regression. A Random Forest model builds different decision trees on bootstrapped data from the training set. The decision trees are different because each tree can only consider a random subset of the total number of predictors at each split.  Thus only predictors in that subset have the opportunity to be the splitting factor. This characteristic of the Random Forest model creates decision trees that are not correlated and reduces the variance of the model. After tuning our hyperparameters our model yielded an RMSE of 0.1239. Like the Random Forest model our last model is based on the premise of combining many weak learners into a single model that is a strong learner.The fourth model that we used was Gradient Boosting Regression.  Gradient Boosting Regression makes an initial prediction; then the algorithm improves on that prediction by adding an estimator term to the initial prediction.  Next it optimizes the model by fitting the estimator term to the residual using gradient descent. Our optimized Gradient Boosting model had an RMSE of 0.1301. After optimizing each of the individual models we used them to create two different metamodels.The first metamodel averaged the predictions of the four base models.  We weighted each of our predictions equally. If we were to continue to develop this metamodel we would perform a gridsearch using cross validation to find the weights that yielded the optimal result via a weighted average of the predictions. This metamodel yielded an RMSE of 0.12582 when submitted to Kaggle.The second metamodel stacked the base models.  We took the predictions of each of our individual models and created a fourdimensional matrix.  We then performed multiple linear regression to yield our final prediction.When we submitted this model to Kaggle we yielded an RMSE of 0.1267.In order to effectively predict residential home prices it's important to examine external factors as well. This includes supply and demand indicators such as construction pipeline net absorption days on market. Furthermore when predicting something like home prices it’s important to take into account the local economy. This would include investigating job growth income levels and familymix in the local area.,NA,Which factors contribute to the sale price of a house? There are plenty of answers: size matters as do the building materials. Location is important. Does it have a backyard? A swimming pool? What’s the garage size like? How old is it? As you can see the number of ways you can assess a house’s price quickly gets out of hand. It’s nearly impossible to accurately and heuristically judge a house given a large set of variables. But is there a way to look at those variables simultaneously and quantitatively to accurately predict the market sale price?The answer is yes. The power of machine learning provides us with the tools we need to look at a large data set and spit out a predicted value. Using a dataset containing information on houses in Ames Iowa our team leveraged different machine learning techniques to predict respective sale prices.Upon receiving the dataset we first performed an  to assess the completeness of the data and to see if any variables needed to be altered so that they would be usable by our models. For example categorical variables would need to be dummified or converted to ordinal format and certain numerical variables might need to be transformed to ensure a usable distribution.Next we identified or engineered  and conversely dropped variables which did not strongly correlate with sale price.With a condensed dataset we then began applying different types of to the data to get a sense of which models had the most predictive power. Critically we performed a series of rounds where we  iteratively to minimize our prediction error. After identifying the most successful models we ensembled different model combinations together to further improve our predictions.Finally with a set of sale price predictions we  to see how well we fared against the competition.We first explored our target variable Sale Price. In order to meet a key requirement of regressive models we applied a logarithmic transform to this variable. This gave us a more normal distribution from which we could train our model.The 79 features were a mix of 28 continuous and 51 categorical variables. 34 features contained missing values and our team used a variety of techniques to handle this including dropping variables outright that had 99%+ missingness (e.g. Utilities) and the application of imputation methods on features we deemed critical for our analysis (e.g. Lot Frontage).We generated the following criteria to transform features:We established a correlation plot to identify features that were highly correlated with one another. Based on this analysis we removed a few additional features allowing us to address multicollinearity.With a clean dataset we were ready to begin modeling our data.The four models we combined in each of our metamodels were Ridge Regression Lasso Regression Random Forest Regression and Gradient Boosting Regression. We optimized each of our base models by performing a grid search and 5fold cross validation to find the optimal hyperparameters that yielded the best prediction. Next we ensembled (or “stacked”) different models into metamodels. In the first metamodel we ensembled the models by averaging their predictions.  In the second metamodel we stacked the four models and used multiple linear regression to make our final prediction. For a more detailed modelbymodel breakdown see directly below.Our team used this opportunity to simulate a reallife data science project. It was a great opportunity to learn from one another and collaborate to productionalize machine learning algorithms.However given limited resources (a relatively small dataset) we made do with what we had. The project was fantastic practice in tweaking models to maximize predictive power and minimize model overfitting. These skills will surely prove useful as we go on to future projects in the data science field.,NA
Social Network Analysis with Scalable User Behavior Scores of a Music Website,14,https://nycdatascience.com/blog/student-works/social-network-analysis-with-scalable-user-behavior-scores-of-a-music-website/,"To build the desired social network we need to take into account various kinds of user behavior information including following and rating to artists saving spotlighting and listening of songs of an artist etc. We count and convert all the available user behavior data into different feature variables which will then be used to calculate an overall usertoartist fanscore. The fanscore accurately represents/defines the relationship from a certain user to an artist (or to another user in the case of one user following another user). Based on that a social network graph can be accordingly derived.To measure the activeness of an individual user we take into account the total number of user activities on the website along with some other specific user activities e.g. sharing and commenting etc. For each user behavior feature variable we intend to calculate a score of the same consistent range from 0 to 100. In order to do that we need to conduct some preprocessing on the various different feature variables. First all of these behavior variables are different kinds of activity count. We normalize all of them with their respective measuring period of time in terms of the number of months. As such each activity variable becomes a certain number of ""count per month"" which is very straightforward to interpret and control. the NetworkX Python package for SNA. We built a For the entire social network graph of all the users and artists of the music website we tested and conducted user influence analysis in many different ways (i.e. with ) and thus got an influence score for each user/artist. Specifically wWith all the above user scores and network works done finally we are able to give our answers to the targeted questions of the project. One future direction is to add unpopular artist handling. We found that f",NA,"In a music website there are artists and regular users as critics. While the artists launch and share their original music songs through their profiles  all the other regular users enjoy finding and listening to the songs and interacting with the platform in many different ways such as saving spotlighting music rating commenting sharing and following artists of interest. Moreover one user may also follow another user(s) if he/she finds out that the other user always finds and shares music that are similar to their taste. These elements make up a dynamic and changing online community of music publishing listening sharing and rating etc. or a so called ""social network.""In this musicoriented ecosystem how can we build an efficient network model such that all the aforementioned various user behaviors can be well captured such that for marketing purposes valuable information may be properly derived? Specifically for a certain specified artist can we find out the top influential users from his/her fan network? Can we identify who his/her true fans are? Also of the social networks users who of them are the most active users by themselves?These are the specific questions/tasks we received from a music tech startup company which become the very problems/goals of this team capstone project. Moreover of particular emphasis/challenge is to make use of the available data as much as possible and derive accurate and practically reliable/usable insight information while at the same time the involved algorithm is readily scalable to accommodate more user behavior variables in the future.After time normalization we identify some certain user behavior variables of much larger variation ranges than the others for which we apply log transformation to reduce their respective variation range to be similar as those of the others.For the calculation of a usertoartist fan score in order to make it be easily scalable for many more user behaviour variables possibly added in the future we design and apply exactly the same algorithm (but just with different parameter values) to calculate a score for each variable respectively. The scores are all of the same range and thus can be easily aggregated together via simple weighted averaging operation into one final overall integrated fan score. Similarly we calculate an overall user activeness score.The fan score calculation is based on the scores of a user's music listening and other artistoriented behaviours like following rating listening etc. while a user's activeness score is calculated more based on a user's overall website activity.Given the fan score we need to further detect whether a fan of a certain artist is a true fan or not. For that sake we need to further consider what fan scores the certain fan user have toward the artists other than the concerned one. In case a fan shows strong interests to many other different artists at the same time i.e. having high fan scores for multiple artists that fan's truefanship score should be somewhat adjusted to be lower than the original fan scores. In practice we developed a simple and robust algorithm to conduct such adjustment properly.With the usertoartist fan score we can build a social network of all the artists and regular users and conduct proper analysis as needed. Herein we use directed graph with weighted edges between users and artists where each edge starts from a user to an artist and the weight of each edge is the related fanscore.e tested with 4 different influence scores: closeness centrality betweenness centrality Katz centrality (i.e. an extended version of eigenvector centrality) and the famous pagerank score. Finally we chose to use just the single pagerank score as our default for now.  is a famous algorithm from Google which has superior performance when measuring the importance/influence of different nodes in a directionally linked networks. An illustration graph of the algorithm is as shown below.A demonstration of the entire network graph is provide as follows. Herein the red nodes represent artists while the blue nodes represent regular users. The edges represent the ""following"" relationship between a user and an artist where a dark end/arrow indicates the outgoing direction.Given a certain specific artist we can identify all the artist's directly and indirectly connected music fans (i.e. the socalled ""ego network"" with the concerned artist being the central or ""ego"" node).Moreover as mentioned earlier the involved user score calculation algorithms are readily scalable when more variables may be added in the future.or artists that have no any or very few fans we give no or very poor recommendation on their top influential users. In fact it's better than nothing if we can find similar but more popular artists (e.g. of similar genre and/or other music characteristics etc) and return their top fan result instead to serve as some more meaningful recommendations for the unpopular artist.Another direction is that: besides the current top fan recommendations for the artists a music or artist recommendation solution can be developed as well to better serve the use of regular users.Thank you for the interest. 🙂",NA
Predicting House Prices Using Machine Learning Algorithms,15,https://nycdatascience.com/blog/student-works/predicting-house-prices-using-machine-learning-algorithms/,Inspired by the accomplishments of the women in the movie “Hidden Figures” we named our team after the movie. We are an allgirls team of three who come from diverse parts of the world  Lebanon India and China. This blog post is about our machine learning project which was a past kaggle competition  The dataset contains “79 explanatory variables describing (almost) every aspect of residential homes in Ames Iowa…[and the goal is] to predict the final price of each home” ().We collaborated on certain parts of the project and completed other parts individually as if it were a research project. The goal of the project as aspiring data scientists was to utilize our arsenal of machine learning knowledge to predict housing prices. The following blog post is categorized into four main parts: Exploratory Data Analysis & Feature Engineering Creating Models Conclusion and Relevant Links.We started the project research by analyzing the data and visualizing it. Real estate is a new area for all three of us but we managed to gain some interesting insights. For example some variables are closely correlated with one and other. Some pairs are correlated by nature such as “Basement finished area” and “Basement unfinished Area” while other pairs were correlated by deduction such as “Overall condition” and “Year built.”Next we explored the data to see if there were trends in sales prices associated with seasons. We found out that there are more sales during summer. Research shows that high supply doesn’t necessary means high price since the price of housing normally peaked around summer. One theory is that when there are more options in the marketplace people are more likely to find their ideal house and put down the deposit.The neighborhood is an important factor when it comes to buying houses. Since the raw data does not contain school district information and crime rate neighborhood was an important factor implying above factors. After plotting out the graph down below we went back and checked the accuracy neighborhood with the higher price was equipped with highend facilities besides school districts and great locations. We can also see the variance of an expensive neighborhood is typically higher which explains the skewness of the sales price density as well.After trying the linear models approach it is good to see the data from a different angle so I decided to try the treebased models approach. These are the steps I followed:Why start with Random Forest? Random Forest Model is considered one of the best models for feature importance analysis. The reason is that the treebased strategies used by random forests naturally ranks by how well they improve the purity of the node. The below graph shows the 30 most important variables in our dataset:The accuracy score of Random Forest Model on the house price prediction is 0.88. Here comes the question: Is it the best accuracy score? The following step is a comparison between several treebased models to check which model has the best accuracy score in predicting House prices.After applying each of the following models on the data set different accuracy scores were achieved as shown in the following graph:The Gradient Boosting Model has the highest Score with 0.90 and with error 0.095. So in the following steps I relied on Gradient Boosting Model in my analysis.To have a better performance of the gradient boosting model on our data set I used the GridSearch function to tune the parameters of the model. After several trials of GridSearch the following parameters were chosen with specific ranges:Huber or LsRange( 0.0001 0.0010.010.11)Range(100 1000 10000 14800)Range(1 > 15 )Here are some of the parameters that gave me the lowest mean squared error:Since stacking or averaging several models together might give a better accuracy score in the prediction two gradient boosting models were used in the analysis depending on the first two rows of the above table as parameters.On the other hand linear models might help in improving the score as well. I used the following two models that gave me good accuracy scores as well.In Ridge model analysis after the parameter tuning step I chose alpha to be 10. The error of this model is 0.11. In Lasso model analysis the same steps of the previous models were applied and the error is 0.11.To sum up the final model was the average of the 4 best models together: Gradient Boosting Model 1 Due to the lack of observations the first step should be linear models. I started by implementing Lasso and ridge both yield 0.92 (pretty strong) CV scores. Since the data contains a lot of categorical variables I was curious how well tree based model fit the model. Random forest Extreme Random forest XGboosting all yield okay result at the best during crossvalidation. The best performer has to be Gradient boosting which Fatima mentioned in details. I also explored a little in SVM and finally combined my models using stacking.Each one of us collaborated on the initial exploratory data analysis and feature engineering part. Then we worked on creating predictive models individually. Thanks for reading this blog post. Feel free to leave any comments or questions and reach out to us on through LinkedIn.,NA,“House Prices: Advanced Regression Techniques.”“Everything should be made simple as possible but not simpler” said Einstein and I took this advice when I started creating models. First I focused on simpler models such as lasso and elastic net before creating more complex models. Besides lasso and elastic net I utilized gradient boosting regression XGBoost light gradient boosting and random forest algorithms to build models.To ensure how well the models were predicting sales price I split the training data into two parts. One part was used to train my models and another part was to check how well the trained model predicted sales prices. Through crossvalidation techniques such as parameter and hyperparameter tuning the best possible metrics were calculated to check for model performance. This metric was called crossvalidation score.Although I created several models I selected the top 5 best models based on the crossvalidation scores and combined them by averaging.1. Random Forest Feature Importance Analysis2. Comparison between the following regression models3. HyperParameter Tuning of the model with the highest scoreLoss: Learning Rates: Number of Estimators: Maximum Depth:   4. Linear Models: Lasso & Ridge 5. Averaging Several ModelsGradient Boosting Model Ridge Model and Lasso Model. ,NA
How Many Data Science Jobs Are There? A Scrape of Glassdoor,16,https://nycdatascience.com/blog/student-works/how-many-data-science-jobs/,AbstractThe QuestionsThe Scrapping ProcessThe PlanningImages showing the relevant information that I would scrape (highlighted) from the job posting and the company description pages respectively.The Hiccups With ScrappingA picture of what was being “seen” by Scrapy due to the data being called by JavaScript. The company information is missing.The network file that Glassdoor uses to see which website to call to present company information.The URL and the page containing the company’s information.Despite Glassdoor stating that had 791 pages of listings…...I would get an error for any page past 30.The formula for the URL to search through all the listings for all of my cities.Extracting Description DataThe parts of the description that we still need to extractThe VisualizationsFurther Exploration and Next Steps,NA,Once we all graduate from NYC Data Science Immersive Program we will have become data scientists. We will have knowledge on advanced machine learning algorithms natural language processing and how to analyze and visualize large data sets. But then what? We probably should get a job in that field. Although people say that the data science field is really ‘hot’ right now what does this mean in terms of job prospects? How many jobs are offered to data scientists and how much do they earn? With these questions in mind I decided to scrape data science job postings from Glassdoor.When I first started this project there were three big overarching questions:Since these three questions are so broad enough many sub questions can be created and could affect my analysis. Some of the factors that I wanted to control for were:After writing down a checklist of questions and potential avenues to explore I was ready to scrape.Using Scrapy my plan was to look up every Data Science job posting on Glassdoor.  After viewing the posting I would be able to retrieve information such as the job title the company’s name the estimated salary of the posting and the description of the job (which includes the programs needed and educational requirements). I would also be able to get information about the company such as the industry it was in and its size. The data collection seemed like it was going to be relatively straightforward but there were a few unforeseen problems that I encountered.One issue that occurred was my attempt to retrieve information about the company. Even though it was listed in the source code of the website the code that Scrapy looks at to gather information my program was unable to scrape it properly. When attempting to debug my code looking for a solution I found that even though the information on the company was in the source code the website uses JavaScript something that Scrapy does not recognize to get the information from another link.After learning this I searched through the network files that was being transmitted in hopes of finding the source of the JavaScript. I ended up finding a promising file titled “companyOverview” which contained a request URL. That URL which was the same for every posting except for a string of numbers which was the company ID contained all the information needed for that company.Once I found this separate page which hosted the missing data I was able to scrape the company information successfully.Another problem I encountered was a strange error where my code would break whenever it reached to the 31 page of Glassdoor. My initial thought was that Glassdoor was recognizing that I was scrapping their website and banning me from their website. I soon concluded that this was not the case since I was able to access. As I investigated the problem I learned  that Glassdoor only provides the first 30 pages for its viewers. Even though it states “30 out of 731 pages” there is no 31 page (or any page past 30).In order to gather enough data I decided to look up 16 different cities across the US and find all the data science job postings located in those areas. I also discovered a way for my spider to go to the next website programmatically by decoding what each part of the URL meant.After overcoming these two obstacles I was able to scrape all the necessary information from Glassdoor.Once my scrape was finished I still had one final task to do before I could run my analysis. I designed my spider to capture all of the information listed in the description and convert it into one long sentence for every job posting. I still needed to extract the important pieces of information such as the necessary skills and minimum education levels from the job description.In order to get all the relevant information I used regular expressions. I couldn’t only tokenize the words in the description because there were multiple ways the skill or requirement could have been listed. For example a job stating a minimum of a master’s degree could say things such as “MA” or “Masters Degree.” or a “Graduate degree”. If I only tokenized the words in the description I would extremely underestimate the job that would require a masters degree. I also had to make sure I wasn’t getting false positive from sentences containing the word “master” (e.g.: candidates need ay in a financial modeling). I did this for various programs that a data scientist might use as well as for different educational levels required for data science jobs.Once all my code was extracted and cleaned I started my analysis and visualizationThe first visualization I wanted to see was a correlation plot between the variables I scrapped from Glassdoor and the salary the salary that was posted.: A Variable Correlation MatrixMost skills appeared to have a mild to moderate increase in estimated salary with Python being the highest. Job postings requesting Excel appeared to be lower paying job having a negative correlation with salary estimate. Salaryhigh and salarylow have completely correlated with estimated salary because the estimated salary on Glassdoor was generally the mean between those two figures. This should not be stated that all excel jobs are lower paying but the linear relationship between these two variables is negative. Further analysis would be needed to uncover the true relationship between all of the variables.The interactive graphs below are created through the python program Plotly.Number of Job Postings in Comparison to Required Educational Level.In this graph I measured the number of job postings in comparison to the required educational level for job posting. Note that jobs which state two different educational requirements (e.g.: “this position is open to people with a bachelors or a masters”) would be counted twice. I believe the reason why MBA listings are so low is because most jobs state that applicants would need a masters or graduate degree in a relevant field such as economics or finance.Salary Range of Job Postings in Comparison to Required Educational Level.I measured the salary range for each of the job postings and compared it to the required educational level for the job postings. The further the applicant’s education goes the higher the median and mean salary is. That said even though the salary of a graduate degree is significantly higher than an undergraduate’s we must factor in the opportunity cost of years spent not earning an income and further the applicant’s education. I also tested the significance of the pay between educational levels with an ANOVA and found the results to be significant.: Number of Job Postings in Comparison to Required Skills.Another factor that I wanted to know was what skills were needed in order to get a job as a data scientist. In our program we talked about the importance of SQL and it shows in this graph. It is the most desired skill for data scientists followed very closely by python. Other languages such as R and Java are popular as well but not to the same extent. This graph shows the number of job postings that ask for a specific skill or program.: Number of Job Opportunities and Median Salary by IndustryAs I continued exploring the data I was curious to see how many jobs were being offered in certain industries and how much they were willing to pay. In the graph above you can see the median salary for all of the industries and compare it to the number of jobs being offered in the industry. The average median salary for all of the industries was around $102000. The highest paying industry was Retail which Information Technology ($115000) offered the most number of jobs (with a median of $112000).: Data Science Salaries Compared to IndustryAlthough certain industries such as retail and information technology paid well I was curious to see if they had a large number of outliers causing the data to be skewed to the right. As you can see both information technology and retail have similar IQR (interquartile ranges) and not too many outliers. The mean of retail was also similar to the median providing evidence for a normally distributed salary range. This was true also industries such as finance insurance government food services and aerospace and defense.: Data Science Salaries Compared to Company’s SizeWhen analyzing my data I was curious to see how big the companies were that were hiring data scientists. I wanted to see how well small start up companies were paying in comparison to multinational corporations. From my findings size did not seem to make a major difference in the amount they are willing to pay for their data science positions.: Data Science Salaries Compared to Company RatingsAfter looking at other factors regarding salary I was curious to see if companies that paid more were more highly rated. The black line is a simple OLS regression estimating the data science salary based on the company’s overall rating. This regression does not have much explanatory power and is mainly used here as a visual showing which companies pay more controlling for their rating.: Number of Job Postings in Comparison to StatesAlthough everyone is talking about how Silicon Valley is hiring tons of data scientist I was curious how many data science positions were offered in other locations in the US. This graph provides a count on the number of jobs posted based on their state (or district for Washing DC). This bar chart should be taken with a grain of salt since cities had to be specified to retrieve enough data since Glassdoor only provided 30 pages of listings. I will address how I would improve this given more time in the Next Steps section of this blog post.: Data Science Salaries Compared to LocationFor this graph I selected the 25 cities that offered the most number of data science positions. Even though certain locations might offer a lot of data science positions are they well paying? I plotted the salary ranges for each of the cities in the graph above. As you can see the two cities with the highest median incomes for data scientists are San Mateo CA and San Francisco CA.: The Ratio of Data Science Salaries Compared to Yearly Rent Based On Compared to LocationAlthough California appeared pay their data scientists the most I also knew that these cities could be rather expensive places to live. In order to control for this I found rent information for each city from Apartmentlist.com. From the website I collected median rent information for a one bed room apartment for each of those cities and divided the estimated salary range they offered. When you control for cost of yearly rent it shows that places such as San Mateo San Francisco and Palo Alto do not pay the best. Instead jobs that are located in cheaper places to live such as Houston and Chicago offer better pay. Although this is a simplistic control for cost of living it does illustrate that higher pay does not always translate to more money being earned.This project is rather interesting to me and I plan on improving it after I graduate form the immersive program. One way I would want to improve my project is to get a larger number of cities represented in my scrapping of Glassdoor. I would like to include at least one major city in every state to get a more accurate representation on the number of companies that are hiring data scientists. I would also like my scrape to be updated on a regular basis since the number of postings change so frequently in todays market. I would like to set up a way to automatically scrape Glassdoor (and other job marketing sites such as indeed and monster) every two weeks to keep up with the ever changing demand for data scientists.I would also improve this project by getting a more accurate way to represent cost of living in each city. Although rent for an apartment is a start there are many other indicators I would like to use such as price of food transportation and entertainment. If I can more accurately control for cost of living factors then I can see which cities offer the highest paying jobs while simultaneously being the cheapest place to live.Another way I will want to improve my project is to create a model predicting the monetary value of each skill a candidate has. For example the question I would like to answer is how much more valuable is a candidate to a company if they gain mastery over a certain program such as Python or Spark. This would also lend itself very well to people looking for jobs but unsure on what they should learn to stay competitive in the field. I can envision this project becoming an application where a user would put in information about themselves (e.g.: their education the skills that they know the industry they want to work in) and they would receive output on what their estimated salary should be along with jobs that offer skills they already know. It can also recommend certain skills that they do not have to increase their value to a company thus increasing their salary.,NA
Mushrooms,16,https://nycdatascience.com/blog/student-works/r-shiny/mushrooms/,"When we hear the term ""mushroom"" this is the definition most of us think of. How much do we really know about mushrooms and the benefit of mushrooms. How many species of mushrooms are there? Which are edible and which are poisonous? Mushroom market was valued at $35.08 billion in 2015 and expected to reach above $59.48 billion in 2021.  With the food trend leaning towards vegan and natural foods mushrooms are the preference that a lot of vegans consumers tend to depend on due to the fact that they are great protein source and full of vitamins and nutritions. Most mushrooms you see in the grocery stores or super market today have been commercially grown by farmers in controlled environments.Mushrooms have a very short shelf life and cultivation is heavily labor intensive and requires high operation costs.  The rapid demand growth within the last ten years also means that mushrooms farmers will have to grow enough supply to fulfill the demand forcing them to use unwholesome cultivation methods which include excessive chemicals and fertilizers.  Such growing methods have consumers shifting away from conventional mushrooms and leaning towards organic or wild grown mushrooms.   This is one of the many reasons why mushroom hunting is still very popular in certain areas around the world where natural growing environments and climate allows. In the United States mushroom picking is popular in the Appalachian area and on the west coast from San Francisco Bay northward in Northern California Oregon and Washington and in many other regions.Mushrooms have been used in medicine as an ingredient in cooking in many cuisines and as a natural dye for natural fibers such as wool for clothing. Mushrooms for a long period of time now have been known as the ""meat"" of vegetable world.  There are about 100000 species of mushrooms and only 10000 are known to the North America region. So the question is should you eat wild mushrooms? The answer is yes and no. Yes because there are so many great tasting wild mushrooms out there.  No because there are more ""bad"" than ""good"" mushrooms grown wild. About 4% of the wild mushrooms are consumable and can be use in cooking. 25% are edible but not recommended and about 50% of all mushrooms are inedible with 1% of them being extremely poisonous.  Therefore only positive identified mushrooms should be eaten and do not combine different mushrooms type while mushroom hunting. ",NA,According to the graphs above mushrooms grown on paths  should not be consume since most are poisonous. As for the mushrooms that grow in the woods there are more edible varieties compared to poisonous but there are still a good number of poisonous mushrooms.The data shows  that most poisonous mushrooms have these common 4 colors : red gray brown and yellow. Most edible mushroom have no odor. Any scent of odor especially foul spicy or fishy indicate that the mushroom is poisonous.Gill color and spore print color on mushrooms can also be use to help distinguish if a mushroom is edible or inedible.Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.) New York: Alfred A. Knopf,NA
Web Scraping: Building an App to Find the Perfect Tennis String,16,https://nycdatascience.com/blog/student-works/web-scraping-building-an-app-to-find-the-perfect-tennis-string/,Finding the ideal tennis racquet string is a challenge for many players because there are thousands of different types available varying in material construction shape/texture and thickness. To add confusion two strings with the same ‘specs’ can play very differently and the same string can play differently when used by different people or strung at a different tension. Because of this dizzying array of possibilities many players just ask their stringer to pick a string for them and do not put much thought into optimizing this important piece of equipment – the only part that actually touches the ball during play.For players who do try to find the best string for their game the only thing to do is test out a variety of strings until you find what works. As an avid tennis player who tinkers with different string combinations I have found  to be the best resource for finding strings to try because the site has so many reviews (17500+ reviews by 4400+ unique reviewers) and covers almost every string on the market (2350+ varieties). However even though it is the best resource currently available the website has very basic search filtering and ranking capabilities that limit its usefulness.This is especially tragic because the site gathers great data! For each review stringforum not only collects information about the the  (ratings across seven categories an overall satisfaction rating the adjectives that best describe the string and a text review) but also about the  (gender age bracket playing style ability level swing speed and how much spin they use in their strokes) and the  used in testing (manufacturer model frame size string pattern string tension level). In addition to data gathered from reviews the site also has general information about the strings (price thickness material construction and features).‘comfort’ ‘control’ ‘durability’ ‘feel’ ‘power’ ‘spin’ ‘tension stability’ and ‘overall rating’. The site's rankings are not very useful however because users can only sort by one characteristic at a time. This would be fine for a player interested in only maximizing control or only maximizing power but any player who has preferences for more than one characteristic is out of luck. The problem is every tennis player I know has some degree of preference for all the characteristics  the only question is how much. Instead of asking  the player prefers a better ranking system would list all the characteristics and ask the user  to put on each. One player may place a high emphasis on comfort and control low emphasis on durability and power and medium emphasis on the others. Another may assign entirely different weights. The point is that it's natural to take all the characteristics into account when deciding what makes for a good string and a ranking system should reflect this reality. It’s also a shame that users on stringforum aren’t able to rank strings based on adjectives. Each review includes a list of adjectives to describe the string being evaluated from a list of 22 possibilities (e.g. ‘soft’ ‘lively’ ‘explosive’ ‘spongy’ ‘springy’ ‘stiff’ ‘precise’ ‘dull’ ‘boring’). Since there are a finite number of options it would be easy to rank strings according to how often reviewers chose to describe them by an adjective. Just like for string characteristics users should be given a list of all 22 adjectives asked to provide weights for each and get an individualized ranking based on those preferences. In this case however the user should be able to provide negative weights in case he/she wants to penalize strings for certain adjectives.Review Criteria String Rankings and String Profiles and they For characteristics users are allowed to assign weights from 0  10 with a default of 5. This means that each characteristic counts a medium amount in the overall rankings by default and the user can decide if it should instead count for nothing a small amount or a large amount. It makes sense that the default is medium because all eight are important components of good strings. A user who uses the default ranking would still get a perfectly acceptable (although bland) ranking with all characteristics weighted equally.,NA,I scraped tennis string review data and built an app that allows players to find the string best suited to their particular preferences skill level and playing style.My app is an improvement over any resource previously available for researching and ranking tennis strings because:  I scraped review data from stringforum and built an app that leads users through a three step process for finding the right tennis string:  Not all tennis string reviews are relevant to all players. A beginner uses strings under very different conditions than an expert so the opinions of one may not be informative for the other. The same goes for players using heavy vs. light spin slow vs. fast swing speeds and may of the other tester attributes. A useful string ranking system would allow users to filter reviews to select only those from players similar to themselves (as long as they leave enough data for an accurate analysis).The same also goes for many of the racquet and string attributes. Users whose racquets are strung at low tensions would want to filter out reviews by testers using high tensions. Users who like thinnergauge strings would want to filter out reviews about thickergauge strings. I'm sure most users would want to filter by price.My app allows users to filter the dataset by 20 review criteria (7 string criteria 7 tester criteria and 6 racquet criteria). The table in the lower panel is dynamically updated when the user adjusts preferences in any of the three input tabs.Reviews on stringforum include ratings in eight categories which I will call 'string characteristics' and users of the website can rank strings by any of them: My app allows users to rank strings in three ways: using characteristics adjectives or both. Users are able to input preferred weights for up to 30 categories and view a ranked table. The scores are easily interpretable and color coded to show percentile.After getting a ranking it's time for the user to look at detailed review information for the top strings.The best format for reading reviews is a single table that displays all the review data in separate columns. This way the user can sort the dataset by any desired variable and find out for example what reviewers who rated a string poorly for spin had to say about it (and also scan info about those reviewers and their racquets to spot patterns).It would be nice to also give the user a graphical representation of the review text. Word clouds aren't the most informative visualizations but they are easy to implement and are suited to this case.For the characteristics and adjectives ratings users should be able to view an analysis of how a selected string compares with the filtered and full datasets. The comparison should be displayed in both percent and absolute terms with percentile and zscore being my choices (I prefer zscore over rank both conceptually and aesthetically because users can interpret it without seeing the sample size).My app displays detailed review data for a selected string in four ways: a table for reading reviews a word cloud and separate tables for characteristics and adjectives analyses. As with the ratings table scores are color coded by percentile.You can find the code at my For scraping the review data I used Scrapy which is a web crawling framework in Python. The main task was to instruct a web crawling 'spider’ how to navigate through the site URLs and provide it the XPath code to identify the data to collect on each page. The review pages on the site followed a predicable URL pattern and were organized in tables which made the job relatively straightforward. One slight twist was that the site often encodes its data as symbols (smiley faces plusses etc) rather than text or numbers so I had to identify those and encode them as numbers.The initial dataset fresh from scraping had 17517 observations (reviews) of 19 variables. After wrangling the variables almost tripled to 55. Much of the work involved text string manipulation  extracting the various pieces of tester racquet and tennis string information as separate variables. I also created separate variables for each of the 22 adjective choices (allows for faster and more efficient processing than working with a single variable containing all the selected adjectives).I allowed the user to decide how to deal with missing data. For each of the 20 criteria in the filtering section the user is given a choice whether to include or exclude reviews with missing values.On the stringforum site these were displayed as plusses or minuses with the number designating the degree (three plusses meant 'amazing' three minuses meant 'terrible' and a white circle was 'neutral'). When scraping the data I encoded these on a scale from 3 to +3 for the number of plusses or minuses (neutral was 0). However I wanted to encode these into a more intuitive scale in the app.My solution was to convert these numbers to a %max. This way when a user sees a score of 100 it's intuitive that the string has a perfect score (whereas a score of 3 could mean anything without context) and a score of 0 is the lowest score (whereas 0 was the middle score in the earlier encoding). The scale is also easily interpretable. A score of 44 for example means that the string received exactly 44% of the maximum possible score for the metric.Adjective scores need to be encoded because these will also be used for ranking. I needed a metric that was in percentage terms (to be fair to strings with few reviews) and give each review equal influence in the rankings. Reviewers are allowed to select as many adjectives as they want from a list of 22 and I did not want a review with 1 adjective selected to count less than a review with 5 10 or 22 adjectives selected.My solution was to calculate the prevalence of each adjective  the % of reviews in which the adjective was selected. It is fair because it treats each review as having 22 votes one for each adjective and both 'yes'(selected) and 'no'(not selected) votes are counted. Prevalence is also easily interpretable and on the same 1100 scale as our other ranking metric.I built the app using the Shiny package for R and the shinydashboard sidebar layout. The three menu items in the sidebar are correspond to the three core functions of the app (filter rank and research).For the output tables I used the 'DT' interface to the JavaScript DataTables library which has excellent builtin search and sort capabilities. Users are able to search for text found anywhere in a table and sort by any column.For the adjectives users are allowed to assign weights from 2 to 2 with a default of 0. This means that no adjectives count in the overall rankings by default and the user must specifically choose which ones should count at all how strongly and whether to reward or penalize a string for having reviews with that adjective. With 22 adjectives and some of them negative it makes sense to let the user initiate the ranking and not count any by default.For string rankings the user has a choice of three output tables: ranking by characteristics ranking by adjectives and ranking by both. If the user chooses to display the combined ranking a panel appears asking for weights to assign each of the components (characteristics and adjectives) on a scale of 110.The cell backgrounds of each String Rankings output table are colored according to that string’s percentile within the filtered dataset in 5% increments. If the mean value for a string is above the 55th percentile for a metric its cell will be green. If it’s below the 45th percentile then its cell will be red. The middle percentiles are white and shades of color get darker toward the extremes.I succeeded in building an app that significantly improves upon the best resource previously available for researching and ranking tennis racquet strings. However the app is still a prototype and its functionality and user interface can be further improved.In terms of functionality a 'string comparisons' feature showing a headtohead analysis between strings would be useful. So would a 'find similar strings' feature for when a user has a string he/she likes and wants to find other ones like it. 'Tester profile' and 'racquet profile' menu items would allow a user to play around with the data and explore how a particular tester rated different strings and how a particular racquet brand or model performed against others. I also hope to add an 'EDA' menu item that allows users to visualize and plot the data.There are several tweaks to make in terms of the user interface  moving some information out of tables and into information boxes is one obvious improvement and the general 'look' could benefit from some text styling and CSS wrappers. I am eager to make these changes as I continue to develop the app.For this project I explored using the scraped data for building the string finder app but the same data could be used for example to gain insights about what players of different types like and dislike about tennis strings. This type of information would be fun to extract and useful for manufacturers and marketers to know. I hope to pursue an analysis along these lines in another blog post.  ,NA
Dubai International Airport Flights Analysis - Kayak.com Scraping project,17,https://nycdatascience.com/blog/student-works/flight-booking-scraping-project/,the first onethe second oneThe data from these visualizations is  filtered. Now the analysis is applied on flights going to Dubai from London and mainly in the mentioned days.,NA,As people might think that a busy airport and passengers traffic is a good sign about the country sometimes the case is different. In Dubai the passengers traffic caused a lot of problems like flights delays flights cancellations passengers missing flights..etc. The questions here are: Is it possible to avoid these problems? Do airlines and flight schedules have impact on that?A flight booking scraping project of Kayak.com was done to analyse the flights arriving to Dubai from 10 other busy airports in the world over a whole month (March 2018). The main target of the project is to know the main factors that are affecting the traffic and if there is a solution for that.is a flight booking site that provides the user all the flights options he can have on a certain date and to a specific location. In this project since it is about analyzing the traffic in Dubai International Airport the destination was fixed to DXB (Dubai International Airport). As for the Departure it was changed frequently to get the flights coming from 10 different airports which are: . For each of these airports the flights information gathered was from March 1 2018 till March 31 2018. were consideredis for the total number of flights coming from each airport to Dubai through out the month and it consists of . As for  it was the details of each flight and it consists of The following graphs will answer the following questions:This graph shows that the highest number of flights arriving to Dubai is from This graph shows that the total number of flights coming to Dubai in March areon  and From these two visuals and the previous one it shows that most of the flights arrive in This graph shows that flights coming from London on the busy days are mainly arriving at One of the clear things is that the airlines with high number of trips to Dubai (Alitalia Etihad Airways and Multiple Airlines) arrive at the same time to DXB. The solution is that these airlines have different arrival times to DXB during the day. This can be applied depending on the airlines schedules to find the alternative timings that are good for all airlines. In a later stage more  analysis can be done to different airlines airports and timings for more results. ,NA
Some Simple Observations of Used Cars on U.S. Market,17,https://nycdatascience.com/blog/student-works/some-simple-observations-of-used-cars-on-the-u-s-market/,This project aims to survey and observe some simple but interesting patterns based on the data collected for used cars on the U.S. market. As a car enthusiast and an owner of several cars (not simultaneous ownership; I am not rich) I feel I can give my insight from the consumer side to talk about what those patterns mean.,NA,The raw data set is scraped (using the Python Scrapy suite) from  which is like an Amazon.com for used cars. This website lists detailed information on cars/dealers. I decided to make this website the source of my data because it is reputable comprehensive (you can virtually search for any cars at all U.S. zip codes) and provides cleancut and wellformatted information. The website helps you locate the dealers who have cars you're interested in which is information from seller side. But recall I said I surveyed patterns from the consumer side. Is that a contradiction? No. The logic is this: Dealers of used cars stock vehicles based on their years of understanding of consumer needs/preferences so as to maximize sales volume. Thus their stock information reflects consumers’ choices fairly accurately. I even feel their information is more objective than say from certain car review magazines whose ratings about cars are often strongly opinionated (e.g. based on magazine editors’ personal preference catering to automobile manufacturer’s sponsorship etc.).Prior to doing this project I browsed what previous NYCDSA alumni did for webscraping projects and found one project authored by someone who was also enthusiastic about cars (quote: “I am a big car fan”). He looked at patterns from a different angle: comparing the East versus West Coast usedcars markets from seller side. His project resonated with me and inspired my selection of topics.The following figure shows a typical search result from www.carfax.com. The key information to be scraped is listed as the boxed text in the figure. To locate those key pieces of information the reader can look at the redboxed texts in the lower part of the figure (to the right of Audi A6).After obtaining a raw data set (about 20000 plus vehicles) I added the cars’ manufacturer country information (e.g.: Audi: Germany). I also did a simple filtering:based on practical consideration as well as my own experience. For example I used mileage rather than years to judge the age of a car. One of the reasons for that is dealers rarely stock cars used more than 12 years anyway.The following figure consists of pictures of the two Audi A4s I have owned along with the odometer reading for each. I show them to convince you (the reader) that I  despite being a pedantic math person  do know something about cars and the above filtering consideration as well as ensuing discussion are based on my own experience.After some data cleaning with the scraped information we have a table shown as follows (here it displays information for ten randomly selected vehicles). You can also see that the table consists of 17358 records. From this structured data table we are ready to do statistics and make further observations.We can do a lot of summary statistics once data is structured as the table shown above which any modern computational software (R Python Matlab etc.) can do quickly. Let us focus on some simple but interesting observations. The first set of observations shows several top fives in terms of volumes available on market and arranged by descending order according to respective volumes. See the following table:Alternatively we can use bar charts to visualize the above information (volumes) but since here it is the text information (car makes/models) that matters most we put the information in text tables directly. Here are some other interesting top fives:What do we observe? At least two things: (1) Japanese cars are very competitive on U.S. used cars market. (2) The sale strength of U.S. made vehicles is in the large sized pickups and SUVs.Usually more interesting observation is made by doing comparisons. Here we compare three models of the same class: Audi A4 BMW 3series and MercedesBenz Cclass. They are all made in Germany compact entrylevel premium cars that are designed for personal rather than business use. As you read along you will find we can even infer about the three car makers’ different design philosophies to target different categories of consumers. But let us observe some simple distributions to begin with:It seems there is nothing more to be observed but behold when we compute a simple quantityand plot its distributions for the three car models that yields a distinct and interesting pattern as shown in the following figure.From the above distributions of the ratio price/mileage you can observe that when the ratio price/mileage is small BMW 3series dominates; when the ratio price/mileage is higher than some threshold value Audi A4 dominates; in both cases MercedesBenz Cclass is in the middle place. In fact this reveals those three car makers’ marketing philosophies. How is that so? Let us first understand what the ratio price/mileage suggests. Consider the following two sets of examples in each case bluecolored ratios give smaller values than the redcolored:The above inference is not surprising. Since I was in high school I have noticed that BMW likes to advertise its “sheer driving pleasure” (quote which also includes the pleasure of winning attentions on the street). But for Audi its commercials focus on Audi’s calmness and steadiness such as how it drives on ice steadily like a heavy ship. In fact I think BMW’s philosophy is to make a car like a vanguard jetplane whereas Audi’s philosophy is to make a car like a stately ship. For MercedesBenz it takes a balanced philosophy and to be honest I feel it does not care about making Cclass that much; it merely introduced its Cclass so as not to completely lose a foothold in this particular class of market. As a result you can see MercedesBenz Cclass ranks in the middle in both categories.It is amazing to me that how such a simple calculation of objective and coldblooded numbers (price/mileage) can reveal something that is subjective and humanrelated:: It may seem that other readers need some domain knowledge (about cars here) to make useful observations. But the point is as long as a data analyst can use his/her domain knowledge to invent metrics to bring about revealing patterns (fancier term is: feature engineering) the readers or coworkers or clients will notice those patterns and do their research to further. For example I am not familiar with Japanese cars of the class discussed in this section (Acura/Infiniti/Lexus) but I can conduct a similar study as given above and infer about those cars’ characteristics and their car makers’ design philosophies.We now look at how various quantities are related to each other in terms of the socalled Pearson correlation coefficient. The correlation coefficient lies in between 1 and 1 with positive (negative) values indicate positive (negative) correlations and larger (smaller) magnitudes indicate stronger (weaker) correlations. To visualize correlations we generate a heat map. But before showing the heat map let me introduce one more cookedup feature that is simple enough to calculate although it also reveals a lot:The higher the ratio years/mileage the gentler a car has been used. The reason is simple: Consider two cars A and B both of which have been used for 5 years but Car A has been driven for 5k miles and Car B for 50k miles. Obviously Car A has been used more gently than Car B. Accordingly I consider this ratio (years/mileage) an index for the Gentleness of Usage.By the way some may wonder when we cook up new features such as Years/Mileage will the new feature be redundant with respect to old features such as Years and Mileage? The answer is generally no as long as we did not simply take linear combination of old features to derive new features. Everyone with elementary physics knowledge knows that from distance and time (two features) we can compute a new feature speed  distance/time. But (the new feature) speed provides a new dimension of information about the state of motion which cannot be known from (old feature) distance or time alone. In formal mathematical analogy: An integrand and its derivative in general have no direct proportionality.The correlations between various quantities are shown in the following heat map. Note that the calculations are done based on all the vehicles from the data table (17358 in total).Some of the correlations are obvious. For example the price of used cars is negatively correlated (0.32) to mileage. Some of the correlations are interesting. For example the ratio price/mileage (recall: it indicates consumer preference or car’s personality) is positively related (+0.32) to the ratio years/mileage (recall: it indicates the gentleness of usage of a car). This is understandable; consumers who prefer lowprofile cars tend to drive more gently. But there are also some nonobvious correlations. I will leave it to the readers to find surprising correlations.This is merely a preliminary survey of the U.S. usedcar market. A lot of improvements can be made in the future:The lists for improvement can be endless but the goal remains discovering patterns that are simple to understand and make use of. To that end I would recommend talking to car dealers because they know what are the actual interesting and worthwhile business questions that call for analysis and answers. In other words they have real domain knowledge on which to base the data analytics.,NA
What are the most under-rated hiking destinations?,17,https://nycdatascience.com/blog/student-works/web-scraping/what-are-the-most-under-rated-hiking-destinations/,Using data collected from the web apply exploratory data analysis to find hiking destinations that are underrated.,NA,"Note: From the outset I had to define what ""underrated"" means. I concluded that I wanted places that have one or multiple of the following:Visit the link to the github page here: ",NA
New York City Weather and Vehicle Collision Data Analysis,18,https://nycdatascience.com/blog/student-works/new-york-city-weather-and-vehicle-collision-data-analysis/,"“” website followed its “robots.txt” rule. The website is well organized and I mThe data set includes: To get the originally downloaded weather data set ready for analysis I first did the following data preparation tasks.To get collision data set ready for analysis I did some similar work on data cleaning and some different tasks as follows.To check correlations I tried/tested w/ several different plotting tools.While ""Clear"" and ""Overcast"" have the two largest number of collisions they are also occurred most of the time. Therefore for a more fair check of each weather condition's impact on collisions I divided total number of collisions by total number of occurring hours i.e. the collisions per hour graph. Herein heuristically set minimum hours threshold  10 (for now).The more steep slopes/variations we see in a frequency graph the higher impact that weather factor may have on collisions. Herein roughly we can see that: snow and humidity has more significant impact than that of temperature and visibility.Note that herein in order to calculate total occurring hours of a continuous variable we have to properly binning them so as to treat them the same way as for categorical variables. Also the result of pressure and wind speed is not shown here as the simple binning between min and max is not working well w/ them and I need better binning approach to handle many outliers of them. As for the result of dew point and wind direction there is no much steep variation slope and hence no much potential impact observed which is also omitted herein.  The frequency graphs w/ the consolidated severity index quantity show a slight promising difference with those w/ the simple total number of collisions graphs where we can see a little more steep variation herein than that w/ the number of collisions. However whether or not or how much this much difference can help on modeling/prediction performance is a problem that definitely needs a careful check/comparison in actual modeling practice.   Overall it is unfortunate that: from the graphs we cannot see any significant correlation coefficients between different weather factors and collisions. Herein the actual question is: how we should interpret this result properly. Esp e.g. for snow factor for which we observed significant impact in frequency graphs. To collect more insights either on collisions data itself or on its correlation with the weather factors I also checked the correlation plots involving top collision causes and/or top involved vehicles. ticeable significant correlation spots are:Thank you! 🙂 ",NA,This work is a continuation of the previous work of . While the previous work is focused on analysis of the collision data set only in this work I further include the weather data of New York City (NYC) and investigate their correlations.  Specifically the objective of the project is to practice web scraping and data analysis using Python. I scraped 20132017 NYC hourly weather data from the “” website (high quality reliable data by Central Park weather station) and studied correlations between the weather data and motor vehicle collisions data to find some good features to predict/model vehicle collisions.The ultimate outcome of the project would be a model to predict the number of vehicle collisions for the whole city a borough or various different areas/spots of the city etc. given weather and some other data sets e.g. traffic volume data. The possible practical applications are:To achieve these business goals a big technical project plan would include or further consider:I found and scraped high quality hourly NYC weather data on the ostly just followed labdemoed scraping techniques using Scrapy and didn't encounter much new/unexpected technical challenges in practice.The finally collected data set is high quality and reliable NYC hourly weather data from 2013 to 2017  observed by Central Park weather station. With the two data sets ready I then calculated and drew collisionsperhour frequency graphs to check different weather factors’ impact on collisions (using the basic Matplotlib plot).For example a nice looking Pandas scatter matrix plot is shown below which shows scatter plots and kernel density estimates of the number of collisions and different weather factors between them and by each variable itself respectively. We can see some clearly significant positive correlation exists between temperature and dew point.In fact this result of very low correlation coefficient is not so surprising given the nature of the snow condition variable a dummy/binary variable and occurring not very often in time. But then how can we properly measure their relevant/concerned correlation impact on modeling? And how to effectively make use of these weather variables for better modeling?... All these questions/problems need further thinking/studies in the future.Some interesting observations are highlighted as follows.As for the correlation plot between top involved types of vehicles and weather factors some noIn the future the project can be further pursued in the following several directions.,NA
Sneaker Reselling Solutions,18,https://nycdatascience.com/blog/student-works/sneakers-reselling-solutions/,There used to be a long line waiting outside the Nike stores to buy sneakers not for themselves but to make money. People resold their sneakers on eBay and publicized their business on Twitter and Facebook. At the prime time of sneaker reselling the market was worth one billion dollars per year. Back in the day I took part in that market as a seller. After a while I noticed that sneaker reselling business was declining. The resell price of sneakers on eBay was even lower than its original retail price meaning you would lose money by reselling it. So I stopped buying sneakers with the intent of selling them.  Based on my personal observation I decided to find out if my experience was a true representation of a change happening in the sneaker resell market. I designed my project to determine whether the sneaker reselling business was actually on a decline. First  I gathered data and analyzed the changes in reselling price. I mainly concentrated on Air Jordan sneakers as they are the most popular sneakers to resell. Second if the sneaker reselling business was declining I want to find out the reasons. Third I wanted to find out if  a sneaker reseller could still make a profit in the current market.To determine whether the sneaker reselling business was declining I looked at the numbers on  a website for buying and reselling sneakers. I scraped all the resale transactions including sneaker name resell date size price from 2011 to the beginning of 2018 and created a graph. Before we look at the graph it is important to understand the term “price premium.” Price premium is a calculation utilized to figure out whether there is a profit or loss when an item is resold. If price premium is positive it means you will make a profit. If it is negative it means you will lose money.The graph shows that the price premium of Air Jordan sneakers was still profitable from 2011 to 2015. However from 2015 the premium started to decrease which confirmed that Air Jordan reselling business was in decline.Competition was another cause of the Air Jordan price premium decline. When Kanye West left Nike to join Adidas he also brought his sneaker brand Yeezy with him. After Adidas released Yeezy in 2015 the premium of Air Jordan began to drop.In the winter of 2015 Adidas released another style of sneakers called NMD which affected the price premium of Air Jordan. Interestingly NMD's price premium also started to drop in 2016. When Adidas initially released them in 2015 NMD sneakers were limited editions. People needed to download an app and sign in to get a pair of NMD. However starting from 2016 Adidas decided to stop limiting the number of NMD sneakers that people could buy. That decision had a major impact on sneaker resellers.Here's the question how can one continue to make money in the current sneaker reselling market? Most importantly don't put all your eggs in one basket. In addition to Air Jordan sneaker resellers should consider stocking sneakers from other brands such as Adidas.It also comes down to style. Let's take Air Jordan for an example. There are a variety of styles of Air Jordan such as Air Jordan one Air Jordan two so on and so forth. On the graph we can see that Air Jordan one sold the most for the past seven years. However it  did not make the most profit. When you look more closely at the data the price premium of Air Jordan one was negative. Actually the more Air Jordan ones the resellers sold the more money they lost. When I multiplied number of sales and their price premium of all the Air Jordan styles it is clear to see that reselling Air Jordan eleven and twelve can make the most profit.,NA,The next step is to figure out why the reselling business of Air Jordan declined. I plotted the number of styles of Air Jordan released each year. There was a significant increase of number of styles from 2015 to 2017. One important thing about those released styles is that most of them were not limited editions. When merchandise is not rare or limited especially when it comes to sneakers buyers usually will not pay good money for it.As it turns out when it comes to sneakers size does matter. I created a graph showing the price premium of different sizes. It seems that you could make money from selling sneakers at size 6 as its price premium is the highest. However when you look at the number of sales there were only about ten pairs of size 6 sold over the past seven years. When the price premium multiplied with number of sales it turns out that size 10 and 11 are the most profitable sizes.Numbers don't lie. Sneaker resellers can still make a profit and reduce their losses if they stick to limited editions choose specific styles and offer their right sizes.,NA
The Facebook Effect,18,https://nycdatascience.com/blog/student-works/the-facebook-effect/,Yet this does not mean more cannot be done to better leverage this relationship. Although manipulating the treatment trending on Facebook is not feasible another method would be to find a comparison group that is similar to the treatment group in every way except the treatment. ,NA," On Wednesday February 21st social media enthusiast Kylie Jenner tweeted:""sooo does anyone else not open Snapchat anymore? Or is it just me... ugh this is so sad.""Kylie Jenner has nearly 25 million followers on twitter and this single tweet alone was ""liked"" 350 thousand and ""retweeted"" 71 thousand times. Kylie Jenners tweet about Snapchat is a microcosm of the way social media has engulfed how businesses operate and how it can both be a major benefit or in the case of Snapchat detriment to success.So for my webscraping project I seek examine two questions? What is the effect of something trending on social media? and subsequently how to increase the likelihood of something trending. And I chose to examine the largest social media company of them all... FacebookFor the first part of my analysis I wanted to focus on the effect Facebook exposure. Companies are constantly writing content sharing links and commenting on post in hopes that increased exposure on Facebook results in larger publicity. What better way to examine this then to look at Facebook's ""trending"" list literally one of the most viewed items on Facebook. Implemented in 2014 Facebook created a ""trending"" functionality on their homepage in which they provided viewers a list of popular topics being discussed and shared on Facebook. The actual algorithm by which this list is constructed is moot for the purposes of this project instead what cannot be denied is that when something is deemed ""trending"" by Facebook it is seen by all of Facebook Users. So if we can find a way to measure the effect of things trending on Facebook we can begin to unpack how much exposure on Facebook really matters? But that remains a big if. Without being employed by Facebook or paying some type of fee having access to things like number of clicks on a post is impossible. What I need is information that is both publicly available and directly related to facebook trends. And I turned to Wikipedia.What of the most commons patterns after something goes trending on Facebook is for someone to seek more information on that topic. For example an actor or actress may go trending on Facebook because they got married or were involved in a scandal. One of the first reactions for me is to get more information. Where did I see that name before? what movie were they in? wait weren't they already married!? And as google can profess when a person is searched 9 out 10 times the first item returned is their Wikipedia page. So if I could link facebook trends to their wikipedia viewership at a granular level I would have a great way to examine the relative effect of something trending on Facebook. What do you know Wikipedia publishes information on the hourly page view count of EVERY WIKIPEDIA PAGE.1. I record Facebook trending data from the week of January 30th to February 5th.  This information is obtained at 8am (ET) daily one of the most active times on Facebook.2. I download data from wikipedia on the number of views a page receives hourly from the same time periodThe bulk of the work then involved accurately combining these sources of data. Each wikipedia file contained information on the hourly page view counts for every single wikipedia page (which was slightly over 5 million). With seven days of data and 24 wikipedia files for each of those days downloading and subsetting the data took a substantial amount of time and computing power.This included:In total for one week of Facebook trending data I had over twelve hundred observations.So does something trending on facebook affect its wikipedia page view count?The graph above indicates that it seems so. The y axis is the number of views the x axis is the time of day in three hour intervals (each tick represents the sum number of views from the preceding tick) the blue line represents when something goes trending on facebook and the black line represents the average for all Facebook trending topics. As you can see after something goes trending it has a sharp increase in the number of times it is viewed.Aggregation can obscure important individual distinctions therefore the graph above shows the wikipedia page view count for every Facebook trending topic that week (where views is logged so all graphs can be on the same y axis scale). Overall we can see that there is a general trend of where after something goes trending of facebook it receives a sharp increase in wikipedia page views  for several hours before tailing off. Some trends that stick out in particular includeAnalysis of how these effects vary by group reveal even more interesting insights.When we group Facebook trends into three categories politics popculture and sports we see the same overall pattern of growth in wikipedia views. However sports related trending topics differ from the others; namely it receives both the largest uptick in wikipedia views immediately after going trending and has the sharpest dip when the views begin tailing off. This suggests the Facebook trends have different impacts depending on the topic of what is trending.But are these difference significant? To answer this question I run two analyses. First I create a box plot for the number of hourly views for a topic before and after it goes trending on Facebook. Second I run a T  Test comparing the mean for the number of hourly wikipedia views before and after trending on Facebook.The box plot demonstrates that the median and overall distribution for the number of views in the posttrending group is higher than the pretrending group. Furthermore with pvalue less than .001 the ttests suggests that the mean number of hourly wikipedia views posttrending group is significantly higher than pretrending.So then can we argue with confidence that having something going trending on Facebook increases its wikipedia page view count by nearly 70%? Not necessarily. The ttests does show that the mean difference in hourly wikipedia page views pretrending and posttrending is significantly different. However it does not prove that going trending on Facebook is purely responsible for that increase in page views. Indeed a reasonable counterargument would be that something goes trending on Facebook because people are talking about it and that in and of itself is enough for an increase in wikipedia views.Indeed with the current way the study is constructed I cannot rule out the counter argument that the effect of something going trending on Facebook is actually being confounded by the fact that both ""Facebook Trending"" and Wikipedia page views are driven by real world events.In order to truly find a ""causal"" relationship between something going trending on Facebook and wikipedia page views I need to isolate the effect of going trending on Facebook from other potentially confounding effects. Taking a page from the experimental methods it could be thought of as having a treatment group and a control group then measure the differences in the outcome.As a thought experiment what would that look like? The treatment group would be what we have already observed in the data; a real world event occurring then that event going trending on Facebook. The control group would then be that same real world event occurring but that event NOT going trending on Facebook. When working with observational data where treatment and controls cannot be manipulated this is an impossible task.In order to do this I turn to historical data of trending twitter topics. Twitter trends particularly historical twitter trends offers an interesting comparison to Facebook trending topics. Something that is trending on twitter is likely trending for the same reasons something is trending on Facebook; because it was initiated by some realworld event. So that clears the condition of find a group (or phenomena) in this case that is extremely  similar to the Facebook treatment group. What remains is that group not observing the treatment going trending on Facebook. However if something goes trending on twitter it is also likely to go trending on Facebook. And in the case it is not an argument for selection bias could be made specifically that the topics trending on twitter but not on facebook are qualitatively different. That is why historical twitter data is best suited as a point of comparison. The functionality of Facebook trends dramatically changed in January of 2017. Prior to that point in time Facebook personalized what users saw in their trends section according to the preferences of each person. Although it is impossible to know in entirety the black box behind Facebook's trending algorithm after significant push back from media and the general public Facebook reported that they they would no longer personalize Facebook trends to reflect users personal interests. Instead ""Everyone in the same region will see the same topics"".Therefore if we could gather twitter trends that existed before Facebook changed its algorithm theoretically we would have a group of real world events that occurred and would have been trending on Facebook (if its current trending format existed)but  did not.Alas a control group.Therefore I go back and obtain additional data for my study. First I scrape data from trendogate.com which keeps track of twitter trends as far back as 2015.  I scrape data from the week of January 18 2016 nearly two years prior to my facebook data. I do this in order to make sure I had twitter data from a point in time prior to Facebook trending algorithm change as well as make my this data as comparable as possible to the Facebook data.Second mimicking the earlier data collection process I obtain data from wikipedia on hourly page view counts for times corresponding to the twitter data.This graph shows the relationship between when something is trending on twitter and the number of hourly views that topic receives on Wikipedia. It is important to note here that the green line which represents when something is trending on twitter is at 2pm. This is because trendogate.com both updates its website roughly in the late afternoon and because unlike facebook twitter tends to have its highest rate of activity in the early to late afternoon. Whether this is a fair assumption is a valid concern and one I definitely wrestled with. However given the limitations of data I decided this was the best course of action.Again I run a ttest in addition to graphing a box plot to examine whether differences in wikipedia hourly page view counts pretrending and posttrending on twitter are significant. And the results show that at a pvalue of less than .05 the means do not differ.The data from the Facebook analysis show the number of views a wikipedia page gets is significantly higher after it goes trending on Facebook. Supplementary analysis uses trending topics from twitter from 2016 as a comparison group in order to isolate the effect of facebook on wikipedia hour page views. The results from the twitter analysis show that have something on twitter does not significantly impact that topics page view count thus lending support for the fact that increased exposure on Facebook does result substantial increases in publicity as seen in the large increase in wikipedia views.So back to the original question. Is Facebook exposure important. Yes. having something trending on Facebook will nearly double its search rate on wikipedia.We established trending on Facebook does wonders for exposure. Great. So now everyone who wants to increase their publicity should go trending on Facebook. Easier said than done.So in part one we identified an area of need (exposure on Facebook) now I want to analyze . Having something trending on Facebook is nearly impossible but we can take the underlying principals of Facebook's trending section. How can you maximize content exposure on Facebook?Contrary to what many believe it is not by flooding facebook with endless posts and images. Described as the ""Zombie Scroll Syndrome"" Facebook users are constantly scrolling through content ignoring posts images and especially advertisements until something catches their eye. Indeed today marketing companies actively trying to find ways to create content that breaks this zombie like scrolling and brings meaningful attention to their information.One of the best examples of this are how media outlets share their news on their Facebook accounts. In stark contrast to when users come on media outlet's websites to look at articles and post when on Facebook users are not directly there to see whats going on in politics sports or entertainment. Therefore how they  interact with information is qualitatively different. Correspondingly most major media outlets do not simply just post their stories verbatim or share a link to their website. Instead they craft posts intended to pique the interests of Facebook users in order to navigate these users to their websites.To understand how post and content can best be crafted to increase Facebook exposure I conduct a case study of one of the largest media outlets in America: The New York TimesI use data collected on all the Facebook posts made by The New York Times from 2012 to 2016. This data contains  the facebook post written the title of the article or video shared its underlying description and number of likes. Initially in order to continue to build my web scraping skills I was scraping data from the archives of the New York Times to collect additional information on how they shared their posts on their website. But after several scrapes I realized that the information being collected from the website was exactly the same as the information under the title and description section of the Facebook data. Therefore proving that I could scrape the website I felt my time was better spent analyzing the data.The first observation that sticks out is that compared to descriptions on their website the New York Times Facebook posts are on average longer.From the dashed lines on both graphs (which represents the means) we can see that when The New York Times posts on Facebook they use both more words (three more words) and characters (twenty more characters)  both at pvalues less than .001.Indeed a scatterplot of the number of words in a facebook post versus the number of likes indicate that there is a positive correlation between the number of words in a post and the number of likes that post receives.Besides using more words and characters how does the language used on Facebook differ?Above are wordclouds of Facebook post(L) and Website description(R). As you would expect they look fairly similar for the most part. When trying to detect such nuanced differences other methods of analysis might be preferred.FacebookNew York Times WebsiteAbove shows a simple text analysis (via Google's Cloud Natural Language) of a sample post on Facebook compared to the corresponding article post on The New York Times website.We can see here that when posting on Facebook The New York Times mentioned Ghandi but did not on it's website description. While this is just a simple example from one post this pattern is also evident in the overall corpus of text. When posting on Facebook The New York Times was significantly more likely mentioned names like ""Obama"" ""Romney"" and ""Mitt Romney"" compared to when they posted article descriptions on their website. Additionally when posting on Facebook The New York Times is more likely to pose a question or use quotes.Exploratory analysis of The New York Times provides insight into how The New York Times uses language in their Facebook posts to maximize their social media exposure. This includes:After Kylie Jenner's tweet Snapchat was estimated to have lost 1.6 billion dollars in worth. Was her post responsible for the entirety of that loss? We may never know but we can say with certainty Jenner strong presence on social media did significantly effect the outlook of Snapchat.Indeed in this project I sought to unpack the effects of content exposure on social media and Facebook in particular. I found that when something goes trending on Facebook it results in a 70 percent increase in searches on wikipedia. Using wikipedia views as a proxy for increased awareness we can see that Facebook exposure has a enormous impact on product awareness. Furthermore to ensure that I was isolating the effects of something trending on Facebook and not other potentially confounding effects I compare the effects of something trending on Facebook to trending topics on twitter. I find that the effect of trending topics on twitter were not significant lending support to the hypothesis that in my data Facebook was largely responsible for the 70% increase in wikipedia views.I then conducted a case study of how The New York Times posted information on Facebook compared to their website to gather insights on how major companies use social media to increase their social media presence. I find that post on Facebook are longer include names of important figures (e.g. Obama Ghandi) and use more questions and quotes.While my study is the first to examine both how facebook trends impact wikipedia views and compares Facebook posts to website posts for large media companies my study remains exploratory in nature. Future work that may contribute to the robustness of my findings include:Thanks for taking time to read a long post. Please reach out if you have any questions or suggestions!",NA
Where to Find a Clean Restaurant in San Francisco,18,https://nycdatascience.com/blog/student-works/where-to-find-a-clean-restaurant-in-san-francisco/,The Department of Public Health in San Francisco conducts unannounced inspections of restaurants at least once a year. It checks food handling food temperature personal hygiene and vermin control and gives restaurants inspection scores. Unlike New York City where the higher the letter the worse the score in San Francisco the higher inspection score means indicates  more sanitary conditions. My purpose of developing this shiny app is to help the users find clean restaurants in San Francisco.The inspection data used was obtained from Kaggle.com. The dataset contains the name address zip code phone number and inspection score of each inspected restaurant in San Francisco. On the first map I categorized the restaurants by their inspection score and added them to the map. To show the restaurant score at a glance I opted for emojis. My first idea was to plot restaurants using different colors to represent different categories of inspection score. However I changed my mind after noticing that users may have their own preconceptions about what colors means. I saw a map online  that  used blue and red to represent clean and unclean. When I first looked at that map I thought blue markers represented the clean restaurants and red markers represented unclean restaurants. When I realized that the author intended it the other way around I thought that in order to avoid possible confusion I would use graphic icons to represent restaurant rankings rather than colors. The restaurant with an inspection score between 100 and 90 is thumbsup meaning that that is a very clean restaurant. When you zoom in on the map you will see the name address score and phone number in case you plan to make a reservation. I used a smiley face to represent restaurants with score between 89 and 80 which means that this place is clean enough to eat in. For restaurants with score 79 to 70 I used fearful face meaning that you should probably not go there. For restaurants with score below 70 I used vomiting face which is pretty intuitive and selfexplanatory representation of why you should never go to those restaurants.On the second map I grouped the restaurants by their zip code. Knowing what restaurants are available to you is one consideration when moving to a new neighborhood. When you use my app simply select the zip code; it will show you how many restaurants have thumbsup smiley face fearful face and vomiting face. In fact you can get the exact number of restaurants in each category from the info boxes.On the third map I plotted the top five most sanitary foodie streets and the five least sanitary foodie streets in San Francisco. To do this I grouped the restaurants by their street name and took the median of restaurants’ inspection scores for each street. Using this map it is very easy to find the most sanitary foodie streets and the least sanitary foodie streets. If you were to hang out with your friends in San Francisco on a Friday night you wouldn’t want to go to a street that is full of unclean restaurants right?,NA,This is my app if you would like to find a nice clean restaurant in San Francisco please use this app.See my Shiny App .,NA
Crime and Demographics in New York City,19,https://nycdatascience.com/blog/student-works/crime-and-demographics-in-new-york-city/,Note: Staten Island data were incomplete from the dataset between 2000 and 2012 and an executive decision was made to disqualify these years,NA,"Some American fixations: football taxes and crime. Like its kin crime sits squarely in the national consciousness; untold resources have been devoted to understanding dissecting and analyzing all its facets. Obsession over criminal activity is perhaps nowhere more salient than in New York City a city which found itself mired in crisis in the 1970s and 80s. The 4/5/6 subway line which today handles the greatest share of riders was affectionately called the ""Mugger's Express"" due to high incidences of daylight robbery. Meanwhile gangs prostitutes and corrupt officials roamed the city unchecked.Of course if you're reading this you know the end of this story already. With mayors David Dinkins Rudy Giuliani and Michael Bloomberg in office New York crime plunged to unprecedented levels. Soho once an industrial wasteland of sweatshops and abandoned factories is now one of the most gentrified neighborhoods on the Eastern Seaboard. Brooklyn once afflicted with staggering amounts of criminal activity is now a hot zone for the new generation of yuppies. Indeed The Economist ranked New York City as the #10 safest city in the world on its Safe Cities Index all but memorializing the Big Apple's transformation into an alpha city.So how did New York dramatically reduce its crime rate? Any prospective analyst would find challenge not in finding an answer (of which there are many) but rather in crafting a succinct narrative from the enormous hoard of American crime data. Approaches could be as varied as measuring the effectiveness of stopandfrisk or evaluating the  introduced under Michael Bloomberg.For my project I chose to look at two separate data sets: the  and the Census Bureau's .The NYPD dataset grew out of Rudy Giuliani's Compstat initiative introduced in 1994. This initiative enforced a statisticallydriven approach to crimereduction; since its inception all criminal offenses have been logged in a central database along with relevant data on geographical location offense type and time. These data are further grouped by precinct. Datasets are updated weekly providing impressive granularity and access to New York's crime trends. The currently available data span from 2000 to 2016.The ACS is a nationwide demographic survey conducted by the United States Census Bureau which was founded in 2005 out of a need for annually aggregated household data. The ACS contacts approximately 3.5 million households per year and presents the data in an opensource easily accessible format. Data are gathered on multiple categories including income education and ethnic information. High geographic resolution has also been recently introduced by the Census Bureau in the form of Public Use Microdata Areas (PUMAs) which in essence are census blocks. Interestingly these blocks do not correspond to any other geographic delineation.My initial vision was to unify the NYPD and ACS datasets. In doing so I would construct a longitudinal study comparing demographic data with crime rate grouped by geographic subareas within New York City.Ideally I would have tried to analyze the initial decline in crime rate which occurred throughout the late 80s and 90s. The criminal offense data were either not available online or were not recorded altogether. Thus any study seeking to use NYPD data could only feasibly catch the tailend of the crime decline from 2000 on.I ran into further limitations with the ACS data. While nationwide New York ACS data are available online from 2000 data standardized into PUMA are not available until 2011. Any longitudinal study combining NYPD and ACS data grouped by geography could then only take in years 2011 or after.But the most serious limitation came when I discovered that the geodata I had been using could not be overlaid on top of each other on my data visualization. And while it was indeed possible to collate the data in a different format the problem was discovered too close to the project deadline to make a change. When I revisit this project I will seek to rectify this problem and give the visualization the treatment it deserves.Ultimately I could not combine the data geographically and I could not compare the datasets directly. But I decided that I could construct two separate studies and qualitatively assess the impact of certain variables. What you see below is an amalgamation of two different data visualization studies: a longitudinal study of crime in New York grouped by NYPD precinct and a demographic snapshot of the city grouped by ACS PUMA.My first goal was to visualize crime and demographic data in a choropleth map. Below you can see each precinct color coded by crime rate (with a dropdown menu allowing selections between different types of crime [i.e. major felonies minor felonies misdemeanors and violations] and a slider allowing selection of different years from 2000 to 2016). Figure 1.1 depicts the former and showcases the hoverover function I implemented into the map.My next step was to go through the same process but with the ACS data. Figures 1.3 and 1.4 depict the same process but with PUMAs instead of NYPD precincts. You will notice that areas of high crime (i.e. The Bronx and South Brooklyn) from Figures 1.1 and 1.2 roughly tend to correlate with areas of high unemployment and high labor force disengagement with the exception of midtown Manhattan. The outer edges of New York proper also exhibit high rates of labor force disengagement. I posit this is due to the outskirts being a more suitable residential area for the retired and familyrearing population a trend we see in suburban commuters.My second goal was to construct a handful of graphs which visually represented the data and exposed interesting bivariate trends. I first confirmed that the New York crime rate had indeed dropped significantly (see Figure 2.1). What's astounding is that since the turn of the millennium the citywide crime rate dropped from just under 250000 offenses per year to a little above 150000 offenses per year almost a 40% decrease since the beginning of the NYPD data set. Some interborough disparities in crime volume can be explained by each borough's population size with Brooklyn having by far the largest population. But in hindsight a similar graph adjusting for population size would have been interesting to ponder.I next looked at income bracket distributions throughout the city to see if income correlated with crime rate. Not surprisingly in 2015 Manhattan had the most number of families that made more than $200000 a year. The Bronx stands apart with the most number of households with the least amount of income and the least number of households with a high amount of income. Brooklyn exhibits a similar pattern with a bolstered right tail probably due to the gentrification of neighborhoods such as Williamsburg and Brooklyn Heights. Qualitatively neighborhoods with fewer rich households in proportion to poor households seem to have a higher crime rate.Finally I plot mean and median income against unemployment rate. We can see that there is a relatively strong correlation between the two variables. ",NA
"NYC's Seven Major Felonies - What, Where, and When",19,https://nycdatascience.com/blog/student-works/nycs-seven-major-felonies-what-where-and-when/, My MotivationEven though people in New York City are streetsmart crime is always a possibility and certain areas are more dangerous than others. But where are these unsafe areas? What crimes are taking place and when are they happening? Using NYPD’s Historic Complaint dataset I decided to look into these questions. The application can be found on my . The code used to create and run the application can be found on .The QuestionsAs I was envisioning the app I thought of 6 questions that I wanted my app to answer:The DatasetI used the  which includes information on all felonies misdemeanors and violations that have been reported to the NYPD from the start of 2006 to the end of 2016. The data set included information about the crime such as the date time a description of the crime committed and the location of the incident (including latitude and longitude). I focused on seven major felonies that occur in NYC: Murder Rape Felony Assault Grand Larceny Grand Larceny of Motor Vehicle Robbery and Burglary. I also knew when looking at the crime rates for each borough I would have to control for population. In order to do this I found the  for 2010 and its estimate for 2016. In order to estimate the population for the missing years I used the logistic growth formula. When I first downloaded the dataset there were around 5.6 million observations. I initially filtered the rows to include just the seven offenses I was focusing on. After inspecting the data I noticed that a number of the offenses had missing labels or were labeled inaccurately (e.g.: the row’s three digit code did not match the listed offense description). I went through the data and made sure what I was collecting was accurate and inclusive. After making the appropriate edits to include and correct for these entries I created helper columns to provide additional filtering for the mapping and statistics of these crimes. I also consolidated information together from certain columns to add information to the maps and dropped the unnecessary variables that were no longer needed. Once my data was cleaned having just over 1.2 million observations I added in my estimated population to the cleaned file. Once I had my data I began coding the shiny app.Visualizations and Statistics:The icons are color coordinated based on crime type making it easier to see what types of crimes occur near them when they do not have a specific crime filter set. If you click on a crime pinpoint information appears stating the type of crime that was committed where the incident occurred and when it happened.In order to help a person find crime in a specific area I add an address lookup feature. This allows users to quickly focus their search to specific areas rather than have to zoom in and constantly having to orient themselves to find their area of interest. In order to provide a more highlevel overview on where crimes occur in the borough I created a heat map. Although you are not able to see the specific details about each individual crime it lets the user know generally where crimes occur and how the location changes as time passes.  The map can be filtered with the same specifications as the cluster map.Crime Trends Based on Crime Type and Borough When I started the assignment I wanted to accomplish two tasks in terms of statistics. I wanted to be able compare trends of specific crimes within each borough as well as see if a specific crime differed between borough. In order to accomplish this I created two statistics tabs focusing on each question specifically. Both tabs provide information on the overall trend of crimes per 10000 residents as well as the frequency that specific crimes occur within various months day of the weeks or time of day.In this tab you can pick a borough to focus on and  see how the crime rates compare within each borough. The graphs measure the total amount of crimes that occurred within that time factor (year month day of the week or time) and displays how the percentages changes. So for the example above we can see that the number of  felony assaults in Staten Island have been increasing per 10000 residents while the number of grand larcenies have been staying the same despite a dip for the years 2009  2011. Grand larceny also appears to increase as the year goes on while assaults are more frequent in the spring and early summer. We can also see that assaults are  more likely to occur on the weekends while grand larceny is more frequent during the week days. The final notable takeaway from this example is that felony assaults are more frequent in the early morning and late evening while grand larceny generally occurs in the late morning to early evening.In this tab we can compare specific crimes and whether or not there are significant differences between the borough.  So for this example we can see the number of robberies have been going down for almost all the boroughs with the exception being Staten Island. However we can also see that the likelihood that robberies take place controlling for months days of the week and time of day don't appear to differ very much between boroughs. Viewing the DataOn the final tab you can view the underlying data filtering based on the types of crimes the day of the week the time of day as well as for specific boroughs. Similar to cluster map it provides you with the date the crime was committed the time of day the type of crime and where it occurs. You can filter the data based on these categories as well.Further ExplorationAs I continue to add onto this application I would like to include information about the income distribution for each borough. I would also like to look into whether educational spending has an impact on crimes rates within each borough. I also would like to add weather as a factor that I could potentially control for seeing its impact on crime rates. This app is only for trends and analysis but I add more data to the application I would like to run regressions measuring the effects of income inequality education spending and gentrification on crime rates for each borough.,NA, The cluster map allows the user to see the location and details of specific crimes by filtering based on month and year type and by the borough. As you zoom into a location the clusters move apart. If you zoom in enough you are able to see individual pinpoints on where specific crimes occurred for that time frame.,NA
NYC Real Estate Analytics - Manhattan 2017,19,https://nycdatascience.com/blog/student-works/nyc-real-estate-analytics-manhattan-2017/,When it comes to real estate business choosing the is the hardest step!'Note: The application still needs some enhancements and updates. Why there are two different months for the best investment?! and which month is better?,NA," 'Investors search for  properties that have higher tendency to be sold with better price. The more deals they have the more money they earn. However it is not that simple! Having the wrong deal in buying properties means losing millions of dollars. So prior to buying a property there should be deep analysis about the property itself the previous sales the previous prices the location etc.  A lot of factors help in having the The following application helps investors in making their decision by analyzing previous sales transactions. It provides the user some computed analysis results with statistical insights and user interactive analysis features. For the time being this project shows all the property sales that occurred in Manhattan for year 2017.  The data is mainly collected for 36 different locations through 17464 sales transactions.
It provides the following about each transaction: Neighborhood Building Class Category Tax Class at Present Block Lot Easement Building Class at Present Address Zip Code Residential Units Commercial Units Total Units Land Square Feet Gross Square Feet Year Built Building Class at Time of Sale Sales Price Sale Date.From an investor's point of view this data set provides the investor all the information needed to analyze previous sales. He/She will get answers for
the following questions:
1. What is the best location in Manhattan to invest in? (whether because it has the highest sales rate or the highest sale price).
2.What is the best time of the year to invest? (depending on the sale prices changes during the year)
3. What location do people prefer for residential units as well as for commercial units?
4. Where are the old buildings located? Are people still buying apartments or houses in an old building?
A lot of questions can be answered from this data set.The statistical insights tab answers the user two questions:This graph shows that the best time of the year for investment in Manhattan based on price is  As for the minimum average of sale price it is  261156 $ and the maximum average of sale price is 5411472 $.In this graph we can see that the best time of the year for investment in Manhattanbased on number of sales is The answer is in the hand of the investor.. it depends on whether  he/she wants to earn more money or sell more properties.The variation in this graph shows that the best place to invest in is  this is based on the sale price.As for this graphit  shows people's interests for some locations over the others since there are more sales in those locations. The highest number of sales is in  and the second highest is  (which is the same place that has the highest average price as well).The map shows the 36 locations in Manhattan with average sale price and number of sales for each location. The map is user interactive as well.The interactive analysis feature provides the user the option to analyse the information himself/herself. The user can modify 5 inputs which are: the locationthe type of building show only old buildings feature show only new buildings feature and the price. After that he can visualize the result in a table or graph.Since people's interest in buying or renting a property varies with time it is important to keep track with what people are up to! nothing is better that analyzing google trends to visualize people's searches on buying in a certain location. This tab will show the investor on daily basis and even lively what locations people are interested in the most.",NA
American Food Deserts: Analyzing the development of,20,https://nycdatascience.com/blog/student-works/food-desert/,This is my shiny project examining the development of food deserts in American neighborhoods. Minorities today disproportionately suffer from poor health outcomes. African Americans are twice as likely to have diabetes compared to whites and nearly 40 percent of Latinos are overweight or obese .Food deserts are commonly characterized has places that have limited access to affordable and nutritious foods and have a surplus of restaurants fast food chains bars and convenience stores (instead of grocery stores). And it make sensestands that such access to food sources plays a role may contribute to thesein persistent health differences across groups.Therefore for in this project I aim to:On top of that the NAICS has over 10000 categories. Consider what that amounts to with over 40000 zip codes in American and nearly 15 years of dates used for this study. The data required A LOT of cleaning.Caption: From left to right and top to bottom choropleth map of fast foods unfresh grocery stores fresh food and alcohol in the San Francisco Bay areaCaption: Map comparing number of fast food restaurants in San Francisco (L) and New York (R).Caption: Changes in the number of fast food restaurant in New York (Left to right; top to bottom  2000 2003 2006 2009 2012 2014).,NA," Much of the existing debate as to why such stark disparities exist have focused on issue such as healthcare coverage and socioeconomic status. However galvanized by Michelle Obama’s ""Let's Move Initiative” a growing area of attention has been the prevalence of food deserts and how their existence may help understand why nearly 40% of children are overweight or obese in black and hispanic communities.To accomplish the goals at hand I needed a data source that had several characteristics namely:A tall task indeed!Fortunately for me one of the richest public data sources available is the U.S. national census. In addition to the decennial census it offers neighborhoodlevel information from the annually collected American Community Survey (which began roughly in 2009) and the Zip Code Business Pattern Dataset which contains information on the number of different types businesses (categorized by the NAICS) since the 1990s for every zip code in America.Unfortunately for me one of the messiest public data sources available is also the U.S. national census. In addition to the overall large nature of the data which makes data processing and cleaning slow and tedious the census also regularly changes coding schemes and concept definitions year to year. Not only that but also the NAICS has over 10000 categories. Thus with over 40000 zip codes in American and nearly 15 years of dates used for this study you get the picture. The data was required A LOT of cleaning.Nonetheless after much cleaning I had a dataset that consisted of 20 cities with:Knowing the number of fast food restaurants fresh grocery stores nonperishable food sources and liquor stores in conjunction with the shape files associated with each zip code allowed me to create a choropleth map that demonstrates how areas in a city differ on these food source categories.The colors on the map indicate how much of that resource is in a given area. The darker the shade the higher the frequency and the lighter the shade the lower the frequency. The bins for the color scheme were constructed using the distribution of each outcome; therefore they change from graph to graph.Having 20 cities and 15 waves in my dataset allows me to explore differences in food deserts both across different cities and over time.​In the graph above we can compare the number of fast food restaurants in the San Francisco Bay area with those in New York City.In this graph we can analyze  how the number of fast food restaurant changes over time in New York. In particular we can see that there seems to be a large growth in Brooklyn.Given that the data contains sociodemographic data that goes along with the number of food resources in a neighborhood it is easy to observe the racial and socioeconomic characteristics of these neighborhoodsClicking on the marker in a zip code reveals its zip code number racial composition and median income thus allowing users to explore the descriptive characteristics of areas they want to learn about. However the basis for incorporating sociodemographic characteristics of zip codes was to garner insight into how food deserts correlated with neighborhood demographic composition. Unless users want to individually click every marker in the graph the choropleth map fails to provide insight into that relationship.While a choropleth map may be one of the more appealing visualizations it was not the best one to focus on the association between my two variables of interest. I proceeded to try some bivariate analyses to see if I can unpack the relationship between neighborhood racial composition and food resources. Given the size of all the waves of my data and the nature of bivariate analyses and visualizations I took a crosssection of my data and only examine the year 2014.If the narrative that minority neighborhoods are deprived of access to healthy food and instead have a plethora of unhealthy food options we would expect to see some sort of either positive or negative correlation between percent white and the outcome variable. But as the graph above suggests it is difficult to infer any type of relationship. Is that to say that we should reject the narrative that food deserts are concentrated in minority neighborhood? Not necessarily. The lack of a distinct pattern in the scatter plot may be more of a function of the relationship between neighborhood demographics and food resources. That is say an increase percent white from 20% to 22% may not significantly affect the number of fast food restaurants but crossing some threshold may.These graphs best illustrate this point. When I transform my xaxis from the continuous variable percent white to a categorical variable where neighborhoods are categorized as predominantly white black hispanic or heterogenous the previous scatter plot now turns into a box plot where we can examine the differences in the means and distribution of these groups.Yet still these findings lack robustness. Though we can now see that predominantly white neighborhoods have the  lowest number of fast food restaurants we cannot rule out cofounding factors. Namely do minorities neighborhoods have more fast food restaurants because they also tend to have lower levels of socioeconomic status? What about population density? Midtown manhattan likely has the largest overall number of fast food restaurants but could the overall foot traffic and population density be inflating the box plot results?To control against these confounding conditions I ran a negative binomial regression. Although my outcome variable is continuous I decided to use a negative binomial regression. instead of OLS. This is because my outcome is a count variable  and therefore by definition it will have a lower bound of zero. The effects are twofold 1) they would prohibit the conditional errors from following a normal distribution and subsequently and 2) they inherently make my errors heteroskedastic. Furthermore I chose a negative binomial regression over a poisson model because a negative binomial regression relaxes the assumption that an outcome's mean is equal to its variance. In my model I used city level fixed effects and control for area of a zip code log of median income and poverty level. I used zip code population as my exposure variable which fixes its log value at one thus essentially making the outcome a rate. Finally for my variable of interest neighborhood racial composition I utilized percent black percent Asian percent Hispanic and percent other with white as the reference category. Consequently the effect of each group must be interpreted relative to the effect of the reference group percent white.Negative Binomial regressions are modeled such that the outcomes are measured as the log of rates which ultimately detracts from its interpretability. Therefore I constructed bar charts where I used my negative binomial regression model to compute predicted values. The sidebar slider manipulates one demographic group (while holding the others at their mean ratios)thus allowing for insight into the effect of increasing a given demographic group in a neighborhood. For example in the graphs above the green histogram bar represents the predicted values for fast food restaurants fresh food stores alcohol serving institutions and non perishable groceries when percent white is manipulated. The green bars in the first graph displays the predicted values when a neighborhood is 50 percent white. In subsequent graph the green bar now represents the predicted values when a neighborhood is 70 percent white. Therefore the predicted values portion of my app allows for analysis of both the relative effect of increases in a demographic group (e.g. the effect of increasing percent white from 50% to 70%) as well as the comparison of effects across demographic groups (e.g. the first graph where the predicted values are compared at 50% white vs. 50 % black vs. 50 % asian vs. 50 % hispanic).The following analyses are not included in my shiny app because the nature of the results did not lend itself favorably to interactive graphs. However I felt it was important to run these additional analyses. The above statistical analyses were all run cross sectionally. That is I used only data from 2014. That was necessary due to constraints on the shiny server the nature of the models as well as for optimizing visualizations. But in doing so I dramatically reduced my sample size (and therefore the power of my models) and more importantly lost the immense leverage of using panel data to further tease out relationships. Therefore I conclude my analyses for this project using latent growth curve models to examine differences in the trajectory of growth of different neighborhoods.In contrast to typically used fixed effects models growth curve models allow for analysis of instead of effects. In this case that means that instead of having (e.g. 20 percent black in 2000 to 25 % in 2014) as the basis of my model I instead examine differences in the overall trajectories of growth (e.g. predominantly white neighborhoods in 2000 vs predominantly black neighborhoods in 2000).For ease of understanding the graph above shows imaginary data for how this growth would appear. Each line represent a neighborhood so they each have their own distinct growth pattern. However each individual line can also be categorized into a group: Asian black Hispanic and white. So then the questions becomes are there features of these groups that distinctly affect the slopes and intercepts of growth? This is the foundation of latent growth curve models in a nutshell.Growth curve modeling takes the simple notion of growth as slope plus intercept and expands on it by examining the latent factors that affect slope and intercept.In the case of my model I use characteristics of neighborhoods as either predominantly black Hispanic or integrated (with white as the reference category) to construct the latent variables for slope and intercept and use poverty level log of median income population density as time varying covariates.In the figures above I only examine  fast food restaurants and nonperishable groceries (and only include only the estimates for neighborhood racial composition for simplicity). The results for the other outcomes can be viewed in the supplemental materials section but I focus on these two figures because of the additional insights they contribute to my negative binomial regression model.For fast food restaurants the results from the cross sectional negative binomial regression suggest that an increase in percent black was associated with less access to these food sources. However the question remains has this disparity always existed? The results from the latent growth curve model shed light on this question. The effect of predominantly black neighborhoods on the intercept of growth for fast food restaurants is negative; however the effect on the slope is positive. This shows that historically predominantly black neighborhoods have had less access to fast food restaurants (compared to whites) but these differences have decreased over time. Given that the cross sectional models show that black communities have less access to fast food restaurants in 2014 we can infer that while there has been an increase in fast food restaurants concentrating in black communities this increase is still overshadowed by initial differences in starting points.For nonperishable groceries the cross sectional negative binomial regression demonstrated that black communities have more access to these resources.  However the growth curve models suggest a different pattern for how this difference has developed over time. Predominantly black neighborhoods have a positive and significant effect on the intercept and a non significant effect on the slope. This indicates that in regards to nonperishable groceries black communities have historically had more nonperishable grocery stores compared to white communities. However racial composition has no effect on the rate of growth of such grocery stores.  This suggests that the differences in access to nonperishable food stores between white neighborhoods and black neighborhoods results from initial starting point differences rather that differences in growth analogous to two parallel lines with different intercepts.When I began this project I had three goals in mind.Using a choropleth map within a shiny application I was able to successfully accomplish the first two goals. The third goal proved more challenging. Simple bivariate analyses proved inappropriate because of the extent of confounding factors. Negative binomial regressions provided insight into a snapshot of how neighborhood composition controlling for socioeconomic status affected access to food resources but ignored questions of how food deserts develop over time. Finally latent growth curve models while the most computationally demanding and statistically complex reveal differences in trajectories of growth defined by their intercept and slope for analysis of between group variation in food deserts.So back to the original question are minorities disproportionately living in food deserts? I believe that my project shows mixed support for this narrative. For black communities the results show that they have less access to fresh food and more to nonperishable groceries compared to whites but they also have less access to fast food restaurants. This suggests that black food deserts exist in the sense that they are more likely to be characterized with 711s and corner stores than Whole Foods and Costcos. But at the same time this image of McDonalds Arby's TacoBells Applebees overflowing in these neighborhoods may also be false and in reality they may just be communities barren of stores restaurants and businesses altogether.However the narrative of minority food deserts may be more accurate to describe hispanics and Asians. Coinciding with the narrative of minority food deserts these groups have more access to both fast food restaurants and nonperishable groceries. Yet they also have more access to fresh food stores. Again this suggests for these neighborhoods food deserts may exist in the sense of easy access to fast food chains convenience store style groceries but they also have increased access to places primarily engaged in selling fresh fruit vegetables etc. like bodegas and family markets.The predicted values and growth curve models add an additional element where we can also see the practical effects of each group as well as analysis of how changes over time are affected by neighborhood racial composition. Namely we saw that differences in fast food restaurants have existed historically between black and white communities but have actually been within the past decade. There has also been historical differences in nonperishable groceries have also existed but these disparities have remained stagnant over the past 15 years.This has important implications for how we now understand and talk about food deserts. We can begin to examine structural differences in food resources that exist in communities today as well as evaluate whether these differences have been exasperated over time. And as my analysis show answers to these questions involve a lot of nuance. And indeed broad strokes answers and methods can overlook important patterns and phenomena that are going on in the data. And as my project shows there are a lots of different way to approach a problem with each garnering different insights. I only highlighted select models and groups in this blog for this reason so I invite users to browse through my app as well as my growth curve models to gain a more detailed view of food deserts. Please feel free to reach out with any comments or suggestions!Supplemental Graphs:Footnotes:",NA
Department of Consumer Affairs Charges,20,https://nycdatascience.com/blog/student-works/r-visualization/department-of-consumer-affairs-charges/,"""NYC Admin Code § 20822(a)  SALE OF EXPIRED MEDS:  BUSINESS OFFERED FOR SALE OVERTHE COUNTER MEDICATION LATER THAN EXPIRATION DATE ON THE LABEL.""                                 ""NYC Admin Code § 20708  STORE DID NOT CONSPICUOUSLY DISPLAY THE TOTAL SELLING PRICE AT POINT OF DISPLAY FOR ITEM.""                                                              ""6 RCNY § 570(a)  NO PRICE LIST FOR SERVICES DISPLAYED""                                            ""6 RCNY § 570(a)  PRICE LIST NOT DISPLAYED CONSPICUOUSLY""                                           ",NA,"The NYC Open Data project provides the .  The Department of Consumer Affairs was started in 1969 with the goal to ""ensure compliance with local consumer protection and licensing laws and State and federal regulations.""  They oversee almost  in NYC.  This data set describes the charges brought against NYC businesses and through analysis can reflect how the enforcement plays out in practical terms.It contains 50 000 rows which includes location data some categorical information such as borough and business category but not too much numerical information.  This project had to rely heavily upon  of charges for the visualizations.  A lot of time was spent on parsing through the Charge descriptions which were challenging to categorize.Here's a sample of what the Charge descriptions looked like:I parsed the first part which describes the rule and is before the dash of the description then counted the number of charges vs each categorical description.  I would have liked to generalize about what each rule was and turn this into a category as well.  However I realized that would skew the analysis since for some categories it's difficult to conclude what the general description might be.  I also tried to find a source about the rules which I could then cross reference but only found  which is difficult to scrape data from.Are you an activist citizen who is concerned about what regulations your local businesses are required to follow?  Or perhaps a policy maker who is interested in the applicability of the current laws and rules that protect consumers?  Would it be surprising to discover that most of the charges fall under just a few categories?  That industries such as pharmacies and tobacco retailers get enforced more heavily?  Or that more charges were brought against boroughs which have stronger retail business climates?  In analyzing the data I discovered these kinds of trends.More insights can be garnered by visiting the  and manipulating the parameters.  I chose a very simple design and intended that the parameters could be applied to multiple plot types which can then be viewed by navigating through the tabs.  I left it in the user's control to decide how many ""top"" industries or charge types they were interested in analyzing.I also provided a word cloud to highlight what the most dominant words in the text of the charge descriptions.  I used some R packages to remove the extraneous English words and additionally I removed words like ""near"" ""required"" and ""upon"" which don't relay any meaning.And now without further ado please feel free to peruse my code.",NA
A.I. Development for Two Sigma Halite II Challenge,21,https://nycdatascience.com/blog/student-works/development-of-game-ai-for-two-sigma-halite-ii-challenge/,"Halite is an open source artificial intelligence programming challenge created by Two Sigma where players build bots using the coding language of their choice to battle on a twodimensional virtual board. Each game starts off with either two or four players in which they  compete in a match to either occupy the most planets or destroy the most enemy ships. Each player starts off with the 3 ships and you must dock on planets to gain control and produce more ships. The purpose of our project was to utilize machine learning techniques to develop a bot which would learn how to play the game. In order to do that we fed batches of games from the best players into our deep learning models with the hope that our bot could mimic highlevel strategies from the best players’ bots. Below is an example of a 4player game:Our first challenge was to understand the features outputs and coding pipeline that was included with the game. These aspects were important for us to build accurate models. Once we understood the framework of the game we needed to learn how to navigate through python’s tensorflow package and create our deep learning models. Once this was completed our execution challenges included learning how to deal with the computational complexity of the model and train it in an efficient way. Finally once the bot was trained we needed to translate our predictions into game commands and making sure our bot was fast enough in order to avoid it timing out.We used an adaptive agile approach that was inspired by Bernard Ong data scientist and guest speaker for NYCDSA fall 2017 cohort. The fundamental basis of Agile Process is to maximize the productivity of teams through implementing a divide and conquer strategy in a quick and parallel fashion. By incorporating iterative and parallel tracks the team can quickly disregard approaches which don’t work and either stick with the current approach or explore other approaches. The idea is to fail fast and move quickly.The Agile Process is a “standard industrywide software development and engineering life cycle where strategies and solutions evolve through collaboration between selforganizing crossfunctional teams”.  Unlike the traditional Agile Process our modified agile process is designed for machine learning purposes and AI development. Our agile process contains different components but follows a similar parallel architecture to Bernard’s proposed machine learning agile process.The components for this project included game framework understanding cloud computing navigation pipeline navigation and engineering data preprocessing feature and prediction engineering  algorithm selection hyperparameter tuning model fitting model evaluation model reengineering compilation and submission. In traditional Machine Learning pipeline each process is executed sequentially; model fitting could only be achieved after feature engineering is decided and the process of feature engineering could only start after data preprocessing was completed. However with our agile process approach we were able to leverage multiple individuals in the team to run data preprocessing feature/prediction engineering and model fitting in a staggered parallel fashion. By the end of the first week of our timeline we submitted our deep fully connected bot which placed us in the top 10%. Below are some of the approaches that we explored in parallel among the team: Feature Engineering was a fundamental component in all of our approaches to improve bot performance in the game. The bot’s performance lies not only in the complexity of the model but what features we feed into the model and what predictions come out of the model. These predictions could then be translated into game commands to the bot. Therefore we had to constantly update our features and predictions to best suit our approach and improve bot performance.Some of thefeatures that we engineered to suit our approaches:One of the crucial steps before implementing a convolutional neural network is to transform the dimensions of our features into a 3D array (width * height * num_channels) which then could be fed in batches into the CNN. We came up with 3 different channels: health ownership production. The health channel is a 2D map frame array which contains all ships and planet health regardless of ownership. These values are placed in the array corresponding to their respective coordinates in the actual map. Any coordinates which do not contain ships and planets in the map will have a 0 value in the map frame array. Likewise the ownership channel is a 2D map frame array which has 1 1 or 0 values. These values correspond to our bot’s territory enemy’s territory or uncharted territory in the map frame. The production channel follows the same structure as the health channel.After building the structure of the 2D map frame array we use minmax normalization to normalize the health and production values to (0 1) and standardize the size of our map frame arrays. In each game the size of the map frame changes but maintains the same 3:2 aspect ratio.  By standardizing the size of our input 2D image array into a set dimension (100 x 100)  we can maintain consistency in the output dimension size for each batch of 2D map frame arrays after the convolution and pooling operations in the CNN. We also proposed the idea of mirroring and rotating the input channel frames by 90 degrees to increase the effective data size by 8x.  To make our predictions independent of the game state and applicable to any amount of ships that we have we utilized the kmeans approach to analyze ship movement. Our idea was to try separate player fleet into up to 10 clusters so it is easier to analyze and predict the issued commands. After the model is trained it would predict the coordinates of the clusters and then the ships are distributed to move to those coordinates.The idea was implemented in our CNN network but we were not satisfied with its performance and the decision was made to move forward with predicting ships coordinates directly.We also explored the different ways of representing ships positions on the map. The most straightforward way is to use the x and y position provided by the game code however this is not a robust solution because the map size is randomized and the prediction will be only relevant for the game with a particular map size.The better approach is to normalize the map coordinates so that they are ranging from zero to one. Furthermore we could reshape the map to be the square size this way we can rotate and mirror it this way we expand our dataset 8 times.Another approach is to switch from Cartesian coordinates to polar coordinates. This way we don’t have to worry about the exact positions of the objects; instead we can just provide angles and distances between them. To address the complexity of the network and stay within size requirements and time constraints we decided to transform our input features into sparse tensors and sparse matrices. The reasoning behind it was justified by having large areas of empty space in our map representations. This would also help with data preprocessing times when we were extracting data from JSONs.One of our first approaches was to develop a baseline neural network model and establish a baseline rank for our game bot. We leveraged the template provided by Halite to expedite our approach.The template constructed an architectural code pipeline to develop the game bot as well as provided default features and prediction labels to train the neural network.  Using the template we built a shallow fully connected neural network on Tensorflow with 2 fully connected layers (12 and 6 neurons in respective layers) 11 default features 28 prediction outputs and softmax activation for the output layer. The default features are as follows:The prediction outputs were the allocation distribution of ships to send to each planet on the map (max of 28 planets). The total number of observations is variable depending on the number of games we download and the number of frames in each game. This could be formulated as follows:Total number of observations  number of games * number of frames* number of planetsIn our case we trained 430 games a total of 56014 frames. Thus there were around 1.6 million observations. Above is the cross validation and training loss for our baseline neural network. We defined our loss function to be the crossentropy loss because of the nature of our prediction outputs as probability distributions. Each step corresponds to a feedforward and back propagation process and we used 1000 steps to reduce the errors of our weights and biases. Our final cross validation loss was 3.025 and once we compiled the model and submitted the bot we established a baseline rank of top 70%.In conjunction to establishing a baseline neural network model we developed a deep fully connected neural network. The deep fully connected neural network had the same inputs and outputs as our baseline neural network but with different number of layers and neurons per layer. This approach decreased our cross validation loss to ~2.50 and increased our bot rank from top 70% to top 10%.Finding the optimal hyperparameters (number of layers number of neurons per layer) was a challenge for this approach and subsequent neural network approaches. Machine learning algorithms such as Random Forest and Gradient Boosting can use grid search and bayesian optimization for hyperparameter tuning within a reasonable time. However  implementing grid search and bayesian optimization for neural networks have significant challenges particularly being time and computationally expensive. These are some challenges for Bayesian Optimization using Gaussian Process with Expected Improvements (GP EL):Because of these drawbacks we decided to hand tune our hyperparameters (number of layer number of neurons per layer). We experimented with 579 layers and 20 50 100 neurons per layer and compared cross validation loss for each layer neuron combination setting for the model. We found that the optimal layerneuron combination was 7 layers and 50 neurons each layer.There were different design strategies experimented to structure our CNN including networkinnetwork skip connections batch normalization and very deep networks. Although these methods increased bot performance the computational cost outweighed their benefits.The most successful architecture proved to be a very simple convolutional network with 2 convolutional layers with leaky relu activations (slope  0.3) 2 max pooling layers (2x2 and stride of 2)  and 5 dense layers with softmax activation for the output layer. The number of filters per convolutional layer was determined by incrementally increasing it up to a point before timing out became too excessive. At the end of the network  the default prediction: 28 probabilities of ships to send to planets will be returned. Although the cross validation scores for our CNN model was ~2.3 our CNN bot only ranked in the top 30%. In the early phase of a game our bot would get stuck in position; going back and forth. We hypothesized that this was due to the lack of early game frame data as well as the nature of our prediction output. In each game turn our model will predict allocation of ships to send to a planet. These probabilities are turned into game commands which will move the ships in the direction of a certain planet. It happens to be that in early games there are a lot of different starting combinations that ships can go. With only a few hundred games to train there is not enough information to capture all the early game shipplanet combination resulting in abnormal behavior from the bot. Furthermore if differing game commands are issued in each turn the bot will exhibit the erratic back and forth behavior.We had a few things in mind to improve the performance of our model. First we would eliminate the need for pooling layers in our CNN. Pooling layers help to reduce dimensionality of the array and provides rotational and translation invariance. They are perfect for use in classification situations because they are insensitive to rotations of the image array and the location of objects in the image array. In our case however we are not classifying objects in our map frame but we are concerned with the specific location of our individual ships in each turn. Therefore we do not want to throw away information in that regard. Our prediction outputs would need to be updated as specific coordinates for individual ships to move to instead of allocation of ships to send to each planets. Finally we plan on rotating and mirroring the image arrays to increase our data size. In traditional neural networks we assume that all inputs and outputs are independent of each other however for many tasks this can be a major shortcoming.There are multiple such cases wherein the sequence of information determines the event itself. For these types of cases we need a network which has access to some prior knowledge about the data to completely understand it. We applied various types of recurrent neural networks as they are especially useful with sequential data because each neuron or unit can use its internal memory to maintain information about the previous input and can handle arbitrary input and output length. In our case we feed the network a sequence of turns. One of the early challenges we faced was that the number of frames (turns) varies from game to game. To combat this we used tensorflow’s dynamic unrolling feature which allowed a dynamic variable in terms of the number of time steps. Internally it uses a tf.While loop to dynamically construct the graph when it is executed. 
The basic RNN design struggles with longer sequences but a special variant—long shortterm memory networks (LSTM) — can work with these. LSTMs don’t have a fundamentally different architecture from RNNs but they use a different function to compute the hidden state. The memory in LSTMs are called cells and you can think of them as black boxes that take as input the previous state and current input . LSTMs resulted in a slight improvement of score compared to basic RNN network as they were able to include information from the early phase of games. To combat overfitting we used a common regularization technique dropout. Additionally we used multiple combinations of learning rate training steps neurons number of layers activation functions (relu tanh leaky relu). In an LSTM model there are 3 gates an input output and forget gate. This gating mechanism is used to help with longer sequences. Similar to LSTM GRU models are used to avoid the issue of vanishing gradient. The main difference between the GRU and LSTM model is that GRU only has 2 gates and contains less parameters which make it more efficient and faster for training. Although the performance for the GRU was very similar to the LSTM this technique allowed us to experiment with different parameters because of its faster speed. The cross validation for our RNN model was ~2.6 ranking in the top 25%. Similar to the CNN bot the RNN bot struggled during the early phase of the game. We hypothesized that this was due to the lack of early game frames and small sample size (game replays).  However the bot adjusted in the middle and late stages of the game and on numerous occasions it was able to overcome the poor early game play. In order to improve the performance of the RNN model we would like to incorporate a Bidirectional RNN. This model will value past inputs and future inputs in order to predict the current state. We believe that this can help with the early game poor performance. Neural networks are considered to be black boxes: given input returns output. Internal information about the model such as the architecture optimisation procedure or training data can be hard to visualize and explain to an audience. Tensorflow recently introduced Tensorboard which makes it easier to understand debug and optimize TensorFlow programs. Tensorboard acts like a flashlight on the black box of neural networks by visually breaking down the mechanics under the hood.We added a lot of details to the TensorBoard so that we can observe while the model trains. Our main intention using TensorBoard was to assist in debugging as well as visualizing the Tensorflow graph to track the various transformations (reshape transpose etc.). In the future we plan to use it to plot quantitative metrics and show additional data like images that pass through it such as the input for CNNs. It can also be used to see what the model is learning especially when the training time increases.GPU accelerating computing is the use of graphics processing units (GPU) together with a CPU to accelerate machine learning deep learning and engineering applications. They help power and accelerate platforms ranging from artificial intelligence to selfdriving cars and drones. GPUaccelerated computing segment computationalintensive portions of the application code to the GPU and the remainder portions of the application code to the CPU. Although GPU cores are slower than CPU cores they make it up with their large number of cores and faster memory for parallelization of operations. That is why they are suitable for handling expensive computations for deep learning. Meanwhile CPU by itself is still faster than GPU for sequation code processing.We were able to maximize efficiency and maintain our agile process through the use of Atom and associated packages Teletype and Remote FTP. Atom is a free open source code editor which contain numerous capabilities and packages. One of those packages that we used was Teletype which allows users to share their workspace and collaborate on code in real time. Another package Remote FTP can edit files directly on a server without having to create a local project. Therefore we don't have to download the files of the complete project from  AWS virtual machine but simply connect and edit our remote files from Atom. When saving the files they are automatically updated on the server.Our next steps would be to utilize reinforcement learning to create a bot which has direct control over issuing game commands. For this we are planning to use DeepQ network where we use a combination of reinforcement learning techniques and neural networks to predict expected rewards for each possible action.To reduce the preprocessing times we could offload the JSON conversion and initial matrix operations needed for feature engineering to big data systems. Distributed computations will help speed up the process and the convenience of PySpark would make it a relatively simple migration.",NA, where Ownership (101) and ship health (0255)When training a deep learning model two major operations are performed: feed forward pass and backward propagation. Both of these operations are essentially matrix multiplications of input arrays and weight arrays. While these operations are simple in a mathematical sense these computations are time expensive when we scale our neural networks (increasing the number of layers and size of our input array). Thus to compute these expensive matrix operations in a faster manner we leveraged GPU accelerated computing from Amazon Web Services which offers parallelization of these computations. We were able to connect to the AWS virtual machine and train our deep learning models by launching a spot instance and activating Tensorflow using AWS Deep Learning AMI.One of our future goals is to combine our convolutional neural network with our recurrent neural network. By combining both neural networks we can take advantage of the feature extraction from CNN and temporal aggregation of features from RNN to generate a more accurate model for our bot.,NA
Predicting clicks in mobile advertising: An experiment,21,https://nycdatascience.com/blog/student-works/predicting-clicks-in-mobile-advertising-an-experiment/,"Advertising is a multibillion dollar industry that acts as a bridge between companies and their customers. While most people are conscious of the ads around them they likely underestimate the power of those ads and the influence of advertising in general. Research suggests that simply making someone aware of products events and brands increases the odds of that person actually buying those products attending those events or supporting those brands.Mobile advertising is a form of advertising that takes place on mobile devices such as smartphones and tablets. Mobile ads are served via Real Time Bidding (RTB) an auction process that happens in mere milliseconds. A mobile device user together with the available ad space on his or her device comprise a . Advertising companies bid to serve ads to these bid requests and the winning bid results in that company's client's ad appearing on the mobile device. Such an ad referred to as an  also provides the user with the option of obtaining more information about the ad by directing them to a website when the ad is touched; this is called a . Mobile advertising companies use the ratio of clicks to impressions known as click through rate (CTR) to gauge the success of their clients' ads. The process is well described by the following graphic from insight.venturebeat.com:To illustrate these points consider the following hypothetical scenarios.Yellow  0.08 Orange  0.15 Red  0.2. The above assumption led us to our second one which is that Thus we endeavored to determine if campaignspecific models those that were trained only on data relating to a specific campaign were more accurate in predicting click probabilities than a general model that was trained using all available data.The general methodology we employed to investigate this question was to conduct an experiment in which we compared the performance of two campaignspecific models with the performance of a general model on campaignspecific data. This allowed us to make a direct campaigntogeneral model comparison. The figure below describes our workflow; each section of the diagram will be discussed in more detail below.Ads Anonymous provided us with three data sets: Unfortunately Ads Anonymous was unable to obtain data for the bid requests on which they bid but did not win. For this reason we chose to exclude the bid requests data set and used only the impressions and clicks sets to train our models. Because the clicks data set was a subset of the impressions data set we were able to merge the two together to determine which impressions did or did not lead to a click effectively leaving us with a single data set. Finally at Ads Anonymous' suggestion we filtered the data for only those impressions that occurred in the U.S. and those that belonged to an ad campaign with a nonzero CTR. This reduced ""general"" data set of 73 million impressions with a CTR of 0.63% is what we used to train our general model. We created our two campaignspecific data sets from this general data set but only after cleaning and preparing the general set for modeling.A caveat to our approach is that because we were unable to analyze lost bid request information our models were inherently biased towards bid requests that Ads Anonymous bid on and won. Therefore we made a (nontrivial) assumption that the impressions on which we trained our model were representative of all possible impressions.Our now single general data set included several types of information for each impression including variables relating to the user the user's device the ad the app on which the ad was served and the bid request from which the impression came. Detailed below are examples of how we treated different variables.Useless InformationOur data set included some variables that possessed little to no variance and thus provided limited predictive power. For example all our data came from impressions that occurred in September 2016 thus the variables Month and Year were uniform for all impressions. .Incorrect TypeThe variables Ad and Campaign were labeled with unique numeric identifiers but these numbers do not have an inherent ordinality. That is Campaign 234 does not have an ordered relationship with Campaign 235. Thus the Ad and Campaign variables are actually categorical rather than numeric with each unique identifier representing a different class or level. Machine learning algorithms however would treat these numbers as having an ordinal relationship. Buried InformationWhile the variables themselves contained information useful for prediction often additional information exists ""hidden"" in the values after manipulating them in some way. To access this hidden informationAn example of a feature we extracted was derived from the variable Day which included values ranging from 1 to 22 corresponding to the days in September from which the data were collected. We hypothesized that people may have different click behavior depending on different days of the week. Thus we created the new variable Weekday which described the day of the week (e.g. Monday Tuesday etc.) the ad was served.Other examples of extractions are:An example of a feature interaction we made was the combination of Weekday and Hour. Similar to our rationale regarding the creation of the Weekday variable we hypothesized that click behavior might be different both on different days and at different times of different days. People may be less active on their mobile devices during weekdays than on weekends but this may differ depending on the time of day. That is Friday evenings might be more similar to Saturday evening click behavior than during that same time on Sunday.Other examples of interactions are:Messy LevelsA large majority of the variables in the dataset were categorical and several of them contained a large number of levels. Many of these levels however contained the same core information and thus would be more appropriately treated as the same level. An example of a variable with messy levels that we reduced was BestVenueName which describes the app or site on which the ad was served. Seen below on the left are values for BestVenueName for three different impressions. Each observation contains the core information ""Meetme"" but if left untreated a machine learning algorithm would have treated them as separate levels. We cleaned these impressions such that each value was replaced with only the value on the right ""meetme.""Infrequent LevelsSome categorical variables still contained a large number of levels even after cleaning the messy ones. In several cases a majority of the impressions only included a small set of these levels whereas other levels were seen only a few times. An example of such releveling was the variable Carrier which describes the platform from which the device is receiving Internet service. This variable contained more than 350 levels most of which corresponding to only a small number of impressions (see below top). We releveled Carrier by grouping all levels that constituted less than 0.5% of all impressions into an ""other"" group (see below bottom).Minor MissingnessThe Location variable comprised of the latitude and longitude of the user at the time the bid request occurred was roughly 4% missing. This geospatial data is import in understanding the demography of the user and thus is useful in targeting the appropriate audience for a particular ad. Machine learning algorithms do not handle missing data well so Major MissingnessWhen preparing datasets for modeling variables with high amounts of missingness are often either dropped losing any information in the variable or the missing values are imputed in a systematic way artificially populating the variable. While both strategies have advantages and disadvantages the one employed is often based on the perceived importance of the variable in question.For our data almost half of the impressions were missing the variable Gender. suggests that gender is an important factor in a determining a person's response to advertising. In a metaanalysis of more than 30 years of research a scientist found that women will purchase products marketed towards both genders whereas men will only purchase products marketed towards men. Based on this information we considered gender to be an important factor in predicting clicks therefore we opted to impute the missing values with machine learning.To execute our imputation we used a random forest to predict the missing values. We only included variables that were descriptive of the user and their device and excluded information pertaining to the ad or click. After a coarse crossvalidation process our best random forest model included 40 trees a max depth of 20 and included four variables at each split. The model resulted in an 85% accuracy in predicting gender. Once our data had been cleaned and all feature engineering was complete we created our two campaignspecific data sets. For the general model we ended up using ~73 million impressions (with clicked/unclicked labels) and from this we sectioned off all impressions related to company Hair Care and all impressions related to company Sports Bar. Hair Care included 3.2 million impressions and a 1.45% CTR and the other campaign Sports Bar was comprised of 1.9 million impressions and a 0.55% CTR. These campaigns were chosen for their manageable size their duration and for clickthrough rates that straddled the general data set's CTR (i.e. 0.63%).For each of the campaignspecific datasets we had to further split it into training validation and test sets. To approximate a production setting in which we would be using past data to predict on future data we opted to do our train/validation/test splitting by time. Our data spanned from 9/1 to 9/22 in 2016 and we used impressions from 9/1  9/17 as our training set and impressions from 9/17  9/22 as our test set; this meant that our training set constituted the beginning 80% of the campaignspecific data and the test set constituted the last 20%. To obtain a validation set we further split the above training set. For this we used a  such that our new smaller training set constituted roughly 60% of the overall campaignspecific data and the validation set constituted roughly 20%. In hindsight it would have been more consistent to also use time for this split. However our approach was motivated by wishing to replace the crossvalidation step on the 80% training set with a single validation step on the 20% validation set and we believed that using a random subset of that 80% set was still justified.Predicting clickthrough rate is a binary classification problem which we decided to approach using logistic regression with stochastic gradient descent. The advantages of using logistic regression on this particular data set are many:Information surrounding Logistic Regression and its tolerance to imbalanced classes is both limited and mixed so we decided to run a subexperiment. One common way of addressing class imbalance is through undersampling the majority class. That is by throwing out some percentage of the majority class observations in order to diminish the degree of class imbalance in the model training set. The risk of undersampling too much is that a model can be left with too little data to train on which can lead to underfitting. To see how undersampling affected our models we trained models with different degrees of undersampling. We trained each model three times: once on the entire training set (no undersampling) once on a training set where we undersampled the majority class to 10% and once on a training set where we undersampled the majority class to 1% (reaching near parity between clicked and not clicked). The results of these different undersamplings on our models are given in below in Prediction and Evaluation.  The other side of an underfitting problem is an overfitting problem which happens if model complexity is too high and the model fits to noise in the underlying data. To prevent this Spark's logistic regression has builtin regularization in the form of ElasticNet; it has an alpha parameter that controls the mixture of Ridge and Lasso (L2 vs L1 penalty to the cost function) and a lambda parameter that controls the size of the penalty term.Since we were working with big data we had to be deliberate in any choice that could result in long running times. As such we chose to reduce the number of parameters we tuned from two to one. We used pure Lasso regularization (alpha  1) and only tuned the lambda penalty parameter. For our grid search on lambda we also had to make some practical sacrifices. Rather than run a kfold cross validation we used one validation set and we kept our grid search fairly coarse. We evaluated the fit of our models using loglikelihood. As a guide for an appropriate starting lambda we used MLLib's objective history for the unregularized logistic regression model because it captures the training log likelihoods. With an idea for a starting lambda value we then searched 5 values at different orders of magnitude around that.0.5",NA,"For this capstone project we  Aaron Owen Kathryn Bryant and Paul Ton  served as consultants for the mobile advertising company Ads Anonymous (not their real name). Ads Anonymous’ main objectives are to maximize the number of client impressions served via the RTB process and more importantly to maximize client CTR. Our role was to build a machine learning framework that could be used to inform Ads Anonymous' bidding strategies by providing accurate click probabilities for incoming adspace auctions which in turn can help Ads Anonymous more effectively achieve its goals.Our approach to providing accurate click probabilities was founded on two assumptions. The first assumption is that These three situations are captured by the following graphic:After training our models we can feed in new bid requests and the model will return a probability of whether that bid request will result in an impression that will be clicked. In order to make a prediction of  ""clicked"" of ""not clicked"" from the probability we needed to decide on a decision threshold. To decide on a best threshold we had to decide how we would evaluate the quality of our predictions.With any classification model that predicts two classes (here ""clicked"" or ""not clicked"") there are four categories of outcomes: true positives (TP) false positives (FP) false negatives (FN) and true negatives (TN).  In order to choose an appropriate evaluation metric for comparing models we needed to understand what these four outcomes would mean in the context of mobile advertising. Consider the following (diagram credit: ):Suppose TP denotes the total number of true positive outcomes for a model FN denotes the total number of false negatives FP denotes the total number of false positives and TN denotes the total number of true negatives. These outcomes can be used to compute various basic measures of success for a model.One obvious easy measure of success is  defined as (TP + TN)/(TP +FN + FP + TN). We opted to immediately discard accuracy as a candidate for model evaluation due to the extreme class imbalance in our target variable. With such an imbalance any model could achieve high accuracy by simply predicting every bid request to correspond to an ""unclicked"" outcome. But since we care more about predicting the minor class  than about predicting the major class this is a highly inappropriate way to evaluate the success of our models. We also threw out (or ) defined as TN/(TN + FP) because our focus is on clicks (positives) rather than nonclicks (negatives).Thus the two basic measures we wanted to consider were  (sensitivity) and .Recall is defined as TP/(TP + FN) and also goes by the name of . In context recall answers the question “Of all the clicks to be had what proportion did you get?”. High recall can be interpreted as taking good advantage of relevant advertising opportunities. Low recall corresponds to taking poor advantage of relevant advertising opportunities i.e. losing money in an abstract/hypothetical way; the company doing the advertising does not show its ad(s) to as many receptive people as it could have. Note that recall can always be maximized by simply buying every single bid request (thereby making FN  0) but this is unrealistic for any company with a budget (that is).Precision is defined as TP/(TP + FP) and can be thought of as. In context precision answers the question “Of all the impressions you bought what proportion yielded clicks?”. High precision can be interpreted as putting money in the right place. Low precision corresponds to putting money in the wrong place i.e. losing money in a tangible way; the company doing the advertising gets very little return on the money they spent on impressions. Precision can be maximized by only buying bid requests for which the click probability is nearly 1 so that FP is roughly 0 (or in the extreme not buying any bid requests at all so that FP  0) but of course this defeats the purpose of mobile advertising.With the goal of making Ads Anonymous' customers happy we recognized that precision and recall needed to be balanced so that AA's clients maximize both the number of receptive viewers to which they show their ads and the number of clicks that result from the impressions they buy. Two metrics came to mind for model evaluation that included both precision and recall: F Scores and Area Under PrecisionRecall Curve (AUPR). Given the implications of low recall versus low precision we felt it was more detrimental for a company to lose money in a tangible way than in an abstract way so we wanted a model evaluation metric that we could weight in favor of precision. This led us to F Scores.In general an F Score is a harmonic mean of precision and recall. A harmonic mean of precision and recall can be thought of as a ""pessimistic"" measure of center in that it always lies between the values of precision and recall . (In contrast the usual arithmetic mean lies in the exact center between them.) The formula for an F Score is as follows:As the formula suggests different choices of β allow for varied weighting of precision or recall. At β  1 the usual harmonic mean is returned in which both precision and recall are left asis so that the smaller of the two measures  whichever that may be  is preferenced. For 0 ≤ β < 1 precision is made artificially smaller and is therefore weighted more heavily than recall; for β > 1 precision is made artificially larger therefore recall is weighted more heavily than precision. Given that we wanted to prioritize precision in our models over recallWith this chosen evaluation metric we tuned our clicked/not clicked decision boundary for each model so as to maximize the F0.5 Score.Recall that our experiment involved training three different logistic regression models on Hair Care impressions on Campaign 2 impressions and on General impressions one for each degree of undersampling. The 1% undersampled model trained on Hair Care impressions was used to predict on the Hair Care test data and from that prediction we obtained an F Score for the model; we also used the 1% model trained on General impressions to predict on the Hair Care test data to get an F0.5 Score for that model as well. We repeated these calculations for each combination of undersampling and campaign. Here are the results:In bold we see the better F0.5 Score for the two models being compared. For both campaigns undersampling the major class down to 1% proved detrimental to the campaignspecific models likely because the data didn't contain the minimum required sample complexity (not enough data was seen in our training sets to appropriately handle new/test data). Interestingly enough every single model improved as we undersampled less suggesting that that logistic regression robust to extreme class imbalances . We suspect that the size of the data and the complexity of it both influence the efficacy of under/oversampling as a way of dealing with class imbalance and we recommend exploring this issue on a casebycase basis.The official result of our modelcomparison experiment is as follows:Although the campaignspecific models achieved better F0.5 Scores overall. Hence we sought to unpack our results further and dig deeper into the business implications of our findings. In particular we wanted to compute the following for each model:For these computations we used TP FP FN and TN outputted by each of the models as well as two other quantities: average price per 1000 impressions (by campaign) which we denote by 'price' and downstream return per click which we denote by 'x'. The requisite formulae/equations are as follows:1. Total spent:2. Total saved:3. Required ""downstream return"" per click in order to profit (must solve for 'x'):4. Return on Investment (ROI): The values for TP FN FP and TN and the four above computations for each of the best models (in all cases the nonundersampled models) are given below. Note that the ROI computations are done with x equal to the larger value (of the two models) needed for downstream return per click for profit. This was done under the assumption that a true single x value exists independent of any models and for comparison's sake it we made it a value that would enable both models to profit.Across the board campaignspecific models are more conservative/less risky from a business standpoint than the general model. By using a campaign model to inform bidding a company will spend less save more and get a higher return on investment. Furthermore if the downstream return per click 'x' is unknown a company is more likely to turn a profit by using a campaignspecific model since the downstream return per click needed to profit is lower for the campaign models than for the general model.Overall we see that the findings of this businessbased analysis of our various models corroborate the findings of our academic analysis. Specifically with higher ROIs and lower downstream return per click values in order to profit the campaignspecific models are better than a general model for predicting click probabilities for individual companies.Our project focused on finding out whether companycustomized machine learning models for click probability were better than general ones. We were successful in answering this question for two carefully chosen campaigns but it would be prudent to repeat our experiment for  much larger sample of campaigns.Although our experiment helped us determine which of a campaignspecific model or a general model was better for predicting click probabilities it did not produce for us the absolute best model for this task. We could certainly shift our focus from a comparative one to an absolute one and pay attention to whether our results were objectively good rather than just comparatively good. To this end there are a few avenues to pursue:",NA
"Alumni Spotlight: Katie Critelli, Data Scientist at Deutsche Bank",22,https://nycdatascience.com/blog/alumni/alumni-spotlight-katie-critelli-data-scientist-at-deutsche-bank/,My advice would be to first of all take it seriously. If a TA or instructor says something they’ve been through it before and really know what they’re talking about. Take advantage of that and try to pick up all the pieces of information you can. The other thing I’d say is to have fun with it. You could have a creative idea or come from somewhere different from everyone else so even if you aren’t the most advanced person in the room you shouldn’t underestimate what you can do.,NA,"Katie Critelli had spent years doing research when she was considering an academic career. When she decided that she wanted to have greater flexibility and apply her skills outside academia she recognized that the path of the data scientist was the one she wanted to pursue. To obtain the necessary skills and the assistance in launching a new career she enrolled in NYC Data Science Academy. Her role in the newly formed antimoney laundering team at Deutsche Bank lets her apply her skills and creative thinking while learning more on the job. We sat down with Katie to learn more about her background and why she decided to add Data Science to her resume.I’m from Darien CT and I went to college at UPenn where I majored in neuroscience and minored in Italian literature. I spent about 4 years doing lab research as I had intended to pursue an academic career but then I realized that wasn’t the right path for me.After graduating college I decided that I didn’t want to pursue an academic career and moved to Washington DC to work as a military healthcare consultant for Booz Allen. In the fall of 2017 I moved to New York with the intention of enrolling in a Master’s program at Columbia but I decided to drop it because I wanted to gain a practical skill rather than just more knowledge. I found NYCDSA would help me do that.I decided to pursue a career in data science because it seemed like an incredibly important field where the skills I gained could be applied to any industry. Data science combines what I loved about research and what I did as a consultant  searching for data analyzing and making sense of it using it to tell a story building useful models and communicating insights to others to inform decisionmaking.I considered multiple programs in the New York City area but I chose NYCDSA for three key reasons:
1. I wanted to focus on data science as opposed to software engineering or just programming.
2. The program promised students not only classroom experience homework and project work but guidance during the job search and interview process. Plus3. I immediately liked the team at NYCDSA when I went to meet them in person.My experience at NYCDSA was really incredible. I felt like I got everything that was promised and much more. The TAs and instructors were knowledgeable nice and always available. The curriculum was truly cuttingedge and the projects involved tackling realworld problems. I met so many other motivated students and learned a lot from going over homework and project work with them. Something else that stood out to me was that the atmosphere was one of constant improvement. We would have pulse checks every Friday to see what students did and didn’t like about the bootcamp. If students had suggestions for ways to improve things they were taken seriously. I often saw these suggestions implemented within days.When I came I had used and Python and other tools but only knew them in a limited way and didn’t even know what gaps I had in my knowledge. Now I know Python and also know what I don’t know. That kind of knowledge is much more useful as it is something I can build upon and apply in a practical way. For that reason I feel much much more confident about my ability coming out of the bootcamp.It took me about 1.5 months to find my current job. NYC Data Science Academy helped with the process in many ways including organizing a  where I got my first leads on potential jobs and gained interview experience. I was contacted a month later by a company from the event. Even though I hadn’t spoken to the representative at the event they were interested in my background and invited me to interview which eventually led to an offer.I’ll be working at Deutsche Bank in the antimoney laundering team. It’s a new group whose function is to identify signs of suspicious activity in large amounts of transactional data and to build models based on previously detected cases of money laundering that will flag suspicious transactions. Though I’m working as a data scientist my role there is somewhat fluid as it also involves data engineering web scraping and filtering news from the web researching data mining pulling data from lakes etc. I expect to learn a lot on this job.I found that I was very well prepared for the interview questions. In the case of Deutsche Bank which was looking to fill a somewhat creative role they wanted to know how I would approach a problem. I felt comfortable discussing a lot of different approaches that could be applied to particular situations.This interview was originally posted on .",NA
Big Data Analytics: from Impressions to Clicks,22,https://nycdatascience.com/blog/student-works/impressions-to-clicks/,122 impId/dimensions combinations has more than 5 clicks per impression (0.025% of all combinations in Clicks) We took a closer look at them...Of 27 ads with multiple clicks per impression 14 had multiple clicks on more than 1 occasion.Ad sizes 320x480 & 300x250 were most likely to had multiple clicks on many occasions  despite their lower share in Clicks data (share  proportion of rows for a given Ad size).Of 15 campaigns with multiple clicks per impression 9 had multiple clicks on more than 1 occasion  not in line with their share.3rd party videos were much more likely to have multiple clicks per impression despite their low share.Toyota.com for example had experienced much larger number of multiple clicks per impressions than its overall share would dictate.Now our new column in Impressions could be used as a target variable for predictions (1  click 0  no click)The following columns had no missing values and were used as is: Ad Size (7 levels) Ad Type (3 levels) Device Type (4 levels) Exchange (4 levels) Venue Type (4 levels) and Target Group (1200+ levels).Looks like Males are more frequent ‘clickers’.Clicks seem to be somewhat less frequent for ‘US Cellular” and some minor carriers (fall under “unknown”)Larger ads are clicked on relatively more frequently.3rd Party Video ads are clicked on relatively more frequently.PC users seem to be less generous with their clicks.Ads purchased on Rubicon seem to attract fewer clicks.Ads shown on apps seem to attract more clicks than those shown on websites.Relative frequency of clicks is different from state to state.Relative frequency of clicks is different from url to url.Looks like people are less likely to click on ads in the morning.Looks like on Friday people have better things to do than clicking on ads!Our target variable (clicks) had imbalanced classes: only ~0.6% of all impressions were clicked on. We tried to address this issue by::15 IAB dummies were fed into the model as is.All predictors were then standardized by default by LogisticRegression procedure (imported from pyspark.ml.classification)All categorical variables were label encoded using Spark’s StringIndexer. The encoded categoricals were then fed to the random forest classifier along with the unaltered numeric variables.Top Positive Regression Coefficients for Logistic Regression:Top Negative Regression Coefficients for Logistic Regression:Predictor importance results from Random Forest were partially in line with logistic regression’s findings: (with 1200+ levels) was the most important predictor.The top predictors were:,NA,"We were very excited about this project because it gave us an opportunity to finally deal with the ""BIG DATA""!A digital company involved with placing bids for clients' online ads provided us with the following:The table below shows columns we decided to use as predictors of clicks % of Missing values (NAs) for each of them # of levels and how we recoded the columns for modeling.Several graphs below show the results of our exploratory data analysis devoted to the bivariate relationship between predictors and clicks. For each predictor we first show its levels and their incidences and then % of clicks for each level.The table below shows the diagnostics for several models we've run. Each of them took a long time to run and our attempts at running parameter grid search using crossvalidation failed because the Amazon cluster we used was not large enough.Area uROC stands for ""area under Receiver Operating Characteristic Curve""Area uPR stands for ""area under Precision/Recall Curve""We tried to run the models with and without case weights. We discovered that using class weights* is analogous to using no weights and then predicting clicks/nonclicks using a probability threshold that maximizes F1 measure: It raises Recall but reduces Precision & Accuracy.We did not have much time and each model took a long time to run. This is why we only ran a few Random Forest models. For the last model in the table below our several attempts to produce an accuracy table timed out so that we are not reporting Accuracy Precision and Recall for it.",NA
"Kaggle's Competition: Predicting Housing Prices in Ames, Iowa",22,https://nycdatascience.com/blog/student-works/kaggles-advanced-regression-competition-predicting-housing-prices-in-ames-iowa/,Response VariableMissing DataOrdinal CategoriesNominal CategoriesOutliersSkewnessNear Zero Predictors,NA,"is a website designed for data scientists and data enthusiasts to connect and compete with each other. It is an open community that hosts forums and competitions in the wide field of data. In each Kaggle competition competitors are given a training data set which is used to train their models and a test data set used to test their models. Kagglers can then submit their predictions to view how well their score (e.g. accuracy or error) compares to others'.As a team we joined the  Kaggle challenge to test our model building and machine learning skills. For this competition we were tasked with predicting housing prices of residences in Ames Iowa. Our training data set included 1460 houses (i.e. observations) accompanied by 79 attributes (i.e. features variables or predictors) and the sales price for each house. Our testing set included 1459 houses with the same 79 attributes but sales price was not included as this was our target variable.To view our code (split between R and Python) and our project presentation slides for this project see our shared GitHub .Of the 79 variables provided 51 were categorical and 28 were continuous.Our first step was to combine these data sets into a single set both to account for the total missing values and to fully understand all the classes for each categorical variable. That is there might be missing values or different class types in the test set that are not in the training set.As our response variable Sale Price is continuous we will be utilizing regression models. One assumption of linear regression models is that the error between the observed and expected values (i.e. the residuals) should be normally distributed. Violations of this assumption often stem from a skewed response variable. Sale Price has a right skew so we log + 1 transform it to normalize its distribution.Machine learning algorithms do not handle missing values very well so we must obtain an understanding of the missing values in our data to determine the best way to handle them. We find that 34 of the predictor variables have values that are interpreted by R and Python as missing (i.e. ""NA"" and ""NaN""). Below we describe examples of some of the ways we treated these missing data.1) NA/NaN is actually a class:In many instances what R and Python interpret as a missing value is actually a class of the variable. For example Pool Quality is comprised of 5 classes: Excellent Good Fair Typical and NA. The NA class describes houses that do not have a pool but our coding languages interpret houses of NA class as a missing value instead of a class of the Pool Quality variable.Our solution was to impute most of the NA/NaN values to a value of ""None.""2) Not every NA/NaN corresponds to a missing attribute:While we found that most NA/NaN values corresponded to an actual class for different variables some NA/NaN values actually represented missing data. For example we find that three houses with NA/NaN values for Pool Quality also have a nonzero value for the variable Pool Area (square footage of pool). These three houses likely have a pool but its quality was not assessed or input into the data set.Our solution was to first calculate mean Pool Area for each class of Pool Quality then impute the missing Pool Quality classes based on how close that house's Pool Area was to the mean Pool Areas for each Pool Quality class. For example the first row in the below picture on the left has a Pool Area of 368 square feet. The average Pool Area for houses with Excellent pool quality (Ex) is about 360 square feet (picture on the right). Therefore we imputed this house to have a Pool Quality of Excellent.3) Domain knowledge:Some variables had a moderate amount of missingness. For example about 17% of the houses were missing the continuous variable Lot Frontage the linear feet of street connected to the property. Intuitively attributes related to the size of a house are likely important factors regarding the price of the house. Therefore dropping these variables seems illadvised.Our solution was based on the assumption that houses in the same neighborhood likely have similar features. Thus we imputed the missing Lot Frontage values based on the median Lot Frontage for the neighborhood in which the house with missing value was located.4) Imputing with mode:Most variables have some intuitive relationship to other variables and imputation can be based on these related features. But some missing values are found in variables with no apparent relation to others. For example the Electrical variable which describes the electrical system was missing for a single observation.Our solution was to simply find the most common class for this categorical variable and impute for this missing value.For linear (but not treebased) models categorical variables must be treated as continuous. There are two types of categorical features: ordinal where there is an inherent order to the classes (e.g. Excellent is greater than Good which is greater than Fair) and nominal where there is no obvious order (e.g. red green and blue).Our solution for ordinal variables was to simply assign the classes a number corresponding to their relative ranks. For example Kitchen Quality has five classes: Excellent Good Typical Fair and Poor which we encoded (i.e. converted) to the numbers 5 4 3 2 and 1 respectively.The ranking of nominal categories is not appropriate as there is no actual rank among the classes.Our solution was to onehot encode these variables which creates a new variable for each class with values of zero (not present) or one (present).An outlier can be defined with a quantitative (i.e. statistical) or qualitative definition. We opted for the qualitative version when looking for outliers: observations that are abnormally far from other values. Viewing the relationship between Above Ground Living Area and Sale Price we noticed some very large areas for very low prices.Our solution was to remove these observations as we thought they fit our chosen definition of an outlier and because they might increase our models' errors.While there are few assumptions regarding the independent variables of regression models often transforming skewed variables to a normal distribution can improve model performance.Our solution was to log + 1 transform several of the predictors.Predictors with very low variance offer little predictive power to models.Our solution was to find the ratio of the second most frequent value to the most frequent value for each predictor and to remove variables where this ratio was less than 0.05. This roughly translates to dropping variables where 95% or more of the values are the same.Feature (variable or predictor) engineering is one of the most important steps in model creation. Often there is valuable information ""hidden"" in the predictors that is only revealed when manipulating these features in some way. Below are just some examples of the features we created:Now that we have prepared our data set we can begin training our models and use them to predict Sale Price.We trained and tested dozens of versions of the models described below with different combinations of engineered features and processed variables. The information in the table represents our best results for each model. The table explains the pros and cons for each model type the optimal hyperparameters found through either grid search or Bayesian optimization our test score and the score we received from Kaggle. Our scores the root mean square error (RMSE) of our predictions which is a metric for describing the difference between the observed values and our predicted values for Sale Price; scores closer to zero are better.For brevity we will not describe the details of the different models. However see the following links for more information about how each model is used to create predictions: .Below are plots summarizing variables that contribute most to the respective model's prediction of Sale Price.For most models predictors related to square footage (Area) quality (different Quality measures) and age (Year Built) have the strongest impact on each model's predictions.There is no visualization for our best model which was an ensemble of four other models. The predictions for this ensembled model are calculated by averaging the predictions from the separate models (two linear regression models and two treebased models). The idea is that each model's predictions include error both above and below the real values and the averaged predictions of the best models might have less overall error than any one single model.One note is that treebased models (random forest gradient boosting and XGBoosting) cannot provide an idea of positive or negative influence for each variable on Sale Price rather they can only provide an idea of how important that variable is to the models' predictions overall. In contrast linear models can provide information about which variables positively and negatively influence Sale Price. For the figure immediately above the strongest predictor residency in a Commercial Zone is actually negatively related to Sale Price.The objective of this Kaggle competition was to build models to predict housing prices of different residences in Ames IA. Our best model resulted in an RMSE of 0.1071 which translates to an error of about $9000 (or about 5%) for the averagepriced house.While this error is quite low the interpretability of our model is poor. Each model found within our ensembled model varies with respect to the variables that are most important to predicting Sale Price. The best way to interpret our ensemble is to look for shared variables among its constituent models. The variables seen as most important or as strongest predictors through our models were those related to square footage the age and condition of the home the neighborhood where the house was located the city zone where the house was located and the year the house was sold.",NA
"Kaggle Competition : Predicting House Prices in Ames, Iowa",22,https://nycdatascience.com/blog/student-works/machine-learning/kaggle-competition-house-pricing-in-ames-iowa/,. The dataset which consists of 2919 homes (1460 in the training set) in Ames Iowa evaluated across 80 features provided excellent learning material on which to perform exploratory data analysis imputation feature engineering and machine learning (linearbased models treebased models and ensembling). Our main objectives for the project were 1) to gain facility in the endtoend process of a data science project in a collaborative environment and 2) to better understand the implementation and evaluation of various supervised machine learning techniques. In order for machine learning algorithms to provide meaningful insights we needed to ensure that the data was relatively clean. For our dataset we had to change some feature types and also handle missing values.In many cases we want the model to treat observations with missing values as a separate category. For example we know from the data description that a missing value for ‘PoolQC’ means that the house does not have a pool. It is important to let the algorithm know that some houses do not have pools because this may affect their value so we flag the missing values as ‘none’. The only exception is 'Functional': we still want to flag the missing values for this feature but we assign the value ‘typ’ instead of 'none' because the data description says that missing values here mean ‘typical functionality’.For numeric features when the house does not have attribute being measured it usually works to impute zero. It makes sense for example that the area of a missing garage is zero square feet and that a missing basement has zero bathrooms.  It is usually the case that quality features produce better models and one way to improve the quality and variety of features is to strategically create new ones by combining existing ones. However just adding more features isn’t necessarily helpful because one might encounter such issues as multicollinearity the ‘curse of dimensionality’ increased processing time and overfitting. Since there is a cost to adding features we had to exercise judgment in which ones to add. Maybe townhouses are predictably cheap in one neighborhood but predictably expensive in another. And conversely maybe a particular neighborhood tends to have expensive townhouses but cheap singlefamily houses.,NA,"In recent years machine learning has been successfully deployed across many fields and for a wide range of purposes.  One of its applications is in the prediction of house prices which is the putative goal of this project using data from a Our workflow consisted of a full development cycle divided into five stages: exploratory data analysis and preprocessing feature engineering modeling hyperparameter tuning and ensembling. The following is a workflow chart illustrating the five stages:The machine learning models we used for this project were the following:To gain a sense of the relationship of the features with each other and with house sale prices the target variable we employed a diverse set of data visualization tools including the following: density plots scatterplots boxplots and correlation plots.The first EDA we performed was to examine the distribution of the home sale prices. The histogram of home sale prices appeared to be rightskewed. We therefore performed a log transformation of the home sales prices to make the distribution more Gaussian.  The following are histograms of home sales prices before and after the logtransformation:The second EDA we performed was to create a matrix of Table Plots of the features (xaxis) against the target variable (home sales price).  To interpret these density plots in general we looked for two things: 1) a linear relationship between the feature and the target variable and 2) variation in the density of each feature value versus home sales price. The following is an example of some of the density plots for features we found to have a strong relationship with home sale price:Other plots we used to explore the relationship between features and home sales prices included scatterplots and boxplots. Examples include the following:Lastly we also explored the correlation between the various features using correlation matrices.  The following correlation matrix shows the correlations between some of these features with darker colors indicating higher correlations.Based primarily on these various EDA results we narrowed the list of features to around twenty which we considered in our base multiple linear regression model.  This will be discussed later in the blog post.First we had to change the data types of the below features from numeric to string.The values for each feature above represent different categories and not different amounts of something. This is easiest to see in the case of MSSubClass where the numbers encode different categories of houses such as 2Story 1946 and Newer (60) and 2Story 1945 and Older (70).  It is harder to discern in a feature like GarageCars where each value seems to count something (cars) but in actuality represents the garage capacity and therefore represents a category.We used the below strategies for dealing with missing values.For these categorical features we knew the house had the attribute being measured so we could not impute 'none'. In all the cases there was one dominant value for most of the data and so we decided to impute the mode as the value of the missing data because assuming the data are missing completely at random it is probable that they (like most of the observations) have the most typical value.For LotFrontage we needed to impute a value because it does not make sense that a house is actually missing the attribute. Instead of imputing the median value for the entire dataset we decided to impute the median for the neighborhood the house is located in to give us a more accurate estimate.Derived features are obtained by performing arithmetic on two or more similar features to produce another one. For example we added several similar features describing the number of baths to obtain the overall number for the house and we obtained total square footage and total highquality square footage by performing arithmetic on features that measured square footage.There are sometimes features in the dataset that interact with each other which happens when a change in one feature increases or diminishes the effect of another feature. For example if you have two houses with a '5' for 'ExternalCondition' but different scores (a '2' and a '10') for external quality the '5' should be weighted differently based on the quality score. The same condition score means something different when a house is low quality versus high quality and the same applies vice versa (the same quality means something different when it is of low condition versus high condition). The top three rows in the table above list features that like 'ExteriorQual*ExteriorCond' capture interactions between two different measures of the same thing.We included the feature in the last row because we think there may also be an interaction between the type of house and the neighborhood.For our base model we used a multiple linear regression model.  We selected 20 features that we believed were the most promising based on the EDA we performed as described previously. We further narrowed the list of 20 features by organizing them into five main categories based on our understanding of how a typical home buyer or investor would assess a home.  The table below summarizes our analysis:The features highlighted in red were the ones we ultimately selected to run in our initial multiple linear regression model.  With the exception of 'GrLivArea' all the features are categorical features.  Before dummifying these categorical features we further grouped the values of these categorical features into quantiles based on the relationship of each feature with the target variable.  One reason is to limit the number of explanatory variables in our model after dummification so as to lower the possibility of multicollinearity issues. We also performed a variance inflation factor (“VIF”) analysis to gain further comfort. In general a VIF score above 5 indicates that multicollinearity might be an issue.  After dummification all the explanatory variables we chose had a VIF score below 5.Our model selection process was the following:The Rsquared values across the various models we trained and tested ranged from approximately 80 percent to 81 percent. As part of our residuals analysis after we fit the model against the entire training set we examined if there were any influential points that may have had an outsized influence on the regression. As the following graph shows there are two observations (#523 and #1298) which stand out based on their influence as represented graphically by the sizes of their circles.Based on further review we noted that the sale prices for these two houses were very low relative to their living areas even in comparison with other houses in the Edwards neighborhood where they are located. Because we couldn’t detect any patterns or features that might explain this we made the decision to exclude these two data points from our analysis. After rerunning the regression without these two data points we arrived at a Rsquared value of around 81 percent for the entire training set.  We further note that the regression is significant at the 5 percent significance level and that the features with the highest absolute beta coefficient values were the top quantiles in terms of house quality and type of sale (for example normal sale or a short sale).The process we used to train and test the Ridge and Lasso linear regression models was similar to the one we used for the multiple linear regression model. The major difference was the further complication of tuning the model hyperparameter that affects the L1 and L2 penalty terms.  The following was the process we used:For the Ridge linear regression model we expanded our features list to include many of the interactions described previously. Given the presence of the L2 regularization term we felt reasonably comfortable with expanding our features list. The following graph illustrates the result of our gridsearch analysis used to tune the hyperparameter. We note that the optimal hyperparameter in which the root mean squared error is at the minimum is an alpha of 18.7.Using an alpha of 18.7 we note that the Rsquared values across the various models we trained and tested ranged from approximately 91 percent to 94 percent. We also note that the Rsquared value is lower for the test set than the Rsquared value for the training set which indicates that there may be some overfitting.  Consistent with the results from the baseline multiple linear regression model the features with the highest absolute beta coefficient values were those related to the quality and condition of the homes the neighborhood and the interaction between them.For the Lasso linear regression model we regressed the same set of initial features from the Ridge model against house sales prices. We also employed the same hyperparameter tuning process but interestingly the optimal hyperparameter for the L1 regularization term was much smaller at 0.01.  The following line chart provides a graphical representation of our grid search results.Using an alpha of 0.01 we note that the Lasso performed worse than the Ridge model in terms of predictive accuracy.  The Rsquared value for the Lasso models we trained and tested hovered around 84 percent.  However consistent with the results from the previous linear regression models the features with the highest absolute beta coefficient values were those related to the quality and condition of the homes and neighborhood. It is also interesting to note that the combination of home sale type and sale condition had the highest absolute beta coefficient value.Treebased methods empower predictive models with high accuracy stability and ease of interpretation. Unlike linear models they map nonlinear relationships quite well. They are adaptable to solving either classification or regression problems.We developed three classes of treebased models: decision trees random forests and gradient boosting. It was highly interesting to evaluate and compare the distinctive characteristics and performance of each of the three separate treebased models.To better understand the underlying behaviors we plotted the root meansquared log error (""RMSLE"") against a range of hyperparameter values. These plots were instrumental in:Subsequently cross validation grid search was performed on each treebased model with their respective hyperparameters. This resulted in the selection of the optimal hyperparameters based upon the average RMSLE score against the test set.It was interesting to note the distinction between each of the treebased models by examining their prediction profiles. Evidently decision trees exhibit clear discrete steps in the prediction plots owing to its simplistic model. The random forests and gradient boosting models achieved a better fit to the actual prediction profile from the training set. This is attributable to the bagging and boosting procedures in the random forests and gradient boosting models respectively.Furthermore the variable importance plots gave valuable insights on the various treebased models. It uncovers which variables were ultimately utilized by the models and their relative importance. Here are some interesting takeaways:Ultimately gradient boosting combines weak learners to form a more accurate and robust decision ruleset.After creating the linear and treebased models above we decided to combine them in an ensemble in order to increase the prediction accuracy and improve the overall confidence level of the predictions.  The various models capture different aspects of the dataset such as outliers thereby making the ensemble more robust.A number of transformations and imputations were made to the dataset in this stage in addition to those made in the earlier stages before running the models in the ensemble. These included unskewing all features with a skewness greater than 0.75 and removing outliers. These outliers were detected visually from the plots and confirmed using the Bonferroni outlier test.We used the StackingRegressor from the mlxtend package. The ensemble model consisted of lasso regression ridge regression and random forests models and used lasso regression as the second level model (metaregressor). This is illustrated in the image below.The following is a summary of the RMSLEs for the various models against the public test set and also against Kaggle's private test set used to score the submissions.Based on our experience with this project we were able to gain valuable insights on the application of a range of machine learning models. For example we discovered that features engineering and hyperparameter tuning proved to be vital steps and could have a big impact on the performance of the machine learning models.  Also ridge regression and lasso regression within generally outperformed the treebased models with the exception of gradient boosting. This is most likely due to the size and nature of the dataset which appears to lend itself more to the application of regression models.",NA
Webscraping running shoes portal runrepeat.com,22,https://nycdatascience.com/blog/student-works/webscraping-running-shoes-portal-runrepeat-com/,What are popular shoe brands?What are the popular shoes for specific needs?What features may have critical influences on customers satisfaction?For my web scraping project I decided to scrape  a running shoes discovery and review platform. It has over 134867 expert reviews and over 1000 shoes for users to choose from.In order to narrow down my research scope I focused on the top women's running shoes in all categories. I was able to scrape 400+ shoes with top scores in terms of popularity and top reviews. For product datasets I scraped brand name shoe name overall product rating run score rank summary and reviews. Plus the web scraping review dataset includes shoe details like terrain use release dates score reviews review summary etc. My web scraping codes are available on    ,NA,Whether you run for fitness or you are a marathon runner finding the bestfitting shoe among the many choices at a running store isn’t always easy.,NA
Web Scraping and Analysis of Wines on Vivino.com,23,https://nycdatascience.com/blog/student-works/web-scraping-analysis-wines-vivino-com/,"As someone with a taste for  good red wine and coffee but with the limited funds of a student I decided to webscrape my favorite app  by using Python and Selenium where I scraped information about 16690 bottles of wines. Vivino is the “goto” app when you want to discover good red wines especially on a student budget. Thus the prices range from $10  $6000 a bottle so it should be possible finding a wine matching almost everyone preference and budget. After couple of hunts for good affordable wines I started wondering: . Those are the questions I explore  throughout this blog post.Vivino is a Danish founded company and is today the most downloaded wine app used by more than 26 million users around the world. With millions of wines featured the database makes up the most extensive wine library in the world. Vivino has established a community and developed an app which especially is great for us who love red wine but aren’t sommeliersintraining. The Vivino app enables consumers to snap a photo of a bottle’s label and the app will immediately pull up information about the wine its rating score reviews and much more.I scraped the following variables on Vivino.com:The box plot reveals that the median rating for red wine tends to be slightly higher compared to white wine and sparkling wine. The majority of the red wines obtain a rating approximately between 3.5  4.0 (the interquartile range box). Further it should be noted that the rating for  red wine white wine and sparkling wine seems to be normally distributed though there are a few outliers. An interesting observation is that a slightly wider rating range for red wine is observed compared to what we see for the white wine and sparkling wine categories.I expected to see this correlation. The overall average rating score per bottle and price seems to “some extent” to correlate particularly for red and sparkling wine. The correlation coefficient reveals a “somewhat” strong relationship between rating and price for red wine (0.68) and sparkling wine (0.69) while the correlation for white wine is significantly smaller(0.57). However that correlation peters out for red wine priced above $100 a bottle; the rating  only increases slightly in tandem with the price at that point.As observed at the initial boxplot a slightly wider rating range was observed for red wines which is why I decided to look into the “spread of the data points” around the average rating (variance) for red and white wine. I wanted to investigate whether ratings for red wine have a higher variance than white wine.H0: Variance of average(rating red wine)  Variance of average(rating white wine) H1: Variance of average(rating red wine) ≥ Variance of average(rating white wine)The variance test revealed an extremely low pvalue (2.2e16) why the null hypothesis is rejected in favor of the alternative the rating for red wines has a higher variance compared to white wine. The higher variance for red wine rating can be caused by several factors.  Some part of the explanation might originate from the fact that red wine and white wine are served at two different temperature. A common rule of thumb is that red wine is best served between 50F and 65F while white wines are best served between 45F and 50F. As too much cooling causes a  loss of flavor the odds of flavor loss are higher for white wine than for red. Lastly red wines are typically aged in oak barrels while white is aged in stainless steels vats. These two entirely different processes result in two distinct taste experiences. The variety of flavor experience tasting a red wine versus white wine are bigger and could therefore be a part of the explanation why the data points for the rating of red wine are more dispersed.  Nothing really changes here compared to the previous scatterplot. However it’s again worth pointing out that it’s possible to buy a red wine with top rating for around $75  $100. The jump in the year is because no bottles were present in the missing years in the scraped data. The scatter plot reveals a correlation at 0.377 which could indicate that an increase in the year will decrease the rating. Therefore we need to test whether it’s significant or not. An interesting insight here is that older wines tend to have a more narrow rating range an increase in years also increases the range of rating. This could indicate that old wines are quality wines wines which people have saved. While the wider rating range for newer wine indicates that from a consumer perspective that both quality and lower quality wines are produced. In sum the plot makes intuitively good sense when considering year and rating within red wines.The boxplot reveals from my perspective a quite surprisingly insight! Wines in the United States have on average a low price while the median rating is the highest compared to the remaining countries. That would indicate that wine lovers who want a decent quality for a reasonable price should look at wines from the US!I have often heard that French wines are the best. During my internship at adidas I had the privilege to live with a French mate for a few months and we often discussed why people tend to claim that French wines should be a better choice (my mate tried several times to convince me). With the scraped data from Vivion.com I can check if the ratings support the claim of french superiority.H0 : Average rating France  Average rating remaining countries H1 : Average rating France ≥ Average rating remaining countries My test revealed that the scraped data did contain any evidence that the average rating for France wines is higher than the remaining countries.:
Next I wanted to figure out which regions and bottles to look for within a reasonable budget. Therefore I specified the following requirements:Playing around with scraped data revealed some interesting and surprising insights which pointed me in the direction of the following question. Can the predictors price and year explain the obtained rating score for red wines at Vivino? Therefore I decided to test the following model:Rating Score  β0 + β1 √price + β2 #Number of reviews per bottle + εThe analysis was carried out in Rstudio where I obtain the following output of my regression analysis.The adjusted Rsquared for this model was 0.5306 which is decent given the few number of predictors; and  The model reveals that the baseline rating score for red wines at Vivino.com is 3.6. Further every unit increase in √price would increase the rating by 0.053 while each additional reviews increase the rating score by 0.000007265. This means that it takes 137646 reviews to bump the rating by 1 point. It requires a significant amount of reviews to move the rating.",NA,"
The graph confirms the previous boxplot wine lovers who want quality (high rating) red wine for a reasonable price should indeed look at wines from the US!",NA
Webscraping Every Platinum Record: What Happened to the Album?,23,https://nycdatascience.com/blog/student-works/scraping-riaa-drastically-music-industry-changed/,Over the course of the past 20 years the music industry has experienced an incredibly drastic change and in particular it has to do with the way in which we consume music. Music services like Spotify and Apple Music are wonderfully intimate forms of which we listen to music. In the digital age we live in  the user have full autonomy over what we get to listen to. We can press skip we press pause and we can compile songs from albums and singles and put them together into playlist. How has this changed consumer behavior though? Given the digital platform that exists now there has grown an extreme saturation of artists. Anyone can viably learn and instrument record a song on their phone and upload it to Spotify. So with such an abundance of music in the world  how does one go about garnering attention and stand out? Click  if you want learn more details one how the RIAA defines a unit and their certification process.The data was obtained from scraping the RIAA’s website listing all awards ever given out since its inception which can be found on the following link: . Each entry is of the most recent award that a particular album or single received. These for each album or single to be awarded we have listed the: artist title the date it is was most recently awarded the name of the record label the format (single or album) the release date the type (digital or physical copy) the group type (band solo or duo) and the genre.,NA,"In short we can study the trend of the industry nowadays and see that for an up and coming artist. They should stop making albums and release music in singles.To study these trends I chose to look at the most successful albums and singles throughout time as inferred from the Recording Industry Association of America (RIAA). The RIAA is the organization that certifies records as either gold platinum multiplatinum or diamond. These are certified by the organization when applied for by either an artist or label via proof of sales. The RIAA defines gold as the sale of 500000 units platinum  1 million units multiplatinum  multiple of million units and diamond  10 million units. The specifics are defined on the RIAA website but just know that a unit is considered either the sale of a physical or digital copy of an album or single. To adjust for music streaming companies the RIAA in 2016 also defined a unit as either 150 stream per single or 1500 streams per album.To start let's take a look at the top selling artists in the domain of albums as seen below in Figure 1.Now compare this to the top selling artists in the domain of singles as seen below in Figure 2. Do you notice anything between the names of the artists in Figure 1 vs. Figure 2?The trend between the two of these should be relatively apparent. The topselling album artists are all from an era that was specifically preInternet while many of the artists in the topselling single artists are very current and are still making commercially successful music even today. Hmm... The trend here seems to say that singles are more popular nowadays but what does this mean really? Well to answer this let's have at the effect of album versus singles sales in the context of what I consider the two huge digital disruptors in music: Napster and Spotify.If we now divvy up the distribution of albums and singles into eras of post and pre Napster and Spotify eras we start to get a better picture for how much these two industry disruptors really changed the music industry. Furthermore we can see that singles are by far outweighing albums in commercial success as depicted in the time series graph below where the red line is the percentage of all sales that were certified as albums versus the blue line for singles. We see that at the launch of Napster begins an immediate drop in sales of albums generally. Alternatively we see the opposite for singles. We also see that not so coincidentally at the moment of the Spotify launch marks the swap between singles and albums.So what does this mean for an artist? Is there any insight to be had from this information? Well we can see singles are now dominating the charts in terms of music. In terms of success the metric to which the RIAA uses for streams is debatable but it still illustrates that singles are the most prominent form of the major of commercially successful music. Thus it goes without saying that singles are going to be the best way for an artist to tap fame. I think it is important to note that this is for popular music. This is definitely not a suggestion for all genres but more of the charting genres: pop hip/hop alternative and dance music. Lastly many of the artists whose albums were incredibly successful actually include many of the ones that dominate the singles charts. What I think this really means that these artist have garnered enough attention to make a album a commercial success. Thus the very concept of the ""album"" as the most popular medium to which deliver a piece of musical work is arguably outdated and singles are and going to continue to rule the future of popular music for years to come.",NA
A closer look on the effects of the Lending Club Scandal,24,https://nycdatascience.com/blog/student-works/lending-club-closer-look-pre-post-scandal/,Lending Club is an online platform that connects borrowers and lenders. It enables borrowers to create unsecured personal loans of up to $40000 on either a 3year or 5year term. Investors can then browse loan listings on the website and select loans they want to invest in based on a myriad of variables such as amount of loan purpose of loan loan grade fico score etc. Like most secondary marketplaces lending club profits by charging a “middleman” fee for screening borrowers facilitating transactions and servicing loans. Since its establishment in 2007 lending club has revolutionized personal loans in the US and is the world’s largest peer to peer (P2P) lending platform. The company raised $1 billion in what became the largest technology IPO of 2014 in the US. P2P is a new business model and like most business models it has to be tested it has to go through some issues and come out well on the other side. In early 2016 lending club was involved in a scandal over some of the firm’s loans and concerns by the board over CEO Renaud Lepanche’s disclosure leading to a large drop in its share price and the CEO’s resignation.This led me to explore how loans have changed through lending club’s growth and their outlook postscandal.The data used for this analysis includes all of from 2007 up to 2017 Q2  approximately 1.5 millions rows and 140 columns (of which 110 columns were removed for processing purposes).To explore this issue we must first understand the basics and how Lending Club works. Lending club assigns grades to loans as an indicator of risk. Grade A is considered to be the safest or least risk while Grade G loans have the most risk. As witnessed below interest rates increase proportionally over subgrades and there isn’t too much overlap between their interest rate density distributions. Both of these charts show that Lending Club seems to have a consistent model across loan grades.Next we look into their model further on how different variables impact interest rate. Unsurprisingly Fico score seems to have the one of the strongest relationships with interest rates  as fico score decreases interest rates decrease. Credit Age annual income and number of accounts opened in the past year are some of the other variables correspond with interest rates.Now that we have an understanding of how lending club’s model works we can look at changes in their business over time. Geographically California consistently has the highest loaned amount from 2008 onward. Coincidentally Lending Club is headquartered in California. In recent years Hawaii has had some of the highest interest rates however as a whole interest rates seem to be volatile from year to year . Overall both loan amounts and interest rates seem to be converging toward uniformity  a good sign for any business.Next we look at how interest rates have changed over time across loan various loan grades. The safer loans  Grades ABC seem to be relatively consistent. However as you can see interest rates have been rising for the riskier grades as high as 30%!We also look at how loan amounts have changed over the years a good indicator of lending club’s growth. They have consistent and stable growth from 2007 to 2014. While they do see incredible growth PostIPO there is also great deal of volatility.We have to dig deeper into this volatility to identify the scandal. Looking at the monthly distribution across years we see that prior to 2016 July and October have seen consistent jumps in loan amount likely due to holiday seasons. However 2016 saw an abnormal spike in 1st quarter specifically in March  the month of the scandal. In fact the difference between March and the second highest month in terms of loan amounts (Oct 2015) is nearly the amount of loan amount involved in the scandal  $145 million.Finally to get an idea of where lending club is heading we take a look at trends in default rates. Default rates are final once loans are completed. Lending Club offers 2 types of loans: 3year term and 5year term. Default rates for 3year term after 2014 are not finalized as these loans have not completed their term. Similarly for 5year terms we have final default rates up to 2012. For example if we invested in 1000 loans with a 3year term initiated in 2015 payments will be made until 2018. Therefore the current default rates on those loans will only go higher as loans could potentially default in late 2017 and 2018.The first thing that stands out is loans with a 5year term have a higher default rate than 3year term. Also as expected default rates are higher as grades get riskier for both terms. The safer loans (grades ABC) seem to have consistent default rates but the riskier grades (EFG) are trending toward higher default rates. This is a sign of concern and I believe lending club has also noticed this trend as they are increasing interest rates for these grades.Another sign of concern is that default rates are already high for years that we do not have finalized default rates for such as 2015  2017. For example in 2015  Grade F and G for 3year term have the highest default rate more than any prior year and this number will only go up as these loans have not completed their term. Lending Club’s credit rating model seems to be consistent over various factors and allows investors to choose their risk level accurately. After the 2016 scandal and the CEO’s resignation we noticed a dip in investments but their investor base remained confident and investments have bounced back.While Lending club is offering interest rates higher than ever before one should be cautious over investing in loans on Lending Club especially for riskier investments (Grades D through G). Default rates have been increasing. Perhaps lending club is accepting riskier borrowers increasing default rates but also offering higher interest rates than ever before to account for these default rates and to attract more investors.  Now more so than ever before the investor must look at the variables linked to a loan to differentiate the good investments from the bad. Due to high interest rates on riskier loans (grades DEFG) there is a lot of value and opportunity if one is able to identify the correct loans. The next step in this analysis could be to take a deeper dive into these riskier grades to identify patterns or trends that an investor can take advantage of to increase their bottom line numbers. Check out the shiny app . The details of the code can be found .,NA,Why are borrowers using lending club instead of traditional banks? Lending Club tends to offer lower rates than what the market would provide. As seen in the world cloud below it is common for borrowers to use these loans to consolidate debts.,NA
Food delivery: a new revenue source but also more complexity to manage,24,https://nycdatascience.com/blog/student-works/delivery-reviews/,Another significant change in the last years was the market for food delivery that keeps growing with the creation of several websites and apps delivering meals from restaurants that sometimes haven’t traditionally offered the option food togo. For restaurant owners the extra business is often welcomed but introducing a third party can create a large number of problems.,NA,"Over the past decade the number of consumer review websites such as Yelp.com has exploded. These websites allow consumers to share their experiences about service product quality restaurant environment and other aspects. Nowadays it is very easy to acquire information from countless other consumers about restaurants hotels products and it shows a significant impact in the businesses.So given that bad reviews can harm the business and having the delivery service as a new factor to be reviewed this study is intended to analyze how reviews from Yelp website can be compared with Seamless website (delivery service)?On Yelp.com I used Scrapy to web scrape 393314 reviews from 570 restaurants in New York City.On Seamless.com I used Selenium to web scrape 335169 reviews from 5612 restaurants in New York City.The number of reviews per borough from each website can be identified in the charts on the left and the number of restaurants per borough and price can be identified in the charts on the right:Before joining the databases form both websites to do the final analysis a specific analysis on Yelp data was performed.When analyzing the restaurants from different boroughs it is possible to notice that Manhattan Queen and Staten Island show restaurants with higher rates where 75% of the restaurants have the overall rating between 4 and 5.The main purpose of this study is to analyze how the delivery service impacts the review rates the following chart shows the user rates whether the restaurant has delivery service or not based on Yelp reviews. It was possible to notice that restaurants with delivery service have a wider percentage of restaurants with lower rates.In order to check if lower rates on restaurants with delivery service is a general behavior in New York City or if it changes from one borough to another I plotted the following chart.  Manhattan and Brooklyn show the same behavior but Queens shows the opposite behavior and it seems to have no difference on delivery service in Bronx. This indicates that the infrastructure/traffic/service of the borough might have an impact on reviews.Likewise Yelp the same analysis was performed on Seamless data. Seamless make available information about what people are saying on reviews related to the quality of food quality of the delivery and the quality of the order made on the website. I plotted a box plot to check if these variables could be related to the overall rate of the restaurants.It looks like there is no significant ""bad"" reviews related to if the order was accurate or not (when the food is delivery accordingly to the order made on the website/app).To perform the complete analysis comparing restaurants that are in both website I joined the databases ending up with a total of 135 restaurants. The total number of reviews from these restaurants on both websites are shown in the chart below.Initially I plotted a box plot comparing the rated of these restaurants per borough and you can see difference in some boroughs as follows:If you see the overall rating from the restaurants together if it doesn't seem to be very different but to confirm that I plotted the results for restaurants that had more than 1000 reviews on Yelp. The chart shows that some restaurants do not match but it looks like they usually have similar results.Regardless of what both charts showed I tested their correlation (0.0215) which means a very low correlation. I also ran a twosample ttest and the pvalue was 1.9931e223 which means that the samples are unlikely to have the same mean. So the overall rate of people using Yelp (most of the cases going into a restaurant) is different from ordering food online using Seamless.This study was able to show that Yelp and Seamless have different overall rates for the same restaurant. So before starting food delivery service a restaurant needs to aware of the new factors that it may bring to the restaurant management.Some concerns related to starting delivery service are quality and temperature of the food more orders on the restaurants of peak dinner times the prices on online services may be higher than in the restaurant menu which may lead to a bad delivered food experience.A deeper study can be done using sentimental analysis on the reviews to gather more information to prove this approach.If you want to see more information about this study you can check my",NA
New York City Psychotherapists: Who Are They?,24,https://nycdatascience.com/blog/student-works/new-york-city-psychotherapists/,There are many mental health professionals to choose from but it is  difficult to find the information on them Of course not all patients are able to pay the full amount out of their own pockets.  Most therapists (to be in a more convenient location for more perspective patients,NA," New York might be the city with the highest number of psychotherapists in the world. what and where they studied how much on average they charge per session what health insurance plans they accept and what their areas of specialty are. Probably that's the case because psychotherapy is a very individualistic profession. It's possible to find information about an individual therapist but there is little information on psychotherapists as a group.To address this issue I decided to collect information about New York City therapists using Web Scraping. More specifically I did web scraping of Psychology Today's ""Find a Therapist"" .  This site is a wellknown platform used by New York psychotherapists to advertise their services. An individual therapist pays Psychology Today a monthly membership fee and Psychology Today includes his/her profile on its website. A potential patient goes to the site enters the zip code and finds all therapists who practice in or near that zip code. Then the future patient can study profiles of different therapists find one she likes best and contact the therapist to schedule a session.Because it is a selfpromotion platform many therapists provide useful information about themselves like their degrees universities they graduated from their license number session fees insurances they accept their areas of focus and specialties preferred therapeutic approaches etc.I webscraped all New York city zip codes using Python 3's  on October 1719 2017. Then I merged this information with some additional information collected from US Census Bureau: each zip code's median income and number of households. This reduced somewhat the size of my data set because I could not find income/population size information for some of the zip codes I scraped. I ended up with information on 12629 psychotherapists from 168 New York City zip codes.I cleaned the web scraped data using R. Some of the information therapists provided was standardized but some of it was collected via therapists' writeins. As a result there was quite a lot of data cleaning to do. Below is what I found out about New York City (NYC) psychotherapists who selfadvertise on Psychology Today's website.As you can see most psychotherapists are actually Clinical Social Workers (LCSW  Licensed Clinical Social Worker) who hold Masters in Social work followed by Clinical Psychologists (PhDs).The absolute majority of NYC therapists graduated from universities located in/around New York City.It turns out that 5% graduated over 40 years ago and 17.5% graduated over 30 years ago while  64% graduated 20 or fewer years ago 32% graduated 10 or fewer years ago and 10.5% graduated just 5 or fewer years ago which is quite a high number because you have to be courageous to start your own practice less than 5 years after graduation!Here I should probably mentioned that in order to practice psychotherapy in New York State you are required by law to have a license issued by the state. However 1.9% of NYC therapists from Psychology Today listed licenses from other states! And 3% listed no license at all! Are almost 5% of those listed breaking the law? The graph below shows in what states therapists with outofstate licenses were licensed:As expected most of them are from the states around NYC  NJ and Connecticut. But there are some from as far away as California!Many (not all) indicated their ""average session cost"" (""from $X to $Y""). On average this range was $135 to $199. The maximum session cost listed was $500. The minimum was $20. 80% of therapists said they were willing to use the sliding scale (i.e. allow some patients to pay less given special circumstances).71%) said they were accepting insurance. However once you take a closer look at what insurance plans they say they are actually accepting you find that many list just one: ""Outofnetwork"". This means: I charge what I charge and give you a bill  you take it to your insurance and try to get money out of them good luck! In other words the therapist doesn't have a contract with any insurance (and thus is not bound by its rates). I calculated the percentage of therapists who listed at least one health insurance that wasn't ""outofnetwork"". I found that only 50% of therapists accepted at least one ""real"" insurance plan. In other words 71% of NYC therapists say they accept insurance but in reality only 50% do.What are the most widely accepted insurance plans? Below are the top 15 (and 1199 is not a typo  it's a real insurance plan). Insurance names are not capitalized due to the data cleaning process:Let's now take a look at who NYC therapists prefer to work with and what they specialize in. The next chart shows age groups therapists say they specialize in:And here are special population groups therapists say they are interested in treating:Each psychotherapist in his or her profile on Psychology Today could pick TOP 3 specialty areas from a list of several dozens. Below are the most popular specialty areas (all of these are standard areas listed by Psychology Today):As we can see the ""usual suspects"" top the list  anxiety depression relationship issues. It's also interesting to see how ""life coaching"" has crawled up  the list to spot 13. Life Coaching is not a psychotherapy specialty at all and is frequently practiced by people who are not psychotherapists. In fact among therapists without a New York state license a higher percentage (19%) lists Life Coaching as their specialty vs. 3% among those with a valid New York state license:It's also interesting to look at the less popular areas of specialty. Below are the bottom 20 from the overall list of top 68 specialties I focused on:I wonder where 'internet addiction' will be in a few years...I was also curious if psychotherapists in New York city zip codes with higher median income list different top 3 specialties than psychotherapists in zip codes with low median income.Below are the most popular specialty areas in top 20 median income zip codes (median income > $100000):And here are the most popular specialty areas in bottom 23 median income zip codes (median income < $35000):I was somewhat surprised to find fewer differences than expected. Two things that jump at you are: (a) higher incidence of 'behavioral issues' in poorest zip codes (b) lower incidence of 'self esteem' in poorest zip codes.Let's now look at the top 20 issues therapists say they specialize in (each therapist could check several):One should probably clarify that 'gay' 'lesbian' and 'bisexual' are listed by Psychology Today as 'issues' not because it's still considered wrong being gay/lesbian/bisexual but because some psychotherapists feel they are more experienced with/qualified in dealing with issues specific to LGBT+ patients.And now a chart of the less 'popular' issues (bottom 20 from the top 100 issues I focused on):Some of the issues among the bottom 20 are surprising: many women suffer from postpartum depression and most of us suffer from stress but only few therapists say they specialize in those issues.Now let's take a look at the preferred therapeutic approaches. Each therapists could list several. I focused on top 38 approaches in total. Below are the most popular 15:It is very interesting to see how over 65% of NYC therapists listed on Psychology Today say they use psychodynamic approach (and ~27% list Psychoanalysis!). That's New York for you! You'll probably never find such a high representation of psychodynamic approach anywhere else in the US. And again: ""Coaching"" which is not a psychotherapeutic approach at all makes the top 15 list!The next chart shows the bottom 15 approaches (out of the top 38)  which are also interesting :Curious to see that only few therapist use EMDR a highly effective traumatreatment!Finally let's take a look at the concentration of psychotherapists in different NYC zip codes and its possible cause. Fortunately I was able to get US census data on the number of households and median income in different New York city zip codes. Below is a map of NYC that colors zip codes according to the number of therapists per 1000 households (the more therapists per 1000 households the darker the color).We can see that the ""therapist density"" is the highest in Manhattan and affluent Brooklyn zip codes. Below is the list of the zip codes with the highest therapist density.These 10 zip codes have very low therapist density (note  I was unable to find the number of households for certain zip codes so that some zip codes might be missing from my analysis). As expected they are in the poorest sections of the city.What is the number of therapists per zip code related to? Is it related to the zip codes' population size (i.e. number of households)? The correlation between zip codes' number of households and number of therapists is pretty low just 0.20. That's probably because in addition to population size population income matters. Take a look at the scatter plot below. Each dot represents a zip code. Dark brown points are zip codes with higher median income. We can see that most zip codes with larger median income have quite a few therapists  irrespective of their population size.The next scatter plot confirms that income matters more than mere population size. Correlation between zips' median income and number of therapists is higher  it's 0.48 (and if both axes are logged it becomes 0.51):Because both income and population size seem to matter I calculated a proxy for ""total zip income""  the number of households multiplied by the median income. As expected its correlation with the number of therapists was even higher: 0.63. The scatter plot below demonstrates this relationship:Finally is median income related to the average session fee? It is. I correlated the ""bottom"" end of the ""fromto"" range for the session fee and median income. The correlation is 0.44. Overall the higher the median income in a zip code the more psychotherapists there charge per session:My analysis leads to further research questions. For example the number of therapists per zip code might be related to the number of people working there. Many therapists might open offices where many people work . For example Zip Code 10004 has by far the highest therapist density. It covers the southernmost tip of Manhattan and Governor's Island. Not too many people live there but quite a few work there. The same is true for midtown Manhattan. This is something worth looking into.Another interesting topic is: how do therapists' specialties approaches issues they are interested in cluster? Any interesting groupings there?Finally a practical implication of the data I collected: Psychotherapists considering opening a practice in a specific zip code could gain valuable insights that would help them differentiate themselves from competitors.",NA
NYC Leading Causes of Death,24,https://nycdatascience.com/blog/student-works/nyc-leading-causes-death/,"Being a yoga teacher I am a firm believer that health is determined by a person’s individual behaviors. However factors like ethnicity and sex are cannot be chosen and impact the probability of death from a certain cause.The goal of the project was to determine what trends exist in the leading causes of death when looked through the lens of ethnicity and sex. I decided to focus on the four major ethnicities (White Hispanic Black and Asian) that reside in my hometown New York City. To conduct my research I used data  whichUsing dplyr I deleted any information pertaining to ethnicities that were not clearly defined by race (""Other Race/ Ethnicity"" and ""Not Stated/Unknown""). I then renamed the factors under Ethnicity and Sex to more simpler terms. Afterward the number of deaths and death rates were transformed from character strings to numeric values.In the sevenyear span 418760 people died. Of the total 49.3% of them were White 26.5% were Black 17.9% were Hispanic and 6.29% were Asian. In regards to sex 51.2% of deaths were women. Interestingly more White women died than all Asian Black and Hispanic women combined.",NA, I decided to use a box plot to determine the leading cause of death regardless of sex or race. The plot shows it is heart disease followed by cancer which makes up for 59.7% of deaths. Also here we notice three outliers in heart disease which represent White female deaths from 20072009.Diving deeper the data suggest that on average more White men and women die from cancer and heart disease than any other race.Globally we can see that the amount of deaths ascribed to heart disease has been declining while that of cancer relatively remained the same.However that is not the case when we look at specific races. For example for Hispanics heart disease does also decline from year to year. However conversely cancer increases besides having a dip in 2010.There are many ways to analyze the data. To highlight trends both globally and categorically I used R. Shiny an interactive web application in which the user can create live analysis.  Here one is able to input ethnicity gender and year to create a sidebyside comparison between two types of people.The leading cause of death regardless of race sex or year is heart disease followed by cancer. Though death by heart disease has steadily been decreasing in NYC general for some ethnicities it is the exact opposite. For example from 20072009 most Asians regardless of sex has died from heart disease. However in 2010 cancer took the lead and remained the number one cause of death for the next 4 years.,NA
Citibike Business Opportunity: Advertising,25,https://nycdatascience.com/blog/student-works/citibike-business-opportunity-advertising/,                                                               1)West St & Chambers St        2) Broadway & E 22nd St        3) Broadway & E 14th St         6PM         Monday     ,NA,It is hard to wander around New York City without seeing rows of dozens of bright blue Citibikes planted in the middle of busiest nooks and crannies of the city.These bikes belong to Citibike a ridesharing program that allows users to conveniently rent a bike to travel to their destinations without having to worry about the hassles of parking and locking their bicycle.Citibike has quickly become the preferred mode of transportation for many New Yorkers who are tired of the laundry list of issues with public transportation and are looking to get some fresh air as they travel around the city.The premise for the program is quite simple you can choose between an annual pass for year round access or a 3 or 7 day pass as a more temporary option.Pass holders are able to pick up a bike from a station near them and ride to their destination and park the bike at a station closest to their final destination.With over 622 stations in Manhattan Brooklyn and Queens New Yorkers can easily find a convenient location nearby.As a New Yorker who sees Citibike stations in what seems like just about every other corner I came across the interesting realization that Citibike has a great opportunity to sell advertising space at the Citibike stations that are located in prime locations with mass exposure to pedestrian and vehicular traffic.  For my project I built an app that organizes existing Citibike user data to help guide sponsors looking to purchase advertising space with Citibike.Citibike’s public data set is a fantastic source of information because every single ride is documented and released to its data set.Across the 12 month span from Aug 2016 to July 2017 there were over 15 million observations for citibike rides.Due to timing and the limitations with R I will be focusing on 1.5 million observations for the month of July in 2017 (The newest month released by Citibike).Citibike’s raw data set includes the following categories:For the sake of my project which attempts to better understand and observe trends in the Citibike customer I will be focusing on the following categories:Upon simple analysis of Citibike users you can find a couple of interesting facts.First Male riders outnumber female riders by 2:1 ratio.The cause of the discrepancy may be attributed to male vs female transportation preferences but can not be accurately determined from the scope of this project.Next you will notice that two age groups (25 to 30 and 31 to 36) account for just under 50% of all Citibike riders.This information is very telling in terms of the popularity of Citibike among young professionals.Understanding the population of Citibike users is extremely important in considering advertising opportunities with Citibike.The premise of the app that I have built is for potential sponsors looking to buy advertising space with Citibike to use citibike’s user data to better strategize where and when to invest in advertising.As a result the Shiny app allows the user to interact with the app to gain valuable information.My app consists of three tabs outlined below:In this tab the user(potential sponsor) is able to select the gender they would like to filter by in the top left box.Once the user selects a filter the map on the right automatically adjusts to show density of citibike users by the selected criteria.For example if “Female” button is selected the map will show the neighborhoods in NYC(represented by the colored polygons ) by how many female riders are active the darker colors representing higher density of female riders.In addition ten markers are shown on the map representing the location of the Top 10 Most VIsited Bike Stations based on the chosen gender.The second tap in the app mirrors the functionality of the Gender Map tab only now the user is able to visualize New York City according to age range.Once again the user selects the age range he/she would like to study and the map shows the density of Citibike riders in the selected age range in each neighborhood.Top 10 Most Visited Citibike Stations are displayed for each age range through markers.In the final tab the user is able to select both gender as well as the age range he/she would like to examine.The charts below output the hourly activity of the selected gender/age range combination as well as activity by day of the week.In order to better understand a real world application of the app it is useful to examine a sample case study. :What if a upandcoming makeup brand X is looking to target younger female customers wanted to advertise with Citibike?Using the gender map company X can see which stations/ neighborhoods have the most exposure to female riders.Similarly selecting the 1924 age range in the Age Range tab will generate a map showing which stations/neighborhoods will have the most exposure to 1924 year old Citibike users.Finally the Date and Time Tab can be filtered to show the time of day and day of the week female riders are most active!Using the Top 10 stations for females and the 1924 age groupwe can deduce the top 3 stations that Company X should target.As shown in the case study example the app takes Citibike ridership data and organizes the information in a way that can be extremely useful for advertising.Although the app itself was a little specific in scope(limited to Citibike users and NYC region) the application is a great example of using data sets to glean information regarding customer behavior.With simple analysis and visualization techniques the app is able to hone in on locations and times that will provide the most advantageous for marketing purposes. As the technology for acquiring customercentric data becomes more and more powerful( i.e. credit card/ shopping cart analysis viewership analysis email subscription analysis etc.) it will become extremely important for companies to take massive amounts of data and translate into useful contributions to business strategy and innovation.,NA
San Francisco Restaurant Inspection Analysis and Visualization,25,https://nycdatascience.com/blog/student-works/san-francisco-restaurant-inspection-analysis-visualization/,Moreoverthe health department would be able to anticipate when a problem arise. For example  in a scenario where total high risk violations is greater than low moderate or no risk violations for a certain period of time the health department can plan beforehand and act accordingly to prevent a possible food related crisis.,NA,Starting 2012 jurisdictions across the country including San Francisco have begun publishing health inspection scores on Yelp using a standardized scoring system called LIVES. This open data allowed restaurant consumers to make informed decisions based on where they want to eat and motivated a lot of restaurant establishments to improve their inspection score in the hopes of attracting a bigger customer base. However the situation for health inspectors was quite different. Health inspectors found themselves not able to keep up with the pace of inspections due to the growing number of restaurant establishments in San Francisco since 2012. As a result many restaurants  were not inspected for years. It became a problem since some of those restaurants are known to commit high risk violations. If they are not inspected at least twice a year (standard procedure) the risk for foodrelated illnesses will increase dramatically.  In 2014 there were only 30 health inspectors  for at least 4500 restaurants in SF and one third of  those restaurants had committed high risk health violations before.An interactive application was developed to provide a possible solution for the growing risk of foodrelated illnesses due to uninspected restaurants. The following questions were addressed:The app is intended to provide the user a comparative measure of different types of violations committed at a daily monthly or yearly interval and reveal dense regions in San Francisco containing high risk violations.The data was provided through  San Francisco Health Department via the San Francisco Open Data Portal  . There were approximately 50 thousand observations and 17 features. The features includes:From the time series above the aggregate number of low risk violations  in San Francisco appear to be the highest among all the types of risk violations committed throughout 2014 and 2016. Specifically on June 3 2014 the number of low risks violations was 72 while moderate high and no risk violations had much fewer counts than that.The time series help provide a macro and micro perspective of the inspection performance of restaurants in San Francisco. In the case for San Francisco health department they can use this particular visualization and its interactive features to either discover the general trend of violations committed or specify a month and day to find the respective violation counts.  One of the objectives of the health department is to reinspect restaurants that failed their last inspection due to the number of high risk violations committed. Therefore the following heat map only consider high risk violations. The heat map reveals high risk violation counts for each post code.  Seven postal codes (94133 94103 94109 94110 94102 94122 and 94108)  contributed 52% of the total high risk violations in San Francisco. Upon inspection those postal codes correspond to some of the busiest areas in  San Francisco including Chinatown Pier and most of downtown San Francisco where the number of restaurants is much higher relative to other postal codes.  Further statistical tests would reveal more insights on the correlation between number of restaurants and the number of high risk violations and other types of risk violations.Similar to the heat map the bar plot above depicts the number of  risk violations by individual postal codes in San Francisco. The main difference is that the bar plot provides a visual comparison between all different types of risk violation  in various postal codes. Analysis of the bar plot reveal that postal codes which have the greatest high risk violations also have the greatest low and moderate risk violations. One possible explanation for this finding was mentioned above. Another explanation is that there may be interaction effects within each type of risk violations.  Statistical analysis would test the validity of the hypothesis that as the number of restaurants increase the number of risk violations committed also increase. It could also verify the existence of possible relationships between different type of risk violations.After narrowing down the top 7 postal codes which contribute most to high risk violations individual restaurants were analyzed to see which restaurants within each of these postcodes contribute the most for the  number of high risk violations. For postcode 94133 one restaurant had  10 high risk violations; the most of any single restaurant among the 167  in this postcode. In fact the top 20 restaurants contributed 28% of high risk violations in postcode 94133.San Francisco health department should allocate a larger fraction of existing health inspectors to post codes with high density of high risk violations. This approach would ultimately decrease the growing risk of foodrelated incidents by placing heavier attention on restaurants that commit more high risk violations. Of course hiring more health inspectors  would equally solve the problem and decrease the overall time spent for inspection in San Francisco. However if the San Francisco health department is under a tight financial situation or under other circumstances which prevent the hiring of more health inspectors this short term allocation solution will suffice. An interesting follow up project would highlight the specific details of the approach including the  number of health inspectors to send to respective postal codes  the time and date to inspect restaurants that optimize time effectiveness and the inspection order of restaurants. This could be done by building a  predictive model to determine which restaurants are more likely to commit high risk violations first.The application could be found on  and respective code could be found .,NA
Crypto Currencies as an investment vehicle,26,https://nycdatascience.com/blog/student-works/r-visualization/crypto-currencies-overview/,Note: The 'market' column means market capitalization and is the sum of the values of all coins in circulation.Nevertheless the pace of new ICOs globally remains strong.As a few coins clearly dominate trading we will look at their relative share of the overall crypto currency pie as measured by market cap. This graph shows a key trend in the past year: the emergence of other coins such as Ethereum and RippleNote: when we say 'emerge' we are talking about market cap not when these companies started.Finally I plotted the value of the final portfolio as a function of the volume threshold at which I would buy each coin. The gains get exponentially higher as the threshold diminishes. That means that if I buy each coin “right away” say when the daily volume is only above $1000 then some of the coins will experience a meteoric rise and the portfolio will grow more during the holding period. The same conclusion applies here: we would need to check with exchange data if these coins were truly available at such small volumes. At the very least we can conclude that this trading strategy would only be available to small players.,NA,"The file contains 1129 different coins. Here is a random sample:The following graph shows the constant rise of reported coins. We observe some volatility in the number of new coins in last few months. One factor was China’s decision to ban initial coin offerings (ICOs) on September 4th. Looking at the distribution of coin market caps we observe a large skew towards smaller valuations. In fact 230 (20.37%) of coins have no market capitalisation as of September 28th 2017 (the market cap reported by the website is $0). 678 (60.05%) have a market cap below $1 million.This graph shows the evolution of each coin total market cap over time (only top coins are shown). (22% and 5% of total market cap respectively) and a reduction of the Bitcoin total share from 90% to 50%.
In the graph below all coins prices were normalized to start trade at $1 on April 28th 2013. Plotting prices or log prices for all coins isn’t a viable option here. The coins should be clustered so that it’s possible to read the data. This analysis is outside the scope of this document.In order to better understand the behavior of these coins I implemented a basic dollar averaging trading strategy. Starting 5 years ago when data is available everytime the daily trading volume passes a certain threshold ($100000 to start) for a given coin I buy $1 of it and hold it forever. The composition of the portfolio on the last day where data is available (September 28th 2017) is shown below:Conclusion: 50% of the portfolio is made of just 6 coins. Stratis the biggest position went up 200000 times. The next step would be to check if those coins indeed could have been purchased at the initial ‘low’ prices reported on Coinmarketcap.com. Historical exchange data would be needed for this.The value of the portfolio over time (net of the cash investment represented in red) is plotted below. Investing in about 600 coins would have resulted in a net gain of $3400. This represents a significant return but concentrated in the last year. Very few long strategies would have failed in 2017 and this basic one is no exception.In response to comments from the author of the data file I created 2 more graphs. The first one looked into the alleged strong returns of Bitcoin in the first days of the month. This is not supported by the data which shows no relationship. Note that I focused on the last 12 months for this analysis as there was limited activity in the previous 4 years.The second shows the returns of a coins index (with ‘traditional’ market cap weights) in relation to Bitcoin returns. Supposedly Bitcoin price surges were causing alternative coin holders to sell their holdings in exchange for Bitcoin. We see that in fact all crypto currencies seem to move in tandem.Although those weren't covered here coins carry significant risks as investments including:Because coins are highly correlated to Bitcoin an investor in crypto currencies couldn't meaningfully diversify his holdings across coins. Following this basic analysis I would recommend considering any allocation to a basket of crypto currencies to be conservatively sized.",NA
Washington DC Crime Shiny App,27,https://nycdatascience.com/blog/student-works/washington-dc-crime-shiny-app/,2012 2017 Washington DC Crime ReportThe capital city of the United States of America Washington DC does not have a very good reputation for public safety in the history. This shiny app is focused on giving the user information about Washington DC’s crime from 20122017(July 13th).As an International Student I know my personal safety is always the priority concern for family and friends at my home country. This shiny app can also provide information to newly arrived International students in search of a safe area in which to live.This Bar Chart shows total crime recorded from different shifts: morning night and midnight. The user can choose different years to see the change of the quantities of crime that happened on different shifts. Although the specific time range about morning night and midnight are not provided from the data source. We still can observe a relatively low quantity of crime during midnight time range. The Bar Chart gives information about different types of crime that usually occur in the Washington DC area. Assault is a defined as an assault with dangerous weapon or caused serious injury. Theft F/Auto include stealing motor vehicle parts and anything inside of the motor vehicle.  It is very important to find out which area is relatively dangerous and which area is a good place to stay. The Bar Chart provides some information about each district’s total crime quantities.The cluster map is clearly giving information about the number of recorded crimes on every street. It intuitively shows valuable information for the user.The density Map provides another vision of the location of crimes. The user can select different crime types to see which is most common in a particular area.Crimes are said to escalate during holidays. The definition of holidays for this plot is New Year Independence Day Thanksgiving and Christmas. The result actually shows the total number of crimes occurring during holidays is a relatively minuscule percentage of the total number of yearly crimes.The growth rate shows the total quantities of crime is slightly declining since 2014.This shiny app can give the newly arrived international student a very simple and clear information about Washington DC’s public safety situation. It is a very helpful tool for them when choosing a place to live.Link:https://xiaoweicheng666.shinyapps.io/XiaoweiChengShiny/Code: https://github.com/nash13cxw/1stshiny,NA,I created the following section to better analyze and visualize the data.The red zone defined as top 10 repeated crime location.  This map plot is created meant to warn for certain area that people might get a higher chance to be a victim of a crime.,NA
Facial Expression Recognition with Tensorflow,28,https://nycdatascience.com/blog/student-works/facial-expression-recognition-tensorflow/,As in the study of Artificial Intelligence we want machines to be able to communicate and serve human.,NA,"What's Deep Learning? If you have a basic understanding of Neural Network then it's easy to explain. A Deep Learning Network is basically a Multilayer . With its special Backpropagation algorithm it is able to extract features without human direction. Some experts in the field believe that Deep Learning will replace most of other Machine Learning algorithm in the future.Although  is not part of my program I am very interested in this subject. As Deep learning technique is getting more and more popular these days adding it into my skill set is definitely a plus. Moreover I am taking it as another aspect of adventure in Data Science and Artificial Intelligence.If I can build a system which is able to read human facial expression it can be very useful in several areas e.g. Sales Marketing Human Behavioral Analysis Artificial Intelligence (build an AI to make people happy?)...So what I'm going to do is to build a Facial Expression Recognition model with a Convolutional Neural Network. The entire project can be found on my GitHub repository: At the start of this project I found a data set from a Kaggle challenge which is linked . The data set contains 35887 faces retrieved from Google and labeled by human labelers. However the data set is very messy. There are tons of mislabeled blurred faces and even anime face in it. So I chose to scrape my own data set and clean it by myself.A brief explanation of what my code is going to do is:The detailed code is as below:I  ran the code 7 times for ""Angry Human Face"" ""Happy Human Face"" ""Disgusted Human Face"" ""Fearful Human Face"" ""Neutral Human Face"" ""Sad Human Face"" ""Surprised Human Face""Then I manually went through each picture and deleted the false ones.After all I ended up with: 433 ""Angry Human Face"" 510 ""Happy Human Face"" 425 ""Disgusted Human Face"" 339 ""Fearful Human Face"" 369 ""Neutral Human Face"" 436 ""Sad Human Face"" 469 ""Surprised Human Face"".Although it's not as big as the Kaggle data set I was going to build the model with what I got.I had done several steps to get the data ready:As I was working on a AWS EC2 server I use 'ssh scp' to copy all the data onto the my AWS remote machine.As the pictures are saved in its corresponding folder the first part of the code will walk through all folder in the ""Pictures"" folder and document the directory and name of each file in it. Also it will keep track of all the labels of each picture.The face detector I used in the next section only accept JPEG or JPG file so I had to keep only these 2 formats and drop the others.In a Convolutional Neural Network the algorithm will try to find the common features shared in each group. To reduce as much noise as possible in the second part I used a face detecting tool to extract only the facial part of each picture. A certain tool can be found in Python  package which works pretty well in my opinion. (I'm going to study it and figure out how it works exactly)This code will find each face's position in each picture and append it data to a big array. Also it will keep the label of that face in another array.(P.S. I dumped all my data into a Pickle file to keep a backup at every stage)As color won't matter in facial expression I'm going to set all faces into a grey scale. Also I don't want my system to be a racist 🙂The last step is to rescale the face. I choose 100*100 (although the Kaggle Data Set is 48*48) because I want my algorithm to have a chance to capture those ""microexpressions"".The difference between the Convolutional Neural Network and an Artificial Neural Network is that a CNN has another 2 modules as 'Convolutional Layer' and 'Pooling Layer'. The idea of Convolutional Neural Network is to use a small filter scanning over all pixel areas in a picture and extract that matrix as a feature for each filter location. Then the features are sent into a pooling layer. The ConvolutionPooling can be done multiple times before the final result being sent to a traditional Artificial Neural Network.Working in Convolutional Neural Network is very complicated and requires a lot of prior knowledge so I cannot cover it thoroughly here... I also won't be pasting my code here because the code is way too much to explain and it requires other python script to run.So here is the basic structure of my CNN:Also I am using RMSProp in my CNN. RMSProp is a way to use momentum dynamic learning rate decay to accelerate the training process.I also have spread my data into batches as training a small batch a time would not only accelerate the training process but also allows a slow computer to be able to work on it.After couples weeks of model tuning (almost 1 week every time by my AWS Free Tier) the error rate is around 52% which is not bad with only 2503 Pictures. (The winner of the Kaggle Facial Expression Challenge has 34% with 35887 faces).I understand that for a Deep Learning Algorithm. I recently have read a great  about how ImageNet was built and how that amount of data helped building a fantastic object detection algorithm.The question now is: what to do in the future?I have downloaded a facial expression data set from . Although the data is very not large (only 493 pictures) and not labeled I wrote an interface to label those faces by myself:And add the labeled data set to see how much improvement can I make by adding these faces into my training data set.Also I have just got my hands on one of these:It's a cool little gadget called Raspberry 3. It's basically a mini computer based on Linux and capable of adding different modules.I'm going to:From my understanding:I may come back and update this post from time to time. If you have any question or suggestion please email me at  ",NA
NYC Citi Bike Visualization - A Hint of Future Transportation,28,https://nycdatascience.com/blog/r/nyc-citi-bike-migration-visulization/,"Like all other sharing systems  Airbnb the housing sharing system Uber the car sharing system Citi Bike is the network of bicycle rental stations intended for pointtopoint transportation.Citi Bike is New York City's largest bike sharing system. It’s a convenient solution for trips that are too far to walk but too short for a taxi or the subway. The bike sharing system is combined with all other transportation methods available in the area for commuters.Any Citi Bike client has come up against two frustrating scenarios: the empty dock at the start and full dock at the end of the trip. Researchers call this as ""rebalancing"" problem as part of ""fleet optimization"" questions.  This problem has attracted the attention of data scientists to develop complex methodologies to optimize the available bikes and open docks.Following I attempt to utilize the shiny visualization app to provide a hint for the 3 questions:The visualization app is intended to provide a way to explore different comparative measures at the route station and system levels with spatial attributes and time series.Citi published Citi Bike Trip Histories  . I used the Citi Bike data for the month of March 2017 (approximately 1 million observations). The data includes:Before moving ahead with building the app I was interested in exploring the data and identifying patterns of rebalancing.  Location wise imbalance (Top 10 popular Station)On the interactive map each dot presents a station.  The visualization will also provide options to identify popular routes by selecting date and hour range. The top popular routes are marked in orange as the lines between the spatial points. The direction of the routes is indicated by moving from the more red towards the more green dots.Interesting patterns are observed. The most popular routes on the west side run through Central Park and Chelsea Pier. Grand central/Penn Station centered routes are also in the hottest route list. Outside Manhattan there are centers in Queen and Brooklyn initiating lots of popular routes. Riders bike more along the west and east streets than along north and south avenue. That makes sense in light of the fact that  there are more uptown and downtown subways than crosstown ones and riders do utilize the Citi Bike as a an alternative transportation option.While not enough bikes available in hot pick up stations the docks are lacking in hot drop off stations. The red dots are where outflow of bikes exceeds the inflow of bikes The green dots are where inflow of bikes exceeds the outflow of bikes. In the other words the green dots are the hot spot to pick up a bike(more inbound bikes) and the red(more empty docks) to drop them off. And The more extreme the color of dot is the higher percentage change of the flows this stations has. The size and transparency of the dot is represented by the volume of  both inflow and outflow of the stations. The more obvious the dot is the hotter spot the station is.What caused the balancing problem? The map based interactive app provides an insight for predicting demand. The information displayed is the accumulated hourly variables based on dates selected. Details of statistic numbers is also available for each stations by zooming in.New York has a classic peak commuter flow .  Most commuters ride bikes towards the center of the town from its edge in the morning. At the end of the day they ride the reverse way to the edge where they live especially at the edge sections with fewer public transportations options.   What about the rider's activities. Is there any pattern involved? The app provides insights of rider's performance for reducing rebalancing demands. By studying rider's activities it will provides suggestions for potential solutions. Below each bubble represents an age and gender group. The age is represented as the number on each bubble.  A negative correlation is observed between age and speed. The younger the rider is the faster he/she rides. In similarity the group in the thirties shows similar miles per trip. The performance between female and male group are also different.  The male groups in blue perform a higher speed level than female groups in red. Is there solutions for rebalancing to cut the cost and improve the efficiency instead of manually moving bikes via trucks bikedraw trailers and sprinter vans from full stations to empty stations?  The moving will take crews travel in pairs 45 minutes to load a truck. Citi Bike sought a way to get the riders to move the bikes themselves. In May it started the pilot Bike Angel. The reversecommuter would be perfect target member of the  What is so appealing about the program is the bike sharing system could self distribute its fleet with the proper incentives. The member can easily make 10 Amazon gift card with a few reverse trips. As a result the demand of manually moving bike around would decrease.The Visualization app provides the real time status of fleets: popular routes inbound/outbound net change time series of stations hot spot analysis and rider's activities. It supports the self distributed fleet by establishing a baseline for identifying ""healthy"" rebalancing within the bike share system. It provides a hint for a future transportation solutions.The interactive app is available on.",NA,Currently there are about a million trips on average per month by Citi Bike riders. The system has 10000 bicycles and 610 stations. By end of 2017 the total size of Citi Bike system will be 12000 bikes and 750 stations. The grey area is the current service area. The yellow and blue areas represent the sections to be covered by end of 2017.pattern,NA
Predicting Housing Tax with Machine Learning Models,28,https://nycdatascience.com/blog/student-works/housing-tax-los-angelas/,Californians who buy a house often experience sticker shock when they get their property tax bill. The reason for the dramatic spike in  in real estate tax is the execution of California's Proposition 13 of 1978 which equates the  tax assessed value with  the purchase price. In an increasing market that will always result in a steep increase of the real estate tax value especially when the house has not changed ownership for an extended period of time. While equating the the tax value with the market value may seem fair the lack of yearly assessments creates disadvantages for multiple parties. As indicated by these disadvantages the problem is a doubleedged sword that creates disadvantages for both the real estate owner and the governing body. A possible solution could be to reassess all real estate every year in order to ensure that the tax value equals the market value; however such a process would be very cumbersome and time consuming in light of the fact that there are 3.5 million houses in Los Angeles alone. Consequently in order to assist in determining the market value of real estate with the aim of setting the tax value equal to the market value the following research question is coined: ?In order to create a model that might assist in the prediction of the real estate tax value market information is required. A current Kaggle competition provides information on 2.9 million houses in three counties in the United States one if which is Los Angeles () That makes it an excellent source of information though we must begin by addressing the generalizability of the sample.The concept of generalization from an academic point of view implies that a sample on a number of characteristics does not significantly differs from the population. Additionally the concept of generalization indirectly assumes that the data is randomly sampled which is required for any statistical test. With regards to the comparison of characteristics of the sample and population (US Census bureau) chi square tests were performed on the building construction year and the estimated value of the house (the house tax value) where both test statistics were found insignificant. However even if these tests might indicate that the data is comparable to the population it’s not a perfect match. The fact that the data was conveniently sampled because transaction information was used to select the houses reduces the generalizability of the data to transferability. This difference is only slight as it implies that the conclusion from the sample dataset can be used for the population; however when the sample dataset would increase the sample data will not resemble the population more.The quality of data input is one of the key factors to ensure accurate machine learning prediction accuracy. In order to ensure that the data quality is sufficient the following  cleaning workflow was performed:Overall with regard to the missing values it is possible to indicate that 41.3% of the observations were missing. The variables with the highest number of missing values are the building class type the story type the size of the basement and the size of the garden. In most of these variables the value is not missing at random as the house could simply not have a garden or basement. However in other case like the building class type clearly the value is missing as every house can be classified by a certain building type. In order to process the information within machine learning models these missing values must be imputed as machine linear algorithm cannot handle missing value.To make appropriate and reasonable decisions on the imputation methodology and data type each variable was compared with the description provided by Kaggle. Additionally logical reasoning was used for imputation. The following methods were used for some of the manipulations:Next the winsorization technique was used on all numerical variables. This transformation is performed to eliminate potential adverse effect that extreme values (outliers) may have prediction model.  Observations that fall below the 97.5th quantile and beyond 2.5th quantile were replaced with the mean value.As part of any data analysis exploratory data analysis must be performed. This analysis will ensure that the researcher has a good understanding of the data and can use this understanding and possible findings as an input for machine learning modelling. One could therefore even state that exploration is a prerequisite for machine learning modelling. The exploratory data analysis can be split into two sections: (1) analyzing the dependent variables (2) analyzing the independent variables.Investigating the distribution of the dependent variable the real estate tax value indicates two distinct peaks an indication that might imply that multiple processes are driving the outcome. The 1st peak is the land tax which differs strongly from the house tax (2nd peak) and might even be referred to as zero inflated. This occurs most probably because the land tax in many cases is equal to zero which is true for example apartments or houses with a very low land value. Attempting to predict the real estate tax value in its current shape could result in problems as the data (log transformed) is not normally distributed. Therefore it is advisable to split the independent variable retail tax in land tax and house tax and predict these individually. However here we’ll focus on the house tax prediction in order to reduce the complexity and length of this article. By reviewing the independent variables it is possible to gain a glance of the real estate market within Los Angeles. With over 50 independent variables only a small selection will be presented within this blog post. For example the real estate total tax value is concentrated between $150000 and $200000.  Second most of the houses within the dataset are built before 1959 with a decline in the total number of constructions within later years. Another interesting variable is the available perimeter. The data indicates that the majority of the houses have 1500 square feet of living space. Additionally the data indicates that the house perimeter ranges between  200 and 2400 excluding outliers in the data. Based on these insights and insights from other exploratory analysis not described in this blog post hypotheses can be formulated.In order to predict the real estate tax value a dataset is required that catches the influential factors which combined result in an accurate prediction. However as indicated in the previous sections the large number of missing values has resulted in the loss of a large number of columns that maybe could have held valuable information. In order to retain some of this lost information a clustering analysis was executed by means of KMeans clustering in which eight groups were identified. These groups were determined by observing the reduction in the gradient of the function of the within cluster variation and the number of clusters. As this process can be arbitrary in the absence of a strong inflection point the overall reduction of the within cluster variation was observed for different K resulting in a K of 8. Further analysis of this new variable indicated that the groups significantly differ with regards to the tax value and could therefore provide additional information that was not available within the existing variables. This new variable can now be used within the real estate tax predictions models. In the presence of a robust data set that has been cleaned and investigated for uncovered patterns hypotheses formulation and testing can be performed. Within daily practice this process is mostly done based on exploratory data analysis; however from a statistical standpoint it should be performed based on causal relationships which can then be investigated through correlation studies. Therefore a set of hypotheses was defined and consequently tested through bivariate analysis.In order to evaluate these hypotheses the Pearson’s productmoment correlation Welch Two Sample ttest and the KruskalWallis rank sum test were used.It is interesting to note that the hypotheses for the building quality type which was hypothesised to be higher for higher levels of structural tax turned out to be negatively correlated. Consequently we were not able to reject the H0 hypotheses and could not add this variable to the model because it was not logical from a causal perspective. The other hypotheses except the unit count which showed insignificant results the other hypotheses indicated significant relations that allowed for the rejection of the H0 hypotheses. Consequently these variables will be added to the various machine learning models. In order to construct a machine learning model a researcher has a large number of options and therefore an initial selection must be made. This selection process first focusses on the type of variable one aims to predict which in this case is a numeric variable making this a regression problem. Within the regression family various models are available ranging from linear regression lasso regression random forests and boosted random forests. With the aim of constructing a parsimonious model that can predict the real estate tax as accurate as possible these four machine learning models will be investigated. It is important to note that for all the applied techniques training and testing was performed with Kfolded separation of data with a 80/20 ratio respectively. A multiple linear regression models is a model that aims to find the best linear unbiased estimators under the Gauss Markov assumptions. Within this model multiple variables can be combined to predict one particular outcome where the relationship between the independent variables and dependent variable are assumed to be linear. Initial results from a linear regression model where the real estate tax value is determined based on the size of the house the construction year the property land type the number of bathrooms and bedrooms the type of airconditioning the number of pools and the earlier introduced clustering variable indicates that this model can predict 66.4 % of the variance within the data which can be considered a medium fit. However condition verification of the Gauss Markov assumptions indicate that the assumption of constant variance is violated making the estimators’ significance unreliable and poses the probability of creating an overfitted model. Consequently with the aim to correct for this violation of equal variance which is driven by the violation of normality the box cox transformation is performed.The box cox transformation aims to reduce the level of skewness within the dependent variable. Reducing the level of skewness should reduce the level of unequal variance within the model. The model result indicates that the R decreases to 60.05 in comparison to the 66.4% without the transformation. This decrease can be explained by certain variables losing significance and no longer contributing to explaining variance within the model. Consequently the box cox transformed model can be considered more parsimonious than the model without transformation. In a further attempt to create the best parsimonious model automatic variables imputation can be performed. This modelling technique is based on a multiple linear regression model where the Bayesian information criterion (BIC) is used to determine the most parsimonious model out of all possible model combinations. The downside of this modelling technique is that the variables that are used within the model are no longer driven by underlying causal relations but only based on their contribution to the reduction of the residual. This can result in models that are parsimonious but prone to overfitting the data. Nonetheless the results from this model indicate an R of 60.08% making it as strong as the box cox transformed model though it is based on a different set of variables which in most cases do not have any causal relation. Consequently from these three models the box cox transformed models is the most reliable and parsimonious. In the previous section variable selection was performed by imputing variables through the BIC; however there are other options available for the selection of variables for a model like the Lasso Regression. In Lasso regression shrinkage/regularization is performed for variable selection where Lasso regression attempts to minimize the error while also minimizing the number of variables used for prediction. The balance between the goodness of fit and the prevention of overfitting is determined by lambda. To determine this tuning parameter lambda 10folds cross validation was performed (see the figures below). Through this technique it is possible to determine the best lambda that minimizes the mean square error which indicates the prediction error. Through Lasso regression it was possible to improve the prediction to and R of 68.1 in comparison to 60.08% from the box cox transformed model which is an increase of 8%.                  In the last two machine learning approaches the focus lied on using numerical variables for linear prediction where categorical variables are used as dummies. However aWith the aim of constructing a model that can predict the real estate tax value as close as possible the boosting machine learning model is used. The boosting machine learning model is based on tree bagging which is used to reduces the prediction variance but in addition uses the last model in order to construct the next model. This technique will enhance the prediction power on the training data set but is prone to overfitting the testing data set. In order to fit a boosting machine learning model three tuning parameters must be determined which are the shrinkage the tree depth and the number of trees. Through cross validation the calculation of the mean square error and the Boosting test error plot (presented below) the tuning parameters are determined to be a shrinkage of 0.001 and a depth of 4. Based on these tuning parameters the boosting model’s R is 88.1% which is a strong improvement in comparison to the random forest model. However as indicated earlier the boosting model is prone to overfitting the training data which implies that the model is weak in prediction out of sample. Consequently validation on the testing dataset only indicates an R of 37.8 which is a very strong decline in prediction power. Within this project a multitude of machine learning algorithms were used with the aim of predicting the real estate tax value in order to automate the real estate valuation process and reduce the bias within the California tax system.  as indicated in the discussion of the machine learning models where the Random Forest machine learning model presents the best results. This medium fit is the result of the poor quality of the dataset used within this analysis. If better information with the emphasis on the less missing values is available higher levels of accuracy can be reached. However analysis of the California tax system revealed one of the underlying problems which prevent accurate prediction of the tax value. The tax value of a house is determined at the moment a house is sold as indicate in the introduction. This implies that two identical properties of equal value can have a great amount of variation in their assessed value even if they are next to each other. Consequently with this dataset. Overall this research project can serve as a proof of value. It indicates multiple shortcomings within the California tax system and that predicting the real estate tax value might be a good approach to automate the real estate tax evaluation process. Nonetheless for further research a complete dataset that contains information on the actual market value of the house would be better in order to prevent the misclassification of the households real estate tax value due to the existing time dimension in the tax value assessment.,NA, s the dataset contains a multitude of categorical variables the Random Forest machine learning method is introduced. Random Forest models are considered as an important statistical pattern recognition tool for prediction with categorical variables. As Lasso Regression the Random Forest machine learning algorithm also requires cross validation in order to determine the tuning parameter. The tuning parameters for Random Forest are the number of variables tried at each tree split and the total number of trees. Cross validation indicated that the number of variables tried at each split of 4 provides the best fit while reducing the computation time and a total number of trees of 100 is sufficient to capture the total reduction in the prediction error. Based on these tuning parameters the Random Forest model predicts with an accuracy of R 01.66% which is comparable to the Lasso regression model.,NA
R Shiny Visualization and Insight of MTA Fare Swipe History,29,https://nycdatascience.com/blog/student-works/welcome-new-york-metrocard-city/,There are two sources you can download the Fare Card History data starting from May 2010.One is the the MTA website:nhtesNY state open data website:.Either way you can get the latest dataset up to the previous week. For the shiny app the raw data set were downloaded at July 11th 2017. Since most of the downstream analysis are performed based on annual result  only data from 2011 to 2016 are used for current R shiny development. To understand the dataset I tried to figure out the dimension resolution and limitation of the data table. The MTA fare card history file contains mainly the date of swipe (first week day) fare type subway station ID station name and total swipe count aggregated by week. It is intriguing that the fare type in some degree provide demographical information of its user for example the Senior/disable fare type apply only to the Senior or Handicapped Rider while 30 days unlimited type is best for daily commuters.  The analysis will focus on  weekly activity.One way to narrow down the questions shiny app can possibly answer is to target the potential user or shark holder. By creating interactive data visualization I am targeting the user who is interested in swipe count information for any form of decision making. For example the app can help the manager determine whether to add extra service or/and maintenance to meet actual usage needs in a subway station. Another potential target stark holder would be the local business owner near a station who might benefit from adapting the business to according to the passenger demographical information.Here I designed an R shiny monitoring application to visualize the fare card swipe data in an intuitive way.Click the links  to open shiny:and source code at my repository:First I made an overall manual tab which contains a bar plot of both annual total subway fare card swipes and average monthly swipes from 2010 to 2016.  After calculating the total swipe count in each subway station or each fare type stations or fare types are ranked based on total swipe counts. Consequently it becomes much easier to find the top station or most common fare type using the ranked bar plots.There is  access to information of each subway station or specific fare type.  Click Station ~ Fare Type tub and use the filter menu to select individual station or fare type. Year based comparison are displayed at each quadrant. The next menu tab “Timeline” is designed to check a specific time window between 2011 and 2016 and  provide a detailed view if needed. The shiny app also made it easy to look at the swipe count changes through the years both overall and in specific fare types and specific station.Finally the raw dataset is provided for further analysis.This shiny app made it easy to extract MTA fare card swipe count information based on user's criteria. For example the overall card swipe count showed a slow increase of usage from 2011 to 2016.  While there wasa slight drop during 2015 that may have been due to the fare hike in November 2015. However checking by fare type reveals a clear decrease of Full fare type usage since 2011. There was  increased usage of Senior/Disable fare and Mail/Unlimited fare type from 2011 to 2016.  Overall MTA fare card data provided a straight approach to explore the MTA subway activity. Using the shiny app it become easier to monitor the passage volume change for a specific card type or in a specific subway station thus helps the decision making using filtered data and plot for both MTA passengers and business owner.[1] http://web.mta.info/mta/network.htm,NA,“Welcome to New York. This is a MetroCard City.”. This is what we often hear when boarding a New York City MTA Bus. In fact MTA (Metropolitan Transportation Authority) is largest public transportation system in North America. 1 Everyday million of people both residents and tourists reach their destinations on the MTA system. The subway alone comprises 24 lines throughout 5 boroughs. There are more than 19 options for fare types to fit the various needs ranging from of daily commuters to people just visiting for the day. Now the biggest question is how to deliver a better service to the MTA passengers while maximize the profit for shareholders as well as the local business as a whole?First of all MTA need to collect enough data to understand the problems and then solve them. They already have access to the data from their fare card swipes in each subway station MTA bus Air Train and Tram. Every day there is large set of fare card swipe records are loaded into the backyard of MTA database. Some of the datasets are available to public and we can use the data to note trends in subway ridership and discover more business opportunities surrounding subway stations.For businesses since there are more Senior and disabled rider using MTA subway it is highly recommended to add more elevator service or maintenance and additional seats inside the subway station. For additional revenue I recommend discount coupons flyers or other forms of business promotion can be mailed along with  fare card to Mail/unlimited card user. Small business owner who mainly rely on the subway commuter  should pay attention to population shifts at their neighboring station. ,NA
Visualizing Trends in Primary Education,29,https://nycdatascience.com/blog/student-works/visualizing-trends-primary-education/,Primary education is a fundamental requirement for success. Regardless of how one might define the term “success” the skills attained in primary schooling are vital. Given that the nonpoor essentially have full access to primary education we can safely assume that those who aren't enrolled are also poor. These areas tend to be the poorest and are most affected by regional conflicts.Using available data visualization tools within R like plot.ly we can graph specific indicators that were provided by the World Bank () to better portray our findings. In the graph above comparing the cumulative dropout rate to the last grade in primary education indicates the proportion of pupils enrolled in a given grade who are no longer enrolled the following school year.Comparing GNI per capita using the PPP (Purchasing Power Parity) method we can find SubSaharan Africa and South Asia lagging behind the rest of the world. In the next graph we can then look at the net enrollment rate and find these two regions at the bottom end of the scale as well. This reaffirms our assumption that poorer countries and regions are most likely to lack access to primary education. While trends have shown some improvement there is much more work to be done.Now that we’ve identified where the problem areas are physically we can do a little more digging to understand some of the factors leading to the success (or failure) of primary education. Using Pearson’s method of correlation we can use R to calculate the covariance of x and y divided by the standard deviation of x multiplied by the standard deviation of y. This will better display how various indicators correlate to one another. Essentially we are quantifying the interdependence between two indicators in order to try and identify what drives primary education success.For the sake of the data we have available we can define success of primary education as the literacy rate in the adult population. In addition to serving as an indicator of primary education success literacy is an important factor in reducing poverty. A study by the World Bank linking education and poverty found that “in all cases where detailed analysis of household data has been carried out poverty rates are highest for households headed by illiterate people and decline with increased education of the household head.”The correlation plot identifies the drivers of increased (or decreased) literacy rates in the population aged 2564. Enrollment ratios and GPI (Gender Parity Index) have a positive impact on Literacy rates while Pupil to teacher ratios have a negative correlation. Gender parity displays the access of females to males in terms of education. The closer to 1 the more equal access is. Gender parity is especially helpful when looking at certain countries and regions that may not prioritize female education. Additionally we note the dropout rates rising with the pupil to teacher ratio. In other words as we have more students per teacher they are less likely to succeed and stay in school for the following year. This leads to another cycle of illiteracy and in turn poverty.If we want to get serious about tackling the lack of primary school to those in need we must use data and data visualization tools to shed light on the issues. It’s time to end the cycle of poverty and we can do so by providing basic primary education to all children. Ensuring that students are not vastly outnumbering their teacher proves to be an important factor to keeping students enrolled and improving literacy rates. If we can collect more data we can answer the questions we need and raise additional ones we should be asking. Further research shows that the highest returns in less developed countries come from primary education. We must prioritize our future generations and give them a fighting chance to attain something better.A child’s success in life should not be dictated by the region in which they are born.,NA, to my ShinyApp. to my GitHub repository.https://data.unicef.org/topic/education/primaryeducation/http://siteresources.worldbank.org/INTMENA/Resources/Povertych306.pdf ,NA
Are Premium Airlines Also Premium in Performance? / A Shiny Application,30,https://nycdatascience.com/blog/student-works/r-shiny/premium-airline-performance/,Have you ever found that the flight for which you paid a premium fare did not live up to your expectations? Maybe you would like to be able to select an airline that has a better track record. But where to find such information?As airlines transported 3.6 billion passengers in the year 2016 which is about 800 million more than in 2011 (IATA 2016). Over the last 40 years global air travel has increased tremendously as it has become more affordable. This decline in fare price is the response to the airline deregulation act from 1978 and to the dynamics in the elasticity of demand for air transportation. This decline in the fare price is desired by most passengers as research indicates that the ticket price is still the most important feature on which passengers choose an airline despite other investments such as loyalty programs and the overall improvement of the customer’s experience (Skyscanner 2009).  However booking the flight with the lowest fare is not always the best strategy. low cost carriers are often also leaders in late arrivals low customer satisfaction uncomfortable cabins and unsatisfactory frequent flyer programs (NBC News 2017). While one group of passengers aims to minimize flight fare a second group has other priorities. For example a flight is far more valuable for a salesperson who suddenly has an opportunity to visit an important client than to someone visiting a friend or relative abroad. Consequently the salesperson is more willing to pay a higher fare in order to make the appointment (Airline Economics 2017). As the price of the flight is of secondary concern the salesperson is interested in other performance indicators for flight selection. That raises the following research question: To assess airline operation a wide variety of indicators are available; however not all of these indicators are relevant for the passenger. Relevant indicators of operational performance might be: percentage of flights delayed average minutes delay per flight percentage of flights delayed per reason and for example the Number of flights to a particular destination (NWDS 2017). In order to present such insights to passengers a reliable longitudinal data set that allows for the analysis of multiple airlines over an extended period of time is required. The time period should be chosen carefully as airline performance tends to sway over seasons which could sequentially influence the results (Bureau of Transport Statistics 2002). Consequently the time period is set to the last month of flight information in order to prevent seasonal bias and to ensure that passengers are informed about the current state of affairs. A dataset that allows for such an analysis is the U.S. Airline OnTime Performance dataset from the Bureau of Transportation Statistics U.S. Department of Transportation () .In order to inform passengers on airline performance indicators an application was constructed using R and Shiny. The application can be found at the following internet location:.To evaluate the functionality and usability of the shiny application a flight scenario consisting of a flight from Atlanta Airport to San Francisco Airport on the 16th of Aug returning on the 23rd of August was designed.. The possible flights options were determined by using Sky Scanner as well as the pricing options for these flights. Sequentially in order to determine the “best” flight the presented performance indicators and statistics from the Shiny application were used.For the trip from Atlanta to San Francisco there are three possible options for direct flights: Delta Airlines Frontier Airlines and United Airlines. These options were found using Sky Scanner and align with the information available in the Shiny application. For these airlines the following airline wide information is available:From these results one could infer that the performance of Delta Airlines with regards to delayed flights is better in comparison with Frontier Airlines and United Airlines. This might be a first initial indication of the airlines performance for the route from Atlanta to San Francisco; however does not necessarily mean that Delta Airlines performs on average for that particular route. Consequently route specific analysis should be performed.The Shiny application also provides the functionality to compare specific routes as route performance might deviate from overall performance. When the airlines are compared for the route Atlanta to San Francisco the following route performance is presented:For this route the presented route performance indicates that the flight duration (taxi out + air time + taxi in) is the longest for Frontier Airlines and comparable for Delta Air Lines and United Airlines. It is also possible to infer that the overall delay for Frontier Airlines is longer than Delta Air Lines and United Flights which is in line with the earlier findings from the average airline performance. However from this information as Delta Airlines and United Airlines are somehow comparable it is not yet possible to conclude which airline performs better. Consequently the flights are compared using ANOVA and a TuckeyHSD posthoc test (a test to compare the average flight duration for more than two groups). The table below presents the results and indicates that Delta Air Lines flights and United Airlines flights are both statistically shorter than Frontier Airlines but that there is no difference between United Airlines and Delta Air Lines.At this point of the selection process the date airports and airline have been selected which leaves the flight time. As Delta Air Lines provides seven opportunities on the 16th of Aug to fly to San Francisco. The passengers can select the flight time while taking into account factors that might influence the delay of the flight. Delay information for Atlanta International Airport indicates that as the day passes more flights become delayed. Consequently you’re better off flying earlier in the day if you want to reduce the probability of a delayed departure This selection can be made without having to take into consideration any price changes as the flight fare is the same during the entire day.Since the era of mass transportation the price flights have become ever more important for commuters. However another group of travelers exists that next to the price of a flight also highly ranks the punctuality of a flight. To facilitate this group of travelers in the flight selection process a Shiny application was constructed using data from the Bureau of Transportation Statistics U.S. Department of Transportation. The results from a scenario study indicate that information on flight performance can assists in the selection of airlines for specific routes within the United States and assist in the selection for specific flight times in order to reduce the probability of delay. However the findings presented within the document are limited to U.S Airlines for the period of April 2017 and therefore do not reflect future airline performance,NA,The USA Airline OnTime Performance dataset contains 479000 observations on 12 airlines observed over the period April 2017. Furthermore the dataset contains a multitude of variables from which 11 were used for the analysis:Therefore based on the average airline performance and specific route performance it is possible to conclude that Delta Airlines shows the best punctuality and flight duration and therefore is the preferred airline for this particular route and time.,NA
How safe is New York public school drinking water?,30,https://nycdatascience.com/blog/student-works/ny-public-school-water/,            Beginning of this year we received a letter from my daughter’s middle school regarding water sampling from all the water outlets in the school for lead level  in the water. The test was done during 2016 school year. We were very surprised to know that seven outlets had more than 15 ppb(parts per billion) which is the Environmental Protection Agency’s threshold for action. Highlevel of water lead is known to have cause numerous health problems specially to younger kids and unborn children.  This is obviously very concerning for all parents of New York public schools. I immediately asked my daughter if they were also informed by school staffs and know which are those seven outlets are and she said “Yes”.       But what about other schools? I wanted to know. That is the motivation of this shiny project. This shiny app is created using R language with various R packages specially ggplot2 shinydashboard plotly and leaflet in shiny web framework. The main purpose of this app is to locate the New York public schools  and provide available data about 2016 water sample testing for lead level on the leaflet map. This app can also be used to generate various plots to visualize the data to see the distribution of counties with highest number of schools with most water outlets above federal lead limit for action. ,NA," By: Surya GurungThe dataset is provided by New York State Department of health. It is downloaded from the NY health data site. You can click  to download the dataset. The dataset has 4602 rows and 27 columns and each row is a test result for unique school. I removed some features and missing values from the original dataset and added few columns. After data cleaning there are still 27 features but 4366 rows in the cleaned dataset. The 27 features are:So the cleaned dataset has 4366 unique schools and 62 counties. There are two type of organization(org.type) BOCES and Public Schools.The three most important features of this app are ""School Search"" widget bubbles on the map and tooltip. As the title indicates ""School Search"" widget is the tool to locate the school of interest on the map and it zooms to the school location with a blue marker.       The bubbles have three different colors red blue and green. Red bubbles are bigger than blue bubbles and green bubbles are smallest. Bigger the bubble size bigger the number of water outlets with lead level greater than 15 ppb. So the schools with green bubbles have fewer number of water outlets with greater than 15 ppb lead than the schools with blue and red bubbles.          When mouse pointer hover over any bubble a tooltip appear with school name address and total number of water outlets with more than 15 ppb lead in its water sample.  App also have four tabs for various visualization plots and a tab to navigate dataset.The BoxPlot tab shows you the box plot of number of outlets with  greater than 15 ppb water lead in each school grouped by counties. As you can see there are few outliers whichshows there is one school in Westchester County with 290 water outlets one in Dutchess County with 255 outlets one in Suffolk County with 230 outlets and one in Oneida County with 205 outlets with more than 15 ppb water lead. It is unimaginable to have more than 200 water outlets with more than federal water lead limit in a school where young kids goes to study.Now if you look at ""BOCES vs PS"" tab this shows a group box plots grouped by organization type (shown below). These are box plots of percent of water outlets with more than 15 ppb water lead in each school. In public school group there are few schools with 100% of outlets with more than 15 ppb water lead. This is even more scarier situation.Even though above two box plots showed very terrifying information most schools have none or very few percent of outlets have more than 15 ppb water lead. The Histogram tab shows a histogram of percentage of water outlets with more than 15 ppb water lead. This plot shows that most schools have small percent of outlets with more than 15 ppb water lead.It has been known for a longtime that high level of lead in drinking water can cause many serious adverse effects on human health specially on young children. This app's visualization of the dataset of New York State public schools drinking water lead level testing we found out that there are few schools with more than 200 water outlets with more than 15 ppb water lead. Not only that there are few public schools with 100% of water outlets with more than 15 ppb lead. This is very serious problem. I am sure every school must have taken appropriate action after their water sample testing result but every parents need to be worried and be informed about what kind of steps are being taken to resolve this potentially serious health issue for every kids who goes to New York public schools.",NA
NYC Data Science Academy Hiring Partner Event June 28,30,https://nycdatascience.com/blog/community/nyc-data-science-academy-hiring-partner-event-june-28/,There are no fees for graduate recruitment. If you are interested in hiring data scientists from us there is more information at  or send us a message via .,NA,Data Science graduates and hiring partners came together on June 28th at the Grand Central WeWork location to the 10th NYC Data Science Academy hiring event.We are greatly honored that professionals and representatives from many companies such as JPMorgan Chase Citi Pfizer Publicis CKM Advisors National Grid Verizon Fareportal Inc Conde Nast Revmax Rocketmiles were able to attend.Two graduates (Chao and Samuel) have received offers from the event to work in the same data science team . Many others have been busy with interviews after connecting at the event.Data Scientists (Arda Berkay Kosar Lydia Kan) from previous cohorts also joined the event to speak to newly minted graduates regarding opportunities in the field,NA
Runtime vs. Success (Using IMDB),30,https://nycdatascience.com/blog/student-works/imdb-runtime-vs-success/,"Years ago students of film writing were advised to make their feature length movie scripts between 90 and 129 pages.  A page is expected to be one minute of runtime.  Today these numbers have been revised to a simple target of 110 minutes.  A little fun fact connected with this:  my professor tells the story of an insider’s view from development and how the decision makers at one of the giants used to pick up a screenplay weigh it in his hands and say “too light” or “too heavy."" Then the script was rejected without ever getting a single page read.  Are these page length numbers arbitrary or is there something to these guidelines?With the bin size set very small in the above visualizations we can see movie releases increasing over time with the rate accelerating much more starting in the 1970's. Increasing the bin size from 2 to 28 further illustrates how in recent years there are so many more movies then in the past. A quick look at the history may shed a little light on what's going on here.  The first motion pictures were created in the 1890’s and were little more than short black and white moving images.  As the novelty of this waned next came serial shorts from which the term “cliff hanger” originates.  When real fulllength stories started to evolve they were  expensive and hard to produce.  As a practical matter only small numbers could be made per year. Today the bar has been raised but technology keeps lowering the cost of what it takes to meet that bar.  Independent and ancillary markets have widened the distribution channels and even YouTube is becoming a part of this expanding network.  It’s just a matter of time before low budget movies get made on smart phones if they haven’t been already.  Nothing earth shattering here but the visualization does help show that the runaway escalation started during the time when “” “” and ""” all made their debut.  Many see this time as “the beginning of the blockbuster.”As shown here the data used in this application is organized into 5 subsets of data:There is some overlap in the data sets shown on the application's ""Data"" tab:Click on the ""Visualization"" tab of  to obtain basic stats on each of the aforementioned  datasets (Min Max Mean Median and Quartiles) for these 4 numeric variables:This tab also provides a line plot using  linear regression which we can analyze for the general trend in movie runtimes that we are looking for.  If we start with the plot for “all the data” in our application outliers are mostly clustered pretty close to the general confidence interval for the model.  No outliers outside this range appear after 1990 and only a small number of points barely outside the confidence interval appear from 1960 to 1990.Since there is a limited outlier effect the mean seems like a reasonable metric.  It is 109.2 minutes. Of more interest to the original question this plot essentially reflects a 5000+ record sample of what got made.  The hills and valleys of the line seem to range between 105 and 120 minutes up through the 1970’s.  Then the line becomes a slightly downward sloping trend up through the present with our data points mostly appearing at around the 110 minute mark. Though anecdotal in nature the original 110 minute recommendation for movie scripts would seem to be supported by the data.  The confidence interval around the line though might suggest a range from about 100 to 120 minutes.  If our movie gets made we may then be concerned with what are its chances of making it into the top or bottom? Starting with the Top 250 Movies:The mean for the Top 250 movies was 130 minutes.  The curve trends upwards over all with a range in recent years (1990 to the present) that fell between: 130 and 140 minutes.  There are a larger scattering of outliers on this plot but based on how the outliers mostly cluster not too far from the confidence interval after 1990 the mean still seems reasonable to use.  If we think about why these better received movies are often longer than the general trend for what gets made I’m sure there are many factors involved.  For one thing big name directors and  producers have the kind of clout to make us sit through longer movies.  Consider “” saga which collectively was over 9 hours long with each of 3 movies lasting around 3 hours a piece.We don’t want to end up in the bottom so we'll take a look at this too:The mean for the Bottom 100 movies was 92.26 minutes.  This curve is also showing a distinct upwards trend over all but with a range in recent years (1990 to the present) that was from about 90 minutes to 115 minutes.  There are fewer outliers but there is also less data in this grouping.With more time and development a more thorough analysis could be developed from IMDB data. (available on Git) were used to gather source data.  A full workflow from initial input files to what got used in the application is provided in the .  As this process was experimentally developed and modified while creating the source data for this project R markdown was used to step through the process and keep notes on what was going.  High level:The CEO of a company I used to work for would tell stories of how senior management of various IT areas under him would plan and design solutions on a napkin in a bar which formed the basis of software project requirements.  For this shiny app the “napkin” was actually a piece of notebook paper captured as a  for your amusement.  Only a small piece of this plan has been realized so far.",NA,The content in this blog comes from a shiny application proof of concept using IMDB movie data:Note: Updates to this code are in progress.  When posted the code demo (accessible by the hyperlinks) will have more to show than is explained in this blog post.Note that for this application about 6000  records were obtained. In 2017 alone IMDB recorded  new releases by July and the year was still young at that point int time!   Further analysis is indicated and performing the analysis with more data would also be helpful.  Given the small size of the data findings here should not be taken as an absolute.  Given that here is a summary of what the data suggests so far:,NA
A Hybrid Recommender with Yelp Challenge Data -- Part I,31,https://nycdatascience.com/blog/student-works/yelp-recommender-part-1/,"This is the first part of the Yelper_Helper capstone project blog post. Please find the .This is the capstone project sitting at the end of our 12 week journey in the bootcamp. For this project we would like to work on something that:Based on these criteria we decided on the ‘Yelper Helper’  a realtime restaurant recommender system using Yelp open source data. We believe the experience we gained from this project will be widely applicable.Included in the dataset were five json files which are encompassed users checkins tips reviews and businesses. Yelp also included 200000 photos with their associated labels though we did not incorporate any image analysis in our end product. The total size of the text data was roughly 5Gb. While this barely qualifies as 'big data' our desire to use big data tools and techniques is justified. The richness and variety of the data gave us the freedom to apply a wide range of machine learning techniques to create what we called “Yelper Helper.” The goal of Yelper Helper was to build a recommendation app for users based on keyword inputs location social networks and reviews. This will be explained in more detail in the following sections. Knowing the user’s location and other optional information (user ID keywords) our engine can recommend nearby restaurants and visualize them on a map. The engine is a  recommender. For new or anonymous users we would be able to provide basecase recommendations using only location information. With additional keywords like ""spicy"" or ""tacos"" an NLP (Natural Language Processing) module is turned on that can offer similarity based recommendations; finally with user ID as input the collaborative filtering and social network modules will provide more personalized results based on historical rating activities and friends’ opinions.Before building the app and training the model we wanted to investigate a subset of the data to explore. Among the cities that were available in the Yelp data was Las Vegas which our team thought would be a good training ground due to the number of restaurants visitors reviews and variety in the data. Once the relational database was set up we dove in and performed some exploratory data analysis on the reviews.To get a better understanding of how the words in a review related to the rating that a user gave we performed a highlevel sentiment analysis to identify which words were associated with positive and negative reviews. We took a sample of 1000 negative reviews and 1000 positive reviews from restaurants in Las Vegas and analyzed words or phrases that were used most frequently for those reviews.Words most often associated with bad reviews are on the top left. They include:     and . Words most frequently associated with good reviews are on the bottom right. Among them:     and .Additional exploratory data analysis could be performed such as restaurant trends unique attributes per location etc. Since the data provided by Yelp was already clean with minimal missingness we did not impute any data. Finally because we were able to convert the data from json to csv easily we loaded the data into MySQL database for easy storage and extraction.Our team believed that it would be wise to load the converted csv files into a MySQL database hosted by Amazon’s Relational Database Service. Having the data stored in a relational database would then allow us to extract and access the data with efficiency. MySQL provides many advantages for our recommendation app. First some of the data files contained millions of rows so we spun up the MySQL instance and loaded each dataset into separate tables. This made data extraction easy and fast depending on what we wanted to analyze. Initially five tables were created one for reviews businesses tips checkins and users. A visualization of schema can be seen here:From there we created subtables which only included the relevant Las Vegas restaurant data. An additional advantage MySQL provided was that it allowed us to easily establish a connection via sqlalchemy and MySQLdb to Spark and Python thereby making it unnecessary to create multiple intermediate/temporary csv files.First we decided to analyze the Las Vegas subset using Natural Language Processing (NLP) a machine learning technique that aims to understand human language with all its intricacies and nuances. The 800000 reviews were divided into low ratings of 1 and 2 stars and high ratings of 5 stars. Our goal was to create a supervised learning neural network that could predict the sentiment of a review as positive or negative based on the language used. Restaurant reviews with ratings of 3 or 4 stars were thrown out due to the lack of consistency between reviewers (i.e. one reviewer’s 3 star review could be another’s 5 star review).The low rated reviews were given a rank of zero and the high rated ones were given a rank of one. The text of each review was then tokenized and converted to a sequence using the keras package. A neural network was set up in keras as well using a convolutional filter pooling filter and both ReLU and sigmoid activation function to predict the sentiment of each review as either 0 or 1. The final model was 94% accurate and was spotchecked on newlywritten reviews and on the remaining 3 and 4 star reviews. The model did quite well for our purposes and so we fit all 800000 reviews using the sentiment analysis neural network and then averaged the reviews by restaurant and used the results as a new feature in the locationbased recommendation (See section 6.4). The overall process for the sentiment analysis is outlined below.We also wanted to use NLP to make contentbased recommendations for our app. Itemtoitem similarity is calculated to make these types of recommendations. Think of when you read an article online: there is usually a side menu or a section at the bottom of the page that recommends new articles based on your interest in the current article. Often these new articles are written on the same or a similar topic. That’s a contentbased recommendation.Contentbased recommenders can take a user’s profile and past ratings to make new recommendations to the user. This however presents the cold start problem an issue that arises for a brandnew user who has no rating history. To combat this our NLP recommendation is based on a keyword soft match (similarity calculation) rather than the user profile. So typing “tacos” into our app should bring up a ranking of Mexican joints with the possibility to also recommend some nonMexican places serving tacolike food.To make this happen we once again decided to implement a neural network to understand the language used in the reviews. We used the Word2Vec function from Spark MLlib to create the model. Our process is outlined in the figure below.To preprocess the data we concatenated the reviews for each business together for a total of 6199 restaurants. Then we tokenized the reviews and removed all stop words. Stop words are common words in the English languages that provide function within a sentence but no context such as ""of"" ""the"" and ""has.""Word2Vec then translates each word into a vector of 100 features. These vectors are located in a feature space of 100dimensions with similar words closer together and unrelated words farther apart. For example the vectors for “ice cream” and “frozen yogurt” should be pointing in nearly the same direction but the vectors for “delicious” and “disgusting” would be far apart.Once the words are translated into vectors in order to check if the result makes sense (and have some fun) we can perform some word algebra. We can add or subtract the vectors from one another to find a new word.The word vectors above have been flattened into a 2D feature space. Beef and filet mignon are both foods from a cow while seafood and lobster tail both come from the sea. Lobster tail and filet mignon are exquisite and expensive types of beef and seafood. If we have filet mignon take away the fact that it is beef and add in a new category of seafood we end up with lobster tail.In addition to using word algebra we can determine how similar two words or documents are based on cosine similarity. Mathematically cosine similarity is the dot product of the two vectors divided by the product of the magnitudes of those two vectors. This calculates the angle between two vectors. The smaller the angle the more similar the words the closer to 1 the value becomes. Larger angles are more dissimilar and are calculated closer to negative one. Vectors perpendicular to one another will have a cosine similarity of zero.In the figure above ""burger"" and ""sandwich"" point in somewhat similar directions and have a similarity of about 0.6. Below we can see the results of a similarity search for the word ""Chinese.""Since the business reviews are more than a single word and a user may want to search using multiple words as well Word2Vec averages the vectors of all the words together and then calculates the similarity between the user’s keywords and each of the available restaurants’ reviews. The results are then ranked from most similar to least and returned to the user on the map. Collaborative filtering (CF) is commonly used for recommender systems. These techniques aim to predict user interests by collecting preferences or taste information from many users. In other words CF fills in the missing entries of a useritem association matrix. The underlying assumption is that if person A agrees with person B on one issue A is more likely to have B's opinion on another issue than that of a randomly chosen person.Below is a great visual example from Wikipedia. In order to predict the unknown rating marked with ‘?’ we rely more on the opinions from other users with  rating histories (green rows) thereby arriving at a negative rating prediction (thumb’s down).Mathematically this is done by lowrank matrix factorization combined with a minimization problem (see picture below). The oftensparse useritem rating matrix R is approximated as a product of user matrix U and item matrix PT which are built of latent factors. We then form the cost function J and try to minimize it. Currently in the  library the alternating least squares (ALS) algorithm has been implemented to learn these latent factors. Additionally since we directly rely on the user rating itself our approach is often referred as ""explicit.""For our project we pretrain the model and save it on our Amazon S3 server. When the recommendation engine boots up it will load the model from S3 and use it for prediction. This architecture is designed so that we can keep training multiple models offline as new data comes in. Once a new model is ready the recommender engine will make the switch by editing one line of code.This is a digital version of the classic WordofMouth recommender system  what people have been using for thousands of years.The Yelp dataset is unique in that there is an embedded social network. In the user json file each row describes one user in a dictionary format. For the “friends” key the value stored is a list of encrypted user IDs. In order to quickly convert the unstructured social network data into a structured ""node"" and ""edge"" set (often required by graph theory related packages) we employ the Spark distributed computing ecosystem. This allows us to finish the task within a minute compared to hours in a single machine Python environment.With the social network database now structured and easily searchable a few SQL joingroupbyaggregate commands can quickly answer questions like: “What are the average ratings of the nearby restaurants ?” This interesting feature has the potential to both provide conversationtriggering recommendation and improve user stickiness to the app. As an extension of the algorithm one can easily come up with other intuitive rating estimation schemes such as expanding the network to second and thirddegree connections and applying a weighted average.Users who come to our webpage may want a quick suggestion of all possible restaurants based on only their location. Unlike the methods mentioned so far this requires no additional information and returns recommendations fastest to improve user experience. While yelp provides aggregated ratings for each business these are not always indicative of a restaurant’s quality. For example a restaurant with one fivestar rating would be ranked ahead of a restaurant with ten ratings averaging 4.9 stars. Another problem is that a star rating varies from person to person and is integer based. Finally do we want to take into account reviews that could be irrelevant due to their date e.g. from over ten years ago?Our strategy for dealing with these problems is as follows:How old is too old for a review to matter? As this dataset spans 2004 to present we need to define a reasonable cutoff. Instead of a hard filter we weighted each review based on its age with a sigmoid function centered about 2012 so a new review receives a weight of 1.0 while a review from 2012 receives a weight of 0.5. Because the rate of review writing is steadily increasing approximately 60% of all reviews are unaffected by the time weighting. Our filter effectiveness is demonstrated in the figure shown below.Based on the NLP sentiment analysis described above each review was assigned a sentiment value. This is indicative of how positive a review is with a range of zero to one. In the final rating scatterplot below it is clear that sentiment and star rating given by the user are dependent variables.On the business level we need to address the popularity measures: review and checkin counts. A high review count is indicative of either a popular business or an exceptional one. Checkins are a more direct measure of popularity but it is a more recent feature so even the most popular businesses have counts around 160. With this in mind checkins are assigned approximated twice the weight of reviews.These distributions are extremely skewed as most businesses having a limited number of reviews and checkins. By taking a log transformation we can get a reasonable multiplicative factor that includes but doesn’t overstate popularity.With our final score metric fully defined we can remap stars to percentiles (in Las Vegas). In the chart below it is clear that our new stars need a different interpretation from what is usually assumed. In this system four stars is considered one of the best restaurants in the area and three stars is a good if not great restaurant. Though skewed perhaps this final score distribution is more realistic than a uniformly or normally distributed score. In reality there are only a handful of exceptional—and an enormous amount of average—restaurants.Regardless of the model used we don’t want to check every restaurant in our database if the user is requesting information about a specific area. The most accurate metric for this is haversine distance: the minimum distance between two points on the surface of a sphere. For reference the defining equations are:Putting all of these individual parts described in section 6 together our recommendation engine can react to whatever user input is passed in. In its fundamental locationonly mode we can return recommendations within half of a second. As keyword and userbased models require computation they require 8 and 20 seconds respectively.See blog post Yelper Helper is a userfriendly interface powered by a robust and varied recommender and supported by an efficient data pipeline making it a quick and easy way to find nearby restaurants the customer will love.Having recommendations available for all levels of interaction with the app provides quick suggestions for the casual digital passerby and yet promotes more consistent user engagement with the benefit of more personalized results. The locationbased recommendations aim to provide a quick and dirty service for passing users. The NLP neural networks of the contentbased recommendations allow users to filter restaurants by cuisine or dish. Collaborative filtering and social network recommendations provide individualized recommendations based on personalized taste and friends’ opinions.Building the machine learning models using Apache Spark and setting up a FlaskKafkaRDSDatabricks pipeline creates a powerful and scalable system robust to working with big data and a continuous stream of user requests.  With more time we would improve Yelper Helper with the following ideas.In a short twoweek period we learned a great deal about working effectively as a team by using an agile approach to divide up the labor and regularly meet up to discuss progress and problems. We gained experience in SQL queries Amazon Web Services PySpark programming language and Kafka streaming. We implemented several machine learning techniques and built an entire data pipeline to create a useful and professional product for the modern consumer. Guidance from Shu Yan Yvonne Lau Zeyu ZhangInspiration from Chuan Sun and Aiko Liu",NA, Nowadays every company and individual can use a recommender system  not just customers buying things on Amazon watching movies on Netflix or looking for food nearby on Yelp. In fact one fundamental driver of data science’s skyrocketing popularity is the overwhelming amount of information available for anyone trying to make a good decision.The source of our data was courtesy of the . The purpose of the challenge is to use the provided data to produce innovative and creative insights. Yelp recognizes the top submission with a $5000 prize winning.,NA
Kaggle competition (top 3% ):  Optimizing Russian housing price prediction by a deep dive into the model selection and feature engineering,31,https://nycdatascience.com/blog/student-works/kaggle-competition-russian-housing-price-prediction/,We divided features into 16 subgroups (i.e. demographics). We ran random forests and Lasso on each subgroup.Interpretations: In most groups LASSO would provide that all the features in the group are significant at the minimum MSE level for lambda parameter.In order to select features we have chosen parameters that would go to zero slowest as lambda increases.35 features came out to be ideal in this case with MSE of 0.31 or RMSE of 0.56 for training data set and 0.46 on Kaggle’s testing set score 0.35. We have also tried utilizing Multiple Linear Regression using select features that would make sense in making housing price predictionFor the simplest model we have used ‘full_sq’(size of the unit) ‘ttk_km’(distance to the Third Ring) and ‘public_transport_station_min_walk’(minutes to walk to public transportation station)Result was that this simple model gave a superior result to LASSO model with 35 features with RMSE of 0.499 on training set and Kaggle’s score of 0.37535Using 15 features we were able to lower RMSE a bit further to 0.466 on training set and Kaggle’s score of  0.35189Macro data may not be as helpful as it is time series data and if year/month are included as independent variable it would incorporate the time elementFeatures Selection: 11 main features + 28 selected features +macro features,NA,The goal of Kaggle Competition is to predict Moscow's housing prices provided by Sberbank by machine learning models and feature engineering.We were able to achieve a satisfactory Kaggle Score of 0.314 (RMSLE) by a deep dive into the machine learning model selection and feature engineering. We have achieved a top 3% out of 3274 teams in the final leading board.,NA
Tribots read and write,31,https://nycdatascience.com/blog/student-works/tribots-read-and-write/,Word2Vec is a method that would try to represent the document into a vector formGeneral idea is that for the vocabularies used in the document one would come up with a probability distribution of words that would be used in a nearby expression 2 Methods for doing this is continuousbagofwords and skipgramContinuousbagofwords tries to produce probability distribution of a word given a list of wordsSkipgram tries to provide probability distribution of words that show up in similar content given a vocabularyWe have also tried to apply authorship attribution analysis to the reportsIt seems that doc2vec analysis is not quite effective at distinguishing authors unless the writings are drastically differentOne methodology is to use key features that would give characteristics about the authors and use kmeans clusteringLexical Features:average number of words per sentencesentence length variation(standard deviation of words per sentence)Lexical diversity(number of unique words/words used in document)Syntactical:Frequencies for common Parts of Speech types(singular/plural noun proper noun determiner preposition/conjunctionadjective)Bag of Words:Count the most common words in the documents and apply clusteringIf the company’s stock price  has decreased by 50% we have considered them “bad”If the company’s stock price has increased by more than 100% we have considered them “good” (over 5 years)Neural Network has been around since 1940s but were not so useful until about 5 years agoWhereas regular Neural Networks does not have a sense of time Recurrent Neural Networks try to capture time element while working with neural networks by using previous output as another inputRecurrent Neural Networks by itself is not as effective so people have incorporated LSTM to forget or remember previous outputs10K reports were too large for us to train using RNN Following the example of writing a similar story in Aesop’s fable we have written summary using company profiles in CNBC“the bell outwit met . nobody will all mouse got up and said that is all very well  but he thought would meet the case . you will all agree  said attached chief'”(Aesop’s Fable LSTM RNN output)“other service offerings for play and retailers through the consumer and sale beverages. Hasbro The Investment segment focuses sales and facilitates that sectors. Wholesale in food countries in China its in fundamental storage equity and”(somewhat makes sense),NA,"10Ks annual financial corporate reports is the basis of intelligent investment. These reports often reveals key information about the company.While we made some progress in using natural language processing to investigate 10Ks here are some future research direction:
1. Further analysis is needed to examine the relationship between longterm company health with different sections of 10K (i.e. risks business overview)
2. Features generated by NLP alone might not have the necessary predicative power and therefore more quantitative measures such as valuation models should be included as well
3. 10Qs quarterly financial reports are filed after each quarter and might better reflect company’s current status.
The future of using NLP to understand financial reports are unbound. This similar analysis can be applied to other financial documents that might contain crucial information for intelligent investment. By combining NLP with recurrent neural network (RNN) and long and short term memory (LSTM) machine learning might be able to automatically compose a 10K!",NA
Cluster Analysis of Twitter: Understanding Human Interactions for Business Improvement,32,https://nycdatascience.com/blog/student-works/cluster-analysis-twitter/,"Most of the data readily available in the real world comes unlabeled. Getting the labels often entails manual classification which can be a tedious and expensive process. While dealing with unlabeled data we are limited to using unsupervised machine learning techniques since we do not have a definite response variable to work with. Unsupervised learning is generally used as an exploratory data analysis tool used to understand the data. I decided to work with unlabeled Twitter data in the  form of tweets to attempt  to understand  human thoughts and interactions. A business or organization can benefit a great deal from understanding the individuals in its  target audience and then leveraging this to connect with them on a more personal level. Thus having an automated classification system of such unlabeled tweets can serve as a valuable tool for business improvement.The classification of tweets is a multistep process starting from the preprocessing of unlabeled tweets before applying unsupervised machine learning algorithms for clustering and topic modeling. Once the desired level of accuracy in tweet grouping is achieved one can proceed to manually label them based on similarity in context. Now these labelled tweets can be fed into supervised machine learning models to train the model to understand the underlying similarities of tweets belonging to each category. A fully trained tweet classification model can then be applied to unseen data in an automated tweet labeling system. A visualization of the steps involved can be seen below.The steps outlined in red form the focus of this blog. Only once reproducibility is achieved in the clustering and topic modeling of tweets can one proceed to the supervised machine learning model.The data for this project were obtained directly using Twitter's Streaming API which allows access to Twitter's global stream of Tweet data. The API gives the user control over downloading by specifying the language as well as keywords being used in the tweets. For the purpose of the project I was interested in English tweets.  For some of my data I used the keyword search option to narrow the focus of the tweets being downloaded. The data conveniently downloads in JSON format which can easily be parsed into a Python dictionary.For analysis the data was then transformed from dictionary format into a Pandas dataframe. Preprocessing of the tweets was somewhat of a trivial process since they tend to be slightly different in structure from standard text documents. There tends to be more use of slang and shorthand to get the message across in limited 140 character working space. For that reason one must be careful in removing unnecessary elements from each tweet while retaining features which would contribute to the meaning and context of the tweet.I used regular expressions to remove the characters ""RT"" in any tweet because keeping it would lead to clustering retweets together which would be at odds with the purpose of my analysis. I also removed any user mentions from tweets which can be identified with the preceding ""@"" before the user's name. Similarly I removed any links that were present as well. Finally I used Python's NLTK package to remove stop words like   etc. Stop words are words that are grammatically essential to structure but contribute very little to the context of a sentence. Once the data was cleaned up it was now ready for machine learning.The unsupervised learning algorithms used for this analysis include Latent Dirichlet Allocation (LDA) and Nonnegative Matrix Factorization (NMF) for topic modeling and Kmeans for clustering of tweets. Conveniently all three are available in Python's scikitlearn package.Kmeans clustering algorithm essentially grouped individual tweets into only one of the specified number of clusters which could be problematic if a given tweet fell into more than one category. Despite this limitation Kmeans did surprisingly well in grouping most tweets based on similarities. However there would always be a ""catchall"" cluster which would contain any tweets which the algorithm was unable to properly classify. In this regard the topic modeling algorithms allowed the flexibility of assigning multiple topics to tweets by weight of how likely it is to be a part of each of the topics. The following sections of this blog will show how these algorithms performed with different sets of tweets.The first set of tweets I tested were obtained using different keyword searches related to health & fitness travel sports news/politics art music video games lifestyle dating/relationships wedding/marriage and other life changing events. This was a test to check if the unsupervised learning methods would group the tweets based on the keywords that were used to get them. A word cloud of the hashtags associated with this set of tweets can be seen below. we have the top words used to determine topic using LDA.  Topic 5 is clearly related to US politics as it includes words as trump comey and president. Topic 12 generalizes to entertainment by including the words music video sports and gameplay. Topic 8 seems to be a confused mixture between travel and politics that could partially be due to the mention of President Trump's travel ban.For this set of tweets NMF performed substantially better than LDA as can be seen with the topic assignments below. The top words for each topic relate very well to the keywords used in my search. Topics 1 3 5 7 9 and 13 clearly show relevance to jobs/hiring wedding art sports health & fitness and politics respectively. There were some unclear classifications with certain topics but overall NMF performed very well in this case.Clustering using the Kmeans algorithm also outperformed LDA in coherent grouping of tweets. With the exception of a few ""not so good"" clusters grouping based on sports politics music and health can be seen in clusters 5 8 11 and 13 respectively.The next set of tweets for cluster analysis were streamed with the only restriction of being in english. Given the absence of intentional underlying categories it was interesting to see how the algorithms would perform in grouping randomly obtained tweets. Below is a  word cloud of the hashtags of these tweets.For the openstreaming tweets LDA performed much better as compared to the previous set of tweets. Topic 2 is related to music entertainment and topic 3 is specific to tweets about the elections in the United Kingdom. Topic 28 seems to talk about jobs hiring and careers. Topics 4 and 7 are somewhat unclear in determining the context of the tweets.Once again NMF outperformed LDA in topic assignment as can be seen with the clear context of most of the topics determined using this algorithm. Topic 0 2 7 8 and 10 clearly relate respectively to entertainment UK elections career sports and giveaway advertisements. Topic 11 refers to the unfortunate tragedy of the Arianna Grande concert in Manchester UK and topic 29 speaks of events related to FBI director Comey's testimony regarding President Trump and Russia.Kmeans did a good job at clustering tweets by category as can be seen some of the clusters below. Cluster 5 grouped tweets mentioning different genres of music together in a musicrelated cluster. Cluster 14 and 25 group on the discussion of the UK and US politics respectively. The final set of tweets I obtained were from users following specific organizations and companies. The purpose of narrowing my focus to user tweets was to detect if multiple tweets from the same users can provide insights into topics of interest to the user. For this set I obtained about 100 tweets each from 2000 followers of organizations/companies such as Barclays Fitbit Tinder Democratic/Republican National Party and the Economist. For this blog I will discuss my analysis of tweets from followers of the Democrat or Republican Party.Using LDA topic modeling and visualizing some of the tweets associated with the assigned topics we can see the opposing views from followers of two competing entities. Followers of the Democratic Party seem to be more critical of President Trump in their tweets while followers of the Republican Party are more supportive of President Trump and more critical of Hillary Clinton and former President Obama.Future work on this project will involve cycling between cleaning the text of the tweets to obtain optimal topic modeling and clustering using the algorithms discussed in the blog. Afterwards supervised learning techniques can be used to train a model to classify new tweets based on preexisting or newly added categories.This scope of this project and its methodology can be extended to include image analysis and video analysis to include posts from other forms of social media such as Facebook Instagram and Snapchat. The more data that can be added to this model the better the model will serve as a valuable resource to organizations to understand their current and potential customers.",NA,BelowAs mentioned earlier cluster 9 was the catchall cluster containing all tweets which did not fall into any of the other clusters.The catchall cluster in this case was cluster 22 containing a good portion of the tweets which were unclassified.,NA
Scraping Mobafire for the best champion.,32,https://nycdatascience.com/blog/student-works/r-shiny/scraping-mobafire-com-for-best-champion/,: The map to the left shows an aerial view of Summoner's Rift while the map of the right is a simplified version of the same map. ,NA,"Esports have gained a lot of popularity in the last few years.  (LOL) is one of the highest online played multiplayer online battle arena (MOBA) games in the world. LOL has a community following where the best players (called summoners) compete in world championships across the globe. This has resulted in community based champion build sites where ranked summoners post ways to build a champion. Mobafire is one of the most popular and referred build site on the internet.LOL includes a number of maps but the most played one is . Summoner's Rift is a match between two teams blue team and red team that are set to destroy the opponent's building called the Nexus.Each team selects 5 out of 136 champions. Every champion has a type that can be played in one of 3 lanes or the jungle.The data for each champ was scrapped from an LOL champion building site called . Mobafire is a community driven site where summoners have tested the best way to build their champions. The best champion builds are usually upvoted by the community. Mobafire has collected stats about each champ that include:The scraped data includes:The stats for each champion were not in the form of an HTML table; rather they were embedded in HTML attributes.Scraping this data was easier and the  code However the Moba stats were embedded in CSS attributes. As seen in the picture below each slice of the circle represents one of the five properties of a champion.Luckilyhas a CSS scraper.
What is winning a match dependent on? Is it the champion's popularity or versatility?The pie chart on the left shows the distribution of champions that only have a primary role. The one of the right shows champions with additional secondary roles.  The stacked bar plot shows which position champions have the most versatile roles.Based on Fig. 3 we can see that champions that have the highest versatility are top lane champions followed by mid lane champions. The boxplots show the popularity (pickrate1 and pickrate2) for a champion's first and second role (pos1 and pos2).From pick rates we can infer that champions with higher pick rates are popular among users. Fig. 4 shows that most LOL champions are selected mostly for their primary roles (left); secondary roles were not that popular (right).In determining whether games were won based on champion popularity or versatility heat maps were generated to see if the popular champions regularly won the match for both positions although in Fig. 4 there might be subtle hint that versatility does not contribute to winning a match. The heat map show the popularity of champions and their win rates for their primary and secondary roles.Fig 5 shows a clear indication that popularity or versatility of a champion has little to do with winning a game. The highest win rate recorded in both positions is 57%. In spite of 57% being the highest win rate the popularity for that champion was 80% and 13% for its primary and secondary role.In an ideal situation a highly popular champion (90% and above ) would have a very high win rate (80% and above) for both positions. We can clearly see that the reality is not even close. These analyses show that picking a popular or versatile champion does not lead to winning the game. There are other variables such as team build up and individual champion stats that contribute to winning.LOL expects all champions to keep on leveling up until they reach level 18. It is also expected that in the process of reaching level 18 a user would have the necessary items to make their champions stronger than their enemy team. According to Mobafire there are five categories that can help scale the champion up. Based on these factors the faster a user can increase the damage output the more winning chances the team has.If the 3 point scale of Mobafire are formulated such that damage is the most important determinant we can derive the following formula:
The formula was utilized for all champions and can be seen in the  The app can be used to potentially determine which champion would win in a 1 vs 1 fight. The results were fairly accurate and were extended to team match ups.",NA
U.S. Residential Energy Use: Machine Learning on the RECS Dataset,32,https://nycdatascience.com/blog/student-works/capstone/u-s-residential-energy-use-machine-learning-recs-dataset/,The residential sector accounts for up to  representing a large opportunity for energy efficiency and conservation. A strong understanding of the main electricity enduses in residences can allow homeowners to make more informed decisions to lower their energy bills help utilities maximize efficiency/incentive programs and allow governments or NGOs to better forecast energy demand and address climate concerns.The Residential Energy Consumption Survey  collects energyrelated data on a nationally representative sample of U.S. homes. First conducted in 1978 and administered every 45 years by the  it is a leading data source for residential energy analysis. Methodology for the study is welldocumented  a comprehensive overview of RECS can be found in the program’s .This project applied machine learning methods to the most recently available RECS dataset published in 2009. The primary goals were twofold as is common in many regression exercises:RECS 2009 consists of 12083 observations (each representing a unique housing unit) and over 900 features encompassing physical building characteristics appliances occupant behavior/usage patterns occupant demographics etc. These features serve as independent variables in predicting the outcome variable annual electricity usage in kilowatthours (kWh). Because RECS aims to cover a comprehensive overview of many types of residences for a variety of analytical purposes (beyond energy use prediction) many of the features are sparsely populated collinear or uncorrelated with kWh usage. Therefore a preliminary and recurring task throughout the project was dimension reduction.Since most of the independent variables were collected through occupant interviews  as opposed to exact physical examination of the residence  the values of many are binned as factor/categorical variables. The dataset’s 931 original variables had the following characteristics: Missingness was common in the raw dataset with 73 features having NA values in more than 95% of observations. To quickly gauge whether these features correlated with the outcome variable kWh (and therefore should be retained/imputed) I made use of flexible EDA graphing functions from the  R package. An example is shown below on continuous variables revealing generally low rstatistics in correlation to kWh and having nonsignificant pvalues (not shown).  also accommodates similar exploratory analysis on factor variables via pairwise box plots. Overall although a more complex imputation strategy could have been employed to address these 73 features they were dropped due to their high missingness and little evidence of predictive power over the outcome variable. As previously mentioned the majority of the dataset features were factor variables many of which needed recoding in order to correctly capture the variation being described. As illustrated below nominal factor variables such as WALLTYPE have no intrinsic ordering; it would not make sense to order “stucco” higher than “wood” for example. On the other hand ordinal factors such as AGERFRI1 (age of mostused refrigerator) have a clear order to their factor levels; each level denotes a numerically higher value than the previous. Information contained in variables like AGERFRI1 is often more appropriately formatted in integer/numeric form which can better convey their continuous range of values aid interpretation of the variable coefficient and reduce standard error by decreasing the number of coefficients for model estimation (many algorithms generate a separate model coefficient for each factor level via onehotencoding).In the image above WALLTYPE was left as a factor while AGERFRI1 was recoded as an integer value (using the mid range of each bin as number of years). This factortointeger recoding was applied to additional binned variables relating to age of appliances and frequency of use of those appliances. New factor variables were also created by consolidating information from existing factors into onehotencoded binary values such as presence of heated pool use of electricity for space heating etc.We can visualize multicollinearity among the integer and numeric features using correlation plots again made with R package . In the plot below a conceptual group of collinear variables stands out i.e. those directly or indirectly representing a residence’s size for example total square footage (TOTSQFT) cooling square footage (TOTCSQFT) number of bathrooms (NCOMBATH) number of refrigerators (NUMFRIG) etc. To confirm the degree of multicollinearity variance inflation factors were calculated based on a linear model with the above numeric features  those with high VIF scores (loosely following the rule of thumb VIF > 5.0) were dropped.Despite variable recoding and decorrelation using the methods previously described a large number of features remained in the dataset the majority of them being factor variables. Although principal component analysis (PCA) is a powerful method for dimension reduction it does not generalize easily to categorical data. Therefore feature reduction was continued instead through the use of an exploratory LASSO regression model utilizing the L1 regularization penalty to drive nonsignificant variable coefficients in the model’s output to 0. This was helpful in identifying and dropping features with very little predictive power over kWh usage particularly factor variables that were not covered in the multicollinearity exercise above.The processed and cleaned dataset was migrated to opensource online machine learning platform  which offers highly efficient and scalable modeling methods. In contrast to machine learning conducted in slower native R packages ( ) in the local R environment R package  facilitates API calls to h2o’s online platform sending the given dataset to be distributed and parallelprocessed among multiple clusters. H2o offers an array of the most common machine learning algorithms (glm kNN random forests gbm deep learning) at very impressive run times  lessening the large burden of computational speed in the model fitting and tuning process. It also provides handy builtin functions for preprocessing such as training/validation/test set splitting in this case chosen to be 70/20/10.To best understand variable importance and interpret linear effects of the features on kWh usage a simple multivariate regression model was fit using h2o’s  function with default parameters (no regularization). A variable importance plot (see below) ranks the top 15 features by zstatistic all of which are quite large  an observed zscore of 15 translates to a pvalue of 9.63e54 or virtually zero. Hence all 15 variables show extremely significant statistical evidence of a relationship to kWh with an additional ~100 of the features (not shown) also meeting significance at the p  0.05 threshold. Variables beginning with “hasElec” were feature engineered (previously described) validating the approach of using domain knowledge to add or recode valuable information in new features. Coefficient estimates are denoted on the bar for each feature. We can observe that the presence of electric space heating at a residence (onehotencoded as a factor variable) increases yearly electricity usage at the residence by about 2722 kWh; each additional TOTCSQFT (air conditioned square feet numeric) adds 1.146 kWh/year.While default h2o  provided interpretability and pvalues adding regularization to the linear model enabled an increase in predictive accuracy. H2o allows flexible grid search methods to tune the main  hyperparameters alpha (type of regularization) and lambda (degree of coefficient shrinkage). Because grid search can quickly become computationally expensive I utilized h2o’s powerful  and  functionality to ensure that computation time is not wasted for very small marginal gains in validation accuracy.A primary purpose of grid search is to choose optimal tuning parameters according to a validation metric   in this case rootmeansquarederror (RMSE) of kWh prediction on the validation set  and then train a new model using the optimal values discovered on the full dataset (no train/validation split) to maximize sample size. In 5 minutes of training h2o fit all ~440 model combinations specified in the grid search with the best RMSE model having alpha  1.0 (LASSO regression) and lambda  1.0 (small amount of regularization). These parameters were then used to fit a final model on 90% of the data (combining 70% train and 20% validation sets) and performance was evaluated on the 10% holdout test data which had not yet been seen by the model.To increase predictive accuracy over generalized linear modelbased approaches I used h2o’s implementation of gradient boosted machines. Boosting a nonlinear treebased approach sequentially fits many decorrelated decision trees to slowly reduce overall predictive error (in the regression setting often RMSE). Grid search was performed using the same methods as above and with the following tuning parameters:Following the same process as with GLM parameter values were then chosen from the trained model with the best validation set RMSE. The model was retrained on the full 90% of training data and tested on the final 10% holdout split. Results from the two modeling strategies are summarized in the table below.Final predictive accuracy was similar for both linearbased (GLM) and treebased (GBM) models on the RECS dataset. The two models’ RMSE values represent a large improvement over an RMSE of 7560 which was established as a baseline accuracy by initially fitting a default GLM on the raw dataset (before any feature engineering or reduction). This improvement validates the approaches used  using domain knowledge to feature engineer highly significant variables and grid search for hyperparameter tuning.Aside from the regression setting future analysis could be applied to RECS in a classification context. Since the majority of the dataset’s features are factor variables energy providers could use the rich training set to attempt to answer important energyrelated classification questions about their customers  type of space heating presence of major appliances or whether the home is a good candidate for solar installation. Greater predictive ability over residential electricity use will contribute over time to a smarter cleaner and more efficient power grid.,NA, Additionally predicted vs actual kWh values for either modeling strategy are plotted against the most significant numeric feature CDD30YR (30year average of cooling degreedays in the residence’s region). Both model types demonstrate reasonable ability to predict kWh over a range of annual cooling loads/climate regions.,NA
Predicting the Baseball Hall of Fame,33,https://nycdatascience.com/blog/student-works/predicting-baseball-hall-fame/,The Great Bambino. The Big Unit. Joltin' Joe. Henry Rowengartner. If you're familiar with the sport of baseball you might recognize some of these names from real life or the movies. Since baseball has been engrained in the fabric of America for almost 200 years and since it is my favorite sport I decided that I thought it might be fun to take a look back at some of the best players to ever play the game to see how modern day players stack up against them.Sports analytics have progressed dramatically in recent years. With the wealth of data available for Major League Baseball many teams are employing analytics departments to extract value from their statistics. I decided to scrape the hall of fame players on  to investigate these statistics and determine how good a player has to be in order to be inducted into the hall of fame. Additionally I took a sample of data from players that have played since 1989 in order to predict whether or not they might be eligible to make the hall of fame.The parent URL I used to extract Hall of Fame player statistics was on  a baseball database that has all the baseball statistics one could ever want. I had to write two separate spiders to take into account the different statistics used to measure a batter's statistical output and a pitcher's statistical output. All in all there are 163 batters in the Baseball Hall of Fame which translates to a file of roughly 3500 rows (including all their seasons played). There are 77 pitchers in the hall of fame which translates to a file of about 1600 rows (includes all their seasons played). The data for more recent players was downloaded and filtered to include only batters that had over 500 plate appearances per year and pitchers who pitched over 150 innings per year in order to normalize numbers.First I took a look at batters. I wanted to get a sense of the distribution of number home runs are hit per season for hall of fame batters as well as number of hits per season. The histograms look like this:Hitting a high number of home runs don't appear to be a huge indication of making it to the hall of fame although it probably doesn't hurt. We do see that on average there are roughly 160 games hit per year most frequently which translates to about one hit per game. A batter who bats .300 for a season or fails to get a hit 70% of their at bats is considered a great hitter. I also plotted the total number of strikeouts against number of walks and noticed that many hall of fame hitters had tremendous plate discipline indicated by walks being greater than strikeouts.Finally I plotted the batter based on OPS or On Base Percentage plus Slugging Percentage a general statistic to measure the overall value of a player. Suffice to say I was not surprised to see Babe Ruth at the top of that list.For pitchers I mostly wanted to see their performance in terms of outcomes they could control which are home runs allowed strikeouts and walks. I plotted the strikeouts per nine innings against the walks per nine innings. We see an interesting linear trend in which the higher the strikeout numbers the higher the walks. This could say something about the pitcher’s ability to spin and curve the baseball in order to deceive hitters which might lead to a lot of strikeouts but also less control.In order to measure overall value of a pitcher I measured their FIP or fielding independent pitching. This takes into account number of home runs allowed number of strikeouts number of walks and a constant number to normalize the statistic. The lower the FIP the better.According to FIP Ed Walsh is the best pitchers in the hall of fame followed by the more contemporary Pedro Martinez.Since I had the hall of fame statistics I figure that I could use them as a baseline and try to fit a logistic regression model that would take data for more recent players and predict whether or not they would be included among the players immortalized there. I combined my hall of fame data with the separate subset of more recent players and then used crossvalidation to train the model. Then I predicted and tested it on the test set.For predicting players I used backwards selection and tested the correlation between each variable to make sure there was as little multicollinearity as possible. I then created a binary variable to categorized players that are in the hall of fame versus those who aren't. For batters my best model had hits per season as the most significant predictor (the higher the better) followed by overall strikeout rate (the lower the better). The accuracy of this model was 97% which beat the baseline of 83% by a wide margin. The accuracy was very high and variance was very low though I definitely could have used more data to obtain a better model.For pitchers my best predictor was walks and hits per inning (the fewer the better) and home runs per 9 innings (the fewer the better). FIP also was included in the model though it might have not been necessary as it is essentially a total measure of home runs walks and strikeouts. The accuracy of this model was 95% which beat the baseline model of 90%. Again this was highly biased due to lack of data and crossvalidating my model.,NA, There are a few things worth noting for the model:With more statistics we should be able to create a model that will knock it out of the park (I'm sorry).,NA
Investigating NBA Shot Data,34,https://nycdatascience.com/blog/student-works/investigating-nba-shot-data/,naïvely expect that accuracy would be a strictly decreasing function of distance reasoning that farther shots are strictly less likely to be made than closer shots (all else equal).   So why isn't accuracy significantly decreasing as shooters get farther from the hoop between the interval of 10 to 20 feet?  Certainly many factors could be responsible but to list a few possibilities:,NA,"The goal of this project was to investigate NBA shot data to better understand what variables affect shot accuracy and shot selection.    The data set contained approximately 125000 observations with each observation corresponding to a shot that was taken during a game from the 20142015 NBA regular season.  For each observation approximately 20 different variables were tracked including shooter's distance to the basket distance from the shooter to the nearest defender and much more.  The data was recorded via technology originally scraped from  and posted to .The first feature of the data set that I investigated was how shot accuracy varied with shot distance (1dimensional).  After plotting mean shot accuracy versus shot distance (given to the tenth of a foot in each observation) we notice an interesting feature in the graph:The notable feature in the above graph is indicated by the yellow line:  While accuracy among NBA players tends to generally decrease with shot distance between approximately 10 to 20 feet  accuracy is roughly constant.   One might Of the above hypotheses the second is the easiest to test via the data set.   We can do this by plotting the graph of how distance from the shooter to the nearest defender varies with shot distance:  It does in fact appear that within this 1020 foot range that average closest defender distance increases as shot distance increases.  Perhaps this can at least partially explain why shooting accuracy remains roughly constant within this range (closer shots are easier but more contested longer shots are more difficult but less contested  and within the 1020 foot range these effects essentially balance out with respect to accuracy).  In order to test this hypothesis more directly let's look at how shooting accuracy varies with distance while keeping the distance from the shooter to the nearest defender (""Closest Defender Distance"") constant.  As the latter quantity was also given in increments of a tenth of a foot it was appropriate to aggregate the data via 1 foot intervals in order to address sparseness.We can notice that along most rows (where Closest Defender Distance is ~ constant) we see a more consistent decrease in accuracy within the 1020 foot range suggesting that indeed defender distance was at least partially responsible for the trend observed in the first graph.  Of course to make more definitive conclusions we'd need to account for other relevant variables (shooter/defender height for example) and perform appropriate statistical analyses.The next part of the data that I investigated was shooter height and how it affected shot selection and shot accuracy.  We can first look at a basic plot of mean accuracy versus shooter height to see:At first glance one might be tempted to conclude that taller players are better shooters as their overall accuracy tends to be greater on average.  However we can see from juxtaposing the following plots that this is likely due to a bias in shot selection:The last part of the data that I investigated was shot selection.  Naturally one would expect that the shots most frequently taken are likely to be the most optimal (from an expected value perspective).  Already having plotted accuracy versus shot distance it's simple to calculate and plot the expected value (of a shot) as a function of shot distance:Interestingly enough we can note that the largest EV shots take place at approximately 2.5 feet and 23 feet (3 pointers) from the basket which are roughly the two distances from which the majority of shots are taken as seen in the frequency graph on the right.In the future I'd like to potentially scrape the  website myself to obtain twodimensional data for shot distance and see how shot selection/accuracy is distributed according to actual location on the court (as opposed to simply distance from the basket).  I'd also like to incorporate shooter/defender height disparity into the analysis of shooting accuracy as well.",NA
NBA Teams: How Do Eventual Conference Finalists Perform in the Regular Season?,34,https://nycdatascience.com/blog/student-works/nba-teams-eventual-conference-finalists-perform-regular-season/,As much as Kobe Bryant was one of the greatest basketball players to have graced the game it would probably be easier (even for Kobe) to hit an open shot like the one in the first clip. Statistics could very well miss this.,NA, As the 2017 NBA Playoffs are rolling along a thought popped up in my head: do the conference finalists each year have better stats than other teams in the With this question in mind I searched around and ended up using a dataset of NBA teams' regular season logs from a personal blog named . The dataset came in CSV format with around 12000 observations and 31 columns involving mostly basic stats such as points and assist per game for each team since 2012. While cleaning the data of unwanted observations I noticed two oddities that occurred multiple times: duplicates and outrageous scores. The latter reported an NBA team scoring  in the entire 48 minutes allotted in a game so I decided to set an arbitrary cutoff of 61 points (the lowest score I could find on NBA.com in the time period) for the data set. I also decided that since I wanted to compare the conference finalists to their regular season performance I took out the current season's observations as well in order to have the appropriate context.I split my observations into three categories as structured below:The results are interesting as it seems that conference finalists since 2012 have on average resorted to slightly less three point attempts in the regular season. With teams like the 2014 Spurs 2015/16 Warriors and Cavaliers you would think that 3point shots would be at a premium. On the other hand pointsrelated statistics seem to be about as expected; you do need more points to win basketball games after all.There are also stats that had very similar distributions. My initial guess is that these would be the ones with bigger differences as they would show the quality of a good team beyond just the game results. And then it hit me and I told myself: basketball isn't just about numbers silly.With the multitude of things going on in every single possession in basketball it would be reasonable to say that there would be hardtocapture elements with simple statistics and ratios. Here are some ideas that could be lost in the statistics:So while statistics are useful in a vacuum I think it is equally important to have good background knowledge of what you are trying to observe as you wouldn't be able to separate nonsensical statistics from ones which do tell a proper story.,NA
Otto Product Classification: Kaggle Case Study,34,https://nycdatascience.com/blog/student-works/kaggle-otto-classification/,"The 4th NYCDSA class project requires students to work as a team and finish a Kaggle competition. The 2017 online bootcamp spring cohort teamed up and picked the .  It was one of the most popular challenges with more than 3500 participating teams before it ended a couple of years ago. Although high leaderboard score was desirable our primary focus was to take a handson learning approach to a wide variety of machine learning algorithms and gain practice using them to solve realworld problems.This blog post presents several different approaches and analyzes the pros and cons of each with their respective outcomes.The challenge boiled down to a supervised multinomial classification exercise. The training set provided by Otto Group consisted of about 62000 observations (individual products). Each had 93 numeric features and a labeled categorical outcome class (product lines). In total there were nine possible product lines. The goal was to accurately make class predictions on roughly 144000 unlabeled products based on 93 features. Since the features and the classes were labeled simply as feat_1 feat_2 class_1 class_2 etc. we couldn’t use any domain knowledge or interpret anything from the real world. We therefore sought a modeling approach centered around predictive accuracy choosing models that tended to be more complex and less interpretable.Without much realworld interpretability of any of the features an initial exploration of the dataset was essential.An inspection of the response variable revealed an imbalance in class membership. Class_2 was the most frequentlyobserved product class and Class_1 was the least frequentlyobserved. This gave us a rough idea that the data was biased toward certain classes and would require some method of sampling when we fit it to the models down the road.Next we took a look at the feature variables. A correlation plot identified the highly correlated pairs among the 93 features. The red tiles below show the intensity of positive correlations and the blue ones show the intensity of negative correlations. Using information gained from the plot we could eliminate or combine two features with high correlations.All 93 features were comprised of numeric values so we also looked at their value distribution related to the predicted outcome classes. As the plot below shows some of the features have a limited number of values and can be treated as categorical values when doing feature engineering. It might also be worth standardizing the value ranges for all features if we were to use lasso regression for feature selection.Principal component analysis and resulting scree plot revealed a ""cutoff point"" of around 68 components. This threshold indicates that in attempting to capture the collective variability among all feature variables a significant portion of the variability can be explained with only 68 principal components rather than the original 93 features. Although we opted to keep all features during the project principal components after the 68th (those not contributing much to cumulative variance) could be dropped from the model as a means of dimension reduction.Before the model fitting process it was necessary to understand the Kaggle scoring metric for this contest which would have bearing on the modeling approaches chosen.Kaggle uses multiclass logarithmic loss to evaluate classification accuracy. The use of logloss has the effect of heavily penalizing test observations where a low probability is estimated for the correct class. It also necessitates that the submission be a probability matrix with each row containing the probability of the given product being in each of the nine classes. Given this required format we attempted to develop methods to combine individual model predictions to a single submission probability matrix.We approached this multinomial classification problem from two major angles regression models and treebased models. Each of the team members tried different model types; several methods of ensembling were then attempted to combine individual model outputs into the final contest predictions. In order to conduct our own test before submitting to Kaggle we partitioned the 62000 rows of training data into a training set of 70 percent and a test set of the remaining 30 percent. Through the use of the set.seed() function/parameter in many R functions we made sure that all models were reproducible i.e. built and tested with the exact same training and testing sets and therefore could be accurately crosscompared for performance. While the kNearest Neighbors (kNN) algorithm could be effective for some classification problems its limitations made it poorly suited to the Otto dataset.One obvious limitation is inherent in the kNN implementation of several R packages. Kaggle required the submission file to be a probability matrix of all nine classes for the given observations. The R packages – we used  here – only returned the predicted probability for what it predicted to be the correct class not for the other classes. The inability to return predicted probabilities for each class made the model a less useful candidate in this competition.In an attempt to work around the issue we developed a process to synthesize the probabilities for all classes. We created kNN models using different values of K and combined the predicted probabilities from these models. The attempt didn’t result in accurate results though. The resulting Kaggle logloss score wasn’t at all competitive. The lack of true multiclass probabilities is almost certainly the cause of the poor performance of the kNN models. With only a predicted probability of one of nine classes for each observation there was an insufficient basis to predict probabilities well for the other eight classes. Having inadequate probability predictions for the remaining classes resulted in an uncompetitive model. Generating kNN models was also time consuming. The time required to compute distances between each observation in the test dataset and the training dataset for all 93 features was significant and limited the opportunity to use grid search to select an optimal value of K and an ideal distance measure.Given more time it might be better to use kNN in the process of feature engineering to create meta features for this competition. Instead of using kNN directly as a prediction method it would be more appropriate to use its output as another feature that  or another more competitive model could use as an input. Choosing different values of K or different distance metrics could produce multiple meta features that other models could use.Regression methods could be used to solve classification problems as long as the response variables could be grouped into proper buckets. For this problem we wanted to see if logistic regression would be a valid approach.Procedurally we broke the problem down into nine binomial regression problems. For each binomial regression problem we predicted whether the product would fall into one class and used stepwise feature selection (AIC used here) to improve the strength of the models. We then aggregated the probabilities of the nine classes weighted by the deviance of these nine models into one single final probability matrix.  Using the base R lm() function we found this approach to be extremely time consuming. Running one binomial regression model with stepwise feature selection could take up to an hour for the training set. The multi logloss score was slightly better than kNN but still not competitive enough. The weights assigned to the nine models seemed to have a significant influence on the accuracy of the model. We might be able to combine boosting and resampling to get better scores but the limited computational performance of the base lm() function prompted us to look for a faster and more capable alternative.Since highperformance machine learning platform can be conveniently accessed via an R package h2o’s machine learning methods were used for the next three models. H2o proved to be a powerful tool in reducing training time and addressing computational challenges on the large Otto training set as compared to native R packages.The  function mimics the generalized linear model capability of base R with enhancement for grid searching and hyperparameter tuning. This model was trained on the 70percent training set with a specification of “multinomial” for error distribution. Although grid search was performed over a range of alpha (penalization type between L1 and L2 norm) and lambda (amount of coefficient shrinkage) predictive accuracy was not improved while computation time increased. Ultimately no ridge or lasso penalization was implemented. The overall GLM strategy produced average logloss performance on the 30percent test set.H2o provides functions for both of these treebased methods. Despite sharing many of the same tuning parameters and using similar sampling methods random forest modeling on the Otto dataset – even at a small number of 50 trees – was computationally slow and provided only average predictive accuracy. The gradient boosted trees model in which decision trees were created sequentially to reduce the residual errors from the previous trees performed quite well and at a reasonable speed. This model was implemented with ntrees  100 and the default learn rate of 0.1.Developing a neural network model using the h2o package provided fast results with moderate accuracy but it did not match the most effective methods employed such as extreme boosting.The h2o package’s  function was used to construct a neural network model. The ability to compute logloss values and return predicted probabilities by class made the package suitable to provide results that could be readily submitted to Kaggle or combined with the results of other models.The function offers many parameters including the number of hidden neurons the number of layers in which neurons are configured and a choice of activation functions. Two layers of 230 hidden neurons yielded the lowest logloss value of the configurations. The activation function selected was the tanh with dropout function in order to avoid overfitting.While the neural networks model could be built in minutes it scored average accuracy with logloss values in the 0.68 range. It is not clear that further tuning of the model parameters would yield significant reduction in the logloss value.Combining high predictive accuracy gradient boosting without added computational efficiency the  package provided a quick and accurate method for this project ultimately providing the best logloss value of all models attempted. Cross validation was performed to identify appropriate tree depth and avoid overfitting. Then an  model was trained and applied the test set to score the logloss value.Numerous parameters had to be tuned to achieve better predictive accuracy. Due to time limitations we only tested the following parameters:The multilogloss value for the 30percent test set was 0.51 – the best from all of the models discussed above. When we used it on the real test data for Kaggle submission we got a score of 0.47.Although each model was submitted to Kaggle to test individual performance we also aimed to combine models for improved predictive accuracy. Two methods averaging and stacking were used for ensembling.Model averaging is a strategy often employed to diversify or generalize model prediction. Many models are fit on a given training set and their predictions are averaged (in the classification context a majority vote is taken)  diluting the effect of any single overfit model's prediction on test set accuracy. This method was used to combine test set predictions from our six individual models but did not improve overall accuracy even when attributing larger voting weights to stronger predictors like Stacking was used as a method in building the  and neural network models. We were interested to attempt stacking as the method was employed by the top teams on this Kaggle competition's leaderboard. Stacking involves fitting initial ""tier 1"" models and using the resulting predictions as metafeatures in training subsequent models.For this project we used the predictions from an  and neural network model as metafeatures for a secondtier  model. The multilogloss value obtained from our 30percent test set was 0.56 a worse test accuracy than the model alone.To conclude the best multilogloss value achieved from our experiments was at 0.47 using the  model alone. This put us around the 1100th position on the competition leaderboard as of the end of April 2017.We note the following key takeaways from this classification project perhaps also applicable to other competitive Kaggle contests:",NA,Otto Group is one of the world’s biggest ecommerce companies. It sponsored the competition seeking a way to more accurately group their products into product lines for further business analysis and decisionmaking. The challenge was to come up with a predictive model to best classify products into their respective categories. Generally speaking ensembling is an advanced strategy used in Kaggle contests often for the sake of marginal gains in predictive accuracy. The small number of models and low complexity involved in both our ensemble strategies is a likely reason for limited success in this area (the winning team utilized 30 models stacked across a 3layer architecture). ,NA
Where to Live? An Interactive Geospatial Data Digestion Framework Implemented in R with Shiny,34,https://nycdatascience.com/blog/student-works/interactive-geospatial-data-digestion-framework-implemented-r-shiny-us-county-example/,"Where are the best places to live? How do you answer this question?If you turn to Google there are many ""top 10"" lists generated by someone else who does  know your personal needs. If you have retired you might not care much about salary; if you do not have kids education costs might mean nothing to you; if you have strong political views you might not want them to clash with your neighbors'; if you have serious lung issues air quality might be more important than anything else. We don't all share the same concerns and one size does not fit all.The question is: how do we go about finding the answer that does fit individual need?How about choosing among a variety of available data assigning your priority and ranking the candidates based on your specific input?Here is a prototype of the described solution using R and Shiny. I invite you to play with this  and make your own judgement.",NA," An interactive spatial data digestion framework has been implemented in R with Shiny to help answer questions set up as: ""where is the best place to ...?"" The web app ranks US counties based on user input and visualizes the results using Leaflet map along with other quantitative plots facilitating the ""where's best"" decision making process. Assuming a matrixlike data structure the computational core of the framework combines a rowandcolumn filtering system with a weightedaverage score generator. The delivered prototype product is hosted at  with its source code shared on . For people interested in the thought process behind the scene here is a short version of this blog which is the  given at the end of the 2 week project period. This blog post completes the planned documentation package with special effort devoted to the motivation background and discussion sections.My vision for this project is based on the following considerations:R is an open source programming language and environment known for its statistical analysis capability vectorized programming style and abundant plotting packages. Shiny is a fast growing extension to the R family providing a web app development and hosting environment without requiring html or java script knowledge. While browsing the R Shiny gallery I was immediately impressed by the featured map generation functionality utilizing Leaflet or googleVis.I come from a computational geoscience background. Geoscientists deeply appreciate the power of maps and love making them. Decisions are made on maps from everydaylife decisions (with a GPS in your hand) to billion dollar decisions (think about offshore drilling).One of the biggest personal decisions using maps is where to live. Is it possible to quantify the criteria and measure the tradeoffs? Before we dive into this detailed problem let's generalize the typical decision making process.When facing a large amount of input data and a variety of choices you start by reducing what goes into the decision process by filtering down the data (shortlist generation). This filtering step is often well described with clear quantitative rules (keyword count acceptable range of measurements and so on) but the next step is much more labor intensive and less clear. When a committee need to pick the top 1 from the 3 shortlisted candidates the process might involve a lot more than merely shortening the list from 300 to 3 with consistency and recordability as additional challenges.A few slides were developed to demonstrate one common decision making workflow which could be quantified and automated. The specific version shown is implemented in my project. It starts with a general columnandrow filtering system during the ""300 candidates to 3"" stage and ends after a weightedaverage score generation and ranking step during the ""shortlisted to winner"" stage. The weightedaverage linear system offers not only the benefit of simplifying but of scaling. The weightedaverage routine naturally handles data expansion as I keep adding new columns to the data set. With advantages however there are also some limitations that will be detailed in the discussion section.After the brain storm design stage the project moves into the detailed scoping and execution stage.For any project an ontime delivery is usually the top priority yet passionate developers always want to do more and keep adding sophisticated functionalities. A general good practice is to have checkpoints and fasttrack deliverables planned at the beginning. At any given point past the proofofconcept fasttrack effort make sure to always have a deliverable working version (and yes use version control like Git).Here is my road map:
At the halfway point I created the proofofconcept version. This happens to be a good place to talk about one key data preparation step for this project.The 3 data sets are carefully chosen for the minimum deliverable version as they demonstrate a few key challenges for the weightedaverage scoring system:To begin with all data sets here are numerical. Another level of complexity strikes when trying to include categorical data in this scoring and ranking framework. We will save this till the discussion section.Let's look at the first and third choices in the picture which are Air Quality and Income. The air quality data measures particle concentration in the air with numbers ranging from 4.44 to 15.96. Income data has numbers ranging from 22894 to 125900 (the units are not relevant here as we are blending unrelated quantities like air quality and income together; the result is a number that lacks physical meaning its sole purpose is for ranking). The issue here is that the air quality variation is not reflected well in the final score; without normalization its impact is several digits (or orders of magnitude) weaker compared to income. This is why we need to normalize each quantity to the same range. In this case I chose [0100]  before pushing them through the weightedaverage step.Again looking at air quality and income after they are both normalized to [0100] another issue is that the ""good"" directions are opposed to each other. For air quality a smaller number signifies a good direction. In contrast for income the bigger number is better. Is it possible to simply ""flip"" the numbers in the backstage without bothering the user? Yes but not completely the political data set is chosen to demonstrate another layer of complexity. The political data itself is the 2016 election voting difference for each county and the ""good"" direction is a completely subjective thing. In the final version population density is another column which needs user input to define positive direction since some prefer to live close to the crowd while others prefer more space.After sorting out these key issues I began to enrich the data by adding more variables that people might care about when making a ""where to live"" decision. The data sources are listed at the end. Without time constraints this effort can go on forever.Most of the effort past the midpoint is spent on automatically visualizing analytical insights in a user friendly manner building UI features like linking filtering sliders to tab 2 data table and special calculations based on user mouse click input. In the end the delivered product looks like this which is the hosted web app hopefully you just played with.While all other data sets are preloaded to memory in a way 'static'; this ""distance to"" calculation reads (lat long) from user's mouse click location on the map and does a 'dynamic' spherical distance calculation for each county toward this given point. This is to incorporate the ""I want to live close to / away from"" decision component into the weightedaverage score generation scheme. It has to be normalized too.As I offer more and more optional data sets to be included in the calculation it is nice to have a visual reminder of data considered and their assigned weights.The radar chart (or spider web plot) is designed to help understanding the strength and weakness for the top ranked locations. I decided to plot many measurements for the chosen locationthe user decides not to include certain input data by leaving some boxes in the control panel unchecked.I prefer to look at the full strength/weakness near the end when a small number of final candidates survived through the filtering and digestion funnel and we are very close to a final conclusion (winner). In case certain neglected aspects are so extreme that should trigger a second thought. In other words this is a ""what we've missed before I sign the contract"" visualization.
The 25% 50% 75% quantile are also plotted so there is a visual reminder of what the benchmark crowd might look like. When trying to choose top 510 from 3000+ counties we are really often looking for outliers. I think it would be helpful to have some info reminding the user what an ""average America"" looks like.The chartJSRadar is flexible enough that one can click on the name tags of plotted polygons to toggle them on and off. This is a very nice design by the author of the package as radar charts are very hard to read when too many polygons are plotted.The data table on tab 2 is the filtered result linked to all the sliders on tab 1. The newly calculated score after each ""Update Plots"" button click is added to the left side of the filtered static data table so user can easily sort using the fresh score and drop markers on the map by clicking on the rows of data. It is worth noting that interesting interactive features like this are being developed by R Studio in the ""crosstalk"" package. Eventually these features are going to be much easier to build compared to my solution in the source code.Many of us might ask this question especially after playing with the filtering sliders for a while  how are some of the input data correlated to each other? The tab 3 ""Correlation analysis"" is dedicated to answering this question.A few comments on the offdiagonal strong correlations:Most of these observations are intuitive. The question now is (if you accept the statement that when looking for the best places to live we are really looking for outliers): does statistics matter here? If you find that golden place that scores high on everything do you care about whether it makes statistical sense?There is a discussion section recording my additional thoughts during this project. I appreciate your attention so far if you are still with me. Before I bore you completely I'd like to mention that there are acknowledgement and data source sections near the end.Let's look at these three variables: income living cost and income/cost ratio. The ratio is derived from the first two base quantities. Ratio1 means making ends meet which is a convenient parameter for liferelated decisions. However it is worth noting that when this ratio is offered as a third variable it does not add any additional information to this incomecost system; the degrees of freedom remain two.People with statistical knowledge might choose only up to two of the three parameters in this group in any given calculation. The consideration here is: when all three are checked we are double dipping into something. But this is not unique to this situation.As many things in life are somewhat correlated it is a common problem.Initially I set out to achieve a correlation matrix that allows me to digest the result and think about how to include/drop certain variables and have a PCA. In the end I dropped the last few steps. The reason is that I am  trying to come up with a ""engineered feature"" which is a linear combination of the available data columns that could predict the likelihood of achieving what the users wants to achieve in life.Instead I simply ask users to bring their own ""Happiness  f(x)"" function to the app (with the assumption that the input variables are among the ones I offer here). The ""happiness"" or ""achievement"" is subjectively defined and very hard to measure. My current understanding is that: for what this ""where to live"" app is designed to do I do not have to worry about correlation at all; I just dig for any quantity that the users might care about during the decision making process and offer them all as options.Am I right?As mentioned earlier the weightedaverage engine is chosen because of its simplicity and scalability. At this point I would like to share my thoughts on its limitations in the context of the ""happiness"" discussion. By choosing the linear framework I am essentially saying this for example: while holding other variables constant your happiness grows  with how much money you make; at any given point if you need to take derivatives with respect to a certain variable you'll find the first order derivative is a constant and any higher order derivatives are 0  because I defined a model space with only first degree polynomials.It is always a good practice when choosing basis functions to first think about all the possible mathematical behavior your governing equation might require. For this project I assume for example I would never look at how much happier people become when they make another 10000 $/year. The app itself is meant to use explicit linear functions to go through a forward problem rather then an inverse problem trying to analyze the quantitative impact of money/air quality/safety on our happiness.Another train of thought is: I really wonder what happens when I plug in some nonlinear functions into this system like thisWhile playing with the filters of this app very often you find there are only a handful of counties (out of 3000+) surviving the filtering system especially when you combine multiple sliders. When the whole data set is small enough for the memory to handle the suggested approach should be: after choosing which columns of data should be included (by checking boxes) keep all rows (by not touching the sliders at all) then generate the score. Gain an understanding of where the bull's eyes are then play with filters.It would be nice to have a realtime visualization as I play with the slider of one parameter showing how the possible ranges of other parameters are changing. This would be very easily achievable in the future with the 'crosstalk' package if not already.All data sets used are numerical. Categorical data sets are avoided since it's hard to define the ""distance"" among different categories. When people try to decide where to eat and tell me their favorite cuisine is Italian it takes lots of assumptions and framework building to define the score of American Chinese Thai etc.For this project one categorical data set I found that's potentially easy to include is climate zone data since it's often derived from numerical measurements like how many days in a year people turn on heating vs. cooling. Most other categorical data are hard to incorporate.As real estate value is often a key consideration for people to compare places I really wanted to include that info during calculation. However though Zillow.com offers a lot of good real estate data its county data set only has statistics for slightly more than 1800 counties much less than the total (3000+). I do not want to do quick but wrong interpolation of the data and so was compelled to give up using the set. The proper real estate value interpolation for the other 1200+ counties is a complex project by itself.For other data sets with only a few missing data items I often just googled for the needed data points and manually fixed them. But for the 150+ counties reporting 0 crime rate I suspect quite a few of them are wrong. I did not have time to validate the data so they were used as is.The data cleaning steps are also included in the GitHub code check it out if you are interested in the details.There are two future directions leading to business potential: ",NA
Global Life Expectancy Explorer: What happened when the floor dropped out?,35,https://nycdatascience.com/blog/student-works/global-life-expectancy-explorer-via-shiny-happened-floor-dropped/,"Robert Frost one of the great American poets of the early twentieth century once stated ""In three words I can sum up everything I've learned about life: it goes on."" If we can take his observation and apply it to the average life expectancy of humans across the globe as a single group we can say that life not only goes on but fortunately also up. (see graph below)",NA,"But alas the devil is always in the details. That is where this first Shiny data science project leads me: an exploration into the not so rosy life expectancy data around the world.Although there are numerous open data sets around the web one of my favorites is The World Bank. Specifically for this endeavor the HealthStats portal published via the World Bank Group. The data  provides female and male life expectancy information for 253 countries and covers a time range from 1960 through 2014. It's a substantial collection of data for my initial inquiries.Every investigations needs at least one tool. Here we leverage Shiny to dive past the very general trend line of a global population and graph separate gender box plots of all countries to see if anything stands out.Looking at the plots it becomes immediately obvious that although the median trend of countries is positive we have some serious outliers in the data. Overlaying the gender differences provides extra contrast.The outliers show a significant drop in life expectancy. So what countries are causing these? Ideally the Min/Max selection would already identify the data points and display them below the plot. It's a feature still in work so we can make use of the tab panel that displays the raw data set.If we note the years of interest below the outliers from the box plot we can locate the equivalent year/column in the data set. Sorting the year column in ascending order allows us to identify the countries with the minimal life expectancy numbers. Filtering individually on the top five countries in the primary plot reveals the devil within the details.After searching the affected years in Cambodia we find the following significant events: Cambodian Civil War (19701975) Khmer Rouge regime (19751979). The later event was well known as the ""Khmer Rouge Killing Fields"". 1 million+ people were killed during this .Here we have the  Civil War (19912002) with 50K+ people killed. This is one of the countries of the ""Blood Diamond"" infamy. was cursed with a civil war (19901994) that quickly became genocide (~1994). It was estimated that 800K+ people were killed in 100 days. primarily suffered from a devastated economy and massive food production shortages due to government actions during the affected years.Health issues seem to be the main culprit affecting . Tuberculosis with HIV/AIDS being especially devastating (WHO data in 2002 shows that 64% of all deaths in the country were caused by this illness) contributed the lion share to mortality statistics.An initial glimpse deeper into the data shows some periods of painful rot beneath the healthy global trend concerning life expectancy. Continued enhancement in the Shiny app will provide more efficient insights into the what/when/how of country data points. The following modifications should help with that goal: ",NA
Scraping Trulia and Zillow,35,https://nycdatascience.com/blog/student-works/scraping-trulia-zillow/,For this project I have used Python's following packages:Beautiful soupScrapySelenium.Main goal of this project was to gather data preprocess it and prepare for farther analysis.To scrape real estate listing information from  I used Selenium Python bindings. Itself Selenium is appropriate for creating robust browserbased regression automation suites and tests. In other words it is an automated testing suite. Selenium Python bindings gives access to Selenium WebDriver which enables the user to directly communicate with the web browser and write functions and execute tasks in Python programming environment.When one goes to  and types in the area of interest to buy or rent real estate she is presented with an interactive webpage that has a map of the area dotted with locations of the listings and on the right side 20 listings per page. In order for me to understand what it is that I want to automate using Selenium I first had to brows the listings observe and register my own actions while browsing. This step gave me an initial idea of the algorithm to be written for automation.There are two aspects of scraping zillow.com with Selenium.In order for me to reach the final web page where there are all the descriptions and information for any one particular listing I had to go through several actions such as: This is the rough representations of initial chain of actions I wanted to automate with Selenium.The actual scrapping and writing of information happens mainly in step 3 and 4.Step 3 is required because when inspecting the webpage the xpaths to the information are hidden. They only become visible (hence ‘scrapable') when we click on the ‘More’ button.Step 4 mainly consists of finding the correct xpaths to all the different bits of informations of interest.Step 4 can be broken down into following smaller steps:.The website’s UI is similar to  with listings on the left half of the page and the map on the right side. The key trick to simplifying the scraping process was the following:If the website has it’s metadata stored in a JSON dictionary format thats a score!Steps of discovery:After inspecting each one of the search results I was able to find the tag that contained a relatively large json dictionary in it: a sign of useful information. Closer inspection revealed that it did actually contain all the information I was interested in regarding each listing on that particular page. To be more precise the tag contained several concatenated json dictionaries with different metadata information. That meant that after scraping this information I would have to use regular expressions and python’s string manipulation to extract the dictionary of interest.I used Python’s JSON package to help me with parsing the scraped information into a Python dictionary.,NA," Bellow is the github link to the script of the algorithm described above.
https://github.com/Vacun/Vacun.github.io/blob/master/Selenium/zillow%20with%20Selenium/zillow.pyBellow is the github link to the Scrapy spider for trulia.com
",NA
Soccer Betting Analysis - How to use betting agencies odds to predict match results?,35,https://nycdatascience.com/blog/student-works/r-shiny/soccer-betting-analysis-use-betting-agencies-odds-predict-match-results/,"As a soccer fan with 3 years of work experience as a live soccer match analyst I have thousands of soccer game hours in my repertoire. I follow European soccer on a weekly basis and know most of the teams and players in the major leagues of Europe.Even with all my knowledge and experience I find it hard to predict soccer match results. Like in any other sport the best team doesn't always win. There are many parameters that affect the outcomes of any given game. The skills of the players the tactical formation and teamwork may be the most important ones. But are meeting all these parameters enough to win all the games played? If so we should be able to predict match results pretty easily.The truth is there are many more factors that affect soccer match results  team motivation and spirit player injuries fan support chemistry between teammates and opponents reputation and win history are some of them. The complex interplay between these variables during the fastpaced activity of a game makes every match unique.  Professional betting agencies are making a lot of money from people who want to predict match results. It is safe to say that their betting odds are calculated in a way that maximizes their profits and minimizes their risks.In this project I wanted to examine the degree to which betting agencies' odds correlate with actual match results and to see if there is any way to maximize prediction accuracy. For this I looked at the betting agencies' reported odds for the basic 3way bet (home team vs.  draw vs. away team) and the level of favored outcomes within each game (high favored moderate favored and low favored) and compared them to actual match outcomes to see if betting agencies' odds had any value. I further categorized the match outcomes by the location of the teams (i.e. hosting vs. visiting) the stage of the season and the numerical difference between the payouts in order to find patterns that optimize prediction accuracy.In the end I found that there are three parameters can help predict the outcomes with up to 80% precision: 1) the agencies' high favored result 2) the location of the team and 3) the stage of the season. I used R shiny app and ggplot2 to visualize the data. You can find the full results on the app.
In most soccer competitions draws may be the final result of the game so there are 3 different outcomes to bet on between Team 1 and Team 2:  First outcome: team 1 wins  Second outcome: team 2 wins  Third outcome: team 1 and team 2 drawThe odds are translated into payouts. The result with the minimum odd is the one that is most likely to happen it has the least risk and therefore offers the lowest payout.The result with the maximum odd is the one that is the least likely to happen it has the higher risk and therefore offers highest payout.For example let's take the first match in this betting odds chart of the English Premier League and look at the odds for the full time result. In this game Arsenal is playing against Crystal Palace. For an Arsenal win any dollar you bet will give you $ 1.29 (a $0.29 profit). For a draw in the match a dollar will give you $ 4.98 ($3.98 profit). And a Crystal Palace win will give a return of $ 8.06 ($7.06 profit) for a dollar bet.The data sets were taken from Kaggle a part of a soccer SQLite data base.The data sets include data on more than 25000 matches from 9 different leagues in Europe over 8 seasons (2008/2009  2015/2016). The data includes: match results and dates teams leagues and match betting odds from 9 different betting agencies.The European leagues are: Belgium Jupiler League  England Premier League France Ligue 1 Germany 1. Bundesliga Italy Serie A Netherlands Eredivisie Portugal Liga ZON Sagresand Scotland Premier League and Spain LIGA BBVA.The betting agencies are: Bet365 Blue Square Bet&Win Gamebookers Interwetten  Ladbrokes Pinnacle Sporting Odds Sportingbet Stan James Stanleybet VC Bet and William Hill.It is important to note that there was always consensus between the agencies regarding the probability for each outcome (i.e. they all thought Arsenal had the highest chance to win); the only difference was the magnitude of payout that they offered. Therefore I considered the average consensus as a single entity. For data processing I used RSQLite package for R to convert the different SQL tables to CSV files.                As part of the data cleaning and preparation I deleted rows with missing values and ignored data from 2 betting agencies because their betting odds were uploaded to the SQL server as integers rather than exact numeric values. After this process there were 22434 observations left.Moreover I added columns to the data set to include the match winners  the agencies' average minimum middle and maximum payout and agencies' favored result. For the analysis I defined the result with the minimum payout as the favored result by the betting agencies. The success rate shown in the charts is calculated as the number of times the favored result was the actual final result of the match divided by the total matches played.The favored result level column is a breakdown of the matches to 3 groups using the difference between the payouts (as extrapolations of the odds). My assumption for this calculated column is that the higher the difference between the payouts the higher the chance for the minimum payout to be the winning outcome. Therefore the groups are categorized in the following way:A high favored result  max payout  min payout > 2A moderate favored result  2 >max payout  min payout > 1A low favored result  max payout  min payout < 1The betting payouts have a normal distribution. The maximum and middle payouts are skewed to the right. The minimum payout ranges from a little more than 1 to around 3. The maximum payout ranges from 2.5 to 40. The middle payout distribution looks similar to the maximum payout and range from 1.9 to 10. Below are the histograms of the payouts:   The minimum and maximum payouts are inversely related and the minimum and middle odds are also inversely related. This is explained by the fact that when there is a high favored result (for example one team has a better track record than the other) its payout will be low and accordingly the other outcomes' payouts will be high. On the other hand when there is no high favored result (for example the teams playing have same skill level) the payouts will be quite similar. This is a scatter plot of the minimum and maximum payouts of the matches observed: The first chart depicts actual match results. Here we can see that the home teams wins 46% of the time the away team wins 29% of the time and there is a draw 25% of the time.The next graph shows that the agencies favored the home team 73% of the time and the away team 27% of the time while they almost never favored a draw (13 out of 22434 matches). We can see that the agencies favored the home teams in most cases. This emphasizes the importance of location in the competition. This chart raises an interesting question: why do the agencies never favor a draw result when this outcome occurs in at least 25% of the matches? I did not come across any data explaining how the agencies determine their payouts however I believe that agencies prefer to favor one team over the other because it's easier for them to promote the bet among gamblers. It's just more interesting to have a faceoff. Next I wanted to check how accurate the favored result was in terms of predicting the match outcome. I found that the agencies' favored result (represented by the minimum payout) had an average success rate of 53%.  This was consistent for each of the seasons in the data frame.
  I also wanted to examine to what degree the location made a difference in the payouts given to favored teams. For instance I would expect the payout for a favored home team to be lower than a favored away team. After all it is widely believed that the home team has the higher advantage. I used two box plots which demonstrated that there is If the minimum payout for a favored home team and a favored visiting team are almost identical does this mean that the rates of winning them are the same? In fact no the rates of winning are not the same. As we can see from this bar chart the favored home team had a 55% chance of winning while the favored away team had a 50.5% success rate. Although these are not earthshattering numbers could this be an opportunity for the betting agencies to attract more gamblers? By raising the payout for the favored away team they are promising larger compensation when in fact the probability of the favored away team winning is actually quite low. One of my more interesting findings was that the accuracy of the predicted wins increased during the later stages of the season. The late stages of the season carry higher stakes than the early stages as this when the champion and the relegations are decided. In the chart below we can see a slight improvement in the success rate over the span of a season. 
The results of the away team demonstrate a different trend. While the high favored teams' success rate increases over time the low favored teams show inconsistent success rates throughout the season. It seems that there is no clear effect of the season stage on their performance.  ",NA,I wanted to further visualize the proportion of games that can be cataloged as high favored moderate favored and low favored odds. Because of the league structure like number of games and arrangement of opposition there are more games where the difference in team skills and strengths are large. Therefore we see that almost 50% of the matches were categorized as high favored and more than a quarter that is moderate favored.Next I checked the stage of the season against the location of the team as well as the favored level and I found that the success rate increases over the span of a season when the high favored team plays in their home arena. It seems as if the high favored teams are capable of winning in the important stages of the season while on the contrary the low favored team success rate decreases over time as if the pressure of the last stages of the season and the presence of their fans have a negative effect on their performance.Moreover in my  you can explore the success rates broken down by the location and the favored level. When I did this I found a clear pattern: matches with a high favored result have a success rate of around 65% and matches with a low favored result have a success rate that ranges from 33%43%. Moreover when crossexamining the data I found that there were three parameters that carried the most weight when determining a probability of a match outcome:1) the agencies' high favored result 2) the location of the team (home vs. away) and 3) the stage of the season. Ultimately choosing a high favored team in their home arena in the late stages of a season can raise the probability of winning the bet by 80%.,NA
likePredict: A product to predict,36,https://nycdatascience.com/blog/student-works/capstone/instapredict/,An internal Instagram study showed that teens delete up to half of all their Instagram posts due them not receiving enough likes. In fact Instagram’s new “Instagram story” is in part an effort to counter the vanity imposed by the “likes” metric  . Gathering the data for this project was the largest challenge. In recent years Instagram has evoked an increasingly stringent API policy requiring developers to undergo a number of processes before being given a key. As such we needed to scrape the data. Instagram like many other large organizations has many built in tools to limit and trap web scrapers from getting too much data. Alternatively there are a number of Instagram web viewers that display instagram content without the same regulations. For our project we used two sources: Instagim and Instaliga.Instagim allows photos to be displayed by tag on a single page displaying username likes comments caption filter and some relevant hashtags. Due to the scope and timing of this project we focused solely on photos under the “nature” tag. While we had the photo and the likes it got we were still missing a key metric: followers. We needed to find a website that could be easily crawled for followers and following since they would be important features in predicting the amount of likes for a photo. Instaliga was easy to scrape since it allowed us to append usernames to the end of a URL and then crawl for this information using scrapy. Additionally we were able to scrape the metrics for the last 20 posts from a given user giving us a baseline for their standard level of engagement with their audience. However since each user is it’s own URL the script generated too many requests often being met with server errors. To auto throttle scrapy’s wait time turned out not to be time efficient so we moved forward by hammering their server with requests and getting what data we could.  Some features such as the filter applied to the photo seemed to have even less of an effect than originally thought. These features would likely have larger bias in subsets of photo type: for example selfie photos may commonly use a filter to make a person’s skin look healthier. However for our “nature” photos there seemed to be little correlation.Our plan was to supplement all of the user data with information from the photos to achieve a more accurate prediction. The logic follows that if a user’s followers and general account popularity defines a range within a photo should fall the features in that photo will aid in assigning a more precise prediction for “likes”. After turning the photos into arrays the PIL library was used to to extract summary statistics for each color band. Other features such as luminance were easy to calculate given this data. However we also wanted to extract more complex features. Using the OpenCV library we were able to use a pre trained model for facial recognition and assign a number relative to the number of faces in each photo. We also measured the blur of each photo Unfortunately few of these photo features seemed key in finding a correlation for likes in our subset of data. We compared many of the features against likes and likes/followers ratio to normalize likes but the correlations still seemed somewhat weak.In the future we plan to extract even more features and scrape data from multiple accounts to see if photo features matter more in the realm of a single user. Comparing some of these photo attributes against the mean and median likes for users may also have been beneficial.We began the modeling process by constructing a basic MultiLayer Perceptron with a single layer of input nodes and a single output node  using the Rectified Linear Unit as our activation function. After setting up this basic model using the Keras API with TensorFlow backend the hyperparameter tuning was initiated. Having tuned the model extensively other models were then constructed for comparison.Given the results of the MultiLayer Perceptron we sought to compare with less complex model. Utilizing the GradientBoostingRegressor in sklearn we were able to obtain much better results in our cross validation processes and predictions on our test data. Ultimately we were able to compare our predicted results to the actual amount of likes a post received in our test set and determined that 95% of the predictions were within 30 likes.In the future we’d like to get access to the Instagram API since a scrapy based web application is not a model for stability and scalability. It would alleviate the majority of issues currently present in our process and allow us to expand our models for different categories of photos. Currently we are limited exclusively to public accounts due to data access. Additional features such as time of the week follower involvement/network analysis and a greater variety of image analysis would further reduce the error on our prediction within a given subset of photos. With the right amount of data and computational power this applied model could solve some inherent issues with Instagram’s “likes” metric and even expand to other platforms.,NA,"As a result there is a growing market need for a tool that can accurately predict the likes of a post for a given user. While there are a number of services that provide basic to complex analytics there is a lack of predictive modeling being employed for the average user  hence we developed “likePredict”: A “like” predictor for use in public Instagram accounts.While Instagim can constantly be refreshed to show the latest photos we needed photos that had been posted for at least 24 hours to accumulate likes. When clicking the “load more” button at the bottom of the page we noticed that the website made an ajax call with a unique key. By scraping solely these keys and waiting 24 hours we could replicate the same ajax call from the previous day and obtain photos posted the day prior (we did our best to keep the 24 hour period constant). Using beautifulsoup urllib and the requests library in Python we were easily able to download each photo and get the metrics specific to it.We started off on some basic EDA. The graph on the left here is a histogram of ""likes"" across all of our observations.  As can be seen they are not normally distributed. Followers following and other metrics seemed to follow the same pattern. We debated doing some basic transformations (logarithmic or boxcox) however since our final model was likely to be a neural network or tree based model we deemed it unnecessary.The  other nonphoto features were relatively easy and straightforward. The bulk of our engineering was generating mean mode min and max likes for their previous posts. As intuition would suggest all of these metrics were fairly correlated with the likes received on their newest post unless of course other extraneous factors were at work (sudden influx of followers particularly ""likeable"" post such as a celebrity post etc...).However many other features has more complex relationships. It seems fair to assume that followers would be correlated with likes but although there is a relationship present it is subject to a large amount of variance. These fluctuations are likely to be accounted for by a number of other factors hence our interest in the ""previous likes"" metrics.Finally we created a front end application using Flask. This library lets you easily link backend python code with html templates to build interactive web apps. Once launched this would allow users a simple interface to upload their image and type in their Instagram handle to receive predictions. It’s important to note that we are not web designers so the temporary UX/UI leaves something to be desired.Upon image upload the back end model collects analyzes and exports a data frame. Upon handle input the scrapy automatically collects the previous 20 posts and other relevant data.These data frames are combined to match our models training columns. The model is then applied to the resulting data frame and an output prediction is generated.",NA
"Metarecommendr: A recommendation system for video games, movies and TV shows",36,https://nycdatascience.com/blog/student-works/capstone/metarecommendr-recommendation-system-video-games-movies-tv-shows/,Metarecommendr is a recommendation system for video games TV shows and movies created by    and .  It uses wordembedding neural networks sentiment analysis and collaborative filtering to deliver the best suggestions to match your preferences. It is part of our capstone project delivered at the end of the  program.Finding a piece of media today can be difficult. There are so many games movies and tv shows coming out every week that it is difficult to keep up with. It can take hours to look through blogs videos and reviews to determine if a new piece of media is something you will like. Finding a game from the past that you are sure you will like is even harder. Websites like metacritic.com attempt to simplify this process by aggregating reviews. However there are still some major flaws including:Hence for our capstone project we decided to address these issues by creating an application to improve your search for your next game (and even let you find movies and TV shows if you wish!).  Metarecommendr is a web application that combines a sleek and intuitive user interface with the powers of contentfiltering and collaborativefiltering in order to deliver the best recommendation for you.Metarecommendr was designed and built in the span of 2 weeks. The project workflow is summarized below:One of the reasons we opted to implement both content and collaborativebased recommendations was the distribution of ratings found in our dataset. There were in total roughly a million reviews  half from critics half from users. We found that for both critic and user reviews scores the distribution of ratings were negatively skewed. Hence relying solely on ratings (for collaborative filtering) would not offer enough granularity to produce sensible reviews as most products are perceived positively.Interestingly in our early exploration of the dataset we found that the number of reviews was not necessarily indicative of the quality of a product. I is among the most reviewed items and yet it has a very poor average critic and user review. This makes some intuitive sense. Games that skew either very positive or very negative create more discussion. Extremely bad games can be fun to talk about with others similarly to how bad movies can live on as cult favorites. Mediocre games where there isn’t much to say tend to have less discussion and therefore less reviews.There are mainly two types of recommendation algorithms: contentfiltering and collaborative filtering.Since a big portion of the dataset was composed of text data from reviews the chosen approach for feature engineering on contentbased recommendations was . This is an unsupervised algorithm to generate vectors for documents. It is an extension of the Word2Vec algorithm where a document (instead of a word) is turned into a vector representation.  Its implementation in Python can be found under .Doc2Vec is able to learn semantical similarities among words making its implementation more sophisticated than tfidf. An example output of our model on critic reviews shows that it was able to learn pretty well similar words to the word “Excellent” . Pretty good job!For metarecommendr two Doc2Vec models were trained separately on Summary and Critic Reviews. We opted for not using user reviews since there were not enough descriptive words to yield a meaningful recommendation. On the user interface a user selects a product they like. Products are then recommended according to a cosine similarity metric. The closer to 1 the more similar two vectors(products) are.A major challenge to implementing collaborative filtering on this particular dataset was the high dimensionality and sparsity of the useritem matrix. There were a total of around 27500 products and 63000 users with an average number of less than 3  reviews per user. To reduce the dimensionality of the useritem matrix truncated Singular Value Decomposition (SVD) was implemented. Consider a usertoitem matrix A where aij represents the ratings from user i for product j. SVD states that every matrix Anxp can be approximated by the following equation:where U and V are orthogonal matrices and Sis a nxp diagonal matrix with singular values of A along the diagonal. As S is a diagonal matrix we can obtain a more compact representation through SVD. Truncated SVD takes this approach one step further by using only the k most significant values of S instead of all values. Under this approach we compute a rankk approximation to A such that it minimizes the Frobenius norm error as follows:For metarecommendr the dataset was split into train and test and k was chosen to be 13 according to Cattel’s scree plot.Once we obtain the rankk matrix A' we can make recommendations according to the entries in the matrix.  In the context of our dataset A’ corresponds to a matrix of predicted user ratings where aij'is the predicted user rating from user i for item j. Compared to a baseline where all user ratings for products are simply predicted to be the average user rating (RMSE  7.50) truncated SVD improves 19% upon the error term on predicted user rating (RMSE  6.07) .To sum up for collaborative filteringSVD  a user inputs and ranks a few items. A useritem matrix is then generated and decomposed by SVD. For a given user i this approach allows us to get a predicted user rating for different items and recommend items with highest predicted rating.To better understand the relationship between item review scores we compared items against each other using a modified Pearson’s correlation formula. To help scale down this correlation matrix items with less than 3 overlapping reviews were disregarded and given a score of 0 or no correlation. This itemitem matrix approach also allowed us to make crosscategory recommendations since the algorithm was no longer bound to an item’s metadata(such as in collaborative filtering). On the user interface a user has the option to select a product they like and they receive products with the highest correlation metric. As mentioned in the introduction a major problem with Metacritic’s dataset was the fact that sentiment of reviews did not necessarily match the text data. To address this issue we performed sentiment analysis on the critic reviews. Positive and negative were defined as follows: reviews with scores of 55 and below were classified as negative and those with scores of 85 and above were classified as positive. Reviews with scores in between these values were not used for sentiment analysis. Sentiment Analysis used vectors from doc2vec as features. We attempted a few different machine learning models including: Logistic regression Naive Bayes SVM and different types of Neural Network. The performance of each model is described below:At the end  with the following features: 2 convolution and pool layers 2 recurrent LTSM layers and 3 dense fully connected layers. This model lead us to an accuracy rate above 90%. On Metarecommendr this sentiment analysis is showcased interactively:  a user types in a review and the text is evaluated according to our model. Users are able to receive feedback on whether the given score aligned or diverged from the text. We hope to continue with this aspect of the project to improve accuracy and use it as another preprocessing step for our recommendation systemThere are a few improvements that could be made to metarecommendr including:,NA,"You can . Please keep in mind that for the time being only a scaleddown version of our models is running online due to memory restrictions. Only ""Contentbased"" is functional at this time. The code is .To collect all the data and reviews about our items  games movies and TV shows   we used the Python web scraping framework . In total we implemented 12 spiders  one for each items list one for the summary and details of each specific item and one each for the critics and user reviews of each item. While some spiders were finished quickly the longest one  scraping games reviews  took 10 days in total to finish.Because we were already expecting a rather big amount of data we decided to scrape directly into a database instead of using text files. A preliminary version of our database was set in  a selfcontained SQL database engine which was set up within minutes. After the scraping was finished we exported the data to a MySQL database running as an . To not have to insert 584mb of scraped data from a local machine into a remote database we uploaded all our data to  and implemented an  to directly stream from S3 to RDS via an  instance. This reduced the migration time dramatically by factor 7. Our final app was then ready to read the data directly from the MySQL database.In terms of observations scraped from metacritic.com we ended up with:Since models were built in Python a natural choice was to use  framework to implement our web application.The frontend is an interactive application built on top of and . On the backend The app is able to directly pull data from the aforementioned MySQL database on AWS. Models were exported to Pickle and H5 files which were stored on AWS S3. When a user visits our application such files are loaded from AWS s3.",NA
Numerai Hedge Fund Competition,36,https://nycdatascience.com/blog/student-works/numerai-hedge-fund-competition/,Numerai is a hedge fund that uses a machine learning competition to crowd source trade predictions. Competition is based on proprietary hedge fund data collected and curated by Numerai. Data is encrypted before being made public because it is highly valuable proprietary and its quality provides a competitive edge for Numerai. This enables Numerai to obtain machine learning predictions on private data without ever making it public.Numerai  that high quality proprietary data is expensive to collect and provides a significant competitive advantage. Hedge funds and other financial institutions are in an optimal place to collect and curate this data but they have a strong incentive to keep it private and guard it. But these institutions only employ a small percent of the world's machine learning talent pool. It Meta model construction and its bias over time can also be compared to financial markets. Financial markets are based on decisions made by individuals which when combined determine the market direction. Similar processes are at work with the movement of this meta model. It is build on the predictions of the individual participants and the direction is determined by them. Additionally by using a metamodel formed out of a bag of predictions Numerai is able to take a portfolio theory approach to predictions. Metamodel is made of individual bets made by many data scientists. Averaging of these bets significantly reduces the individual systematic bias and model variance. Resulting leftover bias can be interpreted as learning deduced from the data.Following table shows the time taken accuracy loss and f1score of multiple models. All the algorithms were executed on Google Compute HighCPU (64 core 60 GB memory and Ubuntu 16.04) instances.We optimized and calibrated individual models using bayesian hyperparameter optimization and came up with three ensemble models that consistently produced low variance and low bias. Models were ensemble by soft voting hard voting and using predictions from previous models as additional features. Deep neural network in Keras was used as a meta classifier. These same algorithms were used for predictions on the following week’s dataset. Soft voting ensemble model consistently ranked among top 5 for both the weeks. Other two models consistently ranked between the 5th and the 20th position. Give us a shout out if you want to chat about additional details.In the near future we would like to build a deep learning model using transfer learning along with full generalization and automation from week to week. Implementation using PySpark (for parallelizing) and by incorporating MongoDB (parameter tracking) will also be in the works.,NA,Homomorphic  is used to transform and encrypt the data. Additionally competition data is scaled and normalized. This scaling and normalization leaves limited room for feature engineering and participants have to rely on strong algorithms to achieve success. This makes it an algorithm vs algorithm competition rather than competitors spending endless hours feature engineering.further argues that this makes financial markets machine learning inefficient.With a competition style format on encrypted data Numerai is able to obtain machine learning predictions from a larger pool of data scientist giving it a further competitive edge. Once individual participants submit their predictions Numerai uses them to construct a metamodel. It uses the predictions made by this metamodel for live trading. Crossentropy between the metamodel and user predictions determine the leaderboard rankings for participants.  for using a metamodel comes from the literature on .We tried dozens of algorithms to gauge their effectiveness on the data sets. Algorithms with high time memory complexity or poor learning ability were eliminated step by step. Our aim for model selection was to maintain a quick turnaround time so that we can quickly benchmark results twerk knobs and resubmit.LinkedIn   and Python Anaconda Jupyter Notebook Pycharm Pandas JSON Scikitlearn Xgboost Keras Hyperopt Scikitoptimize Mlxtend Google Compute Linux,NA
Kaggle Renthop,37,https://nycdatascience.com/blog/student-works/kaggle-renthop/,"Finding apartments for rent is usually a challenging task.  one of many websites that try to make the process more convenient tries to help renters by sorting their listing by quality using data. To improve their methods and to better understand the needs and preferences of renters RentHop along with  hosted a competition on  to predict the number of inquiries a new listing will receive based on its features. We took the challenge to see if we could accurately predict the listing’s interest level based on the data provided.Below is an overview of what we did:The participants of the competition are given two datasets: one for training containing approximately 50000 listings and one for testing with approximately 75000 listings in the set. The goal is to predict the interest level of each listing based on the thirteen features provided. The features include information about the location of the apartment time and date the listing was created description IDs relating to the building and manager and basic apartment information. Photos submitted for each listing has also been included for analysis.The interest levels are split into three categories: high medium and low. As the graph indicates there are a lot more listings with low interest listings than medium and high listings and the number of high interest listings is very low even compared to medium.When exploring the data we always need to take into account correlation between predictor variables. Our basic  instincts tells us that bedrooms bathrooms and price might be related. After further investigation however we realized that it probably wouldn’t affect our model too much.One of the most interesting things we noted when learning about our data was that it seems like the hour when the listing was posted plays a role in determining whether the interest level would be high medium or low. It was also interesting to note that most of the listings occur during the middle of the night which might be due to companies automatically setting up their systems to post on renting sites. The proximity of the listings are to the start of the workday seems to correlate more with higher interest levels which is likely due to way RentHop posts their listings on the front page. The newer listings are likely to show up on the front page which in turn lead to a higher interest level.Although we were given a bit of information for predicting interest level we wanted to include more information to help our predictions. Because the rules of the competition do not allow outside data to be used we had to be a little creative when creating new features out of the old. Our strategy was to create as many new features as we could then slowly eliminate features that were not useful for predictions. We broke down the features into smaller related categories to simplify the process.When looking for an apartment every renter cares about the price and number of rooms. The dataset doesn’t specify the total number of rooms but it does include the number of bathrooms and bedrooms. Although the bedrooms and bathrooms are numerical we thought it might be better to treat the information as categorical data so we decided to have the option to dummify those features. Using the bedrooms bathrooms and price features we also created new features describing the ratio between all combinations of the three.The descriptions and apartment features are text data so they were trickier to work with. We used TFIDF to get the numeric values for the frequent words shows up in ""features” and ""descriptions"" then got the mean for every observation. We then used LDA Topic Modeling to get seven topics for every observation based on their TFIDF values. Subsequently we used Google's Word2vec tool to train every word in ""features” and ""descriptions"" and got 100 numeric columns for every word. Then we calculated the mean for all the words in every observation. We also obtained the word and character count for description and the count for apartment features.We grouped the manager and building ID by frequency. If the frequency was within a certain percentile they would be a part of that group. We also had the option to dummify the ID’s.Due to time constraints and limited processing power we decided not to focus too much on the photos provided. We included the number of photos for each listing and tried to extract basic information about each photo. For each image we obtained the minimum maximum and mean of the dimensional properties and the brightness.We simply broke up the listing created feature into month created weekday created day created and hour created. We also had the option to categorize and dummify these new features.For all our models we dealt with categorical features by dummifying them.To start we created a logistic regression model. We included all the numeric features such as numeric vectors from Word2vec TFIDF Topic Modeling. However after we submitted on the Kaggle our results were shockingly bad. After more visualizations we realized that there was too much multicollinearity between some columns like Word2vec. When we deleted those highly correlated columns our results improved. We also had many other variables that were highly correlated with each other because of the way we created our extra features so we carefully selected the features for our model.  We tuned our model using cross validation and grid searching by checking a range of parameters and also testing both L1 and L2 regularization.Although we started with linear regression we focused most of our time on classification trees. This was logical because this was a classification problem and our initial research showed that none of the features showed strong correlations with the response variables. As it appeared that logistic regression was not the best fit model for this problem we attempted to set up to a random forest using our full dataset we included columns that were similar; and used grid search and cross validation to tune the parameters. Our results were decent much better than the results obtained from logistic regression. When doing Kaggle xgboosting is a must. It provides better results in a way that is not computationally expensive. We did not have the computational power or time to tune the model so we used parameters obtained from a public kernel for this competition. We used the reduced dataset from our random forest to obtain our best results with xgboost. When evaluating the feature importance for our model  we see many obvious features that go into deciding interest level like the ratios of basic apartment features and also location. It is surprising to see that hour the listing was created played a significant role in determinine interest levels.We did a simple ensemble with our models using weighted averages. We manually picked the weights according to our score on Kaggle. Our xgboost models were weighted the most and the logistic regression model was weighted the least. This slightly improved our results.Our models worked out fairly well but can always be improved. Because of our allout approach for feature engineering we ended up creating a lot of similar features. We can limit the features more by carefully selecting the features for our models. At the very least we hope to improve our logistic regression model by reducing multicollinearity. We would also like to explore the use of  more unsupervised learning techniques like principal component analysis and possibly build a neural network to evaluate the photos. When we finish selecting our features we would like to optimize the parameters better for each model. Instead of using the standard grid search we would like to try to implement Bayesian Optimization for selecting our parameters. Of course our major goal for the future is to improve our predictions.",NA, The dataset includes many ways of describing the location of each apartment. They include latitude and longitude coordinates the display address and the street address. Using the street address we filled in missing or incorrect coordinates and also categorized the street types. We noted if the address contained a direction (north east south or west) and if it was a street or an avenue. We wanted to get the neighborhood of each listing but were unsuccessful due to hitting the query limit for Google’s API. Instead we decided to group locations in two ways. The first was by simple latitude and longitude boxes and the second was by KMeans clustering. We grouped the listings into twentyfive different clusters.Our basic assumption about using trees was that the more predictors the better which is why we went with an allout approach when creating features. In reality our results had actually improved when we removed a lot of features specifically some of the dummies of categorical variables with many categories and our word2vec for description and apartment features. Because random forest selects a subset of predictors to choose the next best split having many more columns that are not helpful will make it harder for the random forest to choose useful predictors for splits.,NA
Machine-Learning with Renthop,37,https://nycdatascience.com/blog/student-works/machine-learning-xkcd/,"–The ""features"" variable which contained every distinct feature (e.g. Elevator Cats/Dogs Allowed etc.) for each apartment proved more challenging.  To analyze them separately we unlisted and tabled the features then exported a csv for visual inspection.Though there nearly 1300 distinct feature tokens many of these were effectively the same whether due to the use of synonyms or alternate spellings of the same words.  There were 13000 “fitness center”s but also a couple hundred “gym”s and a few dozen “health club”s.  There were “live in super”s “livein superintendent”s and “onsite super”s and so on.To correctly assign features to apartments we made 54 new variables and used regular expressions to match as many features as we could to the listings.  For example we assigned “balcony” and “terrace” (both in the Top 20) to the same ""private outdoor space"" variable.  We also pasted together the “description” and “features” variables since over 1400 listings had no separate feature breakdown.  Based on the table we estimate our approach matched at least 99.5% of features listed in the original data to their listings.To determine which features had any effect upon logloss we constructed a saturated Random Forest model with the new features (alongside the allimportant price variable) on a subset of the training set.  Next we ordered the features by Gini importance in that model then gradually pruned tabulating the model’s logloss on the rest of the set as we went.As the plot shows about 40 of the features manage to lower logloss though the reduction tapers at about 25.  The decline though is quite incremental after more than a few predictors have been included.  Proportionately the features that removed the most logloss were        and . To account for multicolinearity and efficiently incorporate as many of the useful features as possible we conducted a Principal Component Analysis.This screeplot shows that while most of the features are independent a handful seem to be covariant allowing us to get the predictive power of close to 40 features with only 30 principal components.  For the final model we incorporated a 30PC featureset.We assumed that residential area would have a large effect on interest. Living closer to the center of the city is desirable so long as the price isn't too high. As such we decided to convert the latitude and longitude variables to neighborhoods. By physically listing out the name of the area such as “East Village NY” you can use the ggmap library to get the latlongs for the center of the neighborhood. Once you have a list of areas and their respective latitude and longitude you can simply use KNN (k nearest neighbors) to assign the latlongs in the data to neighborhoods. A count of listings by neighborhood – note that this is not normalized for size of that area –  shows that Renthop listings are concentrated in midtown and downtown Manhattan.Next we can map the neighborhoods back to general areas of New York City and New Jersey. Below is a histogram of areas for the training set with interest level as the fill.Unfortunately these area assignments did not improve our score much. The histogram explains why: there seems to be little correlation between area and interest level. All general areas (and even the smaller neighborhoods for that matter) seem to have equivalent ratios of high medium and low interest apartments.Given that price was a strong predictor we also attempted to assign a median price for each neighborhood. After grouping by both price and bedroom we summarized with a median price and joined it back with the original data frame. Finally we assigned a binary variable (“expensive”) that was ""True"" if the price of an apartment was above the neighborhood median and ""False"" otherwise.Ultimately it didn’t improve the score significantly but it helped more than the neighborhoods themselves.  While this work may have been unnecessary in the end it was interesting to see a feature that seemed important turn out to have a small impact. Reasoning that apartment listings have their own specific sentiment language we also tried to generate a realestatespecific sentiment lexicon.  By using keyness to compare the total word frequencies in the descriptions against the Brown Corpus of Standard American English (1960) we were able to highlight words that are particularly common in listing language as seen in the WordCloud to the right.In addition to general positive language like “beautiful” and “great”  words emphasizing size (e.g. “spacious”) and culture/convenience (e.g. “central”) stand out in the visualization.  However despite several efforts to score and/or classify these words they did not improve the final model much.  Apartmentseekers likely see through agents' spin.Our model selection process went through multiple stages. Initially we used a collection of untuned algorithms to gauge a baseline for how these algorithms would work in terms of time accuracy and precision. For the Gradient Boosting Classifier Random Forest Classifier Support Vector Machine and a Multiclass Logistic Regression we used the Grid Search package in Python SKLearn in order to vary the range of values of the parameters as well as the set of features to be incorporated. Through the CrossValidation packages we were able to specifically optimize the algorithms to minimize logloss.We noticed immediately that the treebased algorithms like the Gradient Boosting Classifier and Random Forest performed significantly better at the baseline and devoted more time to tuning these algorithms. Of the four models used in the exploratory analysis the Gradient Boosting Classifier performed the best with an initial logloss score of about 0.59.  Given the success of the Gradient Boosting Classifier we wanted to see if we could push our results  further by using the XGBoost algorithm  in R. Ultimately we were able to successfully improve our results and chose this algorithm in our final model.The caret package has a grid search similar to the one in python. Using this it was easy to search through a few parameters.  However given the scale of the model and the number of final features columns it was computationally expensive to run for a number of hyper parameters. Consequently we didn’t use it to run a wide search but to narrow down options for parameters such as learn rate.Caret can also create cross validation folds. Instead of running a time consuming cross validation you can pull a fold out from the full training set and use it as your subtest in XGBoost. By adding it to the watchlist you can assess the accuracy of your model on a smaller test set while training it.  Of course since this doesn’t compare all folds against each other like actual cross validation it doesn’t account for variance between folds and is prone to some degree of error. Often the test score could be lowered to 0.55 in script but our submitted model received a Kaggle score closer to 0.6.",NA,"For this project we took on the  Challenge on Kaggle.  The rental  website Renthop provided us with a csv of data from 120000 listings and asked us to produce a model to predict whether a given listing would receive ""low"" ""medium"" or ""high"" interest.  The model would be judged by predicting a test set with the logloss formula determining its effectiveness.We approached the challenge ready to explore different machinelearning algorithms and creatively engineer the dataset.  In the process we learned:We created several new features from the dataset immediately.  These included1. : Price emerged quickly as a key predictor with lowerrent apartments drawing more interest.  We reasoned though that a 3bedroom apartment would be more attractive than a 1bedroom at a given price so we mutated a priceperroom variable for each listing.2. : We also reasoned that listings with more material would draw more interest.  Consequently we created variables that counted the number of photos the number of words in the description and the number of listed features. (We also wrote a script to evaluate photo size in lieu of downloading all 300000 photos but did not have time to include this in the final model.)3. : Each listing had a timestamp including both date and time of posting. Since we figured that the time of day might affect how visible the listing would be and that seasonal cycles would influence interest we split these into two separate columns.Finally we were curious as to whether the effusiveness of the apartment descriptions affected interest level.  Did it matter whether a listing's description emphasized the ""stunning"" view?  To find out we used the NRC sentiment lexicon (as implemented in Matthew Jockers's syuzhet package) to evaluate each listing on metrics like ""trust"" and ""positivity.""  However these seemed to have only a small effect in predicting interest.Our most elaborate attempt at analyzing the language of listings involved making a sparse matrix of the highfrequency words in the text then performing a logistic PCA to try to see whether there were any predictive linguistic patterns.  Again this seemed to have a minimal effect.  Most of what the description had to tell us it seems was already in the model.Our final model then was an XGBoost that included a) basic features like price b) mutated features like priceperroom and number of photos and c) engineered features including neighborhood designations and principalcomponent standins for various features.  This netted us a final score of 0.5625.  It's a solid figure though one much higher than the contest leaders.  We came up with some good ideas then but we still have a ways to go before we're machinelearning gurus. ",NA
Renthop Kaggle Competition: Team Data Jedys,37,https://nycdatascience.com/blog/student-works/renthop-kaggle-competition-team-data-jedys/,interest_levelinterest_levelinterest_level.interest_levelprice/roomprice/bedcreatedYYYYMMDD HH:MM:SScharacterPOSIXctweekweekdayhourfeaturesphotosaptIDfeatureaptIDphotofeatureCountphotoCountlisting_idIn order to extract numerical features from the description content we use Tfidf and apply logistic regression models to predict interest levels based on Tfidf vectors for each apartment. The probabilities of the predicted interest level would become new features in our main model.The above figure shows the workflow of our description feature extraction. After cleaning the text we vectorized all nouns and adjectives which generated an apartmentword matrix. We selected the top 1000 terms according to their weights as the input for our logistic regression models.In order to predict interest levels we split the training dataset into equal parts. We trained two logistic regression models with those two subsets and predicted each subset with the trained model on the opposite subsets. For the test dataset we train a new logistic model with the entire training dataset and predicted interest levels with the trained model. Now we can get the probabilities of our predictions giving us three more columns of probabilities of   and  interest levels for both training and test datasets. Finally we used  and  columns as new features in our main model. descriptionOur model selection process began with an analysis of which models were possible to use and which would be best given the nature of the task.  The project is a supervised classification problem. The training set contained nearly 50000 observations and over 100 columns.  Many of those columns were engineered from other columns. While this produced novel and useful information it also created multicollinearity.  One example of this was the calculated cost per bed and apartment price. Our final constraints were time and computing power.  The models indicated that a treebased model was our best option but we tested a number of different approaches.Models we experimented with include multiple logistic regression.  Despite regularization manual feature selection efforts and tuning this model did not produce useful results.  While we could deal with multicollinearity through regularization this forced us to abandon useful information. Support Vector Machines may have been a better option with adequate time and computing power but these algorithms proved too computationally expensive to be useful given our time and computing power constraints.We focused our efforts on trees when a simple random forest model outperformed our other models without serious tuning and feature selection efforts. Ultimately XGBoost proved to be the most effective model. Gradient boosted trees outperform random forest models because they partition the sample space to minimize a userselected objective function and according to userselected regularization parameters. This is different from random forest which selects a random subset of features for each partition. XGBoost outperformed GBM due to more flexible tuning parameters and more efficient processing due to parallel computing.We created a spreadsheet to organize our tuning process and a model to assist in parameter selection. Each team member submitted parameters we tested to this spreadsheet. We then trained a random forest model on that spreadsheet to predict which features might minimize logloss. This model quickly reached local minima and did not prove to be very useful. Going forward a similar model may be useful if we could introduce a randomness parameter. Finally we experimented with a neural network.  We trained a simple neural network using TensorFlow and Keras. Training took 60 hours which precluded any time for any tuning. The resulting model produced a logloss of 0.6. We then ensembled this model with our XGBoost model using the geometric and harmonic mean. Had our results been uncorrelated we may have seens significant reduction in logloss. While the geometric mean did produce a logloss score very close to our XGBoost model (0.57) we were not able to produce superior results using these methods. Once we had determined that XGboost would give us the best predictive accuracy we started tuning the model with a focus on decreasing overfitting for the model. We decided to do tuning manually while keeping a Google Spreadsheet containing parameters for each model along with the training validation and Kaggle logloss error. By doing manual parameter tuning we were able to take advantage of multiple computers and adjust in an more quickly something not possible when tuning parameters using grid search.The parameters we changed from default values in the xgboost package were eta gamma max depth column sampling per tree and subsampling.Eta or the learning rate affects the rate at which feature weights are minimized every iteration. A smaller eta will reduce the amount of overfitting but increase the number of iterations that are required. We found our best results with an eta of .01.The second parameter we tuned lambda controls regularization of the XGboost model.Gamma controls the amount of error reduction required by new branches in the decision tree. The default value for gamma 0 produces a model without any regularization. Increasing this value produces a model that overfits the training set to a lesser degree but does not necessarily minimize validation or test error. We found that a value of .175 for gamma was optimal for reducing the logloss of the test set.Max depth is a parameter that controls the number of splits a tree is allowed to have. A greater max depth value results in more complex trees that are better able to predict the training set but do not predict a test set well due to overfitting. A max depth of 7 produced a model with complexity without overfitting.Our column sampling by tree was optimal at 0.8 which means that each tree sampled 80% of the features randomly which serves to reduce overfitting and produce trees that do not require all of the features.Subsampling was the last parameter that we modulated. We ended up going with a subsampling of 0.8 meaning that each iteration used a random 80% sample of the training set. Like the other parameters we adjusted this served to reduce overfitting while still providing a model with accurate prediction.  ,NA,"This post is about the third of the four projects we are supposed to deliver at the  program. The requirements were:For this project your primary task is to employ machine learning techniques to accurately make predictions given a dataset. The framework will be through the lens of the . While the primary goal of Kaggle competitions is generally focused on predictive accuracy you will be expected to lead your audience through descriptive insights as well. For the purposes of your project you will aim to not only create a model that predicts well but also allow yourself to describe data insights drawn from exploration.Everybody from bootcamp cohort 8 was assigned the same project. In contrast to the first two projects though this was a team effort.  and  contributed to this project.The data at hand were apartment rental listings in New York City from the website . The training set for modeling consisted of 49352 observations and 14 variables:A test set  74659 records x 13 variables  was also available as well as a ZIP file consisting of ~700000 images that were used in the rental listings from the two datasets.The target variable was  which was divided into 3 categories: high medium and low. The goal in this competition was to ""predict how popular an apartment rental listing is based on the listing content like text description photos number of bedrooms price etc.""The training dataset was very imbalanced as there was a greater proportion of apartments listed as low  than at medium (3.1x) or high (8.9x).The histogram and density plot show hat lowerpriced apartments draw more highlevel interest than higherpriced ones.Apartments in this dataset are located all over New York City though the majority are located in Manhattan. This chart shows all the apartments plotted on a map with the color coding indicating the particular The map shows that most of the apartments that fall into the high  category are not located in Manhattan but in Brookly. The majority of the listings for Manhattan only received low interest. Taking into consideration what we learned from the density plot above this leads to the conclusion that the prices for apartments in Brooklyn are lower than those in Manhattan.The feature importance plot below shows the most important features for our best model. We will go on to describe how we created the various features from the columns already present in the dataset.After some initial data cleaning this histogram shows that the average price for an apartment in New York City at least for those listed on renthop.com seems to be $3032 (median: $2950).In the rental market price heavily influences level of interest. However a $2500 1br won't raise the same level of interest as a 3br for the same price. For a more ""applestoapples"" comparison we created a  and  feature. The number of bedrooms here varied from 0 to 4 and number of bathrooms from 1 to 7.The timestamp  came in the standard MySQL format . It was converted from  to  in order to be able to apply date arithmetics functions to in turn extract details such as  ...Although renthop.com is listed as a New York company the timestamp might not be in EST but in PST because the their servers seem to be hosted in San Francisco.Level of interest is also dependent on price with respect to location. A $2000 1bedroom apartment should lead to a higher level of interest in Hell's Kitchen than in The Bronx. However the original dataset only has lat/long information. This subway map shows the median rents for 1bedroom apartments inNew York City as identified by their closest subway stop for the years of 20152016.Because we were limited to the data set provided by kaggle.com we could not link the GPS coordinates or addresses back to information about the neighborhood a certain apartment is located in. We therefore used the density based clustering algorithm DBSCAN to create the neighborhoods ourselves. Given a set of points in some space DBSCAN groups points that are closely packed together (i.e. points with many neighbors nearby) and marks points as outliers if they lie alone in lowdensity regions. 2487 neighborhoods resulted from this clustering.Our intuition was that given the average rental price for a neighborhood level of interest on a listing is expected to be higher for listings priced below market average. Hence with the new set of neighborhoods obtained from DBSCAN we generated ""above market"" and ""below market"" features as follows:Because the aforementioned columns  and  of the training and test datasets were not strings but lists i.e. multiple values for each row we decided to omit both from our overall apartments dataset and created two separate data frames consisting only of  and  or  and  respectively. We ended up with 267906 features and 276614 photos for our training set (test: 404920 419598). From these 2 (or 4) datasets we created  and  columns for the main dataset in order to have some basic information about features and photos in there.From the abovementioned features we picked the top 20 most common and created socalled dummy variables i.e. columns that are coded {0 1} depending on if a certain feature was present or not. This allowed us to separate features to see which are most predictive.We analyzed all the photos that came in the 80gb ZIP file and extracted basic information from them:We then aggregated the above values for each apartment observation (i.e. ) calculating mean and median for:The images were also clustered using kmeans clustering. Unbeknownst to us at the time there are better ways to analyze images in a basic way.The text in the  column might yield interesting insights.  However as with free text entries everywhere format content etc. differ widely. We used sentiment analysis to try to get an idea how the description might be perceived by users of the website. This resulted in 10 new dummy variables conveying the strength of the following emotions for each description:These columns in our case contained values in the interval [0 .. 58] with higher values indicating a stronger presence of a particular emotion.Feature engineering was done by every member of our team. Some of us used R while others explored options in Python. This led to a fragmented codebase with various features being added from different team members and sources over time. We created an R script to make sure that the final data frame we used for our modeling could easily be reproduced. To achieve this we integrated all the various R code chunks and the results from the computations done in Python (csv files). The script once executed would then go on to create the data frames and the files for exporting these data frames based on the train or test input data and then create data frames files for the aforementioned separate photos and features data frames.There are a lot of explorations pertaining to image recognition that could still be explored to improve our model. Going further we could use image recognition to compare number of rooms listed with what is shown in photos. We also could investigate whether the presence of floorplans has any impact on interest level.20 features helped us get more accurate results; however they were very computationally expensive for the XGBoost model. A good way to mitigate this would be to create a secondary model using features to predict interest level and then use those results in the XGBoost model instead of all the features.",NA
Cleantech in the News: Scraping and Analysis of Online Articles,38,https://nycdatascience.com/blog/student-works/r-visualization/cleantech-news-scraping-analysis-online-articles/,continues to to advance with support from technological innovation sustainability projects financial incentives and political programs. Given the field’s large scope there is no shortage of media outlets covering the action. With the goal of tracking and analyzing recent cleantech news I developed a web scraping framework using Python’s  conducted natural language processing on the scraped data with Python’s  and visualized the results using R’s . is a leading provider of online media and research in the cleantech world with an indepth focus on renewables energy efficiency energy storage grid modernization green financing and environmental policy. I scraped 100+ online articles from the previous 3 months for the following information: is an annual ranking of the top 100 upandcoming companies in the cleantech community “most likely to have a big commercial impact in a 510 year timeframe.” The 2017 rankings were scraped from Cleantech 100’s website and the company names were used as anchors for text recognition in the Greentech Media articles.Python’s  framework is a fast and flexible web scraping method based upon the use of “spiders” (scripts with html parsing instructions) to gather online information and store it for further use. In this instance I used a specific  class called “crawl spider” created by defining start URLs a set of rules to inform the spider which links on the start page to follow and set of instructions (specific xpath references) on how to parse extract and save fields from the html at the destination page.Let’s take a look at the spider used to gather information from Greentech Media’s articles.With the scraped data stored locally I conducted text processing on the Greentech Media articles through the use of regular expressions word/phrase frequencies and sentiment analyzers from the Python framework. The following questions were explored:As illustrated below electric vehicles policy and energy storage are covered by 10 or more articles collectively making up about a third of all articles published on Greentech Media in recent months. Interestingly politics are usually of secondary focus on the website. Perhaps more policy coverage has been warranted lately given the current political climate and uncertainty around the Trump administration’s effect on environmental issues.Greentech Media’s readers have the ability to submit their own two cents on a newsfeedlike comments section. We can observe from the histogram below that although many articles go uncommented (far left bin of each facet) a healthy number of articles elicit anywhere from 10100 comments with some generating more than 150. That may be due to the fact that certain polarizing themes spark more comments than a more objective or nonpolitical topic. For example most articles tagged with “energy storage” are uncommented; most articles tagged with “donald trump” show ~60 comments.To get a pulse on the current “players” in the cleantech community I conducted text searching for groups of proper nouns in all the articles. Lists of cleantech countries people and companies were generated adhoc off of background knowledge and a list of cleantech startups as described earlier was scraped from the Cleantech 100 online website.Regular expressions were used to identify total word counts for each proper noun. Additionally using  sentences from all articles were tokenized and analyzed for the relative polarity and subjectivity of that sentence.  gives each sentence a numeric polarity score ranging from 1 to 1 with a value of 1 indicating highly negative content and a value of 1 indicating highly positive content (0 being neutral); it also assigns each sentence a subjectivity score from 0 (completely objective) to 1 (completely subjective). An average for each of these scores was calculated across all sentences containing at least one mention of the given noun.,NA, For each article  107 in total  the attributes (theme tags text body) were then stored as a Python dictionary in JSON format for further use. The 2017 Cleantech 100 companies were scraped using a very similar framework.Since sentiment analyzers are an imperfect product of their training library the choice of training library is an interesting tuning parameter albeit beyond the scope of this blog post. The default sentiment analyzer used by  and in this analysis is trained using the  library.,NA
Examining Billboard Hot 100 Lyrics from 1987 - 2016,38,https://nycdatascience.com/blog/student-works/billboard-hot-100-lyrics-1987-2017/,"The rankings are based on a formulaic approach not the subjective to the musical preferences of the individuals tasked with compiling the list. Airplay on roughly one thousand terrestrial radio stations are tracked to form the foundation of the ranking data. Nielsen provides song sales data for both digital and physical formats which are factored into the rankings. Most recently Billboard added music streaming data to be factored into the hot 100 chart rankings. My goal was to analyze the lyrics by year and find trends in the most popular words used . The Billboard Hot 100 chart data was scraped from  using a combination of BeautifulSoup and Regular Expressions.The twsift unofficial API for MetroLyrics was used to acquire the lyrics corresponding to each song entry in the Billboard Hot 100 charts. This API allows quick access to the lyrical content hosted by MetroLyrics with one major caveat  the song title and artist must be meticulously adjusted (removing non alphanumerical characters replacing spaces with '' and correctly identifying the title & artist) otherwise it wont return the correct lyrics.Click here for lyric scraping code Top 25 Words per Year 198820161989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016Wordcloud by year from 198720161987 19881989  1990 1991 1992 1993 1994 1995 1996 1997 19981999200020012002 2003  2004 20052006 2007 2008 2009 2010 2011 20122013 2014 2015 2016from os import pathfrom wordcloud import WordClouddef get_wordcloud_year(year): wordbag  words_by_year(year)words  remove_nonalphanum(wordbag) print 0 len(words)words  words.split() # Remove singlecharacter & 2character tokens (mostly punctuation) words  [word for word in words if len(word) > 2] print 1 len(words)# Remove numbers words  [word for word in words if not word.isdigit()] print 2 len(words)# Lowercase all words (default_stopwords are lowercase too) words  [word.lower() for word in words] print 3 len(words)#remove stopwords words  [word for word in words if word not in all_stopwords] print 4 len(words)#wordcloud  WordCloud().generate(words)# Display the generated image: # the matplotlib way: import matplotlib.pyplot as plt # plt.imshow(wordcloud) plt.axis(""off"")# lower max_font_size #wordcloud  WordCloud(max_font_size50).generate() (str(words)) plt.figure() #plt.imshow(wordcloud) plt.axis(""off"") #plt.show() print(len(words)) wordcloud  WordCloud(width  1000 height  750 font_path'/Library/Fonts/Verdana.ttf' relative_scaling  1.0 stopwords  all_stopwords ).generate(' '.join(words)) plt.figure(figsize(2012)) plt.imshow(wordcloud) plt.axis(""off"") plt.show()###Generates histogram of top 25 lyricsstopwords_file  './stopwords.txt'custom_stopwords  set(codecs.open(stopwords_file 'r' 'utf8').read().splitlines())all_stopwords  default_stopwords | custom_stopwordsdef get_wordfreq_df(year):wordbag  words_by_year(year).decode('utf8')#vocab.decode('utf8')#words_by_year(year) words  nltk.word_tokenize(wordbag) words  [word for word in words if len(word) > 2] words  [word for word in words if not word.isdigit()] words  [word.lower() for word in words] words  [word for word in words if word not in all_stopwords]fdist  nltk.FreqDist(words) d  Counter(fdist) word_df  pd.DataFrame.from_dict(d orient'index').reset_index() word_df  word_df.rename(columns{'index':'Word'0:'count'})df  pd.DataFrame(fdist.most_common(25)) df.columns  ['Words' 'Count'] df.sort_index(ascendingFalse).plot( kind'barh' x  'Words' title  ""Most Common Lyrics in: "" + year )",NA,"######Generate Wordcloud by year






























 A lot has changed in regards to popular music over the past 30 years but one theme stands the test of time  . Although in recent years its lead seems to be fading (though that may be an artifact of my data collection) is consistently one of the most frequently used words in popular musicThe most frequent words in the Billboard Hot 100 lyrics since 1987 are:I'mLoveDon'tLikeKnowOhJustGotBabyYeahWantYou'reCauseMakeTimeLetGirlSayWayComeI'llAin'tRightGonnaNeed",NA
"Hut, Hut, Hut, Scrape! Scraping 35 years of College Football Player Statistics",38,https://nycdatascience.com/blog/student-works/r-visualization/hut-hut-hut-scrape-scraping-35-years-college-football-players-statistics/,CFB QB Findings: ·High but not record high College QB Ratings led to the most successful NFL QBs Every QB·Drafted from 1985 to 2007 with a QB Rating of 150 or higher started 5 or more years in the NFL.·No strong correlation between College Passing Yards and NFL Success·Most Successful QBs averaged between 7 and 9.5 Yard per Attempt·Most Successful College QBs threw 7090 TDs in college·No strong correlation between College Interceptions and Success·No correlation between College Rushing Yards and Success but poor Avg. Yards Per Rush does correlate with poor NFL success CFB RB/FB Findings:·College RB/FB ended up at 8 different positions when they got to the NFL·>750 Rushing Attempts in College correlates with poor NFL careers or over 4000 rushing yards·Most Successful RBs/FBs average approximately 5 yards per carry·Rushing TDs do not correlate with NFL success·No correlation between receptions and success·Negative correlation between NFL Success and Receiving TDs CFB WR/TE Findings:·College WR/TE ended up at 9 different positions when they got to the NFL·Most successful WRs/TEs played 2040 games in college·Most successful WRs/TEs in the NFL had less than 100 total receptions·Most successful WR/TEs had <1250 total receiving yards in college·Most successful WR/TEs avg. 1020 yards per catch·Most successful WR/TEs had <10 receiving TDs in college·No correlation between scrimmage yards/plays and NFL success,NA,This serves as the next phase in building my NFL Draft Outcome Prediction Tool. Previously I collected 30 years of NFL Draft History and resulting player outcomes. Scraping college football statistics for those players provides more potential predictor variables for NFL Draft Outcomes. The difficult part is data is not available for all positions and some positions do not have as many years of historical statistics available (Source of statistics: http://www.sportsreference.com/cfb/). So the scope of the scraping effort was QB RB FB WR TE (19802017); K P (1990 to 2017). Here are some of the key findings from my tool:,NA
New York City Menu Items - Web Scraping,38,https://nycdatascience.com/blog/student-works/individual-menu-items/,On the map there are markers colored by the price of the item Green for items less than $10 yellow less than $15 orange less than $20 and red for items that cost over $20The map also provides the number of search results as well as the minimum and maximum price,NA,Restaurant reviews are extremely common and used frequently by users to find restaurants with good reviews near them. Yelp Urbanspoon and Zomato are just a few of the popular apps and websites that aggregate reviews. However these sites while incredibly useful and popular could do a better job of providing insights into individual menu items. What's good what's overpriced and where to find it. I wanted to create a way to search by menu item and see prices information and location. I scraped menus from the website Allmenus.com using Scrapy providing me with menu items prices and descriptions along with the locations and names of the restaurants. After data cleaning and removing lower priced items like beverages I was left with about 500000 menu items mostly contained to Manhattan. From there I created a map in shiny using R and leaflet that allows the user to filter by cuisine and more interestingly by ingredient and even cooking method.Clicking on a marker provides info about the item including the restaurant price and description.A heat map is also included allowing the user to get a sense for the density of items with the desired featuresAdditionally there is a table tab so users can see the database the map is pulling from. Using this they can sort by price the name of the item or restaurant or by location.Finally histograms of price are created dynamically following a similar coloring structure lower priced items in green and highest priced items in red. These histograms allow the user to gain insights into the distribution of prices as well as the mean and median. Users can also change the bin size manually.This application could be useful for potential restaurateurs allowing them to get a sense for individual menu items around the city. They could use this information to competitively price their items or find locations where their food is not well represented allowing them to take advantage of this information to better position their restaurant in the marketplace. It could also be useful for consumers allowing users to find menu items with ingredients they like across cuisines. They can also find restaurants near them that  they haven't tried yet but are intrigued by a specific menu item. In the future I would like to add the ability for users to rate individual menu items in addition to the restaurant overall as I feel this would be provide useful information to users about which items to get when they eat at a restaurant as well as allow for a recommendation system based on the reviews individual items.,NA
What I Learned From Scraping Every Single ZocDoc Doctor,38,https://nycdatascience.com/blog/student-works/web-scraping/analyzing-zoc-doc-doctors/,Internet startups have continuously replaced the herculean task of picking up the phone and calling an actual human with a few taps on the phone. Instead of calling a cab you tap a button and one arrives. Instead of calling for late night pizza tap a few times and the order is on its way. Founded in 2007 ZocDoc allows patients to avoid the horror of a phone call and book their doctors appointment with a few taps on their phone.As a lazy millennial I’ve always appreciated ZocDoc’s convenience and use it for the majority of my medical booking. ZocDoc allows me to book doctors based on other patient’s reviews easily reschedule or cancel and fill out forms beforehand by my appointment.Curious about the size of their business the best doctors and basically everything about their service I set to work web scraping every single doctor on their site. After spending 95 hours scraping approximately 1.2 million doctors on their site I discovered that most of doctors on their site are placeholders. See for example . ZocDoc has populated its site with placeholder profiles via data from the American Board Of Medical Specialties. After filtering out these placeholders there are 47363 doctors on their site. Let's dig in. Following the logic used  given about 47k doctors on their site and ZocDoc's annual fee of (as of 2016)   $3000/year annual fee from doctors that use its service we can roughly estimate their yearly revenue to be about 141 million/year. In 2015 in a Series D funding round  $130 million at a $1.8 billion valuation. Thus following the 2015 valuation they were valued in 2015 at 14x of their current revenue. ZocDoc was started and is headquartered in New York City.  So not surprisingly New York state is the home to the most ZocDoc doctors with 13053 followed by Texas with 4569 and California with 4454. The top five states for ZocDoc doctors (New York Texas California Florida and New Jersey) account for 60% of the doctors on ZocDoc. New York city  Brooklyn and the Bronx account for 15% of the doctors on Zocdoc. Other major cities include Chicago Houston and Washington D.C.Doctors By CityOn the night of Feb 11 I collected data on doctor availability for the next fortyfive days  (which includes 32 weekdays) of all the doctors on ZocDoc. The median ZocDoc doctor has 107 appointment slots available in the next 45 days or about 2.3 a weekday. By comparing the first names of doctors against the most common U.S. baby names from 1951 to 1992 (25 yr old to 65 yr old a rough range for when doctors were born) and filtering for first names with at least 20 doctors on ZocDoc  we see names that are more common with doctors vs  the general U.S. born population.  The top five names are Russian: Dmitry Inna Yelena Alla and Igor. Perhaps not popular baby names during the Cold War.,NA,ZocDoc has 1.88 million total reviews.A full review consists of text and a rating in three rating categories: overall bedside manner and wait time.  For overall reviews 85% are fivestar reviews  9% are fourstar reviews 2% are threestar reviews 2% are twostar reviews and 2% are onestar reviews.Bedside manner reviews following a similar distribution.While still mostly positive wait time reviews are a little less positive with more reviews under 5 relative to the other review categories.So ZocDoc's review system appears biased towards positive reviews. Why?One explanation is ZocDoc patients are really happy with their Doctors. Patients have just given good reviews and that's that.Another explanation could be their moderation policy. ZocDoc requires patients to visit a doctor in order to review the doctor and all reviews are moderated.  reviews if they contain profanity personal information pricing specifics accuracy of treatment or diagnosis info and promotional content. This moderation criteria — particularly profanity pricing specifics and accuracy of treatment — seems biased against negative reviews. If you swear at your overpriced doctor who misdiagnosed you your review will not make it passed their moderation.Another explanation is ZocDoc or doctors are artificially inflating their positive reviews with fake positive reviews. Fake reviews are combatted through moderation and requiring patients to see a doctor in order to review a doctor. However fake reviews are common across the internet and fake reviews are certainly possible by a determined actor.ZocDoc has a financial incentive to keep their reviews mostly positive. The doctors pay them to use ZocDoc. If a doctor gets mostly negative reviews then they will be unhappy not get patients through the service and eventually leave.Of all those reviews 1.19 (64%) million do not have any text. This leaves 36% or 685k reviews with text. The text reviews follow a similar pattern the nontext reviews. The vast majority of the overall ratings for reviews with text 88% (599779)  are fivestar reviews.  Only 2% (14672) of text reviews have one star.We can look at some of the most common bigrams twoword combinations  to identify patterns in the top five and one star reviews. These bigrams have been filtered for common words to reveal interesting words.Below are some of the top bigrams. Patients like doctors who answer questions take time make them feel comfortable with friendly staff.The following are the most common bigrams from onestar reviews. People seem to dislike doctors who keep them in waiting in wait room and waste time. Again these top bigrams are most likely biased by ZocDoc's moderation policy which prohibits reviews that complain about price or accuracy of diagnosis.ZocDoc is the home to many different types of Doctors. The most popular doctor is a dentist (14% of all doctors) followed by an internist (9% of all doctors) and a family physician (7% of all doctors). Some ZocDoc doctors list the languages they speak on their profile page. After English which is by far the most popular language spoken by doctors here are the other languages ZocDoc doctors speak in U.S.,NA
2016 in songs on Germany's most popular radio station,39,https://nycdatascience.com/blog/student-works/web-scraping/year-2016-songs-germanys-popular-radio-station/,looking for a unique and interesting subject matter selectsubmit button?hour&date24 * 366  8.784LastName FirstNameLastNameFirstName,NA,"This post is about the second of the four projects we are supposed to deliver at the  program. The requirements were:After I decided to base this project on the songs that were played throughout the whole year 2016 on Germany's most popular radio station .Code and data can be found  while the app itself is .SWR3 is part of the regional public broadcasting corporation Süwdestrundfunk (SWR ""Southwest Broadcasting"") servicing the southwest of Germany (Source: ). On their website  they offer the possibility to  on any given day at any given time.Time and day can be selected by two dropdown filters on the top of the page both implemented as  elements. By clicking the  the page is reloaded with two parameters added to the URL in the format: . This makes it especially easy to navigate through a given date range and get all the songs played in this range.From each page for each hour of a day it was then possible to get each date time artist and title that were played during this hour. To retrieve the content of these elements I used the Python package  and employed CSS selectors to extract the elements one by one. The four elements describing one entry were saved as a dictionary while the data for the whole hour was saved as a list of dictionaries which in turn was appended to an overall list which stored data for the whole execution of the script usually a month at a time. The data was then written to a CSV file.I ended up making  HTTP requests to the SWR3 playlists page one for each hour of each day of the leap year 2016. I did this in blocks of months for two reasons: breaking down the overall scraping time into several chunks and trying to not get banned from the webserver. I ended up with 12 CSV files  one for each month  which were combined into one large CSV file consisting of 113174 rows and 4 variables:After some date/time arithmetics I ended up with the final CSV file consisting of 113174 rows and 16 variables:Most of the variables are straightforward. However I would like to explain two of them in more detail:The data was in very good quality so that I had to only some light cleaning. Usually the artist is stated on the website as . Sometimes however it might be listed as . The reason for this is unclear. I used regular expressions to catch each of the occurrences of the latter form and then converted them into the one mentioned first. When a song had two or more artists this substitution was applied to each of the artists.Also some hours might be missing songs especially when the song count for an hour was much lower than the average of 12.8. This might be due to an error in webscraping but might as well be due to special programming of the radio station.Up until now I was working in Python for scraping the data and computing the date/time arithmetics. For creating data aggregates for the final visualization I switched to R.Based on the CSV file mentioned above the following aggregates were created:All of these aggregates are calculated on the fly because the user is able to filter the underlying data source using up to 8 of these filters:For this project I again turned to  which is a ""web application framework for R [to] turn [...] analyses into interactive web applications"" and the  dashboard package. This makes the data and results more approachable and interactive then just having it all in a rigid report such as in a PDF or PPTX format.The start page of the SWR3 Song Explorer features key insights into the data such as the date range of the data loaded how many songs were played in that range which song and artist were played the most etc.It is supposed to give a quick overview about the data at hand.The table could be seen as the centerpiece of this application. It is sorted in descending order by play count meaning the most played song is on top. The sorting can be changed by the user.Next to the artist name and title of the song it also shows the first and last time the song was played on this radio station and  most importantly  how often.This table as well as all the other charts under Songs which are described below can be filtered by up to 8 filters hidden in the collapsible box right above the table/chart.The calendar shows how many songs were played on any given day of the year. While not yielding much information when no filters are applied  basically every day the same amount of songs are played more or less  it might be interesting to take a closer look at this chart when only filtering for artists or titles.This is a rose diagram which should be read as a 24h clock. For each hour in a 24 cycle it shows how many songs were played. When not filtered by day it shows the accumulated song count played during this hour of every day.For this chart it is the same as for the calendar: it might be more interesting to take a closer look at this chart when only filtering for artists or titles.The histogram gives an interesting insight towards the distribution of songs on this radio station.While the vast majority of songs are played between 1 and 15 times over the course of the year 2016 there are some outliers which are played much more often up to the most played song ""X Ambassadors  Renegades"" being played 35 times more often than the mean (21.6) or even 318 times more often than the median (2).While it is interesting to learn about song statistics taking a look at the artists might not be a bad idea.This chart shows in decreasing order the most popular artists in terms of distinct songs played. The top artist Bon Jovi had 41 of their songs played with the mean and median only being 2.3 and 1 respectively.The word cloud very fashionable in info graphics theses days shows the most used words in song titles played on SWR3. Each song is only counted once in order to not take popularity into account.The word cloud has two more special filters: the amount of words shown can be changed and words to be filtered out can be specified by the user. The cloud is only updated when the apply button is pressed. Because the mascot of SWR3 is a moose this word cloud gets generated in a moose shape when rendered for the first time. The shape can be changed to represent a circle.There were some interesting findings in the data. First of all I was amazed by the sheer amount of songs that are played during the course of one year: 113173. Giving it a bit more thought it makes sense though: ~13 songs per hour over the course of 366 days gives a close estimate.Also very surprising was the distribution of songs. While I had a feeling that some songs were played more than others  which was actually the main reason I analyzed the data in the first place  I was surprised that there indeed seems to be a kind of ""hot rotation"" going on: only 226 songs are played more than 100 times (509; 50) while the rest of the songs  5021  are played less than 100 times (4738; 50) with 1985 of these songs only being played exactly once.The same goes for the artists: only 68 artists have 10 or more of their songs played the rest  2188  have less than 10 of their songs played with 1406 artists having exactly one of their songs played.As stated above code and data can be found  while the app itself is .",NA
Recommendation System and Spam Review Analysis,39,https://nycdatascience.com/blog/student-works/recommendation-system-spam-review-detection/,I. IntroductionII. Web Scraping on Google App StoreIII. Recommendation SystemIV. Spam Review AnalysisV. SummaryWhat's More:References:,NA,"Contributed by Xu Gao. He is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between Jan 9th to March 30th 2017. This post is based on his web scraping projects.  See the IPython notebook at:https://github.com/RayyGao/RayyGao.github.io/tree/master/Xu%20Gao%20WebscrapThis post introduces the recommendation system and spam review analysis based on the reviews from Google App Store Top Charting apps.  Google App Store is the biggest Android app store which not only has numerous applications but also movies and songs. In this project I scraped the apps listed in the Top Charting page. (https://play.google.com/store/apps/top) Scrapy is the key tool in this project and I will introduce the detail in Part II.The recommendation system is based on algorithm ""doc2vec"" developed by Google geeks in 2013 which helps transfer the text to numeric vector. Then I use cosine similarity to find the matched reviews and apps.The spam review is based on ""Sentiment Analysis and Opinion"" from Bing Liu published in May 2012 in which he specifies the different types of spam reviews. In this project due to some limitation I select spam reviews manually and perform an analysis on these results.source:https://play.google.com/store/apps/topIn this web scraping project the item structure is listed in the following table:where Top Charting is categorical variable which includes Top Free in Android Apps Top Paid in Android Apps Top Grossing Android Apps Top Free Games Top Paid Games Top Grossing GamesAnd in the source page there are 3 layers for scraping. I first use the basic parse function to parse the source page. Then through the ""See More"" button to get the next page which locates the right upper corner in the screenshots. Then through the picture of each apps I finally get the target pages to get the reviews of users.Source Code: Scrapy spider.pyFinally I got 8972 reviews from these apps. To show which words have more importance I plot 3 WordCloud from 3 TopCharting categories: Top Free in Android Apps Top Paid in Android Top Grossing Games.Natural Language Processing is one of the most important topics in Machine Learning problem. There are several choices about algorithms to detect words such as term frequencyinverse document frequency(TFIDF) LDA and LSI. What I used in this project is ""doc2vec"" algorithm which was developed by Google geeks in 2013 and free in ""genism"" library in Python.This algorithm can transfer text to numeric vector. Then I use cosine similarity to calculate the similarity between the input vector and every vector in the dataset to find the most similar reviews.First we can have a test. Try to find top 3 single words in all reviews which is similar to ""game"" and ""great"" but different from ""bad"".The result is ""app"" ""job"" and ""love"" with their similarity. Looks not bad! Word ""app"" is quite similar to ""game"" in this dataset and ""love"" means positive attitude which is opposite to ""bad"".Then I write a input ""great puzzle game!"" After calculating the best similarity the algorithm generates the following review.which is quite good. ""Block! Hexa Puzzle"" is obviously a puzzle game. And in this review the user shows a positive attitude. This means this game is at least not bad.But before I continue I notice the similarity is only 0.7963. This makes me doubtful on this problem. Although the first example is cool what about other inputs? So I try another input ""Great RPG Game"". This time I get totally different result.""My Talking Angela"" is a virtual pet game like ""My Talking Tom"". It is not a typical roleplaying game at all. So there must be some shortcomings or mistakes on this recommendation system.In my opinion there are three possible reasons leading to this bad result. The first one is the small dataset. 8972 is not a big number for a review dataset. When people usually do the research about reviews 30000 might be the minimum because of the complexity of natural language. The second reason is spam review. The recommendation system is based on the assumption that all the reviews in the dataset are real and effective. However sometimes people might post biased reviews or fake reviews due to some private reasons. This will make the review dataset unreliable.  The last reason is about the algorithm shortcoming. Generally speaking natural language processing cannot be 100% correct. Since it is based on the training set and labels which are set manually the accuracy of this algorithm is quite uncertain.According to the Book ""Sentiment Analysis and Opinion"" from Bing Liu in the University of Illinois at Chicago the spam reviews can be divided into two types: real spam reviews(I called trash reviews) and fake reviews. The trash reviews mean that due to some reasons comments in the reviews are biased from the real attitude of users or comments do not show any emotion or evaluation which makes those comments spam. The fake reviews are just like their name. They are created by someone due to profit or other purposes.The best way to detect fake reviews is from the user basis. The duplicates will show this review is highly possibly made.(P.S. In my dataset I remove the absolutely same reviews) But the definition of trash reviews is still not clear.  So I use NLP to do the sentiment analysis and compare with the rating and try to do a logistic regression on several factors related reviews on ratings.From this boxplot we can find that sentiment and rating are correlated in mean. But the data out of the 25% to 75% quantile is in big quantity.The Logistic Regression Detail is listed below:´Rating > 3: Positive´Rating <3: Negative´Factors on this regression:´1. Length of reviews; 2. date index; 3. Number mentioned brand/company name; 4. Compound Sentiment; 5. Negative Sentiment; 6. Neutral Sentiment; 7. Positive Sentiment´Result:´Accuracy On Training Set: 69.9%´Accuracy On Test Set: 41.8%From these two facts we can find that the rating and sentiment are not truly correlated in this dataset although they are supposed to be. As a fact I decide to define the trash review as the review when absolute value of difference between rating and adjusted sentiment(15) is larger than 3.abs(Adjusted SentimentRating)≥3Based on the review's app category I make a pie chart to show what kind of apps have high spam reviews. In this graph we can clearly see that Casino apps like card games or other betting apps have the most trash reviews. Then Action and Strategy apps are next.Another topic is about the fake reviews. Based on the user side we can divide duplicate fake reviews as 4 types:In this dataset I try to find the duplicates and find the percentage of duplicates 19.77% approximately.click to seeThis is the pie chart on category of duplicates reviews. We can see that Casino Action Strategy Casual are still Top 4. But in this time Puzzle game gets more duplicate fake reviews.In this project I use Scrapy to scrap the reviews from Google App Store make a recommendation system and do a manual analysis about spam reviews. There are several result we can learn from this project. For all top apps people downloads ""game"" is the top one word. Second""doc2vec"" is a very useful algorithm to treat text to numeric vectors. Third one is how we can define and detect trash reviews manually. And the conclusion from all the techniques above is ""Casino Apps are not reliable.""",NA
"Analyzing Chipotle Pricing, Food Shortages, and More",40,https://nycdatascience.com/blog/student-works/analyzing-chipotle-pricing-food-shortages/,Chipotle has two ongoing food shortages: pork in the United States and chicken in Canada. Some Chipotle locations serve pork from the UK that doesn't meet all of their food standards. This message is displayed when a user visits the page:For simplicity when focusing on different prices we will just look at steak burritos. For the most part menu items prices change together so a model for steak burritos should match other burrito prices.There is a clear regional differences in price especially in New York.Prices also vary based on housing price population and other factors. ,NA," Undeterred by the occasional ecoli and frequent gastrointestinal marathons Chipotle’s super fans religiously eat their burritos.There are the fanatics.  ate Chipotle 425 days in a row (and is in better shape than you).  pledged to eat Chipotle every day for Lent and then continued eating Chipotle for 186 days.  gobbled down Chipotle for at least 109 straight days.Then there are the more casual devotees. Chipotle’s offered rewards to their addicts with their summer Chiptopia promotion and had to  over $20 million.As a frequent eater of Chipotle in New York I’ve always noticed that the price of burrito is much less than Chipotles in my hometown in the Chicago area. There is some on the web about Chipotle price variance but there hasn’t been a rigorous look at it.Curious about price differences and with the super fans in mind I decided to take a deep dive into Chipotle data.With some help from a couple undocumented APIs with info on Chipotle online ordering prices and food shortages I’ve been able to get an almost complete set of data on Chipotle restaurants. (I've pricing data f0r 2176 Chipotles. The data was collected on Jan 9 2017. Their Q4 2016 reports states they operate 2250 restaurants.) I’ve complemented this dataset with Yelp rating data ( as of Jan 27 2017) sale tax data cost of living data US Census data and CPI data.Burritos burrito bowls and salads of the same meat/nonmeat  —  the different forms of the burrito — always have the same prices at each restaurant. However across restaurants menu item prices vary geographically and based on meat/nonmeat type within the United States and internationally.In the U.S. Chipotle generally has three different prices at each locationThere are 11 pretax carnitas/chorizo prices 11 pretax chicken/veggie/sofritas prices and 9 steak burrito prices. The majority of Chipotles in the U.S. sell burritos at the lowest price: 58% charge $6.95 for carnitas/chorizo and $6.50 for chicken/veggie/sofritas and 59% charge $7.50 for steak/ barbacoa.      Below is a map of the median steak burrito price by U.S. county.The Midwest Mountain West and South generally charge the lowest price $7.50 for a steak burrito. Some exceptions: Chicago Midland and Odessa Texas and Cincinnati all charge $7.95.Higher prices appear in the Northeast. New York City's steak burrito costs $9.19 the most expensive burrito in the U.S. Long Island and Southwest Connecticut charge $8.25 for a steak burrito.The West and North West also have some high prices though not as high as New York. San Francisco has the second most expensive steak burrito at $8.75. Silicon Valley and the general area around San Francisco has an $8.25 steak burritoChicken and pork burritos generally follow the patterns of the steak burrito.Internationally in Canada and France Chipotle prices for steak chicken and pork items are all the same. Canadian burritos cost C$8.95 or $6.88 U.S. dollars making them the cheapest pretax Chipotle burritos in the world. French burritos cost €9.3 or $10.01 making them the most expensive pretax Chipotle burritos in the world.  In the UK Chicken burritos cost £6.70 ($8.70) while Carnitas and Steak burritos both cost £6.95 ($8.68) .The pork shortage is an issue in some parts of the Midwest South and Northeast.Fourteen of seventeen (82%) locations in Canada have a chicken shortage. These locations display the message ""FYI This restaurant is currently serving conventionally raised chicken. We'll be back to our unconventional ways ASAP.""In order to determine the best Chipotle in the world I gathered Yelp ratings and sales tax data to complement the Chipotle pricing data. My best Chipotle would have the lowest price while accounting for sales tax along with a high Yelp rating.Based on a review on all of the Chipotles I've determined that the Chipotle in Billings Montana on 1601 Grand Ave is the best.Here's why:Of course the best chipotle depends on your subjective preferences. You can explore this further in the ""Where Is The Best Chipotle For Me?"" section of .So why do these Chipotle prices vary?I've constructed a multiple regression model using the following variables:The model has an adjusted RSquared of 0.753 and appears to be an ok fit for the data.After spending a long time looking at Chipotle data I've learned a lot of random Chipotle facts.Here they are:You can view my code for this analysis in the following repo: ",NA
Pathway of Hope - The Salvation Army's solution to break the cycle of poverty,40,https://nycdatascience.com/blog/student-works/pathway-hope-salvation-armys-new-approach-towards-breaking-cycle-poverty/," Poverty is not an affliction of thImportantly this increase reflects a radical shift in the face of poverty over the last 50 years. In that time .the United States has experienced a disproportional increase in both single parent households and children who reside in poverty.For over 150 years The Salvation Army has sought to combat hunger and meet the need for those in poverty. The Pathway of Hope initiative was introduced to Eastern Territory of The Salvation Army beginning in 2016 and seeks to be a real solution to help families break out of the perpetual cycle of intergenerational poverty.Pathway of Hope is targeted and intensive case management to assist families striving to break free from intergenerational poverty. The Salvation Army forms a crucial partnership with families in need. Families participating in the program possess the desire to change their situation and are willing to share accountability with The Salvation Army for planned actions. Through achieving increased stability these families find a newfound hope propelling them forward on their journey to This analysis takes a deeper look at the initiative running in over 25 local communities within The Salvation Army’s Eastern Territory. The intake process individually evaluates a family in crisis and identifies custom and critical goals ranging from securing employment to finding affordable childcare.Poverty in the U.S. is an epidemic – including one in five children according to the latest Census figures. Children who live in poverty for half their childhood are 32 times more likely to remain in poverty (according to The Urban Institute). The Salvation Army works with many of these families by addressing immediate needs. Pathway of Hope is the next step for  to help them break the cycle of poverty.After a series of conversations with individuals at the Salvation Army's Eastern Territorial Headquarters I received data sets relating to the Pathway of Hope (P.O.H.) with the intention to analyze the current effectiveness of The Pathway of Hope program. Unfortunately the data currently collected by the Salvation Army is insufficient to quantify and rigorously answer this very important question ""Is the Pathway Of Hope effective?"" Upon further inspection the data shows two things:The data in this program is gathered from 26 separate locations with differing computer systems varying structure for inputting client data. Also in its current state covering 26 geographies the Pathway of Hope initiative has been running for only a little more than 1 year. These factors played a major role in the  consistency validity and integrity of the data collected but these hurdles were ultimately overcome by adjusting the scope of the analysis. Rather than attempt to analyze the effectiveness of POH as a whole I analyzed the distribution of Client Goals and their relationship to various factors such as race gender geography and household formation. str(Poh) 'data.frame': . of : : int 14 15 16 17 18 19 20 20 20 21 ... : Factor w/ 134 levels ""1/10/2017""""1/11/2016""..: 68 68 68 68 68 74 74 73 73 74 ... : Factor w/ 30 levels """"""1/12/2017""..: 14 14 14 14 14 18 18 18 1 18 ... : Factor w/ 26 levels ""POH EPA Carlisle(164)""..: 13 13 13 13 13 11 11 11 11 11 ... : Factor w/ 3 levels """"""No""""Yes"": 3 2 2 2 2 3 2 2 2 2 ... : Factor w/ 5 levels """"""Female Single Parent""..: 5 5 5 5 5 2 2 2 2 2 ... : Factor w/ 185 levels """"""1001""""1013""..: 118 118 118 118 118 161 161 62 61 161 ... : Factor w/ 490 levels """"""1/1/1971""..: 333 372 431 401 123 19 442 442 442 427 ... : Factor w/ 9 levels """"""American Indian or Alaska Native (HUD)""..: 9 2 2 2 2 4 4 4 4 4 ... : Factor w/ 3 levels """"""Female""""Male"": 3 2 3 3 3 2 3 3 3 2 ... : Factor w/ 6 levels """"""Client doesn't know (HUD)""..: 6 5 5 5 5 6 6 6 6 6 ...: Factor w/ 11 levels """"""Data not collected (HUD)""..: 1 1 1 1 1 1 6 6 6 6 ... : Factor w/ 70 levels """"""0""""1000""..: 13 1 1 1 1 62 1 1 1 1 ... : Factor w/ 9 levels """"""No exit interview completed (HUD)""..: 4 4 4 4 4 5 5 5 1 5 ... : Factor w/ 6 levels """"""Black or African American (HUD)""..: 6 5 5 5 5 2 2 2 2 2 ...Upon deeper inspection of the Poh data it became apparent that the data was filled with blank incorrect and duplicate values. Of the initial 665 clients 102 clients were duplicated in the data set between 25 times and among duplicate entries the Household.Type Entry/Exit Dates Race and Head of Household entries varied dramatically. This left me with the task of filtering out duplicate entries and trying to determine which entries were a result of the case worker having difficulties inputting information into the database and duplicating the entry or creating a new entry to be used as a placeholder in the database system. Due to the limited amount of unique data for the child clients I focused this analysis on the clients with the attribute Head of the Household  ""Yes"" which corresponds to analyzing the clients on a per family basis. The result is a narrow narrow pool of 148 clients in this analysis hardly sufficient to determine the success of the program but useful to determine areas of need and perhaps influence the deployment of services by the Salvation Army.256713740str(GC)'data.frame': . of : : int 14 14 14 14 14 14 14 14 14 14 ... : int 58 58 58 58 58 58 58 58 58 58 ... : int 4 9 14 19 24 39 51 56 57 67 ...: Factor w/ 17 levels ""Case Notes""""Child Care""..: 8 9 5 7 10 7 16 6 13 8 ... : Factor w/ 89 levels ""Access legal aid""..: 18 4 21 72 54 62 43 34 15 18 ... : Factor w/ 183 levels ""1/11/16""""1/13/16""..: 96 96 96 96 96 143 143 135 135 138 ... : Factor w/ 3 levels ""Closed""""Identified""..: 1 1 1 1 1 1 1 1 1 1 ... : Factor w/ 5 levels """"""Abandoned""..: 3 3 3 3 3 3 3 2 2 5 ... Goals by locationPlease note the top charGoals By Household FormationGoals by GenderGoals by RaceAnalysis of Goal Classification EconomicEducationHousingHealthEmploymentHouseholdNecessitiesLegalOtherChild Care.As a result of removing duplicate and erroneous data the resulting pool of data is unfortunately too small to draw conclusions to apply to the entire population of American families in the cycle of poverty. However I believe that the results of this analysis can be useful in assisting the Salvation Army in fundraising and optimizing the deployment of their assortment of services.  Upon sharing preliminary results with the Salvation Army I was informed that my influence contributed to overhauling the data collection methods and policies for the Pathway of Hope initiative in addition to updating their database infrastructure. In the coming months I plan to continue my partnership with the Salvation Army to add additional data as it is gathered and hopefully deliver useful analysis to help further the mission of .",NA," 
  The data from the Salvation Army came in two separate .csv files below is a brief overview of the two data sets Poh (Pathway Of Hope Main data set) and GC (Goal Classification data set)
























The Goal Classification data set has 487 individual entries ranging from 1 to 21 goals for 95 unique clients. Fortunately most of this subset of Pathway of Hope clients are nearly all Heads of Households so after merging the two data sets(outer join by  ""ClientUid"") the .Below is a table of the 25 Salvation Army Corps that function as the main point of contact for the Pathway of Hope clients.Below is a table of the 12 Goal Classifications that categorize each of the POH client goalsEPA Carlisle has by far the most goals compared to any other Corp participating in the Pathway of Hope.t depicts the number of family goals grouped by the dropdown menu (in this case EE.Provider  Location) the bottom chart depicts the same chart as above but it is additionally filtered by the 11 Goal Classifications so the impact of each goal can be determined.The distribution of goals heavily skews towards single female parent households followed by two parent households at a distant second.The vast majority of families have a single female parent household so it seems natural that the number of female goals is similarly much higher than the number of male goals. Interestingly when filtering by the Goal Classification ""Child Care"" the result was only womenThe majority of families fall under the ""while"" primary race by a large margin but when filtering by the Goal Classification ""Child Care"" the Black/African American families have the highest need.The above Rose area chart depicts the difference in quantity of goals between each of the 11 Goal Categories. Unlike a traditional pie chart each 'wedge' is formed with the same angle so larger categories form 'longer' wedges rather than smaller categories. Most common Goal Classifications from highest to lowest: and",NA
Tracking Exercise Trends with NHANES,40,https://nycdatascience.com/blog/student-works/tracking-exercise-trends-nhanes/,The National Health and Nutrition Examination Survey (NHANES) is one of the foremost assessments of health statistics for children and adults in the United States. Sponsored by the Centers for Disease Control  combines interviews with physical examinations and laboratory tests for approximately 5000 Americans each year. Results are compiled anonymized and made publicly available at the program’s website on a rolling basis. These datasets are a major source of information for further studies ranging from simple national averages for physiological measurements (height or weight) to epidemiological trends for public health policy reform.The 20132014 NHANES publication is made accessible through a  application providing an interactive environment for users to explore data trends. Because of the wide range of information gathered by the survey the app focuses on a subset of the findings drawing particular attention to the prevalence of exercise and associated health outcomes across demographic groups.The app is meant to be used as a starting point to explore highlevel trends in the latest NHANES survey publication. It is recommended however that the program's  be followed in order to draw any robust statistical conclusions from the underlying sample data.The “Overview & Demographics” tab allows users to explore this question. NHANES appears to be a nationally representative sample of the U.S. population across gender education level and income. However certain subpopulations are more heavily represented such as youths under 18 still undergoing much of their physiological development and at therefore at higher focus for many public health concerns.The “Explore Exercise Trends” tab provides exploratory visualization of who is getting exercise in what form and amount and the possible health outcomes associated. Density curves help to illustrate exercise trends and confirm preconceptions about some of the grouping (i.e. demographic) variables involved in those trends.Below we observe that with each drop in level of education – from college/advanced graduate down to high school dropout – the density curve’s average moves from left to right. This implies a direct inverse relationship between minutes per day conducting “vigorous or moderate” physical activity at the workplace i.e. manual labor and education.Minutes per day of physical activity – whether in the workplace recreationally; vigorous moderate or otherwise – are used as the predictor variable in a scatterplot to explore correlations with a variety of health outcomes such as weight BMI cholesterol or blood pressure.Above we observe a clear negative correlation between resting pulse (bpm) and average minutes per day of vigorous recreational activity. Through the use of the interactive graphing package  we gain further information from mouseover tooltips in this visual: among participants getting an average 180 minutes of exercise per day none had a resting pulse of over 98 bpm.Other relationships explored such as cholesterol levels versus exercise habits highlight the limitations of taking a bivariate approach in predicting health outcomes. For example we might assume a negative correlation between amount of exercise and LDL cholesterol levels but in the case below we observe this correlation to actually be positive. This suggests the study could benefit from other known associations to cholesterol such as diet (perhaps individuals with moderateexercise jobs tend to have highercholesterol diets) and genetic disposition.As alluded to in the previous section inclusion of more variables from the original NHANES dataset and the use of formal statistical methods could help to tell a more comprehensive story about the program data. Chief among these would be an exploration of dietary/nutritional and lifestyle habits (e.g. smoking drug use) since NHANES has collected that data for each participant. As NHANES is an ongoing and continuously developing program there is further potential to track the movement of these health trends over time in the general U.S. population.,NA, NHANES divides results into many separate tables for more manageable file download sizes. The app makes use of the following datasets (including but not limited to):The common field across tables is the  a unique identifier for each survey participant which can be used for joining operations across any/all tables. Let’s take a look at the initial raw data import of many NHANES tables and an example  leftjoin operation to quickly aggregate information by survey participant.,NA
Your Price May Vary: US Hospital Charging Comparator,40,https://nycdatascience.com/blog/student-works/price-may-vary-us-hospital-charging-comparator/,In the State Overview tab users can select the year and DRG then the distribution maps and histograms will show the update result. The same data is also displayed as ordered bar charts with corresponding discharges. And the results are also listed in the table where users can filter to see charges of all DRGs in one state.The DRG Comparison displays boxplots of average charges and payments for selected DRGs in selected year. And the average charges and medicare payments seem to increase as complication increases for example intracranial hemorrhage w mcc charges twice as much as intracranial hemorrhage w/o mcc.Year trending of selected DRG in selected hospital is also included in this tab users are able to identify the changes in both hospital charges and Medicare payments in certain hospital.,NA,The Centers for Medicare and Medicaid Services (CMS) released  for the 100 most common diagnosis and treatments for every hospitals in the country treating Medicare patients starting from 2011. According to the U.S. Department of Health and Human Services (HHS) the release of these charge data is a piece of the threepart initiative to increase transparency in the delivery of health care in the United States encourage copetition and provide consumers with more purchasing power. In attempt to help patients better explore the comparative price of procedures and estimate their medical services prior to receiving care I implemented a Hospital Charging Comparator with RStudio's Shiny web application framework.You can try out the application .CMS provided annual data from 2011 but 2014 was the first year for which data was available for all diagnosis groups. So I filtered the 100 most common diagnosis and treatments in 2014 and the application covered inpatient hospital charges and medicare payments of 112 procedures in 3137 hospitals from 2011 to 2013 across the nation.The application has 5 sections:The graphs show immense variation of average hospital charges for a given DRG across the US. For example the average charges for Major Joint Replacement within in Massachusetts is $ 34742  for 11333 discharges whereas the average charges for the same DRG in California is $ 90207 for 32930 discharges.The Hospital Comparison allows users to compare the average charges and medicare payments of selected hospitals in selected year. There is no limitation for the number of selected hospitals but some hospitals did not provide data for all categories of procedures. The results are displayed in bar charts and apparently there are wide variations between hospitals even in the same geographic area. For example in the New York metro area COPD (Chronic Obstructive Pulmonary Disease) at Bayonne Hospital Center part of a chain called CarePoint Healthcare comes in at $ 18995 which is more than four times the prices at New York Community Hospital of Brooklyn and New York Methodist Hospital.Spending Exploration summarises 4year average charges and payments data in bubble plot users can select from top 5 to top 100 most expensive  DRGs (according to Medicare spending which is the product of total charges and average payments) and look at the distribution of payments by the average medical payment per discharge vs. the total number of discharges. The size of the bubble represents the total Medicare payments for selected DRGs.Hopefully this shiny app will help you understand US healthcare spending and be a useful tool while selecting your health care providers.Possible future steps for this app would be combining with the quality of care provided by different hospitals. By aggregating all the data related to both charges and quality the app would be a more accurate guide for patients.R Code is available on . ,NA
Mapping NYC Common Core Scores,41,https://nycdatascience.com/blog/student-works/r-shiny/mapping-nyc-common-core/,"To construct my app I combined four data sources.  I downloaded NYC public school test scores from   I overlaid these results onto a map of NYC's official 2010 census tracts using a shapefile downloaded from the .  I found income data on these tracts from the official US Census Burea's .  Last I found the addresses of all public schools on the  then acquired their geographic coordinates by querying Google Maps' geocoding API.  I combined this material together in R and built the app in Shiny primarily using the Leaflet package.  The relevant code and data are .Let's take a look at the city as a whole first.  Here is the city map displaying test results on the math exam from 2016.  Each dot represents one school with the green end of the color spectrum representing a higher percentage of students who scored at the ""proficient"" levels (i.e. scored at Level 3 or 4) and the red end representing lower percentages.While the correlation between income and school performance is substantial it is not allencompassing.  If we filter census tracts by income quintile we can see plenty of schools that diverge from the general trend in their region.  Let's just look at the lowest income quintile.",NA,"In  the National Governors’ Association and the Council of Chief State School Officers resolved to develop a set of national education standards known as the Common Core. These were intended to unify what had been to that point a set of highly localized state education standards.  The hope of the Common Core was that a nationallyoriented education policy would provide uniform benchmarks to guide and evaluate the American educational system.  By 2013 most states had begun implementing the Common Core.  That year New York State began using the Common Core as the basis of statewide English and math tests administered to all public school students in grades 3 through 8.The Common Core and its associated tests have since come under . Many parents and education professionals have complained that the Common Core has increased the focus on highstakes testing and subsequently ""teaching to the test."" Moreover they believe that the uniform standards do not make sufficient allowance for students to learn at different paces and in different ways and that the money spent on building the Common Core could have been  used to directly support schools.  That is an important debate but it's one for another essay.For now those of us interested in public education in New York City have four years of data to examine regarding school performance on the tests.  To do so I developed the  to chart the performance of all NYC public schools on the math and English tests for grades 38 during the period 20132016.  In this post I will examine the insights provided by this data.  In brief the results of my analysis suggest the following: 1) the results show the clear effect of neighborhood income level though that effect is far from allencompassing; 2) there has been clear progress on the English tests over time both from one grade cohort to the next and longitudinally throughout a cohort; and 3) progress on the math test has been more elusive especially in disadvantaged neighborhoods.Those of you who know the city well will probably nod.  A map charting performance on the math test essentially doubles as a demographic map of the city.  There’s a strong concentration of red and orange in eastern Brooklyn the south Bronx and northern Manhattan.  These are largely lowincome heavily AfricanAmerican and Latino neighborhoods.  Meanwhile the green dots are concentrated in locations like mid and downtown Manhattan and northeastern Queens  predominantly upscale white and East Asian communities.  The yellow dots largely cover middleclass white areas like central Queens southern Brooklyn and Staten Island.  The distinctions are a little less visible but equally present on the map for the English test: the deep red areas in Brooklyn and the Bronx become a little more orange while northeastern Queens moves toward the yellow range probably due to the communities of highskilled Asian immigrants in that area.  Correlation between math and English scores against median household income is respectively r0.47 and r0.51 at vanishingly small pvalues.If we break down the data temporally we can see two contrary stories emerges.  The positive one involves the yearbyyear results.  If we look at the scores on the 2013 English exams the map looks a good deal less friendly than the one from 2016.In fact only 27.6% of students scored as proficient in 2013 compared to 39.3% in 2016.  Over time the scores have steadily improved.  We see a more muted version of that trend on the math exams moving from 31.0% to 37.6%.The realworld causes behind those trends might derive from several sources.  One  might be that the Common Core has done what it was intended to do: slowly improve proficiency by holding students to uniform high standards.  It may however be that students are simply getting better at adapting to the test or that after  about vagueness on the ELA test questions in early years the questions themselves have improved and students can more readily answer them.  For that matter it may also be that the increasing trend of  of the tests on behalf of their children has caused weaker students to be disproportionately excluded from the test pool which would artificially raise the scores without actually improving student proficiency.  Still the fact that steadily more students are scoring at the proficient level is a positive sign.The negative trend however is that students appear to perform progressively worse in higher grades especially in math.  If we take 2016 we see that math performance is relatively fine through the fourth grade at over 40% systemwide.  As we move toward the middle grades though we see noticeable declines.  By the eighth grade there are ugly red markers everywhere on the map with many schools shepherding fewer than 10% of their students to proficiency.Total proficiency is down to 26.4% in eighth grade compared to 41.8% among that year's thirdgraders.  We don’t see that trend in English—there may at most be a few dropped points between elementary and middle school.  In other words the system overall seems to be improving as the years go by but students perform worse as they age.  If we look longitudinally we see the 2013 thirdgraders’ math proficiency is essentially unchanged by the time they are sixthgraders in 2016 with the fourthgraders a few points higher as seventhgraders and the fifthgraders are a few points lower as eighthgraders.  On the English side things are more positive longitudinally—the 2013 cohorts in the third fourth and fifth grades had all improved their proficiency by about ten percentage points by sixth seventh and eighth grades.There's plenty of red but a few patches of green.  Some of these like Columbia Secondary School or AllCity Leadership are not representative of their neighborhood: their specific missions allow them to draw strong students from around the city.  But some green dots represent wellrun neighborhood schools.  Consider for example P.S. 171 Patrick Henry in East Harlem.  The surrounding area had a median household income barely above $22000 in 2010.  Its  is representative of the neighborhood twothirds Latino and a quarter black.  Every student qualifies for free lunch.  Yet on both the English and math tests over 60% of students scored proficient in 2016.  The nonprofit website Insideschools credits its longtenured principal its aggressive pursuit of foundation grants to support computer and labs its strictlystructured culture (uniforms weeklymonitored online assignments etc.) and its general cultural emphasis on reading for the school's high performance.We can see conversely a number of schools in the wealthiest tracts that seem to underperform.  For instance let's zoom into the area around Rosedale in southeast Queens.This region is granted only barely in the top income quintile.  It had median annual incomes of around $80000 a year in 2010 better than 80% of the city but a far cry from the highestearning tracts on the Upper East Side which approach and sometimes exceed $200000.  Yet despite its comfortable affluence it has lowperforming public schools.  P.S. 138 Sunrise for example had fewer than 10% of its students pass at proficiency on math.Why is this the case?  One possibility is that the population of the schools is not representative of the tract.  Nearly threequarters of  qualify for free lunches a number much more in line with the lowermiddle income districts than those in the upper quintile.  There are at least four private Christian schools in the neighborhood which may draw away the more affluent students.  The complicated racial dynamics of the city's education system may also be a factor: unlike most areas in the highest income bracket the majority of Rosedale's residents are AfricanAmerican.  Understanding the struggles in this neighborhood in detail would require more extensive qualitative research.It may be too difficult to extricate from this data the effects of the Common Core on student learning as opposed to the influence of the city's learning curve in adjusting toward a new set of standards.  Still it should be clear that progress is stronger on English than in math with scores improving more rapidly and with higher proficiency rates in the city's disadvantaged areas.  The relatively poor math scores of those areas show the largescale influence that income inequality still exerts on the educational system.  Still the success of schools like P.S. 171 and the lack thereof in some more affluent areas show that a wellrun school can overcome those effects.",NA
Machine Learning Application in Hedge Fund,42,https://nycdatascience.com/blog/student-works/hedge-fund-machine-learning-challenge/,"Most stock market data is not publicly available even though individuals could have access to more market data through Yahoo Finance than ever before. Numerai is the first interface between machine learning intelligence and global capital which manages an institutional grade long/short global equity strategy for the investors in hedge fund transforms and regularizes financial data into machine learning problems for global network of data scientists. People do not need financial domain knowledge for machine learning model development. Numerai has an updated open data source which provides high quality encrypted stock market data for developing machine learning models.The data is clean and tidy and you could apply whatever methods you would like to apply.Firstly let’s take a look at what the data looks like which has been used for competition between Dec 14 Dec 21 2016 (21 features 1 target for prediction 136573 observations for training 21 features 13518 observations for testing). The data has already been scaled between 0 and 1.The model performance for measurement is logloss. Logloss is suitable for measuring the probability of a binary outcome. It considers the confidence of the prediction when assessing how to penalize incorrect classification. For example when you have a binary classification problem a prediction outcome of 0.99 has a more confidence level compared with the outcome of 0.59 through logloss measurement but you could only classify them as one outcome if you set a 0.5 threshold.Firstly we checked the distribution of the training dataset by using barplot boxplot and violin plot as shown in figures from plots we could see the data is evenly distributed and no significant difference among features and we could not extract a lot of information from those plots.Secondly we checked correlations among all the features it is found that more than half of the features are highly correlated and we could do some feature importance analysis to decide whether we could do dimension reduction or expansion.So for this project we have two plans for developing machine learning models for Numerai projects “Less” approach and “More” approach.In the “Less” approach lasso regression random forest have been adopted for feature exploration logistic regression random forest and XGBoost have been adopted for model training and development. In the lasso model for feature deduction the lambda is set as 1e3 and the result shown in the figure is that feature 4610131819 20 21 are significant and should be kept as important features. While in the random forest model results feature 6 20 13 21 10 2 7 9 5 1214 8 11 16 15 17 1 9 4 18 3 are significant which is slightly different with the result got from lasso. Anyway feature 4 6 10 13 18 21 are proved to be important features by both models. Due to the different results shown above and it is difficult to decide whether we should only keep some important features for modeling all features have been kept for the initial model development.
Then we tried random forest algorithm with 300 500 800 trees and crossvalidation the result we got is 0.69501. Finally we tried XGBoost which is famous for machine learning competition. We implement grid search for parameter optimization as shown in the table the process is shown in figure. The best combination of parameters is 0.6 for colsample by tree 0.8 for subsample 0.1 for learning rate 50 for number of estimators and 2 for depth. We put the grid search results into a XGBoost model the results shown in the leaderboard is 0. 69028. Based on the results of these models it is found that logistic regression has the best fitting of the model and the prediction model could be improved more with the more feature engineering work.Logistic regression has been selected for model training since it is easy to implement efficient to try so we applied it to get a quick check about the prediction performance. With the cross validation of the training dataset we got a logloss with a value of 0.68910 on the leaderboard.In this part Python scikit learn has been used for model development since it includes efficient supervised and unsupervised machine learning algorithms. After the cross validation the lambda was found close to 0 which seems that there is no need to add penalty term. If we change the cost function what will happen? Two more  kinds of cost function were utilized to tune the lambda the left graph show the tune with logloss function and the right graph utilize the class accuracy. From these two graph we found the lambda close to 0 too which result is very abnormal in logistic regression model.",NA,"We tried the ridge regression which always determine the lambda by the deviance.Above are the density plots of each transformation. The 1st graph is the density plot of the original feature from which we can see the high density for class 1 at feature 21 smaller than 0.5. We can see similar distribution in each transformation and in the last graph the different distribution of each transformation could improve the accuracy like resolution increasing.Neural NetworkWe train the original date by neural network with several grid search.The best result is the activation as ""RectifierWithDropout"" with the logloss 0.692 on train set but always large error on test set. Most of the observation's response is between 0.45 to 0.55 which make the model too sensitive not stable.Ridge RegressionRidge Regression was used to train three  data with different number of features.From the result data as the number of feature increase the regularization term get larger and the accuracy get improved too. The logloss increase by ~0.001 comparing with the difference between the top and bottom score on the lead board the 5% improvement is significant. ",NA
Machine Learning on Bank Marketing Data,42,https://nycdatascience.com/blog/student-works/machine-learning/machine-learning-retail-bank-marketing-data/,"IntroductionThe DataData Visualization and PreprocessingThe Models and Model OptimizationsThe ResultsThe algorithms used in the study consist of Logistic Regression Radom Forest Gradient Boosting Support Vector Classifier and Neural Network. Without oversampling and parameter optimization all algorithms show around 90% accuracy overall but about 20% sensitivity on fitting and prediction of custmer signups.  After rebalancing the training data and grid search cross validation all the algorithms except the Support Vector Classifier showed improvement on balanced sensitivities and the top two improvements were from Gradient Boosting and Neural Network. The following outputs are then generated from Logistic Regression Gradient Boosting and Neural Network. Here Logistic Regression  is served as a baseline other algorithm to be compared with.
:  :                                                         :                                            :
          precision recall f1score  support              precision recall f1score  support           precision recall f1score  support
 class 0      0.91         0.95     29229     class 0      0.95         0.89    29229     class 0   0.96         0.60     29229 
 class 1      0.67         0.35       3721     class 1      0.34         0.45       3721     class 1   0.16         0.27       3721  
avg/total   0.88   0.90      0.88     32950    avg/total    0.88  0.82       0.84    32950  avg/total   0.87   0.49      0.57     32950      :                                                           :                                               :  
           precision recall f1score  support             precision recall f1score  support           precision recall f1score  support
 class 0      0.91          0.95        7319     class 0    0.95        0.89       7319     class 0   0.96           0.60       7319
 class 1      0.66          0.35         919      class 1    0.34        0.45         919     class 1    0.16           0.26         919
avg/total    0.88   0.90       0.88       8238   avg/total   0.88    0.82     0.84       8238  avg/total   0.87    0.48       0.56       8238
Conclusion",NA,"Retail Banking is a provision of the services by banks to an individual consumer. Such services offered include savings/transaction accounts mortgages personal loans and credit/debit cards. It is a very competitive business.
One of the biggest challenges the business faces is figuring out how to identify condumers who are most likely to do business with it and target those who have suitable needs to sign up for its services. Because of that the marketing of services plays a vital role in winning customers over.In this study we have implemented multiple muchine learning algorithms on a marketing data set of an European retail bank. The goal is to understand the important factors on shortterm deposit account signups and to develop a strategy to help banks focus on those most promising leads in order to win them over.The data set used here is from UCI machine learning repository. It is derived from the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often more than one contact to the same client was required in order to access if the product (bank term deposit) would be ('yes') or would not be('no') subscribed.The following categories of information are included in the data set:Consumer data: Age sex job and marital status education loan status and so on.
Campaign activities: When and how to contact times to contact outcome of previous campaigns.
Social and economic environment data: Euribor 3 month rate employment variation rate consumer price index and
consumer confidence index.
Outcome:  A 'yes' for the bank if there is a term deposit subscribed a 'no' for the other outcome.The data set contains 12 categorical variables and 7 numerical variables as its features and a binary category as its response variable. The following plots show the relationships between its economic numerical variables and the distribution of the response variable.                       The plot on response variable shows highly unbalanced distribution of its binary classes. The relationships between the economic data are nonlinear. Economic data is not normally distributed. In order to fit machine learning algorithms all the categorical variables and response variable are encoded into numerical levels.This data set also contains missing data on one numerical feature: . This feature indicates the number of days that passed by after the client was last contacted from a previous campaign it was coded as '999' if the contact never happened. Over 90% of the records show a number missing for the pdays. In order to implement the machine learning algorthms we need to imput the missing values of this feature in a way to maximize its prediction accuracy.In the study we tested the following appraches with the Logistic Regression algorithm:Since the result shows the highest prediction accuracy on the first apprach. we will keep the data as it is in the rest of study.In order to meet the goal set up above we need to develop a classification model with a balanced ability of giving correct prediction on both consumer signups and no signups. This model is considered as suitable since it can help marketing personnel to focus on those promising leads and reduce or eliminate effort on those who are unlikely to conduct business with them.  To develop such model we implemented:
The highlighted data shows that the sensitivity of signups on fitting the train data has been increased from 24% on baseline to 67% on Gradient Boosting and to 84% on Neural Network Classifier and that the coressponding improvements on the test set have raised from 23% to 66% and then further to 84%. By improving the sensitivity on class 1 we improve the ability of these models to predict customer signups. However sensitivity on class 0 has been reduced from 99% on baseline to 83% on Gradient Boosting and to 44% on Neural Network on train data and from 99% to 84% and then further to 43% on test set. The reduction of sensitivity on class 0 will reduce the ability to predict no signups correctly.Further  let's take a look at the prediction distributions on test set:
Baseline:                                           Gradient Boosting:                                Neural Network:
                                                        Compared with those true values the prediction output on test set shows:
1. The baseline model is accrurate in making predictions in general: It is able to predict correctly over 90% of the time. However it can only predict customer signup correctly 23% of the time. So the model is not informative enough to our project goal.
2. Gradient Boosting has been improved on predicting customer signups correctly to 66% of the time and it is able to filter out those who will not signup 84% of the time.
3. Neural Network has been improved further on predicting signups correctly to 84% of the time however its ability of filtering out those who will not signup was reduced to 43% of the time.
4.  Compared with Gradient Boosting  Neural Network prediction is more expensive because of its failure to identify those who will not sign up.Finally as shown below both Random Forest and Gradient Boosting are able to identify those features which have important impacts on the response variable. These outputs supply very valuable insight to guide further marketing campaign.
From Random Forest:                                                                                           From Gradient Boosting:
                                                    The differences of feature importances reflect the differences of mechanism between thees models.In this study we have applied machine learning techniques to retail bank marketing data and explored how the techniques can be used to help  the bank to conduct its marketing campaign:",NA
Machines for Machine Jobs,42,https://nycdatascience.com/blog/student-works/machines-machine-jobs/,Through use of TFIDF I was able to determine that there was one group that stood out. Though there appeared to be other groups by graphical examination upon numerical examination I determined they were not as consistently separated from other groups. Separating groups is possible but the ability for separation with a clear distinction among the groups requires further study. ,NA,"Patient personalities can be expressed in certain archetypes. Healthcare practitioners have made use of this knowledge by changing their communication style and their frames of mind when dealing with different patients to best educate their patient in order to engage them as active members of the decision making process.Can we create tools to allow people to know their audience ahead of time so we can be focus on being sensitive listeners and building rapport?: In my experience working on data science projects the rate limiting factor in terms of defining a project is primarily finding the data set. In this case I was not able to find a data set specifically regarding patients and clinical diagnosis. I found a research paper regarding patient food diaries but was unable to acquire this data set I then found a publication regarding the categorization of restaurants based on their menu items.: Because I was not able to find the data set I needed I found a data set that had been used in solving a similar problem of grouping observations into categories based on text.: It is a collection of menu items with the features of price name of restaurant type of food price of restaurant: a method of using the frequency of a word in a document and its frequency within a corpus to place a numerical weight on the importance of a word: I initially calculated the TFIDF scores of all the words for the 4 price ranges of restaurant ($$) I then selected the top 50 for each price range and ran PCA on a concatenated dataframe of the top 50 words from each category.: I then moved to KMeans to determine if there was any clustering I could do by using the full data frame of my TFIDF scores using my knowledge of the data set I ran it initially using 4 centroids.: However to ensure that I was not inventing clusters where there were none I ran it on a different number of centroids. I used a graph of the points to determine visually which ones were standing out. However I also validated these findings numerically by comparing the Euclidean distance of each centroid against one another.: a plotting technique that can demonstrate relationships with even with a large amount of features
LSA: a different method of determining which words are important within a text
Cosine Similarity: as opposed to the Euclidean distance which I used in the KMeans comparison
: Using Human responses to develop a training set for future grouping projects

Thanks to Dan Jurafsky author of """" for making the data set publicly accessible",NA
NINKASI: Beer Recommender System,42,https://nycdatascience.com/blog/student-works/ninkasi-beer-recommender-system/,An ubiquitous drink throughout the ages beer is the source of pleasure and the complement for celebration when consumed in moderation. Ninkasi the ancient Sumerian goddess of beer exists to satisfy the desire and sate the heart. Likewise our recommender system named after the goddess aims to deliver personalized recommendations to beer lovers across the country. Our final product consists of web scraped data a contentbased natural language processing model two different collaborative filtering models using Singular Value Decomposition++ (SVD++) and Restricted Boltzmann Machines (RBM) all packed into an interactive Flask webapplication. Our purpose is twofold: to create a recommender system of something fun for others to use and in the process learn how to build a complex recommender system.We obtained the data from  a popular beer website for beer lovers to rate and review their favorite beer. The scraping tool we used was Scrapy a python based web crawling package. Two separate dataset was scraped one dataset containing all the information about the beer and the second dataset containing all the information about the user and the review. We decided to scrape US beer only limited to the top 2025 beers produced in every states plus District of Columbia a total of about 280 000 user reviews were scraped.The left image shows all the scraped beer information and the right image shows all the scraped review information. This is a detailed table for all the fields obtained Our dataset consists of  1269 beers taken from the top 2025 beers of each state and Washington D.C.  Of these beers there are 58 styles and 338 unique brewers: Of the statistics over half the beers had missing values for IBU and mean because the information was not available. Here are some violin plots:The weighted average looks to be fairly normal with mean around 3.7 while both style score and overall score are heavily skewed toward 100. Per the website the style and the overall score are scores relative to other beers and as we chose the top few beers of each state it makes sense that they are skewed as such. The % abv has a median between 8 and 9 and ranging from a bit under 2.5 % abv to close to 20 % abv. F or estimated calories we have two peaks one around 50 calories and one around 250 calories. This is perhaps a separation between socalled light beers and heavier beers.   Our dataset consists of  ~278000 reviews written by ~17000 users. ~9000 users over half wrote only 1 or 2 reviews. (Perhaps a recommender system would increase usage). The dates of the reviews range from 20161204 to 20000426. This means that some beers have stayed on the top 2025 list for quite a while in their respective state. Also there may have been quite some changes to which beers are in the top list. This combined with the number of users who have written only 1 or 2 reviews may not be indicative of the actual number of users who have used the website.After cleaning our data set consists of approximately 11370000 words or enough words to fill 150 350page books. This equates to about 40 words per review without stop words. There are only approximately 127600 unique words or 1.1% of all words.  The top 100 words by frequency are:And the top 100 distinct words by TFIDF score  in no particular order as a word could appear multiple times in the top 100:Lastly we ran a cursory LDA model with 5 topics and 40 passthroughs. Though we chose 5 topics only the first two topics had proportions that were not almost zeroed out:With 1300 beer information and 280000 review data in hand we started to build our recommendation engine. Our plan is to ensemble multiple recommenders to give users the best beer options based on their preference.In this section we expand the discussion on the techniques we used to build the system. As can be seen from the figure above our recommendation system includes a contentbased method which takes advantage of user reviews and a collaborative filtering engine based on user rating. Finally we combine all three techniques in a Flask web app.The idea of contentbased recommendation system can be summarized as follows: (1) User select a beer that he/she likes. (2) Recommendation system find a list of beers that are most similar to the user input. (3) Output results.Figure above demonstrates the workflow. The whole process involves Natural Language Processing (NLP) calculating Term FrequencyInverse Document Frequency (TFIDF) score and Performing Latent Semantic Analysis (LSA). Finally cosine similarity matrix for all documents are constructed.Given 280000 reviews from users we need to have them cleaned corrected and organized into meaningful pieces before performing numeric analysis. Therefore the goal of text preprocessing is to generate a group of words for each beer. Python NLTK package was used throughout the process.Above figure demonstrates the workflow of text preprocessing. For each review string it is first encoded by ASCII then all  garbled symbols and punctuations inside the string are washed off using regular expression. Having all unimportant symbols removed the review string is tokenized into groups of words. Next spell check and auto corrections are performed to each word. In order to facilitate weight calculation stopwords are removed due to having less importance. Finally every word is lemmatized to unify the tense for each verb. The final output is a list of corpuses with each corpus pointing to each beer name.TFIDF is a numeric statistic that intended to reflect how important a word is in a collections of corpus. The tfidf value of a word increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus which helps to adjust for the fact that some words appear more frequently in general. The overall score is by direct multiplying TF points with IDF points. The calculation is performed using the  Python package using the formula given below.TFIDF calculation generates a document(beer)term(word) matrix with each cell corresponding a weight of a word for a beer. Due to the size of the matrix we select the top500 terms according to their weights to build the recommendation system. Before creating similarity matrix among all beers Latent Semantic Indexing (LSI) was implemented to obtain a lowrank approximation of the documentterm matrix. What LSI does is performing Singular Value Decomposition to the documentterm matrix and pick out the first k rank1 components that contribute the most to the original matrix. The advantage of such approximation can not only reduce the noise of documentterm matrix but also alleviate synonymy and polysemy parsing in natural language.When working with ratings data it is often useful to look at the data as a useritem matrix where each user is a row and each item is a column. The matrix will have values in the spots where the user rated an item but will be missing a value otherwise. Because of the large number of items it is often the case the matrix be very sparse since it is unlikely that a single user have tried a majority of the items.In collaborative filtering the goal is to impute those missing values by analyzing the useritem interactions. If two users share common ratings for some items the recommendation system will assume that the two have similar preferences and will try to recommend items from each other’s list that the other did not already rate. This idea can be expanded to consider every user together as one cohesive group. One powerful technique is to factorize the matrix into a user latent factor matrix multiplied by an item latent factor matrix.                                   The idea here is to approximate a single value decomposition (SVD) that would contain the hidden features to explain why a user rated the items the way that they did. In other words the SVD is trying to extract meaning behind their ratings. This algorithm approximates the SVD by treating the user and item latent factor matrices as parameters and training them to minimize the rootmeansquare error for the known ratings. We used Spark’s MLlib ALS package on our data to get a benchmark of 10.75% error.From the winners of the Netflix Challenge in 2009 an enhanced version of this technique was developed to decompose the ratings even further. Instead of just user and item latent factors the model seeks to also model each user’s and item’s bias (deviance from the average of all ratings). For example if two user both like IPA beers but the first user just so happens to rate things 2 points higher on average the system will think that these two users have different tastes. The same is also true between items. In order to combat this the new algorithm explicitly models the biases in an effort to recenter each user and item at the global average. This allows the latent factors to be estimated correctly without worrying about bias. SVD++ goes one step further to also include the number of ratings a user has. The goal of this is to penalize users that have few ratings so that the predicted ratings are more conservative since there is less information for these users.Unfortunately we did not find any standard SVD++ packages publicly available. However after studying the algorithm further we realized that the algorithm is essentially matrix operations and therefore can be efficiently programed using TensorFlow Google’s computational framework for tensor objects and operations.Furthermore we were able to find a regular SVD script using TensorFlow on github so we did not have to start from scratch. Our implementation of SVD++ involved augmenting this script to include the biases and implicit feedback in the algorithm. Additionally we added in features such as kfold cross validation and early stopping. RBM is an unsupervised generative stochastic twolayer neural network. It is derived from the Boltzmann Distribution from statistical mechanics which is a probability distribution that measures the state of a system. It is “restricted” because the neurons must form a bipartite graph which the neurons form two distinct group of units visible V and hidden H. There are symmetric connections between the two group and no connections within each group thus effectively forming two layers.RBM performs collaborative filtering by reconstruct the useritem matrix filling out the missing rating for all user. Consider a dataset of M items and N users we are treating every user as a single training case for an RBM therefore we can initialize the visible layer to be a vector with M element where each element or neuron represents an item with value equals to the rating if it has been rated and zero if not. The visible neurons are modeled from a conditional multinomial distribution of K distinct rating scores thus a “softmax” activation is used. The hidden layer is a latent layer where the hidden neurons can be perceived as binary latent features therefore a conditional Bernoulli distribution is used to model the hidden layer with a sigmoid activation function.To train a RBM the initialized visible layer is propagated through the RBM parameterized by the weight W visible bias bi and hidden bias bj. After we obtain the For RBM all the weights and bias are constant across all user training cases for example if two users rated the same item they must share the same weight and bias for that item between visible and hidden layers.The two fully reconstructed useritem matrix obtained from the two CF algorithms are ensembled linearly to generate the final useritem matrix for new user prediction. To give recommendations for new user we used a neighborhood approach. After receiving the new user rating vector the cosine similarity between the new user vector and the useritem matrix is computed to impute the missing ratings in the user vector using a weighted average proportional to similarity matrix. The final recommendations is obtained from the items with the highest similarity scores.To make our recommender system accessible to the everyday user we decided to make a . Both the content recommender system and collaborative filtering system are accessible using the app. Fun fact: the earliest known recipe for beer is an ode to Ninkasi dating back 3900 years.,NA,We split our EDA into three parts:The first topic looks to describe dark beers such as a stout or porter infused with flavors usually found in those type of beers such as coffee chocolate and vanilla. The second topic seems to describe fruity and hoppy beers with a golden color.This allowed us to program at a higher level while benefitting from the C++ backend and GPU speedup. With the additional use of the GPU alone gave us a 5x boost. Further testing will have to be done to see how much TensorFlow’s backend have improved our efficiency. The end result after doing a hyper parameter grid search and 10fold cross validation was an error rate of 8.25%.The graph on the right is the computational graph for our TensorFlow code. User data is fed through the bottom node and travels through the graph undergoing transformations and operations at each node to finally produce predicted ratings and losses at the terminal nodes.values of the hidden layer it is propagated back to the visible layer from the updated parameters. The visible layer is fully reconstructed and all the missing ratings are imputed. The objective function often used for training RBM is called “Contrastive Divergence” it is looking to minimizing the energy function by maximizing loglikelihood of the marginal distribution over visible rating V. For our model a stochastic gradient ascent with momentum is used as the minimizer. ,NA
Orpheus: A Multi-User Music Recommendation System,42,https://nycdatascience.com/blog/student-works/orpheus-multi-user-music-recommendation-system/,What’s the recipe for the ultimate road trip? Companions who can make you laugh snacks to last the whole trip and of course a good music selection. It has long been an unwritten rule that whoever’s at the wheel has control of the music. This can get boring fast especially when the driver has a bland taste. Some passengers may tune out and start listening to their own music on headphones and disengage from the group. What is essential is a playlist that would cater to the taste of all passengers.In this project we address the challenge of creating a playlist for multiple users with different tastes and preferences and provide a uniformly fantastic listening experience.Imagine an app where upon you and your companions logging in to your music devices aggregates everyone’s listening history and automates a playlist that everyone would enjoy!We created just an app: . With Orpheus multiple users can login to their Spotify accounts and find songs they can all rock out to. The order of the tracks can be based on mood tempo and more. Orpheus was developed in Flask using the Spotify Web API to get user data. Check it out !Orpheus can also be used for parties or as background music for group workouts in the gym with your bros!In order to develop a model for recommending music we needed data. We collected user  website. At a high level the data was used to train a recommendation system for a single user. We employed collaborative filtering using ’s machine learning library to build a latent factor model. This model is then used by an aggregation strategy to determine preferences for multiple users in a group and recommend a final playlist. Finally this playlist is sent to the Flask app where users can get groovy to it. The entire pipeline can be seen below:The following describes each step in more detail.The dataset the recommendation model was trained on was from the Echo Nest Taste Profile Subset.  The dataset consisted of 5 GB of 1019318 unique users 384546 unique songs and 48373586 unique observations of user song playcount triplets.  On average 125 users listen to each song and less than 100 users are responsible for 80% of the songs listened.  Most likely there are a few songs that are highly popular and most songs are listened by a few.  With user listening history in hand the next step was in creating a recommender system. In general recommendation systems aim to predict the preference that a user has for a given item. Items that have the highest predicted preference can then be made as recommendations to the user.There are two commonly used approaches to building a recommender system: Contentbased filtering techniques and collaborative filtering.In contentbased filtering the system looks at the characteristics of the users or items to make predictions. For example in music the system would find songs in the same genre or have the same artist to determine similar songs. Using these characteristics the recommender could look at a user’s items and determine which items are most similar and recommend them.In collaborative filtering the idea is that users similar to you will like similar items. The content of the items is abstracted away and only the interaction between users and items is taken into account. A downside of collaborative filtering is that to make recommendations a user requires historical data with the items: This is known as the cold start problem.Within collaborative filtering there are two types of feedback received from users: Explicit and implicit. Explicit feedback occurs when users actively rate an item (e.g. the Netflix star rating). Implicit feedback occurs based on the consumption of an item for example when a user listens to a song. Our dataset consists of these implicit ratings.We chose to use the collaborative filtering approach known as the latent factor model due to its ability to handle implicit feedback its scalability and Spark’s Alternating Least Squares (ALS) implementation. A good overview of the implementation and its practical use can be found .The latent factor model attempts to reveal latent features about users and products in order to make recommendations. Specifically given a userrating matrix the model finds an approximate lowrank matrix factorization as seen below. The dot product of the user’s latent feature with an item’s latent feature represents the user’s predicted ratings. Predicted ratings for all items are computed and ordered to give the user a final recommendation with the highest predicted rating.Lambda is a regularization parameter used to avoid overfitting to the data.The implicit model works slightly differently. Rather than attempt to predict explicit ratings a confidence that user likes item  is given by the following equation:Where alpha is a tuning parameter of the model. The implicit optimization problem then becomes:Here p_ui represents whether the user liked the item while c_ui represents our confidence they liked the item. The regularization term is the same as in the explicit case.For more details see the paper that invented the implicit feedback collaborative filtering method .Now that we have a model how do we choose the parameters? For that matter how do we evaluate our model?Ranking metrics are a common approach to evaluating recommender systems. Briefly they allow us to assess the quality of a recommendation based on their ranking of predicted items. To evaluate our model we used the ranking metric  (MAP). This was the ranking metric used for the .Given a user’s item history and a recommendation the  (P) measures the proportion of correct recommendations within the topk recommendations. The (AP) is the precision at each recall point  Finally the (MAP) averages the AP over all users.In order to evaluate the model’s MAP we perform crossvalidation. Each iteration of crossvalidation splits the users into training and test users. The test user’s listening history is further split to a hidden and visible set. The model is trained on both the training user’s and the test user’s visible implicit ratings. After the model is trained the MAP score for the test user’s hidden set is computed to determine the quality of recommendations. For efficiency we selected a random subset of 100 users for computing the MAP.To determine parameters to the model we ran 5fold crossvalidation on a one dimensional grid for each parameter: The rank lambda and alpha. We compared the results to a baseline popularity model which simply recommends the most popular songs that a user hasn’t already listened to. The crossvalidation results are given in the plot below.We first ran the explicit model which can be seen in Figure a). The explicit model performs considerably worse than the popularity model which is unsurprising given that the data is not explicit and therefore this is not an appropriate model to use.Figure  b) shows the implicit model as a function of rank. Generally the higher the rank the better the performance as higher rank matrices provide better approximations of the useritem interaction. We chose rank  50 as a good compromise between accuracy and computational efficiency.Conversely alpha had a profound impact on the quality of the model. We found the optimal value of alpha  40 which is also the suggested values by the .We found the optimal parameters to be rank  50 lambda  0.1 giving a final MAP score of 0.1.When comparing this to the closed Kaggle competition we get approximately 25th out of 150 teams. This gave us confidence the model was performing well.We now have a working recommender system and it works great for individual recommendations. The next part is to make Orpheus recommend for a group of users. But how do we convert a single recommender system into a group recommender system?One technique is to get each group member’s recommendation and combine all the recommendations using an aggregation strategy.As much as possible we want Orpheus to come up with  a playlist which would satisfy all members of the group. Assuming there are a couple of different aggregation strategies to employ which one would work best for a small group? What if in our road trip scenario we had a minivan instead of a car would the same aggregation strategy work on a larger group? More importantly how would Orpheus recommend to a group with very dissimilar tastes?These were the questions we needed to answer to come up with the best recommendation for the group. To illustrate how aggregation strategies work we pick some users in our dataset. Suppose User 193650 and his friends User 84250 and User 92650 go for a drive Orpheus knows their listening history and has come up with individual recommendations for each of them. Table 1 shows a subset of recommended songs and how confident Orpheus is that the user will like the song. A confidence closer to 1.0 means the user will most likely enjoy the song. Which songs will Orpheus play first?The least misery aggregation strategy has been used for a . The movie recommender uses it on explicit ratings while here we use it on implicit ratings. For each song we get the smallest confidence rating and set it as the confidence rating of the song for the group. We then rearrange all songs from highest to lowest confidence rating. This is now the group recommended playlist.To measure the satisfaction of each member of a playlist we apply the formula below taken from  which in turn can give us the group’s satisfaction rating on the resulting playlist.Another variable we need to consider is homogeneity of the group. Kmeans clustering on the dataset would uncover similarities of each users and group them together based on their taste. This way we can just get members from the same cluster for a homogenous group and members from different clusters for a heterogenous group. However the main challenge with the dataset is its dimensionality and sparsity. Imagine doing Kmeans clustering for 1 million observations and close to 400000 variables! Luckily in the process of training a recommender system a reduced latent factor matrix is produced. We apply Kmeans clustering on the user’s latent features to come up with groups.To test our aggregation strategies we divided our groups into two categories: homogenous grouping and heterogeneous grouping. For each category we wanted to see how varying the group size might affect the group satisfaction so we made 3 5 and 7 member groups. We made 20 samples for each group for a total of 120 samples. The statistical results below show that the group satisfaction of homogenous groups using different aggregation strategies do not statistically differ from each other. This means that any of the aggregation strategies will result to a playlist where all members are happy in the homogeneous case. For heterogeneous groups we found that the average strategy was statistically significantly better than other methods as can be seen by the ANOVA and Tukey HSD posthoc test results below.We complete our project with a Flask Application designed to generate a Spotify playlist ordered in whichever feature chosen from the tracks of up to six different people.  The diagram below shows how the app works.After the necessary inputs are made the playlist can be launched with a player embedded inside the application. Users can order the playlist by metrics such as energy mood and tempo as can be seen in the pictures below.,NA,The lowrank factors are determined by solving the optimization problem below.We found that lambda didn’t have a significant effect on the performance of the model as can be seen From Figure c). Once lambda becomes too large (> 1) the quality of the model goes down.Basically least misery gets the happiness of the least happy member of the group. The other strategies we tested are: Average strategy  which gets the average happiness of all members; most pleasure which considers the happiness of the most happy member; and multiplicative which gets the product of all members’ happiness.Orpheus gives people the opportunity to enjoy music together. A flask app supported by an implicit collaborative filtering recommender system combined with appropriate aggregation strategies give users the ultimate tool to accompany them on the road at the zoo or in the bedroom.We encourage you to grab your friends and !,NA
Predicting Food Desert via Social Media,42,https://nycdatascience.com/blog/student-works/predicting-food-desert-via-social-media/,Food deserts are areas that lack access to affordable fruits vegetables whole grains lowfat milk and other foods that make up the full range of a healthy diet (CDC 2016). The definition of food deserts is that areas where at least 500 people and/or at least 33 percent of the census tract's population reside more than  from a supermarket or large grocery store (for rural census tracts the distance is more than ) (ANA 2011). The distribution of food deserts (green) is shown in Figure 1.  ,NA,"Access to nutritious food is imperative to physical wellbeing and quality of life. Failing to consume healthful food on a regular basis can lead to a series of adverse health outcomes including obesity diabetes and cardiovascular diseases. A rich body of research has emerged which has identified content and language usage in the platforms of social media to reflect individual’s and population’s milieu. Among the many individuals are known to share on the social media food ingestion or dining experiences constitute a unique category. Twitter for instance captures a number of minute details about our daily lives including dietary and dining choices and prior work has indicated it to be a viable resource that can be leveraged to study ingestion and public health related phenomena.Given the vast amount of available Twitter data and an increasing interest in quickly and precisely identifying regions of the country likely to be food deserts including recognizing their nutritional and dietary challenges. In this project  I wanted to build a monitoring system which can use social media as a ""sensor"" to capture people's dining experiences and the nutritional information of the food they are consuming and predict the food desert status of different areas using the linguistic constructs of ingestion content nutritional attributes and other socioeconomic information such as age sex ethnicity or race education income housing status population etc (Figure 2).Figure 3 shows the framework that fetches and filtered realtime Twitter data (Widener and Li 2014). Two modules are built to completed the tweet data collection task: a retrieval module and a parsing module. The retrieval module is responsible for establishing a connection with the Twitter web server through its . Two rules are applied here to filter out noisy tweets. The first rule is that tweets without location information are excluded. The second rule is applied to fetch tweets about food ingestion only.  A set of foodrelate food words is performed to query Twitter. I compiled a list of 233 food name list based on an  that was used by previous studies ( Sharma and Choudhury 2015) and the . Only food names appearing in both sources will be included in this food list. By applying the above two rules for filtering many irrelevant tweets were removed significantly increasing the number of tweets actually about food ingestion stored in the database.Once the raw tweets are fetched they are saved in a database. The parsing module is used to deserialize the raw tweet data into a textbased JSON object and then extract the desired tweet information including tweet time tweet text location and source.The collection process for this project occurred from 12/5/2016 to 12/16/2016 at which point a total of over  tweets with valid latitude   longitude information were collected.In a parallel data collection task I downloaded cartographic information on  census tracts throughout the US from  (2013) of which  census tracts are officially identified to be food deserts. Census tracts are relatively permanent subdivisions of a county and usually have between 2500 and 8000 people.Additionally for each census tract (both food deserts and nonfood deserts) I obtained the most recent socioeconomic data from the  (2016) by using web scraping (). The SES data consists of four tables demographic population housing and income (see table below for features selected in this project).Since the tweets I collected were with valid latitude  longitude information. I then utilized the  to query the latitude  longitude pair of each tweet for a corresponding US census tract. The API query actually returns a 15 character Census Block number. The first 11 digit uniquely identifies a census tract. For tweet with latlong coordinates outside the US the API returns a null code.After this step I successfully mapped over  to the census tracts in USDA Food Access Research Atlas:  in food deserts and  in nonfood deserts (Figure 4).Since the goal of this project was to predict food desert via Twitter I estimated nutrient intake based on the text of each tweet. Firstly I developed a regular expression matching framework in which each word in a tweet was compared to the list of 233 food names described above ( see Twitter Data part). Then I calculated the nutritional intake of each tweet by matching the food item name to the . For this project I calculated five major nutrients' ingestion:  energy (kcal) protein (g) cholesterol (mg) sugar (g) and fat (g). Figure 5 shows the differences in mean nutrients intake between nonfood deserts and food deserts.In order to create features related to ingestion language of tweets I applied LDA a topic modeling approach in the context of tweets which has been commonly used to analyze healthrelated social media data as well as to cluster foodrelated social media posts.Topic Modeling is different from rulebased text mining approaches that use regular expressions or dictionary based keyword searching techniques. Topic modeling is an unsupervised method used for finding and observing the bunch of words (called “topics”) in large clusters of texts. Topics can be defined as “a repeating pattern of cooccurring terms in a corpus” (Figure 6). For example a good topic model should result in – “health” “doctor” “patient” “hospital” for a topic – Healthcare and “farm” “crops” “wheat” for a topic – “Farming”. Topic Models are very useful for the purpose for document clustering organizing large blocks of textual data information retrieval from unstructured text and feature selection.There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency (TFIDF). LDA is another popular topic modeling technique which is a matrix factorization technique. LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents LDA backtracks and tries to figure out what topics would create those documents in the first place. In vector space any corpus (collection of documents) can be represented as a documentterm matrix. LDA then converts this DocumentTerm Matrix into two lower dimensional matrices. One is a documenttopics matrix and another one is a topic – terms matrix with dimensions (N  K) and (K M) respectively where N is the number of documents K is the number of topics and M is the vocabulary size.Figure 7 shows how I applied LDA topic modeling in this project. For the combined set fo tweets from all food deserts and nonfood deserts I obtained topics by using a version of . I used the default parameter settings and set 25 as the number of topics. Meanwhile I used JensenShannon (JS) divergence to measure the topic distribution of each tweet. It means that there were 25 topicrelated features created for each tweet. The LDA topic results are explored using  which maps topic similarity by calculating a semantic distance between topics (via JS divergence). For example model 1 is about job or work; model 4 is about drinking beverage; and model 13 is about food ingestion (Figure 8).I developed four different classification models with same sets of features to predict the USDA defined food desert status of a tract (). The features include: 5 nutrient intake features 16 SES attributes of census tract and 25 LDA topic features. Meanwhile I applied ensemble methods to combine predictions of  in order to improve the robustness over a single estimator.  The results of modeling are summarized in the table below.Figure 9 shows the receiver operating characteristics (ROC) curves for predicting food desert status of different models.In this project I explored how ingestion and food related content extracted from social media Twitter. Meanwhile I tried to build a pipeline to predict areas challenged by healthy food access by including tweets topic analysis together with nutrient intake estimation and SES attributes of a tract (Figure 10). Results showed that I was able to predict USDA defined food low access status of a tract ""Food Desert"" by utilizing the model. However there are some notable limitations in this project. For example tweets have a lot of noisy information. Also I did not apply sentiment analysis of the text or the content of the images shared on Twitter which might lead to a more accurate prediction.   The next steps for this project are:You may also explore this project via Chuan's GitHub.Analytics VidhyaAlex Perrier Learning Machine ",NA
Project 1: First Term Presidential Election Referendum on Congressional Midterm Elections,42,https://nycdatascience.com/blog/student-works/project-1-first-term-presidential-election-impact-congressional-mid-term-elections/,As a budding data scientist my first project will look at the composition of the US Congress (Senate and House) by party in the first year of a presidential term and two years after the election starting with President Ronald Reagan.  I’m interested in knowing how the congressional composition changes over time by state (D vs. R) and project what congress could look like for President Trump in two years.  ,NA,The impact of congressional changes has a profound effect on the ability of a President to do his job.  The ramifications of these changes are widespread affecting every American and the world too.,NA
Top 9% Open Kaggle Competition - Santander Products Recommendation,42,https://nycdatascience.com/blog/student-works/18037/,In order to support the clients for range of financial decisions Santander Bank offers their customers personalized product recommendations time to time. Under current system not all the customers received the right product recommendations for them. To better meet the individual's needs and ensure their satisfactionthis challenge seeks to improve the recommendation system by predicting which products their existing customers will use in the next month based on their past behavior. Having a precise and strong recommendation system the sales of the bank can be maximized. At the same time the right products can also help the customers utilized their financial plan. The size of training data set is about 2.3 GB which has 13647409 observations. The test data set has 929615 observation. From column 1 to column 24 are the input features which contain 21 categorical features and 3 continuous features. The input features contain customers’ demographic and status with the bank information. On top of this the observations are in the time series format. The data contains each customer’s information from January 2015 to May 2016.From column 25 to column 48 are the output features which contains the product purchased information according to each customer from January 2015 to May 2016. Each column stands for one product and there are 24 products in total. The final purpose is to make a prediction on which products customers are going to purchase in June 2016. In this case the prediction is going to be multiclassifier. To measure the result the competition is using Mean Average Precision @ 7. From the formula where |U| is the number of the users in two time points P(k) is the precision at cutoff k n is the number of predicted products and m is the number of added products for the given user at that time point. @7 here means the evaluation only take the top 7 products into account no matter how many products are in the prediction. At the same time if the customer does not purchase any product the precision is also defined to be 0. In order to manage and implement a great model on this complex data within two and half week as a team the team were following the steps showing below. By doing numeric EDA we dicovered that there were 24 features contain missing value in the data set. Besides having missing value by columns the data also had missing observations. Missing observations means that some of the customers missing data for certain months in between the overall time range. After deeper investigation there were 5 features being dropped before imputation due to over 95 % of missing value and repetitive information of other features. There were about 4 kinds of imputation strategy implement for this data set. For the features in the ‘Unknown’ column the missing values were all labeled as ‘unknown’. The reason is that the features in this column are more customer’s demographic information. Therefore in order not to make any assumption labeling ‘unknown’ was the only way. For the features in the Common Type column the median of each features were imputed for the missing values because those were the features that described the relationship between the bank or continued variable. The features in the others were imputed by couple different methods. Beside the ‘age’ feature the missing values of rest of the features were fill in based on treating those observation as new customers. From the EDA we were discovered that those missing values were the same observations. And within those observations ‘all the account activities were under 6 months which were also the bench mark for being a new customers. For the ‘age’ features the after scaled mean were using for imputation in order to avoid some skewness in the data. Last but not least there were two kinds of products having missing values. Due to the evaluation penalized the false negative we would like to assume that the products havent been purchased yet. At first we would like to take a look at how the product owned related to customer’s demographic information at May 2016. We could see that no matter which segments of the customers the ‘current cash account’ was the dominated product among all. Since the data set was in a time series format it was important to look at the trend of the numbers of the customers. As the graph indicated there were a big amount of new customers appear in July 2015 and keep growing for a bit for 4 to 5 months.  When we look at how many products does each customer own in May 2016 we discovered that there were customers do not own product anymore. Also most of the customers own 1 to 2 products. The following graphs show that if the customers own 1 2 or 3 products which products have the highest popularity. Instead looking at the relationship between products and customers we also did some investigation on how does the product sales over time. Using these two products as example the first one indicates that there were almost no selling activities for the last 6 months. The second graph shows that that product was constantly sold over the time. In addition we also take a look at the income distribution by cities. The graph shows that the income varies in all the cities. From this interesting information we were using it as part of feature engineering later on. In this project we have several rounds of feature engineering which can be divided into 4 stages. In the first stage the input and output features are encoded from letters to numbers and only the original features in the data set are used in the model training therefore it is 22 features in total. Since the data set is way too large and using all the data will run out the laptop’s memory limit so in actual model training one month’s data  is used as training set. However in order to find the month which gives the best prediction three directions of month selection are performed: using the previous month to predict the current month using the month from last year to predict the same month of current year using one month to predict the situation of three months later. After performing all the combinations the pattern between months are not that clear and the scores based on  are not good.Then using the same idea of month combinations combining adding the previous month’s product information as input features meaning 46 input features in total the performance of models is improved. Also it is found that the best way in this dataset to recommend new products is based on the same month from the previous year. Since then the data of June 2015 is used as the train set to give the recommendation for June 2016.In the following steps what we did was adding or dropping features based on time series kmeans clustering and EDA.Our purpose of the model training is to let the model be sensitive to newly added products. Because machine learning is not that smart we cannot anticipate the model training process to understand what we want them to do we need to provide the model the information we want the model to know directly. Therefore we create a change feature. Change here means use the current month’s product information minus previous month’s product information. This change feature has two levels that is “1” and “0”. “1” represents newly added products “0” represents other statuses. The new features are selected based on the results of time series.Change features have a positive effect on the model to further improve the model we reseparate the change features back to ‘1’ ‘0’ and ‘1’ respectively representing close an account no change and open a new account. At the same time 5 products are dropped from the predicting list because the bank doesn’t sell those products anymore. Since the Kaggle system calculation penalizes more on false negative. Attempting not to miss any prediction class weight  is added for output features based on popularity of the products meaning give more weight for popular products.The month selection methods used in the first two rounds of feature engineering are actually to manually search the time effect between months. Combining the information from the manually searching and time series results based on the three levels of change features more product information from different months are added as new features. Here’s an example that how a certain product information of a certain month  is chosen  based on time series.This is the change of pension account through time. ADF test result shows this time series is stationary which is statistically significant the lag number is 4. According to this information since the data of June 2015 is as train set the product change information of February 2015 is added as a new feature. However again this dataset is weird for month 13 and month 14 there are sharp increase and decrease in the chart. At that time the bank has about 50000 pension accounts and in month 13 there were over 10000 pension accounts being closed and in the next month almost 20000 newly opened pension account. This also tells that randomly adding the change features into the model is not appropriate because it is hard to predict the erratic change in the time series.More feature engineering process involves in the model training procedure and more details will be discussed in the following paragraphs.With following new features: adding 5 previous months’ account history a marriage index (combination of age sex and income) removing city and 5 rare products. Our Xgboost model scored 0.02996 on Kaggle Leader Board and Random Forest model which scored 0.02946 is the second best among our single models.The key to build a good model in this competition is to use June 2015 as the train set because 5 correlated main account types (nom_pens nomina recibo reca and cno) show seasonal changes. Using June 2015 as the training month and account history from Jan 2015 to May 2015 enabled us to capture the time series aspect of the dataset. Removing 5 rare products (aval ahor viv deme deco) also contributed to improve our models we saw a 4% increase in our Kaggle Leader Board score by making this change alone.At the end of competition we were working on a new strategy to improve our model. The new customers who joined after June 2015 showed different product purchasing behaviors from the old customers. We could use their data from July 2015 which wasn’t in our training set to build models for them separately. Although the “newcustomer only” model did not improve the predictions on new customers (Kaggle LB score ~ 0.0297) combining them with the predictions from our Xgboost model trained on old customers could provide better predictions.,NA, Feature engineering is key in this project. Based on the results of the model training each time new useful features are added into the models the scores get improved. In the following paragraphs we will discuss our process of feature engineering.The baseline of our model is the recommendations based on popularity of the products at the end of May 2016. Multiple modeling algorithms were tested at various rounds along with our feature selection process including: Xgboost Naive Bayes Random Forest Neural Networks Adaboost and collaborative filter. In the last two rounds of  feature selection Xgboost and Random Forest stood out and outperformed other models.Multiple ensemble model strategies were tested in our modeling process. Voting helped to improve the quality of our model in earlier rounds of modeling when we had diverse models of similar quality however when later we only have two highly quality but correlated models the voting process stopped helping our models improve. Stacking strategies were also attempted at later stages using Xgboost and Random Forest models however due to high correlation of our models neither of these ensemble model strategies helped us to achieve a model of higher score.Our best model at the end of the Santander Product Recommendation Kaggle competition is Xgboost model with aforementioned engineered features which scored 0.0299626 on Public Leader Board and 0.0302852 on Private Leader Board would put us among top 9% of all the participating teams in this competition.,NA
"IoT news headlines, before and after DDoS attacks",43,https://nycdatascience.com/blog/student-works/iot-news-headlines-ddos-attack/,The famous cartoon series “The Jetsons” predicted 53 years ago how our lives would look like in 2063 when all our “daily devices” such as home appliances or cars would be connected to the internet. In 2016 the Internet of Things (IoT) is still evolving to a convergence of multiple technologies including wireless communication realtime analytics machine learning commodity sensors embedded systems automation and more. On October 21 a series of Distributed Denial of Service (DDoS) attacks caused widespread disruption of internet activity across Europe and the US. This attack was also unique because it targeted IoT devices due to the fact they have soft security profiles. As the awareness of what impact connected objects would have on our lives and on our security. In this work we analyze IoT news headlines after October 21 (from October 21 to October 31) last week (November 1 to November 5) and this week i.e three weeks after the attack (November 6 to November 11).In this work I used Python’s library Beautifulsoup for pulling data out of HTML and XML files from Google News to compare IoT news headlines.In order to compare IoT headlines for both the first and second week after the DDoS attack Word Clouds are generated as a visual representation of keywords in a text as shown in Fig.1.Fig. 1 : Word Clouds for IoT headlines two weeks after Oct 21 (left) and three weeks after the attack (right)We can see that from Fig.1 that for both the two weeks after the attack(Fig.1left) the news were kind of neutral and focused on explaining the cause of the attack. We can find words such as: DDoS Botnet security and users. For the third week after the attack (Fig.1right) we start to see some negative headlines and words such as: bad afterthought overload as well as the word security which seems to appear more often.In next sections of this work we will use Sentiment analysis (also known as opinion mining) to analyze the IoT news headlines and extract some meaningful information to determine the emotional communication in a text document. Python’s Pattern library is used for this purpose.In this part the aim is to measure subjectivity in IoT headlines. Fig.2 shows the subjectivity of these headlines for the three weeks after the attacks.Subjectivity is measured based on the adjectives in a text document. A measure of 0 indicates that the text is not influenced (neutral) and a measure of 1 indicated the emotional effect the author wishes to have on the reader. In the case of IoT headlines we can see for the last two weeks before the attack subjectivity was a bit higher 0.45 than this week 0.35.The overall polarity of a document (from Python’s Pattern library) is also based on the adjectives in a text document. Polarity is a value between 1 and 1 where 1 is negative 0 neutral and 1 positive.Fig.3 shows that IoT news headlines are written in a neutral tone.In the next sections we use Python’s Scikitlearn library for text processing in order to determine the similarities separate text by topic and extract some of the most frequent words. This library has several modules that transforms text into a documentterm matrix.First we calculate the measure of similarity between headlines. Since each row of the documentterm matrix is a sequence of word frequencies in headlines it is possible to put mathematical notions of similarity (or distance) between sequences of numbers in order to establish the similarity (or distance) between different texts. The Euclidean distance is used as a measure of similarity between text documents.Fig.4 shows the principle of distance (similarity) between two vectors in space and the equivalent for news headlines in a text document.We can see from Fig.4 that headlines from the first two weeks after the DDoS attack are quite similar comparing to the headlines of this week.Using the Euclidean distance from section IV we use Ward's method to produce a hierarchy of clustering. Ward’s method starts with each text in its own cluster and finds closest clusters and merge them. When the clusters are merged the distance between them is then changed to the sum of squared distances (linkage distance).For our headlines Fig.5 shows the different clusters. We can see how headlines from the first two weeks are grouped in the same cluster.From the headlines we want to extract the number of topics we have. For that purpose a technique called Non Negative Matrix Factorization (NMF or NNMF) to extract topics within a text.Fig.6right shows that there were two main topics during the three weeks following the DDoS attack. For each topic we have the main key words. We can also see in Fig.6left the frequency of some of these keyword by week.In this work an analysis of subjectivity polarity headlines similarities and clustering  along with extraction of keywords has been performed. These analysis are helpful to assess the evolution of headlines from a week to another. For now the headlines about IoT security indicate it is still a “neutral” subject despite the numerous threats and the debate is still going on about the urgency for viable IoT security solutions.,NA,Fig. 2 : Analysis of subjectivityFig. 3 : Analysis of PolarityFig. 4 : Clustering Similarity between headlinesFig. 5 : Clustering based on similarityFig. 6 : Clustering by topics and keyword frequency by week,NA
Kaggle Competition: Allstate Claims Severity,43,https://nycdatascience.com/blog/student-works/machine-learning/kaggle-competition-allstate-claims-severity/,Allstate Corporation is one of the largest insurance companies in the United State. The main products that Allstate offers are car insurance recreational vehicles insurance home property condo renters insurance and so on. In order to provide better claims service for Allstate’s customers the company is developing automated methods to predict claims serverity. Allstate launched this challenge on Kaggle and invited all the Kagglers to tackle down this task with their technical skills and creativity. The goal of this challenge is to build a model that can help Allstate to predict the server of the claims accuratly. At the same time to provide the important factors that have strong impact on the severity. With these information Allstate can proposed or adjust more suitable insurance packages for their customers. On top of this as a team we wanted to implant the data manipulating features engineering and the machine learning skills that we have learn to perform a strategic process and established a model that has strong capability to achieve the best prediction possible. In order to tackle down this challenge within two weeks as a team we were following the pipline showing above from the begining. The workflow is dividing into three main parts: 1) Exploratory Data Analysis (EDA): Due to all the features are annonymous EDA became a very important stage for us to understand more insights of the data. This stage is also crucial for the next stage  Feature Engineering2) Feature Engineering: Feature Engineering is the stage that allow data scientists to exert their creativity. It is also the stage that can help to differenciate the models from others. For the initial features selections we decide to use unsupervised learning methods PCA and Kmeans to see is there any possibility to reduce dimensions and cluster the features. After the first attampt we also used three different encodi09ng methods: 3) The models we have used are:The complete training data set contain    observations 130 continuous and categorical input variables and 1 continuous output variables with no missing value. The density plot of loss below provide the information about the distribution of the output variables. It indicates that the observation is skewed to the right. To prevent high leverage from the outliers we did a shifting and log transformation on ‘loss’ so it can be more normal distributed without changing the order. Second density plot is the result after traformating the output variable. From the numeric graphic analysis it inicates that the data set has 116 categorical variables and 14 continuous variables for the input features. For the categorical variables the lowest number of levels of variables is 2 levels and the highest number of levels that appears in the variables is 326 levels. With such high number of levels in the variables this is the part will be focused on during the feature engineering. After further investigation another import information about the categorical variables is that some variables containing levels only in the test set but not in the training set. The chart below lists out the unique levels for the categorical variables that have this behavior. For the continuous inputs the correlaiton plot provides the insight among the variables. The plot indicates that there are some strong relationship between some variables. This is important information for building model later on. In order to explore the potential dimension reduction the unsupervised machine learning methods PCA and Kmeans were applied to the training set. Based on the resurlts of the scree plots from PCA and Kmeans the dimensions are not able to have a siginificant reduced. For PCA with about 48 princial components around 88% variance is explained under 95%. For KMeans even with 20 clusters the withincluster variance is still very large. As the result we have to move forward with other feature engineering methods to improve the accuracy and reduce the computation expenses at the same time.In this project we did three different types of feature engineering. The findings form EDA showed that there are more than 100 categorial variables in this dataset. Most of them have two levels which is fine for modling. However a few of them have many different levels; some of them have levels greater than 100. So we did a dummy transformation first. We broke categorical variables with many levels into several dummy variables. It means that for a categorical variable when one of its levels have one observation it becomes a dummy variable having (n1) zeroes. Hence after the first step there were many dummny variables which were near zero variance (NZV) predictiors. We could keep all of them or drop the features which had NZV.  we used both of them to build models however the results were not good. On one hand removing data should be avoided if possible. We were not sure if the NZV predictors wrere noninformative. Those NZV features could in fact turn out to be very inofrmation. However keeping all of features took us long time than expected to build models and it might result in overfitting as well.On the other hand we used the function nearZeroVar from the Caret package to remove NZV predictor. By default a predictor is classified as NZV if the percentage of unique values in the samples is less than 5%. The advantage was that this method saved us a lot of time to build models however the drawback was that it increased the error and resulted in losing some potentially important information.In order to balance between time consumption and error reducing this project regroups those categorical variables over 15 levels to keep as much information as possible. After regrouping this project dummifies the new categorical variables with the rest for the third round machine learning. The good thing in this way is that useful information is kept. However there are multiple ways to group variables.Let’s first explain the reason why 15 levels here is a good cutting point. When using near zero variance to drop features for categorical variables it removes those levels with less than 5 percent of observations. For such a large dataset 5% means around 9500 observations. Even if a level having 9000 observations which must have some useful information will be removed from the dataset. For variables like cat116 which has 326 levels only 3 levels are kept after near zero variance feature engineering. Even in an ideal case only 19 out of 326 levels can be used for the further machine learning which is certainly not desirable. Meanwhile this ‘ideal’ situation also tells the fact that 20 is not a good cutting point. To be more conservative this project determines to regroup categorical variables over 15 levels. In other words keep over 5 percent of observations in each level.Here are two graphs. Each graph represents one categorical variable. Each point here is one level. The xaxis is counts. That is how many observations in each level. The yaxis is mean of the loss for each level. An interesting finding is that in most cases the scatterplot is like two straight lines one vertical one horizontal like in the left graph. The right graph Cat112 is an exception.However the ways to regroup are using the same idea: first keep the original levels having over 5% of observations and then split them based on the average loss and counts regroup them into high loss low count group low loss low count group low loss low count group and so on and so forth.By doing feature engineering in this way it also manually builds a connection between loss and those categorical variables. The next step is the third round of machine learning.The first individual supervised model we tried was Multiple Linear Regression. We mainly used this model to check would there be any difference between different feature engineering methods. The Linear model had been tried on 3 encoding methods. The first encoding method did not go through due to high number of variables has 0 variance. After dropping the variables has near zero variance the model gave the RMSE around 0.57659. On top of using the third encoding method which was to relevel the categorical variables that had high number of levels we also droped the highly correlated continuous variables to test. The model returned  the RMSE around 0.56557 this time which indicated that the third encoding method performs better on the linear model. Based on three types of feature engineering the new group gives the best results. In ridge regression the best lambda is 1e05 and the RMSE is 0.56414. In lasso regression the best lambda is 1.592283e05 and the RMSE is 0.56415.For the random forest model we used the data set from second encoding method which is drop the variables have near zero variance. The default setting was used for the initial fit which is 500 trees and 51 mtry. However the model took two days to finish the training without cross validation. Due to high computational expenses we choose to move forward to other individual models. Three Gradient Boosting model were trained:Four parameter were cross validated based on the subtest data such as ntree (number of trees) shrinkage factor depth of trees and number of the minmun obersevation in the nodes(figure 3&4)From the models with the best parameter the model 2 show the best accuracy (RMSE 0.51)but expensive computation(3 days) the model1 lower accuracy(0.54) but efficient(1.5 days)and the model3 sufficiently appoach the balance of the accuracy(0.53)and the computation(2days).Wealso tried XGBoost from Caret package to build models. Similarly the new group shows the best results. Compared with running XGBoost package directly the XGBoost from Caret already reduces the total number of parameters people can tune to seven. After trying different combinations of parameters we found that there were two of them really affected the model which were nrounds and max_depth. The rest were less relevant. The left graph above shows the result of the cross validation which gave us the best model the corresponding smallest RMSE is 0.5436. Meanwhile the most important feaure is cat80D  (see also right graph above) also similar with the results of gbm.,NA," Different error distributions are derived from different models. The nature of stacking is to combine those error distributions into one ""aggregated"" model in order to compensate each other's weakness.For example as the nonlinear features expanding Neural Network usually works better for the numeric variables than Ridge or Lasso regression does. Meanwhile the boosting model costs less computation on the dummy variables. Here the boosting and neural network models are selected to stack through linear regression algorithm and neural network algorithm respectively shown in the figures above. As the result of stacking the Kaggle scores are significantly improved.According to the RMSEs of each models the best individual model is the gradient boosting machine learning. The rationale behind this best performance is multiple attempts of crossvalidation and the third method of feature engineering  new group. However model stacking gives the best predictive accuracy. The greater the differences of the learning algorithms the better performance the stacking model provides.",NA
Kaggle: Predicting Allstate Auto Insurance Severity Claims,43,https://nycdatascience.com/blog/student-works/predicting-allstate-insurance-severity-claims/,"Kaggle competitions are a good place to leverage machine learning in answering a realworld industryrelated question. A Kaggle competition consists of open questions presented by companies or research groups as compared to our prior projects where we sought out our own datasets and own topics to create a project. We participated in the Allstate Insurance Severity Claims challenge an open competition that ran from Oct 10 2016  Dec 12 2016. The goal was to take a dataset of severity claims and predict the loss value of the claim.The training dataset consists of approximately 180000 observations with 132 columns consisting of 116 categorical features 14 continuous features and 1 loss value column. The features are anonymized into ‘cat1’’cat116’ and ‘cont1’’cont14’ effectively masking interpretation for the dataset and nullifying any industry knowledge advantage. To get a sense of what we are working with we examined the distribution of the data by building a histogram. It was obvious that the data was very skewed to the right. To normalize the data we transformed the data by taking the log of the loss. Since there are so many categorical variables we wanted to find a way to see if we could perform some feature selection. We referred to the Kaggle Forums and saw that we could perform a Factor Analysis for Mixed Data (FAMD). This is sort of a Principal Component Analysis for categorical variables to see if we can reduce our dataset or discover some correlations between variables.
Only 4.08% of the variance in our dataset can be explained from the first five components which are the highest contributors to the percentage of variance explained. As a result there is no one particular variable that dominates nor can we reduce our dataset to only a few components. Since there are so many categorical variables we wanted to find a way to see if we could perform some feature selection. We referred to the Kaggle Forums and saw that we could perform a Factor Analysis for Mixed Data (FAMD). This is sort of a Principal Component Analysis for categorical variables to see if we can reduce our dataset or discover some correlations between variables.However a graph of the contributions of the quantitative variables above to the first two components depicts three different groups that may be correlated with each other: a group formed by the upper right quadrant the lower right quadrant and the lower left quadrant. Analysis of the categorical variables is not as clear.XGBoost and Neural Networks are known to be strong learners and we expected them to perform best amongst other machine learning models. However both as a benchmark and possibility for stacking weak learners we incorporated other models to compare the costcomplexity and overall performance between them. For the Kaggle combination the metric is mean absolute error and we report our cross validation scores and leaderboard scores as such.Lasso (least absolute shrinkage and selection operator) is a regression analysis method which can perform feature selection and regularization in order to improve the prediction accuracy providing faster and more costeffective predictors. To preprocess the data we first wanted to remove any highly correlated variables. Looking at a correlation plot of the continuous variables we saw that variables cont1 cont6 and cont11 were highly correlated with variables cont9 cont10 and cont12 respectively. For the categorical variables we dummified the variables converting them from categorical variables to numerical ones. This expanded the dataset’s dimensions from (188318 127) to (188318 1088).Let’s run the Lasso regression model to explore its ability in loss prediction and feature selection. The training dataset has been split in 80:20 ratio and applied 310 crossvalidation (5 is ultimately selected) to select the best value of the alpha (regularization parameter). We can see the regression coefficients progression for lasso path in the graph below  which indicates the changing process of coefficients with alpha value. From plots below (“Regression coefficients progression for lasso paths ” “Mean squared error on each fold”) the best alpha value is 5.377 which could help reduce the number of features in the dummy dataset from 1099 to 326. The error histogram shows the optimized Lasso regression model prediction of the 20% test dataset the R squared value of this prediction model is 0.56.We then fit our data using a KNearest Neighbors Regression model. The tuning parameter is K the number of neighbors for each observation to consider. We start with sqrt(N) which was approximately 434 as our initial guess and tune K from 425 to 440.  K  425 performed the best in this range though that leaves open the possibility of tuning K in a range around 425. The model gave a CV score of 1721 and a LB score of 1752.We also used  Support Vector Regression to fit our data . Preliminary cross validation and parameter tuning on our test set revealed that the algorithm was computationally expensive taking ~12 hours on our machine. Using the sklearn class SVR from the svm module we attempted to tune cost and epsilon using a radial kernel and setting gamma to 1/(number of features). Preliminary tuning revealed an epsilon value of ~ 1.035142  and a cost of 3.1662 giving us a CV performance of 1570. The performance varied greatly amongst the few parameters we chose to test from 10 to the power of [1 0.5 0 1] for C and 10 to the power of [0.05 0.01 0.015 0.02 0.03 0.1 0.5] for epsilon for our grid.The Random Forest algorithm is a good outofthebox model for most datasets since they are quick to train perform implicit feature selection and do not overfit to the dataset when adding more trees. For this model we used the scikitlearn’s package: RandomForestRegressor. To train and validate the random forest model the data was split using 10kfold cross validation. The parameters we tuned were the number of trees and the number of features considered. Initially the model was trained using all features considered per split. This is essentially equivalent to bagging which performed poorly scoring a MAE of 1312 on the leaderboard. To improve on this we decreased the number of features down to the square root of the number of features. This resulted in the random forest only considering 12 features per split. Using less features forces the model to consider different features per split which ended up improving the model’s MAE score to 1188.Training the model on an increasing number of trees improved the predictive power but took more computational power. To prototype the effects of a model quickly we used 50 trees to get a sense of the effect and then 200 trees for more computational power. AddIng more trees will help the predictive power but with decreasing returns. With 200 trees the best random forest MAE score was 1187. This is the baseline score which we wanted to beat with our more competitive models.The Neural Network model turned out to be one of the better performing algorithms. For this competition we used the Keras (frontend) and Theano (backend) Python packages to build a multilayered perceptron. Our best Neural Network consisted of threehidden layers with rectified linear unit activation.Additionally we added in dropout and batch normalization as methods to regularized the network. The model was then run with a 10 kfold cross validation and 10 bagged runs per fold to essentially produce an output that is the average of 100 different runs to minimize model bias. One downside of Neural Networks is that it is computationally expensive. It requires computing many large matrixvector operations. With an Nvidia GTX 1070 GPU our model required 5 hours to train. Furthermore when we tried to add layers or more neurons the model started to overfit. The model resulted in an average validation mean absolute error of 1134 and a leadership board score of 1113 that put us in the top 25%.One of the most popular algorithms currently among Kagglers that proved to be successful is XGBoost. This algorithm takes a linearized version of gradient boosting that allows it to be highly parallelized and computed quickly. This allows large number of trees to be produced per model. We chose a learning rate of 0.01 with a learning rate decay of 0.9995. This model also used an average of 10 fold crossvalidation with a maximum of 10000 trees stopping when the validation error is minimized. The XGBoost model proved to be our single best performing model with a validation score of 1132 and a leadership board score of 1112.In the Allstate insurance dataset the data was highly skewed right with outliers taking on large values. Looking at our validation predictions against the true values the largest errors accumulate around the outlier points. For this reason we wanted to see how well we can classify if an observation was an outlier. We chose the threshold that separates an outlier to be two standard deviations above the average loss value. Although the outlier region only made up 2.5% of the data it made up more than 90% of the range of values.We first tried to use a logistic regression classifier to establish a baseline. This model resulted in a 97.5% accuracy which sounds good at first until we realized that this is only as good as the model guessing that all observations were nonoutliers. Next we tried a more advanced model the XGboost classifier with AUC score as the metric to maximize. However when we looked at the output predictions the probabilities were very close to 0.5 for all observations which told us that the model could not confidently distinguish the two classes. Also the AUC score was 0.6 which was much less than desired.The way this problem was set up it turns out to be an imbalanced dataset problem where the minority class was much smaller compared to the majority class. One traditional method to deal with these type of problems involved oversampling and artificially synthesizing new minority class and undersampling the majority class. Here we used the ImbalancedLearn Python package to readjust our data ratio from 97.5:2.5 to 90:10. Afterwards we again used XGBoost classifier and achieved much better results. The accuracy was raised to 99.6% and the AUC increased to 0.80. Although this was very insightful this new information did not help our regression model much so we turned our attention to other methods raise improve our error rates.Ensembling is an advanced method that combine multiple models to ultimately form an better model than any single model. The idea is that each model theoretically makes its own errors independent of other types of models. By combining the results from different models we can “average” out the errors to improve our score and reduce variance in our error distribution. For this competition we chose to do three different ensembling methods with two XGBoost and Neural Network models: 1. Simple average of the test results.2. Use an optimizer to minimize error of model validation predictions against true values.3. Weighted average of test results. With our best scoring model with a MAE of 1101 we were placed at the top 2% of the leaderboard by the end of the two weeks. XGBoost lived up to its reputation as a competitive model for Kaggle competitions but could only bring us so far. Only after we applied neural network models as well as the method of ensembling we were able to get to the top 2%.While trying to perform competitively in the Kaggle was tough. The two week time limit for this project in the bootcamp definitely amplified the difficulty. In order to perform effectively we needed to have good communication and a good pipeline for testing each model especially since some models took hours to train. It took a while for the team to build this communication and pipeline up but eventually we were able to share knowledge and get multiple workflows running.Although the Kaggle competition was a great way to test our mettle against other competitors using a realworld dataset there were some detractions in this format. By having a dataset given to us in a clean format the process of taking data and churning out predictions was accelerated greatly. Moreover we lost out on attempting to interpret our dataset due to the anonymity of the variables. However this did allow us to focus on practicing fitting and training models — a huge plus given our limited time. In the future we would like to incorporate the method of stacking models to see if we could improve our score even further. In addition we would like to explore other ways in handling the problems with our uneven dataset using methods like anomaly detection algorithms rather than binary classification methods.",NA,Finally when predicting on the Kaggle test dataset using the Lasso regression model the prediction results did not rank into top 200 on the Kaggle Leaderboard score. This was not surprising due to a couple of reasons. First the data does not represent a linear relationship so the model’s prerequisites and diagnostics were not good. Also after the Kaggle test dataset was dummified we noticed that there were variables that were present in the test set that did not exist in the training set.First the simple average resulted in a leadership board score of 1108 already much better than our single best model. Second the optimizing method resulted in a leadership board score of 1105 even lower than the first score. Lastly we chose to weigh the better scoring XGBoost and Neural Network heavier with 40% weight each and the remaining two with 10% weight to sum to a total of 100%. The intuition there was to having the very different models cancel out each other’s errors while focusing more on the higher scoring models. This resulted in our best leadership board score of 1101.,NA
Optimizing Machine Learning Algorithms to Model Allstate Loss Claims,43,https://nycdatascience.com/blog/student-works/optimizing-machine-learning-algorithms-model-allstate-loss-claims/,A viable business model for an insurance company has to involve some way to assess how much payout they can expect for a given customer. If the payout is larger than what the company has accumulated from the customer’s payments then a loss is incurred. Allstate Insurance is trying to determine if they can predict the loss to expect from a customer. To gain a better understanding of how this can be done Allstate submitted a  where the goal was to see if participants could accurately determine the loss incurred for a given customer given various features about them. We participated in the Kaggle challenge with the goal of learning how to best optimize machine learning algorithms use feature selection use exploratory data analysis and a wide range of machine learning techniques. By accomplishing these goals we could then earn competitive score.Through Kaggle Allstate provided a training data set and a test data set with 188318 and 125546 customers respectively. Customer features consisted of 116 categorical variables and 14 continuous variables. There was a lack of domain knowledge as the categorical variables were in the form of ABC  and the continuous variables were scaled to range from 0 to 1. As a preprocessing step for our models we created dummy variables from the categorical variables and we tested the data for nearzero variance. We noted there were 54 factors that were removed every time We eliminated these factors in advance to cut down on preprocessing time.The loss variable from the training data set was greatly skewed to the right indicating extremely large loss values. To transform loss into a more normally distributed form we performed a log transformation. This can help improve the accuracy of methods not robust to outliers. Figure 1 presents the two forms of the response variable before and after the transformation.The left density plot shows the original untransformed response and the right density plot is the log transform of the response. We chose to use the log response to satisfy the regression assumption of normality.A look at the linear correlation between the continuous variables with themselves and with the loss dependent variable in the training data set reveals two important results shown in Figure 2. First none of the continuous variables have a strong linear correlation with the log loss. This suggests that a linear model is not likely to perform well. Second there is a high degree of collinearity among some of the continuous variables. While one could remove these collinearities (for example by performing Principle Component Analysis) we did not need to do so as the nonlinear models we employed do not require such assumptions.In order to streamline our team’s workflow we wanted to make the process of trying out different models as easy and efficient as possible. Therefore we created a model maker pipeline which takes in parameters specific to a model and outputs performance results as shown in the diagram below. The model maker employs the caret package an ensemble suite of many different machinelearning algorithms implemented in R. It also provides convenience functions for performing feature selection crossvalidation and parameter tuning. Utilizing caret allowed us to automate these steps as a streamlined workflow. For example suppose we want to test gradient boosting. We then simply specify in a parameters file the method name “gbm” a grid of parameters to test crossvalidation settings (the partition split sizes number of folds etc.) as well as miscellaneous parameters for that run such as whether to take the log transform. An example model parameters file is given below.Given these parameters the model maker performs preprocessing and crossvalidation on the grid of parameters builds the optimal model and outputs a timestamped folder of results and a Kaggle submission file. Results included the estimated MAE from crossvalidation and the optimal model parameters. Allstate defined the score as the Mean Absolute Error (MAE) between the predicted losses and the true losses. A lower MAE results in a higher score.Parallelization was utilized in R and model tests were run on the server. The server we used was the Brandeis University HPC Cluster. The server had multiple servers available with each server proving 32 cores of computational power and several hundred gigabytes of RAM. This allowed us to attempt more complex models including KNN which requires a large amount of RAM.We trained and tested 4 nonlinear models in the R Caret package: kNearest Neighbors (kNN) Gradient Boosting Machines(GBM) XGBoost and Neural Networks. The advantage of kNN is that there is only one parameter to consider the number of neighbors. The disadvantages are that it takes a long time to run on large data sets (it runs in (n) where n is the number of observations) it requires a 100GB of RAM on this dataset and it is not as accurate as other methods we tried. The final Kaggle score that we got with kNN was 1507.53360 MAE. The advantages of GBM are that it is robust and generally performs well out of the box. We took our final set of parameters from tweaking suggestions on the Kaggle Forum and got a score of 1163.47311 MAE. The final model used 5000 trees with an interaction depth of 5 a learning rate of 0.03 and 20 minimum observations per terminal node. Advantages of XGBoost are that it is well parallelized in R and can yield results significantly faster than GBM while the drawback is that there are many parameters to tune (7 in total). XGBoost is often the most competitive method in Kaggle competitions and some variant is often utilized in the winning solutions. We looked to the Kaggle Forum for suggestions on the parameters and we crossvalidated among four parameters (Figure 3). It is illustrative to observe some trends in this figure. For higher learning rates (eta) higher depths in trees perform worse than shallower trees. Conversely for lower learning rates the higher depth of 12 can outperform the shallower tree models. In other words if the learning rate is high our trees should be simple as to not overfit. If the learning rate is low more complex trees can improve performance. Our Kaggle score for XGBOOST was 1148.65697 MAE and our final parameters included 3000 trees a learning rate of 0.01 a max depth of 12 and a regularization parameter or gamma of 2.Neural Networks have the advantage that they are very flexible and are robust to outliers. The disadvantage is that it is difficult to find parameters that yield the best global solution. We decided to use the  R package and to utilize one hidden layer which simplified the possible network topologies to consider. Apart from the number of nodes in the hidden layer the  package allows a second parameter to tune called the . The purpose of this decay parameter is to prevent individual large values of weights from dominating the network and to prevent overfitting. The use of the decay parameter in  can be compared to the shrinkage parameter in Ridge regression. We performed crossvalidation on the entire training data set and found that ideal values for the  as well as the number of nodes. We decided to use a  parameter of 0.1 and 25 nodes in the final network model. The Kaggle score for this neural network model was 1206.69697 MAE.A summary of model results is given by the following table.To summarize we found that for this problem XGBOOST performed best followed by GBM. In third place after the tree methods was the neural network model and kNN performed the worst. Also even the best submitted Kaggle result still had a mean absolute error of over 1000. This tells Allstate the limit of how much they can forecast is within about $2000 of accuracy. Clearly judging the validity of claims using a model can be difficult.,NA,The collinearity plot used. The response variable loss has been transformed to the log scale. We can see that variables cont11 and cont12 are highly colinear. Loss has only very slight collinearity with continuous variables cont2 cont3 (negative) cont4 cont6 cont7 cont 8 cont11 and cont12.,NA
The Dendrotrons: Allstate Claim Severity Kaggle Competition,43,https://nycdatascience.com/blog/student-works/dendotrons-allstate-claim-severity-kaggle-competition/,"What do you do when as a member of a team called ”The Dendrotrons"" in a Data Science cohort have a twoweek timeframe to work on the  challenge (predict the loss for Allstate claims) and present your results and insights? This article will walk you through our team’s journey for the Allstate Kaggle competition covering our experience in:The team consisted of six members with individual strengths in business engineering development project management production support and academic research. This blend of skills and experience provided a good influx of ideas and early experimentation to determine if strategy and tasks were aligned with objectives. The role of PM (Project Manager) was assigned to a team member to ensure the project timeline and deliverables were being tracked and making progress.Having strong communication skills is core to achieving success either as an individual contributor or when working in a team. For our team we agreed that having a communication protocol was a primary focus to ensure deliverables met timelines during the twoweek project schedule. The topic was discussed during initial team meetings to ensure an agreed upon protocol would work for all team members. The communication process included daily scrum sessions (max 15minute meetings to sync up and discuss project status / updates) continuous feedback via a team channel in  and file management in  and .Working with a two week timeline required for the team to operate under assumptions and expectations that typically take more time to formulate in a team environment. In referencing Agile team development process the team had to transition quickly through the first three phases of Form Storm Norm by agreeing that trust / respect / accountability had already been established given our experience as peers and individual project contributors during the past two months of training and project work in our cohort. This precondition allowed the team to agree on strategies and operate within the Perform phase thus helping the team achieve the core objectives by the project submission times. The scheduling process maintained a continuous delivery of tasks with very little bottleneck across task interdependencies.The team was evenly split into two subteams of three working on two major tasks in parallel: EDA and the Kaggle submissions. This allowed for Kaggle submissions to happen within the first week. ML was used for exploratory analysis and XGBoost was the preferred model used for prediction. The early experimentation process between the sub teams produced continuous feedback within the team; thus influencing the next steps for EDA / prediction. The feedback loop led to synchronization discovery and insight.Given the relative large number of features and observations the first step in the EDA process was developing a method to visualize the dataset as a whole. To this end a novel visualization tool was developed which generated grayscale images from every 25 or 50 observations after the dataset was sorted by increasing log loss. This high level view easily allowed patterns in the dataset to be seen. Furthermore being interactive in nature it allowed the user to simple click on a region in the image to generate traditional box or scatter plots for more detailed insights. This process is illustrated in the figure below.The continuous features (cont1 to cont14) were plotted together with the response variable loss. All features exhibited skewness including the response variable loss. To handle the skewness of the response variable we log transformed the data and performed a BoxCox transformation for the continuous features.The train claims severity dataset has 188318 instances and 132 features or attributes. 116 of these features are categorical variables and 14 are continuous variables. For this case study we need to predict the 'loss' based on the dataset features. Since we can’t use unique categorical features from the test dataset to make predictions an interesting part of our analysis is to determine if our test and train set have the same categorical variables.  We found that 45 variables are presents in the test dataset and not in the train dataset. This analysis could be beneficial in the feature engineering FOR THE PURPOSES OF THE KAGGLE COMPETITION in order to incorporate this variables to our machine learning model to better predict these cases. Figure.1 shows an example of missing variables in the test and train dataset for the categorical variable cat111. The variable F is missing in the train data and the variable D is missing in the test dataset.We first determine the correlations between continuous variables. Fig.2 shows an overview of the correlation matrix of all continuous variables (the darkest color is for the most correlated variables) and Table 1 shows some of the most correlated variables.This correlation analysis represents an opportunity to reduce the features through transformations such as PCA.From the correlation analysis we can see that we there is a potential possibility to reduce the number of continuous feature set. We use PCA to determine how we can use describe our continuous features in a reduced dimension subspace. Fig.3 shows the cumulative explained variance or variability from our PCA analysis and the number of components describing the continuous variable data. We can see that with 7 components we can explain 90% of the continuous data i.e half the total of the continuous features in the dataset (total of 14 continuous attributes).As we can see from Table.2 and as we did for continuous variablesThis correlation analysis represents an opportunity to reduce the 2labels categorical features through some dimension reduction transformation. Since our 2labels categorical features are transformed to numerical 0 and 1 variables we use SVD for this kind of “sparse” data in order to determine as we did for continuous attributes the reduced components that describe these categorical features. Fig.4 shows that 90% of the of the 2labels categorical attributes are described with 26 components i.e 36% of the total number of the 2 labels attributes (in total we have 72 binary categorical attributes).As Allstate did not reveal the true meanings of their predictors any attempt to find relationships among those predictors against the outcome (i.e. the loss) seemed meaningless.  However it is reasonable to speculate that the categorical variable “cat112” is the “State” indicator as it had 51 different values.  With such speculation we attempted to find some possible grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss.  Visualizing the ""cat112” column with loss the group finds that 20 of the states have mean loss exceeding the national loss. On a business perspective this is a helpful indicator for AllState to consider calibrating its policy pricing in these 20 states to minimize future losses. With such speculation we attempted to find some possible grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss.We chose  as our unsupervised machine learning for this exploration.  Following is the dendrogram of average linkage into 5 groups:The first cluster (shown inside the second red box from the left) only has two states (Q & J as ALL is NOT a state).  When we compared the statistics of this cluster against the other four clusters we discovered it had the distinctively shortest distance/range from min to max. The fifth cluster (shown inside the first red box from the left) had only one state (AQ) When we compared the statistics of this cluster against the other four clusters we discovered it had the distinctively longest distance/range from min to max as it contained the min and max of the whole dataset.We thought we could fold the first and the fifth clusters into the second cluster (shown inside the third red box from the left) for two reasons:With such an arrangement we could attempt feature engineering based on only three groups.For details please review [] below.For this project we tested several models both  and . But due to the sheer number of features and observations we tried to reduce the size of the data set to speed up the processing time for a first pass of model training. For example we used Caret’s near zero variance function and we also leveraged a quickanddirty linear regression and selected features for more computingintensive models based on their . Because most of the features were categorical we also tried reducing the number of factor levels prior to . For example cat116 had over 300 levels but because it didn’t have much predictive capacity we reduced the number of levels to three.To reduce the number of rows we used random sampling. We first created an 80/20 train/test split out of the Kaggle training data in order to test our results before running the trained model on the official test data. However within this training set we further reduced the number of rows for initial tuning rounds of new models. For example we might only take 40 features of 30% of the training observations in order to do a quick first pass of a random forest or to test how efficiently  can parallelize the training on our 16core .  We tested a linear regression model boosted trees (of which XGBoost performed the best) and a singlelayer neural network. For classification we used logistic regression boosted trees and a support vector machine. Overall we found that XGBoost was the best single model with a Kaggle MAE of $1126 and a 50/50 average of the Neural Net and XGBoost gave us an MAE of $1124.It proved very difficult to deploy any regularization or observation sampling when finetuning a model. We found that after all but the most cursory training passes we had to use all the data and all the features to tune model hyperparameters. The below chart illustrates this point.The tuning grid which was run for XGBoost on about 50% of the training observations makes it appear as though a learning rate of 0.01 was superior to 0.05. However when we used all the training data we found that a learning rate of 0.05 with a smaller number of trees was superior for both outofsample testing and Kaggle results. This point is further illustrated by the below variable importance chart.The second column “Other” is comprised of the remaining 110 observations that are not depicted on a standalone basis. Though each of them individually is less important that cat113 on the right their collective importance is the second largest contributor to variance reduction. Therefore any reduction of features or observations (via increasing sparsity) is bound to reduce the predictive power of the model.After plateauing at about $26 away from the best ranking Kaggle competitor we tried to determine exactly where our model was underperforming. The below chart shows the cumulative absolute error against the observed log loss. We can see that most of the loss was accumulated against observed log losses of about 6 to 9.5. In other words there wasn’t much to be gained by trying to improve the model for very small or very large observed losses.Therefore when we first plotted this below chart we weren’t overly concerned with what first looks to be very extremely poor predictive power on large losses. For the purposes of the competition it wasn’t important.Below is a detailed view (on a log scale) of the range of observations for which an improvement in model tuning would have the largest impact on the MAE. This chart depicts two main points. First the overestimates are generally larger than the underestimates. And second the underestimates get worse for larger observed losses. Therefore a good strategy to follow would be to seek out a model that is skewed in the opposite direction. Also we tried to use this information to engineer a new feature to help the model compensate for itself. However this was unsuccessful.Ultimately though we were only $26 away from the best Kaggle score this best score was still a $1000 MAE. And this is on a data set where most losses are around $3000. To zoom back to the big picture of what we were doing we plotted a linear regression on 6 features (blue) versus our best XGBoost model on the total set of features. XGBoost is certainly better but its predictive capacity is still limited. This gave us an idea...This insight led us to a classification problem!!We revisited the original motivation for the Kaggle competition. This passage is part of the competition description:“When you’ve been devastated by a serious car accident your focus is on the things that matter the most: family friends and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. “If the goal was to use predictive modeling to reduce paperwork then we thought maybe a classifier question might be useful. After all a major reason you have to go through so much paperwork for an insurance claim is that it protects the insurer against fraud. If we could split the claims into “big” and “small” categories maybe we could identify claims that look “fishy” meaning claims that look as though they ought to be small but were in fact big (the opposite is also a problem but not for the insurance company). If you were reasonably sure that a given claim amount was in the right bucket perhaps Allstate could reduce the amount of paperwork that the lion’s share of customers goes through This improves service for the customers and a reduces cost for Allstate. However if you weren’t sure you could classify the claim correctly the customers would have to go through the same process they do now.We found that half the claims by value were made by 80% of the customers so (following the 80/20 rule) it seemed that creating a categorical feature “isSmall” and splitting the data at the mean was a good starting point.We ran a support vector machine a gradient boosting classifier and logistic regression. As we were running out of time and running into model errors on the logistic regression we pared down the number of features to about 30. We found that all the models had a high accuracy (and not much different from logistic regression with only 30 features!) but we needed to optimize for sensitivity (i.e. “fishy claims”). Logistic regression was the best choice under the circumstances because it expresses its classification as a probability. We found that a cutoff of 50% probability to distinguish a small claim from a large claim may not be suitable due to too high a number of false negatives (claims the model predicted as being large but were in fact not) because that cutoff point yielded only a 50% sensitivity. Rather we needed to minimize the false positive rate.The below plot depicts the tradeoff that has to be made in an intuitive way for a decision maker. Starting on the left we can see that if we set the cutoff very low for the logistic regression more people gain but Allstate is exposed to fraud risk on 20% of their customers. As we move to the right you can see that the fraud exposure is reduced but people move from the “gain” bucket to the “same” bucket. This rate of swapping buckets starts to increase significantly after about 0.70.8. At that point there needs to be a 7080% chance that a claim looks like a big claim to be classified as such. This means that some genuinely large claims may be misclassified as “fishy” and have to undergo the current paperwork process. To determine the cutoff point you would need a subject matter expert to express the “gain” and “risk” in dollar terms and then the decision of where to set the cutoff point would be more clear cut.Recognizing the strength of individual team members and establishing a communication protocol that promotes continuous feedback during the project initiation phase is crucial for mapping tasks to resources successfully integrate the contributions and deliver quality work and service on time. Working as a team on a Data Science problem was the first for many of the team members. Thus we walked away with many lessons learned to be applied to future teambased projects.Working on Kaggle competitions as a team allows for many ideas and experiments but it is important to manage timelines and expectations so that all team members remain on track progressing towards the same end goal. We discovered that using machine learning could lead to further model experimentation through feature engineering. Also that reducing crossvalidation folds is a mixed blessing: useful initially but becomes easy to overfit. The constant flow of communication is what led to the discovery of a business insight and further exploration of the idea with classification models.Given more time:As Allstate did not reveal the true meanings of their predictors any attempt to find relationships among those predictors against the outcome (i.e. the loss) seems futile. However it is reasonable to speculate that the categorical variable “cat112” is the “State” indicator as it has 51 different values. With such speculation we attempted to find potential grouping of states based solely on the basic statistics (mean median 25th and 75th quartiles min max) on each state’s loss. We chose Hierarchical Clustering as our unsupervised machine learning for this exploration.We compiled a data frame with 52 rows (51 states plus statistics of the whole dataset “train.csv” denoted with row name as  “ALL”):We scaled above figures before we calculated pairwise distances.  Then we plotted three dendrograms with three different types of linkage (single complete average).  The Dendrograms of Complete linkage and Average linkage both visually show balanced fusions starting from the bottom.  As Complete linkage is sensitive to outliers we chose the Average linkage for further exploration and we cut that dendrogram into 5 groups:We found the first cluster only has two states (as ALL is NOT a state) and the fifth cluster has only one state:We further investigated the basic statistics of each cluster:Our findings:We thought we can fold the first and the fifth clusters into the second cluster for two reasons:With such arrangement we can attempt feature engineering based on only three groups.",NA, In the claims severity dataset we have 72 categorical variables that are represented by 2 labels A and B. We transform these labels A and B into 0 and 1 respectively to transform them to numerical features so we can determine the correlation. Table.2 shows the most correlated 2labels categorical data.We first grouped a series of confusion matrices into different sets of customers. The customers who “gain” are the ones who get to do less paperwork; in other words they would have to do less paperwork than they do now because the model would label their claim as not fishy (true positive or true negative). The customers who are the “same” have to do the same amount of paperwork they do today (false negative). They are comprised of large claims that were incorrectly labeled as smalllooking. The ones who are “risks” are the customers who have fishylooking claims (false positives). The below table summarizes the categorization:,NA
Tree Troubles -- Predicting Sidewalk Damage Resulting From Trees In NYC,43,https://nycdatascience.com/blog/student-works/tree-troubles-predicting-sidewalk-damage-resulting-trees-nyc/,"The ultimate dataset consisted of the following features ""tree_id"" ""year"" ""tree_dbh"" ""health"" ""spc_latin"" ""spc_common"" ""root_stone"" ""root_grate"" ""root_other"" ""trunk_wire"" ""address"" ""zipcode""  ""boro_name"" ""longitude"" ""latitude"" ""block_code"" ""sidewalk"". Details on these terms can be found at the dataset link above. Clustering was done by first generating a dissimilarity matrix using the “” distance then using the “” function to find the best number of clusters. Using sample datasets (1000 obs) containing all the geolocation related features (i.e. address zipcode boro name longitude latitude) the optimal number of clusters found is 6. These clusters more or less corresponds to the boro the trees are located in. See image below.Removing all geolocation related features with the exception of longitude and latitude the optimal number of cluster is now found to be 2 which corresponds to the sidewalk condition of either damage or not damaged.",NA,"Tree roots growing under sidewalks often cause cracking or lifting of the pavement once the tree surpasses a certain size. This creates significant tripping hazards for pedestrians and liability issues for property owners. Furthermore the cost of repairing such damage is in excess of s. As such this project seeks to:In  conducted volunteerpowered campaign to map count and care for all of the city's street trees. This dataset consist of:The technology employed is a mixture of Python R and Java. Python scripts are used for performing data cleaning and merging as well as web scraping tree species data from . R scripts are used for performing numerical and visual EDA and for running machine learning algorithms.  For the desktop analysis application and to generate heatmaps (see below) Java is used.  Java will also be used to develop a mobile application.In order to quickly check for a relationship between tree diameter and sidewalk damage a heatmap is generated by sorting the data by increasing diameter (from 3 to 70 inches). The magenta color represents the different species of trees. The red and greens pixels represent ""damage"" or ""no damage"" to the sidewalk respectively. One key takeaway here is that there isn't an obvious relationship between sidewalk condition and tree diameter. Another take away is that even though there are 132 tree species in the dataset only a small number make up most of the trees planted (see bar plot below).The associations between the predictor variables in the dataset and sidewalk condition is also compared using either a  function or the  in R.  The strength of association ranges from 0 to 1 with a value of 1 indicating perfect association between two variables.The  was used to run various machine learning classification algorithms on full dataset using the typical 80/20 (train/test) split validation method. The accuracy results are outlined below.Overall the accuracy results for these algorithms was fairly close and given the nature of the problem simple the Logistic Regression models were found to be well suited for use in the analysis application described below.  As for what features are most important in determine sidewalk damage both the tree based and logistics regression models are in overall agreement that having blocks around the trees (rootstone) tree diameter and location play important roles.In order to make the models useful for use by non technical users a desktop applications that performs analysis on the “dead trees” data to predict the potential for sidewalk damage at various years (10 20 30 50 75) in the future is has been developed.Additionally the application also allows for rapid visual analysis by making use of bar plots and links to Google Maps to view the area and even the dead tree in question.By making use of the NYC 2015 Tree Census dataset a classification model with an accuracy of over 75% in predicting root induced sidewalk damage was developed.  Moreover a Java based desktop application was developed around this model to help stake holders assess the likelihood sidewalk damage in the future if a certain species of tree is planted at a particular location.  The next steps for this project are:",NA
Behind the Flight Deal Scrapy Project,44,https://nycdatascience.com/blog/student-works/behind-flightdeal-scrapy-project/,As I mentioned before there can be inconsistencies in the data you want to scrape. Structurally a website is usually wellorganized but without a schema or some sort of validation the data itself can be wildly inconsistent. Here are a couple of examples from Fare Availability on The Flight Deal:,NA,"So you're excited about a great idea you've found a great site that looks easy to scrape time to jump right in and start writing a scraper right?First things first it's crucial to examine the data you wish to scrape. Is it well organized? Will it be difficult to clean later? Sentences can be notoriously hard to clean because sentences have to be parsed somehow to extract meaningful data. You and I can read a sentence and easily get the relevant information but for a computer to parse the necessary information it requires precision. The more variation in your sentences the harder it is to scrape and clean. And if you need to convert whatever you scraped into numeric data or dates then that is perhaps double triple the work. Examples shown in a bit.Secondly some websites provide an API and indicate their preference to not be scraped. Though there are certain advantages to web scraping such as getting data in real time most of the time it is better off to use the API. The data retrieved from an API is most likely well organized and validated by some sort of schema.If you decided the data will be relatively easy to clean and that it's better off to scrape than to use an API if available then:Scraping first involves collecting data from the right locations. We start by visually locating the data we want and any patterns they follow. Is the information in a table? Is it in the same location on every page?For  the data we want are inside the title links located in each box. There are ten per page. As we go through each page the URL stays consistent only updating the page number as the page changes. Overall the structure of the website is quite consistent.Clicking on a few of the links the overall structure again seems well organized. Also the data I want is there such as posting date routing airline in the title price and so on.Pain Point #1: Not recognizing that scraping lines of text or sentences means a lot of cleaning laterPain Point #2: Failing to recognize that the data is spread across entries one per page such that it is difficult to see if the data really is well organized or not.Once we have an idea of how we want our scraper to access the data we can use inspect element to find the HTML to directly locate where our data is. Remember inspect element is your friend. Look for any tags or patterns in the classes that might uniquely identify where the data is.The next thing to do is make sure we can access the data. Sometimes what you visually see may not be the correct path  a class tag may be nonexistent to the scraper some javascript might be in the way and so on. The smart thing to do is to test these paths and see what they link to before implementing them in your web scraper. For Scrapy we can use the Scrapy shell.Pain Point #3: Trying to test Xpaths with a Scrapy spider instead of using the Scrapy shell to test first.The first thing to try is to see whether the data can be accessed using the unique attribute of the tag it's enclosed in or use the tag itself if the data is enclosed in multiple instances of the tag. For example a list or a table. If that doesn't work try accessing the data's parent container. Often sifting from the parent container downwards gives good insight on how to access the elements nested within. Again using inspect element may not always offer the right path but using the Scrapy shell can reveal the proper path to use . I won't go into code here but if you are using Scrapy remember response.css is your friend.This step is the fun part. After you've made sure you have the right paths via Scrapy shell it's time to write the code for the Web Scraper! For Scrapy there's a template that is somewhat easy to follow. Declare your item fields think of the patterns from Step 1 for the spider to follow the target paths from Step 2 for your spider to extract information from  create the pipeline and make sure you set your settings! Download delay is important to ease the load of crawling on a server and to prevent getting kicked off.Pain Point #4 : Forgetting to set the pipeline in the settings for Scrapy and not quite sure why information isn't being gathered.For The Flight Deal I implemented my spider to access each of the ten title links extract the data from within those links via the proper Xpaths or css paths and then move onto the next page with the next set of links. Hint: you can do this by creating a list of urls or using a regular expression to indicate what rules for the spider to follow. In my spider I used a regular expression to limit my spider to crawl from page/1/ to page /999/. I also used BeautifulSoup to extract airport code and city from Orbitz.It would also be wise to do a few test runs before doing a full on run of your web scraper. If you have to access multiple pages testing different ranges of pages and seeing the output is good too.Did you make sure to follow Step 0 and that your data will be relatively easy to clean? Did you also know that even without scraping cleaning data takes up the majority of the time for a data scientist?Pain Point #5 : Spending more than 48 hours learning the pain of cleaning and validating data from text. Yes 48 hours of just cleaning! This involved intimately learning how to  regular expressions think of clever ways to convert date strings into consistent dates with ordering and constant test and retesting. It was like running chemical reactions in a laboratory thousands of times until I got the right formula or designing a product and going through rounds and rounds of user experience testing.Before I delve into an optional section on my struggle with cleaning my data from The Flight Deal I'd like to mention there are two ways to approach cleaning data from web scraping. One way is to parse the data in the web scraper prior to outputting the data and another way is to output raw data and then do it after or both. For my project I thought it would be best to extract the relevant parts of text first in my web scraper then format the data itself later.Valid for travel on the outbound until December.Valid for travel early November  mid December or January 2017.Valid for travel in December.Valid for travel early January 2017  late March 2017 or April 2017  May 2017  July 2017  August 2017.Valid for travel January 10 2017  August 20 2017.Valid for travel November 10th  mid December.It took a many rounds of testing rewriting my regular expressions and scraping and rescraping to capture these cases.Further there are  which I decided not to handle:Using the flexible data search on  we found the following valid travel dates.How would I even parse that into a range that the majority of the data was following?Furthermore notice that some of the text has a day or a year and some of the text has either ""early"" ""mid"" or ""late"".  These date strings would have to be cleaned into a consistent format later on which would require again to handle different cases.Valid for travel November 10th  25th..Why? After careful inspection I missed a single ? next to a space. In most cases there is a month in front of the day with a space in between them. Without the month that space isn't there either!Unlucky for me a majority of the data I wanted did not have a unique identifying attribute or tag. Rather they were all in the same large container under bullet points. So I had to write multiple regular expressions to detect the proper location of the data I desired. Whereas the remaining data had less variability than travel dates there were still issues like :Valid for travel on the outbound until early Decembe. Must purchase at least 3 day in advance of departure.Detecting a debugging can be quite timeconsuming.Even seemingly clean websites can have errors such as the  where I extracted airport information from:""Dalat Viet Nam  Lienkhang DLI)"" .""Skopie Macedonia (FYROM) (SKP)""The two above threw off my code for the longest time as I tried splitting by parentheses to separate city and airport code.Lastly I had to parse the text for dates into a usable context. Detecting and adding the year swapping the day and month so that they are in the same consistent order and handling ""early"" ""mid"" ""late"" all involve regular expressions and are not trivial.  Furthermore because I kept the format of ""early"" ""mid"" ""late"" instead of assuming granularity I assigned these dates a certain value so I can perform comparison and whether a date string fell within a certain date range.Load into Pandas R Data Frames whatever you like. Maybe clean the data a bit more.  Now you're ready to play with the data you just scraped.That's it! Get going!",NA
Scraping NSF Awards to Create Database of Active STEM Researchers,44,https://nycdatascience.com/blog/student-works/scraping-nsf-awards-create-database-active-stem-researchers/,There a numerous use cases for having a searchable database of active STEM (Science Technology Engineering and Math) researchers. For example targeted marketing by companies interested in selling services and products or helping students select the best research institution and mentors based on their interest. Further it can to provide an additional means of getting an overview of active research areas. Unfortunately no so such database is readily available even though most of the needed information is readily on various websites i.e. faculty profiles journal sites etc.An obvious approach for creating this database is of course manually searching through the above mentioned websites. However this would be very time consuming. A more efficient method would be to use web scraping to automate the data extraction process. Unfortunately a small scale test indicated that there is just too much format variation in the relevant webpages to make web scraping practical. For example even within departments at the same institution there is often different formats used for faculty profiles. Moreover since these profiles are infrequently updated the information they contain is often out of date.,NA,To overcomes the challenges listed above a much better approach is to by scraping grant award information from research funding agencies. Not only is award data provided in a consistent format more importantly such information provides a direct measure of a researcher's activity level. For this project the award page of the  was scraped. The process using the  is outlined in the figure below.Briefly a web spider visits links on the wards page downloading zip files containing award information which is stored in XML files. After unzipping relevant information is extracted by the processing pipeline then stored in a . Use of MongoDB allows for a scalable fulltext searching to be done.As mentioned above one use case for such a database is direct marketing. Imagine if you will a small company Acme AFMs who developed a new instrument and want to know if it’s worth going into fullscale production. The first thing they do is establish there is active demand by searching for the grants which are related to AFMs for the last sixteen years. After establishing there is amply demand based on the number of grants awarded the next step is to get a list of the most active faculty/institutions who use AFMs. Again this information that's easily obtained from the database. The figures below provides a visualization of these search results.Web scrapping of grant awards information to create a searchable database of active STEM researchers was successfully done using the Scrapy Framework and MongoDB. Test use cases clearly show the value of this system and its potential. The next steps for this project are to:,NA
The Pains of Growing an e-Commerce Business: A Case Study on Etsy,44,https://nycdatascience.com/blog/student-works/pains-growing-e-commerce-business-etsy/,28143 observation and 7 features available for analysis. After preprocessing the data I performed graphical and numerical exploratory data analysis using R. What follows are my initial findings.,NA,Finding that special gift for your loved one something handmade or vintage might send you Etsy. It's no surprise that this once micro eCommerce website for hobbyist with 650000 members in 2008 had grown into 5 million in 2010 and to 54 million in 2014. Rob Kalin its founder accidentally learned how to make a website to pay for his rent. He himself enjoyed creating things which inspired him to develop a website where other artists like him could sell their work.To get a good sense of the kind of customers and sellers in Etsy I picked a specific product category to analyze. In this case it was baby carriers. Using Python's Scrapy I gathered the following information from the eCommerce site:This yielded to For the baby carrier product category  it was interesting to find out that the most viewed were the ones priced $25  $50. Something that I did not expect from a handmade and vintage marketplace.Product inventory price range for baby carriers appears to be leaning towards lowerend with items $25 and below accounting for the bulk followed by $25$50 range.I wanted to create a model that predicted product demand using product sales volume and views. However on the Etsy website it only published total shop sales with no breakdown per specific product. As a substitute I used the product views to estimate product demand.Upon performing the linear regression diagnostics the summary of results were as follows: ,NA
I Know Which Xbox 360 Games Are Coming to Xbox One,45,https://nycdatascience.com/blog/student-works/know-xbox-360-games-coming-xbox-one/,Microsoft surely takes many factors into account when prioritizing their games but I needed data to analyze so I could think more like them. I decided to collect data from websites using Scrapy a free Python framework designed to allow a programmer to use various methods to extract data from a web page.I grabbed the list of all Xbox 360 games and extracted every single piece of information I could find on their pages including publishers developers user review scores and their counts release dates Smartglass features demo availability multiplayer types sound features and the number of addons themes gamerpics and the like.I then scraped game names users and professional scores from Metacritic along with several other factoids like release dates in case another site is lacking.I know Microsoft has said in the past that they look at UserVoice votes to help make choices so I scraped the votes and forum comment counts.Microsoft has prioritized their first party titles and I believe exclusivity should increase the likelihood of becoming backwards compatible. I retrieved the differentiations of Xbox 360 exclusives (console and regular) from Wikipedia.I grabbed the names and requirements of Xbox 360 Kinect games (required or supported) from Wikipedia. Remember that this is important because Microsoft says they won’t support Kinect required games but games with optional Kinect functionality left me wondering.I manually compiled a list of games that required peripherals. This was greatly helped by UserVoice which includes games that will not be made backwards compatible for various reason.It is important to find how much an Xbox One version of the game in the form of remasters remakes or crossplatform releases had any correlation. I needed to make a manual list of remasters but I decided to scrape certain sites for references to keep me up to date. I started with the list of Xbox One games off Microsoft’s site and I supplemented it by scraping a GameInformer article that gets regularly updated with the latest remasters. Based off these and further research I manually made a new list of games that could be found on both platforms.,NA,"Below is a sample of the Scrapy script I designed to scrape only Microsoft's Xbox 360 store. This is script navigates page by page to find each game's store page and dives into it to gather a variety of information. While in the game's store page the script employs safety measures to prevent mistakes when a page is out of the ordinary.Website: Larry Hryb the Xbox LIVE's ""Major Nelson"" Director of Xbox Programming at Microsoft keeps an up to date list of backwards compatible games on his blog. I notice his list is updated rapidly and is written in a convenient table.Website: Website: Website:  () ()Website: Website: Website: Website:  ",NA
Was the Kyoto Protocol Successful? A Comparison of Europe and the USA,45,https://nycdatascience.com/blog/student-works/r-visualization/europe-vs-usa-kyoto-protocol-successful/,"The Kyoto Protocolwhich was signed upon on December 11 1997 is an international agreement linked to the United Nations Framework Convention on Climate Change. Under the Kyoto Protocol signatory countries agreed to drop total emissions to 1990 levels. A total of 192 countries signed the agreement nearly all European countries. In the US the senate expressed total disapproval of the treaty by stating that it : “"".In order to compare the efforts of a Kyoto(Europe) and non Kyoto(USA) partner the data of the CO2 emissions have been extracted from the annual monitoring reports of the World Bank. Since 2007 marks the 10 year anniversary of Kyoto protocol our comparison will include CO2 emissions between 2007 and 2013 (the last available measurement). In order to assess the level of CO2 emissions I developed an interactive Shiny app to compare both Europe and the USA from 2007 to 2013. I define “Europe” as the countries of the European Union plus Iceland Norway Switzerland Ukraine Serbia Kosovo Moldova…. An explanation of the different features of the Shiny app is presented below.The main goal of Kyoto protocol was to drop the total CO2 emissions per country to the 1990 levels. Since the population of the world has increased and that some countries as China and India have significantly increased their level of CO2 emissions dropping these total emissions levels of CO2 as stated by Kyoto seems impossible regardless of the efforts of both Europe and the US. Recently CO2 emissions per capita have been considered as a ‘fair’ measure to assess the level of effort made by each country. Fig.1 shows an interactive map of both the US and for European countries comparing  CO2 emissions levels per capita for each year.Fig. 1 : Interactive map of CO2 emissions (metric tons per capita) for both the US and European countriesAnother aspect of assessing CO2 emissions for both Europe and the US is to look at the evolution of the CO2 emissions per category including: transport residential (households and private companies)industry and electricity consumption. Fig.2; shows an interactive bar plot of the evolution of the percentage of CO2 emissions per category for both the US and European countries per year.Fig. 2 : Interactive barplot of the percentage of CO2 emissions per category for both the US and European countriesCO2 emissions per metric tons per capita for each year between 2007 and 2013 with the levels Kyoto protocol per capita.Fig. 3 : Interactive barplot of the levels of CO2 emissions (per metric tons per capita) for both the US and the European region. It compares the total levels per year and Kyoto levels.From comparing the data of CO2emissions in both Europe(a Kyoto partner) and the US (a non Kyoto partner) we can see that both regions made an effort to reduce CO2 emissions per capita over the past decade.Even if the Kyoto protocol was about the total level of CO2emissions European countries dropped 1 tonne per capita but didn’t meet the target levels of Kyoto protocol CO2emissions per capita. On the other hand the US has been successful in reducing the level of CO2 by more than 3 tonnes per capita comparing to the stated goals of the Kyoto protocol.In order to better determine the efforts of each region of the world to reduce their impact on global warming a comparison of the evolution of investments and the share of green energies could be a better way to measure the commitment of each country to reduce CO2 emissions.",NA,In order to compare the efforts of both Europe and the US an interactive bar plot shown in Fig.3 compares the levels of ,NA
Which Starbucks Coffee Store Amenities Are the Most Popular?,45,https://nycdatascience.com/blog/student-works/can-avoid-mouse-clicks-starbucks-store-amenities-locator-web-scraping-project/,As I’m walking from Grand Central Station to the NYC Data Science Academy for my first day of class I’m thinking of which Starbucks Coffee store serves breakfast sandwiches. It is a journey in trial and error and wasted time as I walk into stores along my path until I find the right one.  Providing a solution to this problem was the basis for project three of the NYC Data Science boot camp. The scope of the project (solve a business problem using web scraping technology and present your insight) was a great opportunity to use Scrapy (a web scraping framework) for data capture R Studio for data analysis and CARTO to prototype a web based product. My solution allows end users to view the Starbucks Coffee Store amenities and their locations in one place. Go ahead give it a try .The Spyder framework integrates web scraping and Python programming for a flexible and adaptable solution to capture and process webbased content.  R Studio provides a smooth interface and great libraries for EDA to gain insight from the data. The CARTO dataset upload and mapping process is intuitive and allows you to visualize your data on base maps within minutes. Five least common amenities:Combining Open Source and vendor applications (Scrapy R Studio CARTO) allowed me to deliver an  that uses a website as the data source within a two week time line. The web app prototype enables end users to visually explore analyze and find Starbucks Coffee stores with the most / least common amenities. But most importantly you can view a store's amenities with a minimum amount of clicks.,NA,You can view the CARTO based end user product The store locations are distributed across multiple URLs spanning New York City. The web browser image below highlights the geospatial (longitude / latitude) coordinates within the URL.Most of the processing work is performed within Scrapy the magic sauce that allows you to scrape web sites munge and process the data for analysis and feed to CARTO for map visualization. The following steps were performed within Scrapy:EDA in R Studio identified the most common and least common amenities within stores.Five most common amenities:MCA(Multiple Correspondence Analysis) was performed to analyze the systematic patterns of variations with the amenities. The process requires the features to be of categorical data type (factors in an R dataframe).Based on eigenvector values the clusters identify amenities with the most commonalities. In the diagram above the cluster on the bottom right represents the most common amenities across 200 store locations. The cluster to the left has a lower distribution across store locations. Amenity FZ (Fizzio Handcrafted Sodas) stands out as having high direction from the zero intercept. It is the only amenity found in  within NYC and perhaps worthy of highlighting to Starbucks Coffee consumers.Source code is available at .,NA
Dashboards & Decision Making: A Glimpse of What Shiny Can Do,46,https://nycdatascience.com/blog/student-works/dashboards-decision-making-shinyapp/,The ability to see how each market performs and to compare them with one another is essential especially for globalbrands like Starbucks. Starbucks groups its operations in 3 different geographic regions: the Americas Europe Middle East and Africa (EMEA) and China and Asia Pacific (CAP). The 4th chart shows the consolidated sales growth rate for all 3 regions. Immediately the Starbucks executive can see that even if overall the global sales growth rate has been increasing EMEA is not contributing to global growth. There is no need to initiate a longarduousprocess of manipulating financial data.,NA,Executives and business managers need to make decisions all the time and they have to do it quickly to stay ahead of the competition. On a daytoday basis they need to figure out whether to roll out that new product in the pipeline or to continue running last month’s marketing campaigns or to sign that joint venture deal with a foreign partner to open a new store in Myanmar. The impact of the choices they make and their timing affect the company’s success.This underscores the requirement that a robust datadriven decisionmaking process be in place. I have created an executive dashboard prototype that addresses executives’ and managers’ pain points.Business executives and managers are always on the go. Business trips locally and internationally mean they have to have access to information on their mobile devices. This platform needs to be simple and robust enough so they can use it while they are traveling for business especially to emerging markets where Internet connectivity is slower and international roaming data plans costly.Unfortunately most platforms available in the market are very expensive difficult to use and unnecessarily convoluted. Many times users need to access multiple platforms (financial marketing supply chain product development) to get a good picture of what is going on in the company. Loading the app also takes time because the information loaded is not streamlined.More importantly it contains pertinent information about the business and market trends.Considering that the goal of these executives and managers is to maximize the company’s revenues the executive dashboard should provide them with fundamental key performance indicators (KPIs) to make decisions faster and better.In this prototype Starbucks' data will be used to show how target users can benefit from a Shiny app.This chart gives the users a quick view of how the company is doing financially.  In just a few clicks the executive can see the revenue growth rate trend of the company over the past 6 years. There is also an option to view it quarterly. For Starbucks the trend appears to be positive. Its annual sales revenues have been increasing yearonyear.Understanding the dynamics of pricing and quantity in delivering stores' sales growth is very helpful in terms of setting targeted pricing strategy. In this chart the executive can see how customers in each market behave differently.Feel free to check the Shiny app prototype .,NA
MPGView: Find the Vehicle Best for Fuel Economy,46,https://nycdatascience.com/blog/student-works/mpgview/,there are 797 cars owned per 1000 people in America What is described here is a Shiny app built in R that provides multiple perspectives on gas usage in cars.This app can be useful when looking for a car.  For example while everyone know that SUV’s use more gas than a Tesla let’s say there are still choices to be made among SUV’s.  A Lexus RX Hybrid is a better choice than a Range Rover in terms of expected expenditure on gas.,NA,"America is a nation on wheels. According to 2014 stats. Besides paying the price for the vehicle people spend thousands of dollars on gas every year. In fact the annual fuel cost varies dramatically between cars and a small change in a vehicle's parameters can add 1000 dollars to a gas bill every year. Therefore understanding fuel consumption is important especially for those who want a fuel efficient car in the near future. For me a training data scientist and car enthusiast I targeted my project towards MPG and created this shiny app. 2012 to 2017 vehicle data was downloaded from the . Fuel economy data comes from vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor Michigan and by vehicle manufacturers with oversight by EPA.Data merging cleaning and feature creation were performed in R and the app is powered by the Shiny package. All code can be found .When entering the app you will see a welcome page. This page gives a brief introduction to this app. On the top of the screen you will see three additional tabs. ""Understand MPG"" tab will help users visualize how mpg changes between different types of cars. The ""Explore by Manufacturer"" tab is a place for users to see the progress of increasing fuel economy by each manufacturer. Finally ""Select Your Next Vehicle"" tab is a place to get a recommendations on your next car based on the parameters selected. Now let's dive into the app and examine the functions one by one.As shown above users can select the type of vehicle and an MPG measurement on the left. After clicking ""submit"" four plots will be generated to demonstrate the behavior of mpg with respect to other parameters. On the top left is a boxplot showing the distribution of MPG values versus transmission type. Below is a scatter plot and curve fit of MPG versus engine displacement. The two graphs on the right are vehicle distribution among fuel requirements and drive types respectively.Above shows the interface of the second panel. Users can explore the development of fuel efficiency across six years (2012  2017). By selecting a vehicle manufacturer of interest an interactive google motion chart will be generated to facilitate the visualization.In the final panel users can filter the entire data set based on the information set on the left. Selections include:After clicking ""Get range"" button a slider input will be generated based on the information selected. Users can then determine how much he/she wants to pay for fuel each year. After all selections are made click ""Get result"" button and this will generate an interactive table containing all vehicles that fit the requirements along with a variety of key parameters of these vehicles.By exploring this app we can see that fuel economy is highly related to both vehicle type and their parameters. For example most of the SUVs require regular gasoline while most cars require premium gasoline and cars with continuous variable transmission are relatively more fuel efficient.To expand the function of this app more features can be added to the data. In exploring the MPG tab adding features like vehicle weight will result in a more complete exploration. In the recommendation tab including vehicle price in selection bar can help users locate their desired car more effectively. Last but not least besides mechanistic parameters we can further expand our recommendation system by inclusion of vehicle style features.",NA
Pokemon tracker in selected places using inferential statistic,46,https://nycdatascience.com/blog/student-works/pokemon-tracker-select-places-using-statistic-inferance/,Figure 2 select the city: Los Angels and adjust the zoom level to 16 and this show very small range and explicit spawn locationFigure 4  the zoomed central park map showing  the explicit pokemon spawn location and the traffic.,NA,"Recently one phone game spread through the whole world and cause a lot of interesting topic about the technique behind the game. One of the hot topic is: Here I developed an  to predict the pokemon spawning position and the probability of spawning at that position. This is the exciting news for the fans of the game who can easily find the possible spawning position and collect the rare desired pokemon. Next let me introduce my app.Figure 1 show the pokemon (Pidgey) spawn distribution and frequent at each spawn locationFigure 3 using the the satellite map ""Esri.WorldImagery"" to search the very rare kind of pokemon(Pikachu) and investigate the surrounding of the location:Actually there is one very complicated equation to generate the pokemon spawns. In the future I am going to add more features such as wind speed temperature resident population density the local time the moving speed and apply the machine learning algorithm  to track the pokemon more accurate.",NA
PokeViz: Visualizing and Predicting Pokemon Go using k-Nearest Neighbors,46,https://nycdatascience.com/blog/student-works/predicting-visualizing-pokemon-go-using-k-nearest-neighbors/,Motivation:Functionality:Visualization and Prediction:Density and Distribution:Data Source:Insights and Future Updates:Appendix:KNearest Neighbor Algorithm:,NA,"Pokemon Go a locationbased augmented reality mobile game released by Niantic Inc. The players uses the GPS to locate capture and battle fictional creatures in a virtual setting. Like many millennials I grew up playing Pokemon games on my Gameboy Color. So when this game was released I was undoubtedly joined the rank of fellow Pokemon Go trainer running on the streets of New York City looking for the rarest Pokemon. Although the game was a huge success there were many flaws that need to be addressed and new features to be implemented. One of which was a way to locate and predict location of Pokemon. As a hardcore Pokemon fan I developed an Shiny app to offer a potential solution for this problem  .This app has three main functionalities:The map displays all the Pokemons and their previous spawn location. The user can select the filters to choose the type of Pokemon number of Pokemon to be displayed and the continent of interest. Once the selection is complete simply click ""Go!"" button will generate the desired map.The Pokemons are divided into five different classes based on their rarity of appearance: Common Uncommon Rare Very Rare and Super Rare. By clicking any desired location on the map the longitude and latitude of that location will be generated in the top left ""Position"" panel and the predicted probability of each Pokemon rarity class near that location will be displayed in the top right ""Prediction"" panel.Another important feature of the app is to display the density contour map of all Pokemon spawns in major cities. With this feature user can specifically look for areas with higher Pokemon spawn rate within the city.The contour map is a map illustrated with contour lines it shows the different level of elevation as well as the steepness of the slope. For example in the map above the color gradient ranges from red to yellow. The red contour lines represent Pokemon density at lower elevation and as color of the line goes toward yellow the density gets higher. The brightest yellow indicates the highest density thus the most Pokemon spawn.The distribution of each rarity class can be find under the Distribution tab. It shows the frequency of every rarity class in the contour map.The dataset used in this app was uploaded by Kaggle user SemionKorchevskiy and can be downloaded at https://www.kaggle.com/semioniy/predictemall. All data processing were done using R. The code can be find .From this app we can find the general popularity of Pokemon Go. For instance North America and Europe shows the most number of Pokemon spawns and some countries such as China shows no sign of Pokemon spawning.For future directions more Pokemon data will be add to reflect a more accurate predictor. And also the density contour can be divided into five different contours with distinguishable color that corresponds to the five rarity classes which will be easier for the user to target Pokemon of specific rarity.The kNearest Neighbor (kNN) algorithm is a nonparametric algorithm that is capable of both classification and regression. It takes nearest k observations (neighbors) from the input observation and outputs a class membership (or probability of each class) from the majority vote of the neighbors for classification and outputs the average value of the neighbors for regression.The reason I used kNN as the prediction model is because due to the nature of the dataset the observations are location coordinates kNN will be great for this purpose. Also since I wanted to use a model that is capable of performing multiclass multilabel classification for which kNN is easier to implement and achieves better result compared to other classification models.",NA
Where Did All the Bikes Go? An Analysis of NYC Citi Bike Station Capacity,47,https://nycdatascience.com/blog/student-works/bikes-go-analysis-nyc-citi-bike-station-capacity/,I have been fascinated by the number of people riding NYC Citi Bikes to and from work. I’ve asked a few Citi Bike subscribers about their experience and yes some include dealing with a pothole or two; therefore I was interested in the logistics. How are the stations replenished and how do users typically deal with bike capacity constraints when they really need a bike. Do they go to a nearby station or hail a cab? My second project at the NYC Data Science Academy boot camp (build a web based product addressing a business problem using Shiny and RStudio) provided me with an incentive to further research these questions and drive towards the creation of an interactive web based solution. My cohort peer James Lee told me stories of riding a Citi Bike during his undergrad days. And having to go to a station at certain times (before classes would finish) to ensure that he would get a bike. Otherwise he would be left to wander on to the next station to find a bike. My experience with the NYC Citi Bike Station Map website led me to think about how users determine if there will be enough bikes or docks at a given station or destination? And would a ride from point A to point B result in a surcharge if the ride duration were to go above the purchased pricing  (30 mins for day riders and 45 mins for subscribers)?locate feature allows you to look up one address at a time which results in a popup on the map displaying the number of available bikes and docks. You can filter on bikes or docks to reduce the number of markers on the map. However I felt these features were limited. Features that would allow users to determine bike / dock capacity at the station / destination use historical data to show capacity trends during peak times with awareness of the trip duration would improve the ability to plan ahead. I spoke with a few members of my cohort my instructor Chris Makris and a small circle of friends to confirm that the pursuit of a solution for this case would be worthwhile. So I went forward to build a Shiny app with these features which can be found .My original goal for the project was to analyze historical ridership data provided by NYC Citibike through their open data portal. However my cohort peer Joshua Litven referred me to an article that was mentioned by Andy Eschbacher of Carto (was a guest speaker at the Academy the week prior to the project assignment) that provided a detailed analysis on NYC Citi Bike ridership. The analysis by Todd W. Schneider covered ridership trends including peak hours months age and sex. Todd’s analysis provided me with an opportunity to move on to the product development stage using his analysis to influence my development of a prototype dashboard solution that would allow  users to track stations meeting their capacity requirements alongside trip duration and cycling path.The Shiny project technology scope is limited to RStudio and Shiny Dashboard for the development of a webbased solution within a twoweek timeframe. Given the time constraint I limited my exploration of options to popular libraries that also provided examples related to my project scope. My final working prototype met the project requirements by integrating multiple R libraries such as:The shinyapps.io free subscription service model limits you to five Shiny applications with 25 active hours per month. This option made it possible for change releases (as part of the Software Delivery Life Cycle) to be visible to users in the public internet domain. A small test group outside of the cohort was able to validate the  and offer immediate feedback. Present state Shiny development solutions are robust for rapid prototyping of  ideas with options to scale through a simplified model. But the programming syntax requires further simplification to reduce debugging time due to syntax errors. I believe this will further reduce the learning curve making Shiny a great alternative for rapid prototyping of webbased solutions to data related problems.I combined the methodologies from   and   principles and DevOps practices to undergo rapid change iterations with focus on hypothesis testing and user feedback and a fast transfer of technical knowledge from the collective expertise of cohort members and external advisors. As a one man team it meant that I experienced wearing multiple hats to bring the prototype from concept to fruition: Market Research Business Analyst Product Manager Project Manager Developer Production / Customer Support Sales. As a result the first week was prioritized on researching  and dynamic map rendering APIs (Application Programming Interface) with time invested into iterations of trial and error until I found a solution that worked with recently acquired knowledge of R programming. The second week was prioritized on rapid iterations of development and releases to allow for end user testing and feedback. This method was crucial to keeping me focused on developing a product that met initial requirements and was flexible to allow for alignment with the end users' expectations. My approach to dealing with setbacks and limitations after exhausting the collective knowledge pool was to park bugs / request into a 'followup' / 'would be nice feature' checklist so that I wouldn't get stuck on working to resolve an issue or perfect an implementation. In paraphrasing my instructor Chris Makris 'Park the pie in the sky feature requests for a future iteration and remain focused on the core needs of the application to meet the timeline' was sound advice I kept close to heart.The overall process has been an amazing experience for me. I wanted to deliver on a primitive working prototype. Given the twoweek timeline I had to park the historical peak trend analysis feature but am amazed by the significant growth in geospatial and programming knowledge attained and by the number of features I was able to integrate into the final prototype based on user feedback. I will not hide that there were many moments of failure and frustration (e.g. endless troubleshooting of routing coordinates rendering as a polygon shape instead of a straight path on the map) along the way. However my family friends instructors TAs and cohort peers continued to provide me with the support guidance and encouragement to see the project through with the result being a working prototype on the  to aid users in their research / planning. This type of environment was the deciding factor that led me to partake in the immersive.Who would have thought that by the end of my first month of training at the NYC Data Science Academy boot camp that I would have produced a functional webbased prototype that addresses a business problem? The training curriculum cohort collaboration alongside existing industry research in the public domain and my commitment and persistence with respect to  following best practices allowed me to step outside of my comfort zone and achieve a working  that includes the following features:,NA,The The following people deserve recognition for the support provided to me during the rapid product development process:,NA
Cost Benefit Exploratory Visualization Analysis of Public vs Private Institutions,48,https://nycdatascience.com/blog/student-works/r-visualization/public-vs-private-institutions-cost-benefit-exploratory-visualization-analysis/,In 2008 I was a senior in high school applying to colleges around the country eager to start the next phase of my life. Unfortunately my application cycle fell right in the middle of our recent recession caused by the collapse of the housing market in 2006. Although I had gained admission to Northwestern University a prestigious private school I had to decide if it was worth spending my parent’s life savings as well as taking out large loans. My other choice was my native state’s public school State University of New York (SUNY) Binghamton. Although SUNY Binghamton was not as prestigious it would have cost me 4 times less and majorly reduced the financial burden on my family. Ultimately I had to choose between an expensive private college or go a cheaper public college. I ended up going to Northwestern University but if I had more data on the differences I might have chose differently.It is not uncommon for students to have to choose between more prestigious private schools and cheaper public schools. However as college tuition student debt and the need for a college degree are on the rise it is becoming ever more important to choose carefully. In fact regarding the student debt bubble billionaire entrepreneur Mark Cuban has said we are “going to see a repeat of what we saw in the housing market...”. Furthermore according to collegedebt.com the cumulative U.S. student debt is over $1.45 trillion dollars more than the total credit card and auto debt in the country.In this blog we will do an exploratory analysis of the data released by the U.S. Department of Education and look at the costs and benefits between public and private colleges. This blog will focus primarily on predominately bachelor’s degree granting schools and the latest available year’s data (2013). This analysis will focus on the cost debt and earning aspect. We will see a U.S. map of the cost and adjusted cost density plots of the median debt and median earnings after graduation and lastly a scatter plot of net cost vs. median earning. The cost of attendance is the college’s reported estimate of total cost needed per year this includes tuition living expenses fees etc. The public school map is dominated by green and yellow points ($10000 to $30000) whereas the the private school map is dominated by orange and red points ($30000+). It seems that private colleges are roughly about $20000 more expensive. In the above plots we confirm that most private institutions are more expensive than public institutions.However when we plot the adjusted net cost (cost of attendance minus average grants and scholarships) we can see the difference in cost is actually smaller where the private institutions provide more financial aid but still ultimately cost more. The public school map is dominated by blue and green dots ($0 to $20000) whereas the private school map is dominated by green and yellow dots ($10000 to $30000). Roughly speaking the public institutions give about $10000 aid whereas private institutions give about $20000 aid.In the median debt density graph the public and private graphs have different peaks and a portion that overlaps. From this graph it seems that most of the median debt from public schools are less than private schools. This makes sense since in the previous U.S. maps private school costs more so logically students will have to take out more loans to pay for tuition.Surprisingly in the median density graphs the median earnings (10 years after graduation) between public and private schools have little visual difference. The density graphs seem to fall on top one of another with the peaks almost aligned but the private density graph has a little more variance. It seems that regardless of private or public schools in general the earnings are about the same.To understand the distributions a little more clearly 2D density contours are overlaid on top of the scatter points to illustrate where the highest density regions are. The innermost contour line shows the densest region of each group. Here it can be seen that even though the two college types may overlap the peak of each group are separated.In general the cost of private institutions are roughly on the order of $20000 more than expensive than public schools. However private institutions give on the order of $10000 more financial aid resulting in private schools only be on the order of $10000 greater in net cost. On the other side of the analysis in general students tend to leave private institutions with more debt but earn about the same amount after graduation. Finally net cost does not seem to generate more earnings which results in public schools being cheaper but earning around the same as private schools. With all else being it is recommended to go to public schools to save money.In this analysis we focused on a high level overview of whether the monetary investment in more expensive colleges (private institutions) is worth it based on how much debt and earnings one comes out with after graduation. However college is more than just money in and money out there are many other factors that define a good and worthwhile investment. These factors may include faculty to student ratio school size location types of programs and many others. In the future a deeper analysis will be done to include these intangible factors. In addition this study only looked at predominately bachelor’s degree granting institution it would be enlightening to see how schools that grant different level of degrees such as associate degrees medical degrees etc. play into earnings and investment returns.,NA, In order to get a better understanding of how cost and earnings are related to each other for each school it is desirable to show a scatter plot of net cost and median earning. Here it seems that the public and private institutions form their own clusters. With the public school cluster being cheaper than the private school cluster but at about the same earning level. However there is also a small portion of these two clusters that overlap.,NA
How to Help Shelter Animals,48,https://nycdatascience.com/blog/student-works/help-shelter-animals/,"Contributed by Chuan Hong. Chuan is currently in the NYC Data Science Academy 12 week fulltime Data Science Bootcamp program taking place between September 26th to December 23rd 2016. This post is based on her class project  R  Visualization.Each year approximately 7.6 million companion animals enter animal shelters nationwide (ASPCA). Of those approximately 3.9 million are dogs and 3.4 million are cats.  About 2.7 million shelter animals are adopted each year (1.4 million dogs and 1.3 million cats). Meanwhile about 649000 animals who enter shelters as strays are returned to their owners (542000 dogs and 100000 cats). Compared to these lucky cats and dogs finding their families to take them home many shelter animals face an uncertain future. It is estimated that 2.7 million cats and dogs are euthanized in the US every year. Given the differences in outcomes for shelter animals we can analyze the factors that make some cats and dogs more likely to get adopted.In this dataset there are ten variables which are  ""AnimalID"" ""Name"" ""DateTime"" ""AnimalType""(Dog/Cat) ""SexuponOutcome""(Neuteraed Male/Spayed Female/Intact Male/Intace Female) ""AgeuponOutcome"" ""Breed"" ""Color"" ""OutcomeType""(Return_to_owner/Adoption/Transfer/Euthanasia/Died) and ""OutcomeSubtype""(Other/Foster/Offsite/Partner/Barn/SCRP/Suffering/etc.).After a quick check of these variables I decided that""Color"" and ""OutcomeSubtype"" would not be included in this visualization project. This was because that there were 300+ unique colors in this dataset. It was way too many to visualize factor by factor. Meanwhile based on the Sankey plot below we can see that the ""OutcomeSubtype"" is a detailed explanation of the variable ""Outcome"". First let's look at how many cats and dogs we have in this dataset and how different outcomes are distributed. From the two graphs shown below we can see that both cats and dogs were commonly adopted but dogs are much more likely to be returned to their owners than cats and cats are transferred between shelters more often than dogs. It also appears that very few animals died or got euthanized overall.",NA," Two months agohosted a competition to predict the outcome of shelter animals in order to help shelters focus their energy on specific animals who need a little extra help finding a new home. The dataset was from .In this project I did some EDA to investigate the potential relationships between factors and animal outcomes especially adoption situation.There are quite a few cats and dogs in this dataset who sadly don’t have names. I was curious to see if having a name affected their fate. The graphs below indicate that the situation was different between cats and dogs. Cats with names were more likely to be adopted; while for dogs the percentage of adoption was similar whether having a name or not.The ""SexuponOutcome"" (Neutered Male/Spayed Female/Intact Male/Intact Female) variable contains two types of information: if the cat/dog was male or female and if it was neutered/spayed or intact. So there are two distinctive features in fact. I then encoded this variable into two ""sex"" and ""isNeutered"". It seems like the adoption count and percentage were similar between male and female in both cats and dogs.The graphs below show that neutered (or spayed) was a potentially strong factor. Cats or dogs were more likely to be adopted if they’ve been neutered.Further we have information about ""Breed"" in this dataset. Some animals had pure or mixed breed. I wondered if breed purity has some positive impact on the fate of an animal. Then I created three variables from the original variable “Breed” ""isMix"" ""primarybreed"" and ""secondarybreed"". However there were no obvious differences between pure and mixed breeds ( see the graph of the percentage below).The breed variable has way too many levels so for the breed analysis I just selected the top eight most popular breeds in this dataset for cat and dog respectively.(1) For Top 8 cat breedsFrom the graph of the count we can see that the majority breeds of cats are Shorthair Median hair Longhair and Siamese. But the percentage graph shows that the adoption percentage is similar for these top four groups. So the breed may not a strong factor affecting the fate of cats.(2) For Top 8 dog breedsLikewise the percentage of adoption among the top eight breeds of dogs are similar too.Another potential factor is ""Age"" but we have this variable in different units (i.e.  years months weeks and days). So we converted every ""Age"" into ""Ageinyear"" and ""Ageinmonth"" then explored whether there were some different trends related to age.Based on the two pairs of graphs below outcome by age in years and outcome by age in months we can see that most of the animals in the shelter were  01 years old. Meanwhile it seems like that young cats and dogs have much higher chances to be adopted while older cats and dogs with approximately equal probability can be adopted.(1) By year(2) By monthFinally one very important factor is ""DateTime"" which is the time when the outcome happened. It looks like that cats are more likely to be adopted during summer and winter and dogs are more likely to be adopted during winter too (based on the graph by month). Meanwhile we assume that the adoption peaks are weekends and 4:00 pm to 6:00 pm (graph of by hour).(1) By month(2) By weekday(3) By hourTo explore and understand the trends of adoption peak two heat maps with the number of adoption vs. weekday and hour were created. We can see that adoptions are more likely happening during weekends and from 4:00 pm to 6:00 pm. The trend of cats is similar to that of dogs.Based on the findings animal shelters may need to turn to unique promotions to encourage potential owners to take relatively older cats or dogs. Meanwhile shelters can reduce the adoption fee for a cat or dog older than oneyearold and they can bring only older cats and dogs during adoption peak such as weekends to highlight them.You may also explore this project via Chuan's .The American Society for the Prevention of Cruelty to Animals (ASPCA)",NA
Steam Data Visualization using GGPlot2 and R,48,https://nycdatascience.com/blog/student-works/steam-data-visualization-using-ggplot2-and-r/,One of the most time consuming parts of this project involved merging the data because of the game’s names. The names of games on Microsoft’s sites were abbreviated or shortened due to character limits on their own website or are only available as “XYZ Edition”. Users often used abbreviations or modified names when submitting to UserVoice. Wikipedia articles sometimes used the international name of a game. Punctuation trademark/copyright marks and differences in spelling prevent one to one matching.The solution was to make a new column of names where I would remove as many obstacles as I could so that when datasets are merged there are the fewest differences possible. I began by removing words including “the” “edition” “special” “full game” “free to play” and many more. I then removed pluralization and possession from words and converted numbers to roman numerals (“Assassin’s Creed 2” to “Assassin Creed II”). I also removed all non alphanumeric letters as well as spacing and capitalization.I felt that the increase of owners better represented the success of a sale than sales numbers alone because it allows comparisons of smaller games such as indie games to AAA games. For example if an indie game sold only 2000 copies and increased to 2400 copies sold that is very impressive but a 400 sales increase for a AAA may be dismal. The above scatterplot shows how there seems to be a closer relationship to the increase of owners and the number of owners before versus the as opposed to a comparison using sales.,NA,The purpose of my data visualization project was to visualize data about how review scores discounts and campaign length affect a customers' buying decisions. Specifically I wanted to use data sets from Steam Metacritic IGN and HowLongToBeat and use R to combine them into one data frame and then use the GGPlot2 R package to visualize these data sets.PC gaming enthusiasts buy powerful machines to run their games maxed out visualized on beautiful high resolution 144hz screens and amass impressive libraries of games they can play on any whim. Of course PC gamers often the latest and greatest. But what does that mean for games that aren't Battlefield Overwatch or Starcraft II? PC gaming has been on the forefront of digital sales for over a decade due in large part to Steam. For those out of the PC gaming loop Valve Software's Steam platform has become the go to digital distribution platform for PC gaming since its release in 2003. Steam is a service available for Windows OS X and Linux with 11358 games available and 125 million registered accounts. Steam has had as many as 12.5 million concurrent users as of November 2015.The Steam Summer sale has been an annual event on Steam where games become heavily discounted and generates buzz throughout the gaming community. Most games typically go on sale and there are daily and semi daily special deals and packs of games that keeps customers returning each day as well as an encore of some of the best sales at the end.The following datasets were accessed for data visualization:Each dataset required polishing to get values into a proper form including removing non PC gaming platforms and converting the data into the proper data class.Below is an example for how I prepared the Steam sales data for this project. The numerical columns data included symbols that needed to be removed dollar signs commas parenthesis and percent symbols while also including two data points in one column. This required regular expression to retrieve the values. I also dropped and renamed columns for readability and ridding excess information.,NA
Visualizing the Game Style and Shooting Performance among Superstars via NBA Shot-log,48,https://nycdatascience.com/blog/student-works/nba-shot-log/,In the NBA a top player makes around a thousand shots during the entire regular season. A question worth asking is: What information can we get by looking at these shots? As a basketball fan for more than 10 years I am particularly interested in discovering facts that can not be directly seen on live TV. When I was surfing on web last week I found a data set called NBA shotlog from Kaggle. This data summarizes every shot made by each player during the games in the 14/15 regular season along with a variety of features. I decided to perform an exploratory visualization with this data.,NA,Now Let's dive into the shotlog and see what interesting information we can discover in terms of game style and shooting performance among NBA players. I focused this analysis on Stephen Curry James Harden Lebron James and Russell Westbrook who are ranked 14 in the MVP ballot in 2014to2015 season and undoubtedly superstars in the league.The CSV file and the variable descriptions can be accessed . Data cleaning feature creating and graph processing were performed using R. The package used for generating graphs is ggplot2. The R code for data cleaning and feature creation can be found .The graph above demonstrates the distribution of the shot attempts by each player versus shot distance. All four players have a local maximum centered at around 5 feet and 25 feet corresponding to layup region and threepoint region. Curry has the shot density leaning towards threepoint zone while James shot more shots at the paint zone indicating different play style between two players. It can also be seen that Westbrook uses twopoint jumper frequently as suggested by the peak at around 17 feet.The above violin plot summarizes the the shot accuracy for each player throughout the season. Based on the visual inspection of this plot Curry and James have relatively stable shot accuracy compared to Harden and Westbrook (as suggested by a wider shape).After seeing the summary of shot attempt and shot accuracy let's explore how these values behave when other factors are taken into account. Let's divide the shot accuracy according to the match result. From the plot Curry James and Westbrook display a large gap between the won games and the lost games. In contrast Harden shows a relatively small accuracy gap.Then let's look at how the shot number and accuracy change over the season timeline. Westbrook tends to make more shots at the end of the season during which time Oklahoma City Thunder is fighting for the last playoff position. From the graph on the right Curry and James have relatively stable shot accuracy throughout the timeline while the accuracy of Harden and Westbrook seems to have greater variance.Now let's see the number of shots plotted against touch time. Curry performed more shot at a very short touch time indicating his catchandrelease shooting style. In contrast Westbrook tends to have the ball in hand for a few seconds before taking the shot.An interesting phenomenon was observed when plotting shot accuracy against the shot distance. As shown above the shot accuracy decreases from the layup region to around 10 feet. For Curry James and Westbrook although value of accuracy differ with each other they all have a local maximum at around 14 feet. Let's call this region the comfortable zone. On the other hand the accuracy peak of Harden extends out of the threepoint line which is different with the others. When the comfortable zone is passed the accuracy for all players decreases monotonically.When combining defender distance into figure 1 we get a contour plot that can give us a general feeling about the play style of each player. From the plot on the left it can be seen that at layup region the contour plot for Westbrook lies below the one for Curry meaning that Westbrook tends to make more tough layups than Curry. To my surprise Westbrook is even more aggressive at the rim than Lebron James.From the heat map above we can view the number of shots and shot accuracy with respect to each opponent. For example Westbrook made more shots when playing against New Orleans Pelican and Portland Trail Blazers and Harden had poor accuracy when playing against Boston Celtics.Some people believes that making one shot will affect the accuracy of the next shot. Based on the shotlog we can actually explore this effect. A set of plots has been generated. For each player the left most red bar represents the shot accuracy of all shots right after missing one shot. The green blue and purple bars represent the shot accuracy after making 1 2 and 3 consecutive shots. It is interesting to note that almost for all players under study having one shot made seems to have a negative effect on the following shot. The more consecutive shots are made the lower the accuracy of the next shot. When only threepoint shots are taken into account this trend still holds true for Curry and Lebron James.From these graphs we can see that four stars have dramatically different play styles. For example Stephen Curry tends to perform catch and quick release while Russell Westbrook prefers to attack the rim with ball in hand. In terms of shot accuracy Stephen Curry and Lebron James have a more stable performance than Harden and Westbrook. Interestingly in most cases hitting one shot tends to have a negative effect on the next shot. A deeper exploration is needed for more detail about this phenomenon. For the future direction focusing on the defender side of the data is a potentially interesting extension. Further more we could also apply machine learning techniques to predict the probability of hitting a shot.,NA
"Extramarital affairs, some factors...",49,https://nycdatascience.com/blog/student-works/extramarital-affairs-factors/,"In order to determine some of the possible factors of extramarital affairs we will explore a well known survey from the paper “Theory of extramarital affairs”  published in the Journal of political economy in 1978. This survey has a sample of 601 individuals and measures 9 factors like: Having children how the individuals rate their marriage how many years they have been married and how educated they are etc..Although the number of participant is 601 450 individuals in this survey didn’t have an an affair. For practical purposes we will only focus on individuals who had at least one affair during their married life.Fig.1 : Marriage rating vs GenderThe individuals who answer the survey were asked to rate their marriage with a scale from 1 (Bad) to 5(excellent). From Fig.1 above we can see that the average rate for female participants is 3 and the average rate for male participants is 4. We can also say that for this survey the individuals appear to be quite happy with their spouses. In this case how do children influence the quality of their marriage and the number of affairs they could have? Fig.2 : Marriage rating vs Children for both gendersFrom Fig.2 we can see that individuals without children seem to be more happy with their spouses than the ones without children for both males and females. If we look at Fig.3 we can see that the people without children are more likely to have more affairs than the people with children. In addition the number of affairs for male participants with children are almost twice their female counterparts with children.Fig.3 : Number of affairs and children for both genders After examining both the children and marriage rating factors we will next explore some other factors as years of marriage age and education.

From Fig.4 female participants seem they don’t have affairs during the first year of marriage. Between 1 to 5 years of marriage they have more affairs than their male counterparts and we can see that between 5 to 10 years of marriage they have the same number of affairs.  For more than 10 years of marriage female participants have more affairs than their male counterparts.   a degree someone has",NA,"According to the 2012 US infidelity survey 27.2% of Men and 22.9% of Women had at least once an extramarital affair. Several statistical studies have been conducted to understand the causes that led married people to have an affair. In this work we will look at the some of the factors that might influence the likelihood of some individuals to engage in an extramarital affair.In this part we will explore some of the plausible factors of extramarital affairs. For all factors an overview of the behavior of both male and female participant will be shown.Fig.4 : Number of affairs and years of marriage for both gendersIt is quite common to blame what we call ""middle life crisis"" as a major factor for extramarital affairs. It is believed that for both genders there is an age range where an individual is more likely to have at least one extramarital affair.In Fig.5 we represent different age groups and the number of affairs for both genders.Fig.5 : Number of affairs and Age for both gendersFrom Fig.5  the middle life crisis for male participants seems to appear to be around 4147 years old and for female participants it is around 2632 years old where for both gender the number of affairs attains a maximum of 12.  The last factor we are going to explore in this work is Education. For example how do advanced degrees of education and the number of affairs tie up?Fig.6 : Number of affairs and Education for both gendersFrom Fig.6 we can see that the more advanced  the more likely that individual is to have more affairs.Although survey are often biased it is sometimes quite difficult to draw a clear conclusion of the analysis of the data. This work shows that there is no significant factor that could clearly explain extramarital affairs for both genders.",NA
Finding Influencers on Twitter,49,https://nycdatascience.com/blog/student-works/finding-influencers-twitter/,"Have you been followed on Twitter or Instagram by someone you don't know? I get this a lot. And so to avoid being thought of as rude I follow back. Eventually I got tired of following back when I realized that some of these accounts don't really do anything but collect followers. Now why would anyone go through all the trouble of following people in the hopes of being followed back? Why would anyone waste so much time on the internet for this?I eventually realized the answer when I saw that most of these accounts were not personal. A lot of these accounts I encountered were about food some about beach vacations and on some occasion accounts with risque content.Advertising has infiltrated the social network. It used to be just ads on banners but now companies hire personalities on social media to spread the word about their product or event. Companies spend big bucks on celebrities in an effort to publicize their brand and attract a celebrity's fan base. A sponsored tweet could net as much as $13000 as was the case for. Celebrities have multitudes of followers and get paid big bucks by sponsors. So people may have thought that creating accounts and amassing followers would eventually get them sponsorship deals with advertisers. In this exercise we see that sponsors might be looking for some other things other than the number of followers. In a social network a link could represent a relationship as in Facebook or the passing of a tweet as in Twitter. These links determine the flow of information and are therefore a good indicator of a user's influence. I will be presenting two methods of finding potential influencers in a network. One would be by extracting a user's influence measures and the other is by using network graphs. A large database was found on . The database contained a stream of tweets related to NASDAQ 100 stocks extracted from twitter for 79 days from 2016 March 28th to 2016 June 15th. This was selected because of a good mix of accounts representing organizations and personalities. The database also contained information about how many times a tweet was passed along and who the original tweet came from. This act more popularly known as retweeting can be identified in the stream as tweets having 'RT @user' or 'via @ user' at the beginning of the tweet. The stream also contained information about mentions. In twitter a mention is a public conversation between users. A user calls the attention of another user by mentioning them in a tweet. Mentioning is identified by tweets beginning with '@user'.The influence measures extracted from the stream were the following: indegree retweet and mentions. These measures were selected because of how they affect the flow of information in the network. Indegree measures the user's popularity. This was easily extracted from the database by the number of followers a user has. The number of followers shows us the size of the user's audience base. Retweet influence represents a user's ability to create content which other users find worthy of sharing. When a tweet is shared by another user a bigger network of users is exposed to the tweet. From the stream this was extracted by counting the number of retweeted messages for each user. The third measure mention influence was extracted by counting the number of mentions containing the user's name. This influence measure indicates the ability of the user to engage others in a conversation. This represents the topofmind value of the user's name.A total of 96613 users tweeted about NASDAQ 100 stocks during the timeframe. Between them over 680 thousand tweets were broadcast. A word cloud of the NASDAQ symbols most often mentioned shows that Apple represented by AAPL was the most tweeted stock among the group. Users were most active on April 27 where they broadcast over 20800 tweets. This coincides with the day when AAPL stocks slumped following speculations that iPhone sales may decline by as much as 60 million units compared to the same quarter a year ago. The slump in Apple shares dragged the techheavy NASDAQ into the red by the day's end.Users' activity on this day showed that activity was mostly during trading market hours which is 13:30 to 20:30 UTC.Each user's ranking over the three influence categories was assigned by using fractional ranking. For example in assigning the indegree ranking a rank of 1 was given to the user with the most number of followers. Users with the same number of followers receive the same ranking number which is the mean of what they would have under ordinal rankings. Table 1 shows the top 30 users across the three influence measures. Notice that minimal overlap can be seen across each influence rank. The first user to show up across all three measures of influence was ""WSJ"".To see how much users overlap across the three categories a Venn diagram of the top 100 users was derived. Figure 4 shows that among the 239 users in the top list only 10 users can be seen across all three measures of influence.Figure 5 below shows a correlation matrix which represents how a user's rank varies across the three different measures of influence. The correlation matrix represents the strength of the association between a pair of rankings. This matrix was derived by comparing the relative influence ranks of all 96613 users in the database.The users show a strong correlation in their retweet influence and mention influence. The low correlation of the indegree measure across the other two measures show that indegree ranking may not be related to the other rankings.A couple of conclusions can be derived from the correlation plot. First we can say that in most cases users who are retweeted often are also mentioned often and vice versa. Another one is that the most followed user may not be the most engaging user in the group. A user's popularity therefore is a weak representation of the ability to motivate the spread of information.Retweets and mentions have direction. A retweet is the path of an idea from User A to User B. User A broadcast a tweet which was read by User B. User B thought it was worth sharing and retweeted it. This retweet will eventually be seen by users not directly accessible to User A. When User A mentions User B this is again a link from User A to User B.  With this in mind we have enough data to convert our twitter stream into a directed network graph. All users will be a node in our graph and all directed links will be edges. The igraph library will be used to extract information from the resulting network graph.A quick look at the resulting network graph for the whole stream shows that we were able to create a graph with 96613 nodes and 168 519 edges. Because of this size the resulting network graph will not be shown. This is because of the amount of time and computational effort needed to come up with a plot. It would most likely be a crowded mess of dots and lines anyway. However we can still extract some information from the graph object.The density of a network object is the proportion of present edges from all possible edges in the network. Our present graph has a density of 2.799118e05. A very low density would mean that there is a very low interaction between our users.The diameter of a network graph is the length of the longest path across unique nodes and edges. Considering the direction of the links the diameter of our network is 14. This means that we are able to trace an unbroken path across 15 users.The hubs and authority algorithm was developed by Jon Kleinberg to examine the relevance of a web page's content. He categorized pages into hubs and authority pages. Hubs which have more outgoing links are the internet's catalog. This is similar to the early days of Yahoo where it touted itself as the internet's yellow pages. Authority pages have more incoming links presumably because of their highquality content. Translated to twitter activity hub pages would fit the description of a user with high retweet influence and authority pages would be similar to a twitter user who has high mention influence.The hub score and authority score of the network graph was derived using a simple igraph function call. The resulting top hub score went to ""markbspiegel"" while the top authority score went to ""Benzinga"". This is in contrast to the ranking tables where the top retweet and mention belong to ""philstockworld"" and""jimcramer"" respectively.To find out where the discrepancy came from each node were investigated. Although it showed that ""markbspiegel"" had more unique edges than ""philstockworld"" if we consider and sum the weight of each unique edge philstockworld still beats markbspiegel. The same is observed when looking at the edges of ""Benzinga"" and ""jimcramer"". The discrepancy is consistent with how web pages are rated wherein the number of links matter more over the number of times each link was activated. The hub and authority score also does not take into account the weight characteristics of the nodes.To see an actual network graph we narrow down our selection to a twitter stream of users tweeting about CA Technologies.Table 2 shows us the resulting top influentials derived from our ranking method. The first user to cross the three influence categories is ""Benzinga"". The resulting network graph of this smaller twitter stream comes up with 431 nodes and 131 edges. ",NA, There is comparatively more interaction between users compared to our initial network object with the density clocking in at 0.0009550531. The diameter is shorter with just 9 hops across 10 nodes.The resulting hub and authority score show a more consistent result with the ranking tables because the actual number of retweets and mentions were low. This time the number of unique edges were not significantly lower than the total weight of the edges.Figure 7 and 8 show the network graphs with the nodes adjusted based on the hub and authority score. The higher the score the bigger the node size.The fractional ranking method is found to be a more realistic measure of a twitter user's influence. The frequency of interactions between users must be considered in measuring influence even if it is among a usual set of audience. This just means that the user is consistent in producing highquality content that has passalong value.For smaller networks the network graph method may yield additional information that can't be derived from fractional ranking. The key would be to check whether the ratio of the number of edges to the total edge weight is close to 1. The discrepancy between the ranking method and the network graph is expected to be greater when this ratio approaches zero. ,NA
Visualizing Economics and Mortality,49,https://nycdatascience.com/blog/student-works/visualizing-economics/,Countries are economically different in various regions of the world.  The residents of wealthier countries are not only better off financially but they also tend to live much longer.  How does the living conditions compare? What could be a possible reason for this? This project visualizes the economic conditions of different income regions and identifies how these situations compare over time.   The data on this project was found in the databases of World Bank and the World Health Organization.,NA,During my study on Economics at New York University I had seen a clip by Hans Rosling in which he showed a fantastic presentation relating income and life expectancy of 200 countries over the last 200 years.  The presentation was interesting in that it showed how the countries became wealthier and healthier at dynamic intervals.  The countries each had its setbacks from sudden disease outbreaks and political disturbances but nonetheless followed a positive trend overall.  As each country grew healthier with medical advancements the countries began to grow wealthier.  What I wanted to see were the recent changes and possible explanations for them.The GDP per Capita appears to vary widely after between the life expectancy of 70 to 80 years.  Most of the poorer nations seem to be from South Asia and SubSaharan Africa.  Interestingly there is a cluster formed within the longer living but still poor regions found in the top left corner of the graph.  It appears as if there is a particular milestone that must be reached before such an expansion of income can occur.World Bank's metadata was used to categorize nations into 4 different income groups.  After taking the logarithm of the Average GDP per Capita the plot revealed a correlation between the Life Expectancy and GDP per Capita.II. How do Income Gaps Differ?Income Gaps were calculated using the GINI coefficient of various countries reported by World Bank.  A higher income gap or a highincome inequality is expected to relate to lower values in both Average Life Expectancy and Average GDP per Capita.The first plot shows several interesting patterns.  There is a pattern in which a HighIncome Gap (GINI >44.14) decreases from the Upper Middle Income to LowIncome.  However the MiddleHigh Income Gap (37.59<GINI<44.14) increases from the UpperMiddle Income to LowIncome.  This suggests that income grows increasingly concentrated among a select few as countries develop.  However it also shows that there is a greater barrier to entry to wealth among poorer countries as the distinction between the wealthy and poor grows.  This is highlighted in the second plot where overall income gap separated into two levels (High  GINI>34) is shown.  While wealthier countries have an extreme concentration to the select few the overall spread of wealth is more even than in the poorer countries.III. What Causes Such as Life Expectancy?LowIncome nations suffer from high rates of communicable or contagious diseases caused by bacteria viruses and other microorganisms.  Unfortunately infectious diseases also affect the entire population regardless of age and because of that the residents face an earlier death.  The development of medicine vaccines and clean environment allow wealthier nations to avoid such illnesses.  The LowIncome regions are not without hope however.  Over time cases of communicable diseases are decreasing at a rapid pace indicating better health and greater longevity.As noted previously communicable diseases such as infectious and parasitic diseases are much frequent in the Low Income Region than any other regions.  Fortunately there is a definite decrease in such diseases over time everywhere.  However cardiovascular diseases and malignant neoplasms rise over the years and as countries grow wealthier.  This appears to be a direct result of longevity.The benefit in the lower mortality from communicable diseases is evident.  In the year 2012 while all income groups saw a growth in both life expectancy and GDP per capita it was especially more noticeable in the lower income groups which saw an adamant boost.We see a stagnation in growth as countries develop.  For the LowIncome Regions the points become highly segregated in time but in the HighIncome Regions the points overlap mostly showing not a large degree of change.  There appears to be a decrease in the marginal rate of development as countries grow wealthier.There is a strong positive relationship between Life Expectancy and GDP per Capita.  Residents of countries with higher income typically not only earn more money but also live longer.  Wealth appears to be more accumulated to the elites in higher income regions but the real gap between rich and the poor is much evident in poorer countries.  Finally mortality by communicable disease is a serious threat that affects lowincome areas but the conditions appear to grow better over time.Next Steps:In the future this project would be updated in Shiny in which the data in between years can be shown to show a more fluid movement in time.  Other factors should be considered in visualizing the economic situation of the country such as the form of government primary level of production or industry and economic conditions on gender.,NA
Who to Lend to at the Lending Club,49,https://nycdatascience.com/blog/student-works/lending-club-loans-analysis/,"Technological disruption is affecting many industries and dusty old consumer lending is no exception. Peertopeer lending  private individuals lending to one another rather than from banks  has been growing exponentially over the past 10 years and Lending Club is a lead player. But first how exactly does Lending Club work? Say you have some spare cash. Lending Club now gives you an alternative to lending it to your crazy uncle. Instead you can lend it to a...stranger! That may sound scary but Lending Club handles the movement of money back and forth between you and the borrower. Rather than using Lending Club to borrow for new investment or consumption borrowers are typically trying to consolidate existing debt. The below word cloud illustrates the prevalence of ""debt consolidation""  and ""credit card"" in the free text field ""loan purpose"". Borrowers want to consolidate their debt at Lending Club because they think they can get a better interest rate than they pay on their credit cards. This leads to the other important service Lending Club provides:  they estimates the credit risk  the risk of not getting paid back  of each loan. They give each loan a “grade” (just like at school) and based on this grade assign an interest rate. The lender can choose from a spectrum of low risk low return to high risk high return loans. As you can see in the below chart this interest rate has changed quite a bit over time.  The interest rates on the high grade loans have been stable over time but the rates on lower grade loans have gone up considerably. You can also see that the difference in interest rates between loan grades is quite high. For example you may be paying 10% more interest if you have a grade C loan as opposed to a grade A loan.  That’s $5001000 on a $10k loan!
So how does Lending Club determine this allimportant loan grade?  You can see that back in 2007 Lending Club’s loan grade metric was highly correlated with FICO scores: high grades were associated with high FICO scores. However over time the range of FICO scores used to give both high grade loans and low grade loans increased. Particularly intriguing were the low FICO score people getting grade A loans and the high FICO score people getting lower grade loans. As context you would have trouble getting a typical mortgage at all if your FICO score were much below 650. But if you had a FICO score of 750800 you would have banks chasing you down the street with free toasters to get your business! I investigated this latter group further. How do these credit golden children end up with say a grade C loan paying 10% more than one would think they should? I called this group “outliers”.If these outliers were indeed unjustly given lower grade loans than they deserved we should be able to see that in the default rates of the loans. However the chart below illustrates that although the outliers’ default rates are a bit lower than peers with the same loan grade but lower FICO scores they are not generally low enough to justify being bumped up a grade. For example the red C grade bar would have too high a default rate to justify being moved to a B or A grade loan despite the fact that all its constituents have FICO scores above 750. In short Lending Club assessed the outliers' credit risk accurately despite their relatively lofty FICO scores. So how does Lending Club figure this out? How do they know that your crazy uncle with an 850 credit score really deserves a grade C loan? The short answer is we don’t know. However there are characteristics that suggest some possibilities for further analysis. For example in the above graph it's clear that the loan amount seems to be a little higher for outliers than for nonoutliers. Also it looks as though the outliers have more recent credit inquiries and are more likely to have recently opened a credit account than nonoutliers. This is reflected in the shape of the violin plots for Mths_snc_rct_inq and Mths_snc_rcnt_acct. Therefore it may be the case that a potential borrower applying for a larger loan than usual and opening up credit accounts recently may be interpreted by the Lending Club as warning signs that their credit risk is higher than that implied by their FICO score alone.In summary the Lending Club's credit rating practices have matured considerably over the past 10 years particularly with respect to their divergence from the FICO scorebased credit ratings. However the FICO score is still a useful metric for a lender: although the Lending Club has a more accurate algorithm for measuring default risk than FICO within a given loan grade borrowers with high FICO scores tend to default less frequently than borrowers with low FICO scores in the same loan grade. This is valuable info for a potential lender!",NA,This post analyzes trends in Lending Club's loan portfolio and shares an interesting observation that may help lenders maximize their returns.Sample Code:,NA
Bosch Production Line Performance,50,https://nycdatascience.com/blog/student-works/15212/,"In August 2016 Bosch one of the world's leading manufacturing companies launched a competition on Kaggle addressing the occurrence of defective parts in assembly lines. This post focuses on the machine learning pipeline built for the competition and how to preprocess the large dataset for a traditional machine learning modeling process.Manufacturing industry relies on continuous optimization to ensure quality and safety standards are respected while pushing the production volume. Being able to predict if and when a given part of a product will fail the standards is an essential part of such optimization as it leverages the existing massive amount of data recorded in the production line without affecting the process. This argument is particularly relevant in the “assembly” phase since it accounts for 50% to 70% of the manufacturing cost. Bosch among other companies records data at every step along its assembly lines in order to build the capability to apply advanced analytics and improve the manufacturing process.A quick check at the dataset header allows drawing a sketch of the assembly line used for this dataset.The assembly line is divided into 4 segments and 52 workstations. Each workstation performs a variable number of tests and measurements on a given part accounting in total for 4264 features. Different products may not share the same path along the assembly line nor there seem to be a common starting or final workstation. Each of the 1183747 parts recorded in the dataset follows one of the 4700 unique combinations. As shown in fig. 2 one observation  can be interpreted as a series of cells (yellow boxes) where the object is processed. Conversely features may be described according to their popularity (number of rows/parts for which the feature exists) and defective rate defined as the percentage of the parts being measured at a given feature and found to fail the quality test (see fig. 2). It is interesting to notice how features with high defective rate (>0.6%) are clustered around specific areas mostly in line 0 and 1.  
By just loading the first few rows of each dataset it is possible to check all the column names at the same time and discover whether there is any pattern in the structure. As the result each of the tables in training and testing set has the same number of observations respectively which suggests that the data was separated by column from a single table therefore the original table can be restored by simply binding all the three tables together without any advanced joining procedure. This answered the previous question about D codes  it turns out that the last digit of each column is just the column number instead of the feature ID. Each timestamp column is located next to corresponding F column which explains why D(n) columns are describing F(n  1) columns.Next there is a massive missingness within the dataset. To be specific only 5% of numeric values 1 % of categorical values and 7% of timestamps were NOT NULL. This is quite understandable  each observation only goes through a certain number of stations and will not be touched by most of other stations. Therefore the missingness was not at random. Thus if a proper transformation method can be applied to squeeze out those void cells from the data table the physical file size of the data file can be significantly reduced and make it possible to apply machine learning directly on the entire dataset.What are the reasons for producing a defective part? What is the likelihood of detecting an error in the assembly line? It is reasonable to assume that such likelihood increases with the number of steps required in order to produce a part. Similarly the higher the number of measurements the higher the time required to complete the part/product. By using this simple assumption we can use produce (at least) three new features related with the “process” rather than the individual feature. This is particularly relevant for time stamps (Date dataset) where most nonnull features for a given row show only very few (around 3) unique values. By calculating the time lapse (TMAXTMIN) the entire dataset can be reduced effectively from 1156 to 1 column. The other feature namely the number of steps (nonnull features) per row can be calculated for both numerical and categorical datasets.A second major “gain” in dimension reduction can be obtained on the categorical dataset by noticing a large number of duplicated columns (1913) probably referring to the same features measured at different stations. Furthermore the categorical features have only 93 unique values. Rather than encoding the features (preserving the original feature set) we chose to look at the appearance of each categorical value. Combining these transformations the original 2140 features shrink to 93 dummy variables. After feature engineering the dataset is ready to be fed into the machine learning pipeline. Due to the high correlation among variables a large number of observations  and nonrandom missingness patterns of the data the tree models are expected to perform better in this scenario because they are capable of picking up correlations among variables during the training process. However a logistic regression model was still trained and the performance of this model can be used as the baseline for measuring other models' performances. The metric been used to evaluate each model's performance is the Matthew Correlation Coefficient (MCC) which is equally valuing both true positive and true negative rates and the range of this score is from 1 (perfectly incorrect) to 1 (perfectly correct).Next a tree model shall be selected to better adapt to the missingness of the data as well as to achieve a better MCC score. Considering Random Forest is extremely computationally intensive and not able to handle missing values XGBoost was selected due to its high computation efficiency and capability of dealing with missing values automatically. Meanwhile the only hyperparameter of this model that has been modified was learning rate which was set to 1 in order to get fast convergence. A fivefold crossvalidation on training set shows that this basic model has achieved an MCC score of 0.24 which is a huge improvement!",NA,Both of the training set and testing set provided by Bosch was split into three separate tables: One contains numerical values one contains categorical values and one contains timestamps. Each of those tables is roughly 2.8 GB which sum up to 7.7 GB for training data and same size for testing data. With data of this size it is extremely important to understand data before testing any machine learning technique.The first important thing is to understand the naming schema of this table. According to the description document L indicates the assembly line number S means the working station number F means the value of an anonymous feature that has been measured at the station. For example L0S0F1 means the Feature 1 measured at Station 0 on Assembly Line 0. However the special code D is been used differently: columns that named as D(n) records the timestamp that features F(n  1) have been measured. For instance L0S0D10 stores the timestamp for L0S0F9. Why were columns named in this strange way?Finally the sparsity of both numerical and the transformed categorical datasets can be used to reformat the data by means of libSVM. Overall we reduced the data by a factor 5 from 7.7 GB to 1.7 GB.Since the logistic regression cannot handle missing values imputation is needed. However imputing the data set is computationally expensive because all the missing values will be filled and been processed during the training process. Therefore only numerical dataset had been used for this model. In addition L1 regularization (a.k.a. Lasso Regression) was used to narrow down the most important features. As the result  only 22 out of 968 variables were kept in the final model and the MCC score of this model on the test set is 0.14. This is already a great improvement compared with blindly assuming all observations are not defective (response  0).,NA
Predict New York City Taxi Demand,50,https://nycdatascience.com/blog/student-works/predict-new-york-city-taxi-demand/,There are roughly 200 million taxi rides in New York City each year. Exploiting an understanding of taxi supply and demand could increase the efficiency of the city’s taxi system. In the New York city people use taxi in a frequency much higher than any other cities of US. Instead of booking a taxi by phone one day ahead of time NeWe use a joining dataset detailing all ~1 billion taxi trips (14G) in New York City from April and September in 2014 as provided by he NYC Taxi and Limousine Commission (TLC) including information of yellow green and uber taxies. The data associates each taxi ride with information including date time and location of pickup and dropoff trip distance payment type tip amount total amount. Also hourly  weather information is incorporated in the big taxi dataset. A small number of taxi pickups in this dataset originate from well outside the New York City area. In order to constrain our problem to New York City as well as to reduce the size of our data given our limited computational resources we only consider taxi trips that originate somewhere defined in the figure on the right.The information including taxi pickup date time longitude and latitude coordinates is selected which contains 1 billion observations and thus is hard for visualization and modeling. To reduce dimension and data set longitude and latitude variables are rounded to 3 decimals and condensed to 120000 records.We used a python library Geopy to find out zip code from corresponding latitude and longitude. The geopy is a client for several popular geocoding web services which makes it easy for user to locate the coordinates of addresses cities countries and landmarks across the globe using thirdparty geocoders and other data sources.In order to put the raw data into the same form as our input to the problem we group the raw taxi data by time (at the granularity of an hour) zip code temperature and participation of rainfall count the total number of pickups for each timezip codeweather combination and store these aggregated values as data points to be used for training and testing. For instance one row in our aggregated pickups table is “201404 01 00:00:00 49.0 0 10001 375” representing 375 pickups in zip code 10001 on April 1 2014 between 0 aM and 1 aM local time 49.0  and no rain. In total our data set consists of 710000 such data points.After preprocessing the data we did the exploratory data analysis by using Tableau.Firstly  we are exploring how the taxi demand is affected by the weather from two attributes: Temperature and Rainfall.From the temperature graph the overall trend of hourly pick up frequency is getting larger from middle 62 to the right and left sides.  Meanwhile the fluctuation is getting bigger from middle to the two directions as the temperature becoming colder or hotter. Thus the temperature has great influence on the taxi demand which is also very intuitive that people require more taxies to go out when the temperature are cold for example below 36. While when the temperature getting hot more and more people come to the city including travelers which increase the demand of taxies. While the influence from rainfall to the hourly taxi demand is counterintuitive. From the rainfall graph the hourly taxi demand does not increase as the participation of rainfall increase. But the fluctuation of hourly taxi demand is getting bigger as the participation of rainfall increase which indicate that the hourly taxi demand may affected by rainfall but may be not so much or it is a combination with other features to affect the taxi demand.Secondly  we are looking for the relationship among the different types of taxi. From the heat map Uber operation area focus more on Manhattan area. While the green taxi is not allowed to take passenger in Manhattan so there is no green points in the heat map. Last but not least yellow taxi has the most records and wide spread operation area.Furthermore we group the trip information by weekday and hour to see how the taxi demand changes.As you can see  Uber yellow taxi and green taxi follow the similar cyclical trend in both of the above plots. The total taxi demand peaks around Fridays and Saturdays while bottoms out around Sundays and Mondays. Meanwhile  it  peaks around 67 pm and bottoms out around 45 am within a day. It is more obvious by visualizing the hourly changes in animated maps below. The colour faded at the most around 5 am in the morning as we discussed above.By filtering the zip code within New York City our data set has 71000 observations.In order to evaluate the performance of our model we split the data into a training set (80% of data set) and testing set (20% of data set)  where the training examples are all ordered chronologically before the testing examples. This configuration mimics the task of predicting future numbers of taxi pickups using only past data.We chose RMSE to evaluate our prediction because it favors consistency and heavily penalizes predictions with a high deviation from the true number of pickups. From the point of view of a taxi dispatcher any large mistake in gauging taxi demand for a particular zip code could be costly ‒ imagine sending 600 taxis to a zip code that only truly requires 400. This misallocation results in many unutilized taxis crowded in the same place and should be penalized more heavily than dispatching 6 taxis to a zone that only requires 4 or even dispatching 6 taxis to 100 different zones that only require 4 taxis each. RMSE most heavily penalizes such large misallocations and best represents the quality of our models’ predictions.In comparing the results between our different models we also report the R^2 value (coefficient of determination) in order to evaluate how well the models perform relative to the variance of the data set.The multiplelinear regression model allows us to exploit linear patterns in the data set. This model is an appealing first choice because feature weights are easily interpretable and because it runs efficiently on large datasets. The result is showed below.To improve multiplelinear regression and introduce regularization ridge model is applied and the hyperparameters alpha used for ridge model are determined using Bayesian Optimization select parameter values. The result shows that ridge regression does not improve multiplelinear regression.Bayesian Optimization: uses a Gaussian Process to model the surrogate and typically optimizes the Expected Improvement which is the expected probability that new trials will improve upon the current best observation. Gaussian Process is a distribution over functions. A sample from a Gaussian process is an entire function. Training a Gaussian Process involves fitting this distribution to the given data so that it generates functions that are close to the observed data. Using Gaussian process one can compute the Expected Improvement of any point in the search space. The one gives the highest expected improvement will be tried next. Bayesian Optimization typically gives nontrivial offthegrid values for continuous hyperparameters (like the learning rate regularization coefficient and so on) and was shown to beat human performance on some good benchmark datasets.The tree regression model is capable of representing complex decision boundaries thus complementing our other chosen models.  Random Forest is chosen since it prevents overfitting and robust against outliers. And the hyperparameters maxfeatures(number of splits at each tree) and nestimators (number of tress) are determined using Bayesian Optimization select parameter values. Of the values we swept our model performed best with maxfeatures 14 and nestimators of 500.We listed the top 20 important features produced by random forest. As demonstrated in EDA  the hour weekday and temperature features are important. Also some zip codes  By further investigation the true value of taxi demand in these zip codes area have high value and high variance.XGBoost is an advanced gradient boosting algorithm. It is a highly sophisticated algorithm powerful enough to deal with all sorts of irregularities of data. The tool is extremely flexible which allows users to customize a wide range of hyperparameters while training the mode and ultimately to reach the optimal solution. For our model the booster parameters are tuned byBayesian Optimization to find the best combination of hyperparameters (listed in the table) where max_depth is the maximum depth of a tree learning_rate determines the impact of each tree on the final outcome ignorer to avoid overfitting  n_estimators is the number of trees gamma is the minimum loss reduction at each split  min_child_weight is the minimum sum of weights of all aberrations required at each split node  subsample is the fraction of observations randomly sampled for each tree and closample_bytree is the fraction of columns randomly sampled for each tree.Ensemble modeling is the process of running two or more related but different analytical models and then synthesizing the results into a single score or spread in order to improve the accuracy of predictive analytics and data mining applications. We combined our two strong models: randomforest and xgboost for ensemble modeling and used linear regression.The results of all the models are compared in the below table and ensemble modeling did not further improve the prediction as expected. Overall xgboost performs best.In order to visualize how well the models (randomforest xgboost ensemble) perform we plot the true versus predicted number of pickups for each data point in the test set in the Figure.The scatter plots in the figure suggest that the three models perform well on the test set. Most predictions lie close to the true values. The data points straddle the unit slope line evenly signifying that the models do not systematically underestimate or overestimate the number of taxi pickups. For three models as expected absolute prediction error increases as the true number of pickups increases. This effect can be visualized as a coneshaped region extending outward from the origin within which the data points fall.To take a deep look at how absolute prediction error varies the data is separated to 3 subsets based on the true number of pickups: subset 1 with pickups greater and equal than 1000 subset 2 with pickups greater and equal than 100 and less than 1000 subset 3 with pickups less than 100. From this table we can confirm that RMSE increases as the true number of pickups increases. Also xgboost performs best in different subsets.The below figure shows the comparison between the prediction of 3 models and the true value by randomly pick 10 samples from each subset.  At each individual value different models give different performance. So we did the predictions for the next coming week using three models and the result is visualized summarized and compared by shiny interactive application.can help user to compare the number of pickups of three different models (random forest xgboost ensemble) and across different locations in a given time zone  and also visualize the trend of the number of pickups in a 24hour cycle across different locations within New York City. Overall our models for predicting taxi pickups in New York City performed well. The xgboost regression model performed best likely due to its unique ability to capture complex feature dependencies. The decision tree regression model achieved a value of 35.01 for RMSE and 0.98 for R^2. Our results and error analysis for the most part supported our intuitions about the usefulness of our features with the exception of the unexpected result that participation of rainfall  feature is not important for model performance. A model could be useful to city planners and taxi dispatchers in determining where to position taxicabs and studying patterns in ridership. In the future we will implement the 2 below models.Neural network regression: We may be able to achieve good results using a neural network regression since neural networks can automatically tune and model feature interactions. Instead of manually determining which features to combine in order to capture feature interactions we could let the learning algorithm perform this task. One possible instance of features interacting in the real world could be that New Yorkers may take taxi rides near Central Park or when it is raining but not when they are near Central Park and it is raining since they may not visit the park in bad weather. Neural networks could be promising because they can learn nonlinearities automatically such as this example of an XOR relationship between features. In addition to our three core regression models we implemented a neural network regression model using the Python library PyBrain. However we would need more time to give the neural network model due consideration so we list it here as possible future work.Kmeans Clustering: In order to find nonobvious patterns across data points we could use unsupervised learning to cluster our training set. The clustering algorithm could use features such as the number of bars and restaurants in a given zone or distance to the nearest subway station. The cluster in which each data point falls could then serve as an additional feature for our regression models thereby exploiting similar characteristics between different zones for learning.,NA,"


w York taxi drivers pick up passengers on street. The ability to predict taxi ridership could present valuable insights to city planners and taxi dispatchers in answering questions such as how to position cabs where they are most needed how many taxis to dispatch and how ridership varies over time. Our project focuses on predicting the number of taxi pickups given a onehour time window and a location within New York City. The problem is formulated in terms of the following inputs and outputs:Below is a list of feature templates we use to extract features from each data point:Why did we use Bayesian Optimization instead of gridsearch for hyperparameters  tuning process?The result is showed below.Hourly weather is predicted by sin function using information from weather channel. The predictions for the incoming week is presented in shiny app:The web interactive application Uber: Yellow/green Cab: Weather: NOAA climate data website: ",NA
Predicting Horse Racing Outcomes in India,50,https://nycdatascience.com/blog/student-works/horse-racing/,"Horse racing and data go handinhand.  The vast array of statistics about horses jockeys trainers lineage of horses and much more is impressive and the application of this data in determining odds of success is integral to the sport.  For centuries people have worked to understand the relationships among the data in an effort to better predict the success of a horse with a dream of “striking it big”.  With so much data and the possibility of immediate application of predictive models we became quickly enthralled with the idea of building a better model to predict outcomes.  Since half of the final project team was from India and due to the relative ease of obtaining the data we chose to focus on horse racing in India.  Our decision to proceed with this project was easy but was the last easy step in the process.The first challenge was finding data.  The entities that own the data control it tightly.  Results from individual races are findable on the internet but there is not a single location that has all the data nor is there a compiled database that was available to us.  Our first challenge therefore became creating a database upon which we could train models.  We used beautiful soup in Python to scrape over 3500 web pages of data.  We were able to build an initial database with records from the 5 major tracks in India spanning almost 10 years of data.The database was formatted as a single row for each horse appearing in each race totalling 210000 runnings.  The data in each record included information about the horse and its lineage the trainer and jockey and statistics including the horse weight speed rating racetime odds gate draw and age as well as the variables we might want to predict the finish place and finish time.The challenge with this project continued when we looked at the data.  The following illustration highlights the extent of the missingness in the data.The racetime odds were missing from nearly 35% of the data.  We struggled with how to handle this missingness debating whether more bias would be introduced by imputing or by dropping these records.  Ultimately the racetime odds proved to be vital to developing a model with predictive power and the range of odds between the favorite and other horses was sometimes very large.  Because of this we concluded that it would produce less bias if we dropped the records entirely.  We have to proceed with one large caveat: we do not have an understanding of why this data was missing and how it might affect the conclusions if it were available.  However we found it to be the case that odds info was missing from entire races or it was complete for entire races and it was not the case that individual horses in a race were missing odds info.The rest of the missingness was more understandable or was easily imputable without introducing meaningful bias.  For instance occasionally the rating would be missing.  In the case of a missing rating we set that horse’s speed rating to be the mean of all the horses in the race.  We ended up engineering features that had missing data for several reasons and we were able to use the same procedure of imputing those values with the mean of a race.Our first look into the cleaned dataset provided us with some clues about how to proceed.  The first thing that stood out was extent to which the odds alone were predictive.  The implied probability of the favorite horse winning the race was about 42% derived from the racetime odds.  The favorite horse actually won 43% of the time.  The mean odds of the favorite was 1.7/1 which meant that a simple “pick the favorite” strategy would have been profitable.  Our earlier caveat raises its ugly head at exactly this time… if there was some reason for the missingness that has to do with the odds then this would not hold true.  The following chart highlights the implied probability of winning for horses starting in each rank of the favorites table.  Also included is the actual win percentage and the number of observations.  For horses beginning a race as the oddsfavorite the mean implied win percent was the aforementioned 42%; the actual win rate was 43% in over 13000 races.Given the persistence of winning for specific horses trainers and jockeys we wanted to make sure that our featurespace included the history of each of these groups.  We created methods of grouping and filtering in Python to create historical records for each horse trainer and jockey for all races preceding the current race and appended to the appropriate record.  Each record now contained historical run times win place and show percentages for individual horses as well as win place and show percentages for trainers and jockeys.  We then applied procedures to group within races and added features that described the difference between each horse and the rest of the field  comparing prior race times speed ratings and weights. We now had a full featurerich dataset that included 20 more features than the simple data we had scraped.Individual models don't compare well to an oddsonly model:
",NA,Looking at the mean odds of winning for each oddsrank and comparing to the mean odds of the horse that ended in each finish position clued us in to the potential profit opportunity.  If the favorite ran with 1.7/1 odds and the horse that ultimately won had odds of 4.7/1 it seems that there is an additional $3 of profit potential if our model was to perfectly predict winners.We also found that there were particular horses jockeys and trainers with a high percentage of wins.  The following charts highlight the winningness of the top horses races and jockeys (ordered by total count of wins).One of the most interesting trends we uncovered was the winning percentage based on the startgate.  Horses starting at lower positions tended to win more often than horses starting in higher gates.We had the benefit of having scraped 9+ years of data. This allowed us to create three chronologically ordered data sets:We scaled the data prior to splitting the files so that the data could work with any model we decide to use to optimize our results in the future.The primary objective for applying machine learning was to develop a model that could potentially beat a simplistic approach of always betting on the track favorite based on the odds. However it is important to mention that the odds represent much more information than just the probabilities a racing establishment puts on a horse to win a race. The odds also represent information about how much a horse is being backed by racegoers so by definition it also represents latent information of people's emotions  fear greed and maybe even some insider knowledge of how a horse may perform on that day. This information is really hard to deconstruct and it doesn't sit in any existing variables anywhere else.We wanted to assess whether we could use the odds in tandem with the information collected and engineered so far i.e. the past performance of horses jockeys and trainers along with information about the current race to predict the outcome of a winning horse better than just using the odds themselves.So it is a straightforward classification problem for which we combined a gradient boosted machine model and a Neural network model to predict the outcome.We sought to optimize both models on the AUC statistic in an attempt to maximize true positive signals of winners and minimize false positive signals.The AUC we got for the GBM model was about 0.83 on the training and validation datasets.However when looking at the confusion matrix on the test set (below) from among all the signals generated the GBM model only predicted 34% of them accurately.The optimal GBM model had the following parameter settings:The variable importance plot (rescaled to be budgeted out of a 100%) suggests that the main predictive element of the model is the odds itself. Secondary importance measures relate to jockey past performance and how much better a horse's past statistics are versus that of the field.The NN model also yielded an AUC score of about the same as the GBM model (0.84) but the accuracy of predictions from among the signals was much higher  40%.The optimal Neural Network model had the following parameter settings. It is important to note though that it is easy to underfit Neural Networks because of the sheer number of parameters available to tune so improvements to this base model can likely still be made.Stacked models typically yield better prediction strength than individual models.  The basic procedure employed for stacking (as outlined below) was:Using a GBM model as the meta learning model we improved our prediction accuracy to 55%. But one important caveat to note is that it generated far fewer signals (1/3rd to 1/5th of the other models spoken of thus far).In order to compare the true performance of the stacked model to the betthefavorite model we would somehow need to covert them to be on the same scale i.e. we can asses their performance on a per signal basis.More specifically if we assumed that you bet one dollar for every signal that arose from each of the models discussed so far it would yield an average of:Even after stacking our profitability turns out to be just about as effective as a betthefavorites strategy which yields an average of about a 4.5% return on every bet. While the results are on par with a far simpler approach this is just the beginning of the modeling process. We have so far built up a good framework to allow for swapping in or out additional models with better optimized parameters. Thus far we have only explored gradient boosted and neural network models for each of the levels of the stacked model. We'd love to try extending that to additional algorithms for the base layer as well as the meta learning layer.Additionally we may attempt to engineer a few more features. One such feature we would immediately like to look at is the spread of odds across all the horses in a particular race. Usually a horse with very low odds compared to other horses is more likely to win and if this phenomenon is separated from the rest of the data we may be able to focus on predicting winners for closer races potentially ones having higher payouts. We could also optimize models based on dollar payouts rather than a simple accuracy % and we plan to write our own objective function for that further down the road.,NA
Yelper: A Collaborative Filtering Based Recommendation System,50,https://nycdatascience.com/blog/student-works/capstone/yelper-collaborative-filtering-based-recommendation-system/,"""Getting information off the internet is like taking a drink from a fire hydrant"" (Mitchell Kapor). Information overload is a real phenomenon preventing us from making good decisions or taking actions. This is why recommendation systems are becoming common and extremely useful in products such as Netflix Amazon Echo and Facebook News Feed in recent year.Besides in the big data era realtime recommendations may become a new norm because realtime recommendations:Technically we use Spark MLlib to train the ALSbased collaborative filtering models in Yelper (). Below are several brief steps:Now imagine that there are one thousand or even one million of customers who need to access Yelper to get recommendations in different cities. Before we show how Yelper handles this challenging scenario let us first delve a little bit deeper into why this scenario is challenging.○ More graph analysis○ Improve recommendation",NA,"Recommendation is a big topic. Netflix was willing to spend 1 million dollars to summon the best movie recommendation algorithm for their service. Facebook devised hundreds of variables just to carefully gauge the recommended feeds on the first page when you log in your page. Not to mention myriads of other applications in the market.Based on the dataset provided by the ""Yelp Challenge 2016"" ""Yelper"" is a system that:The rationale for building Yelper is that:The GitHub page can be found here: Slides: There are 5 major components to build Yelper as shown below. Here is the overall flowchart of what happens.Yelper was built based on the  which includes:Our system mainly relies on two data sets: the 687K users and the 86K businesses. We performed two preprocessing steps ():In Yelper we deliberately created all business IDs as integers in range [0 1M] (M means million) while all user IDs are in range [10M +∞] because we have much more user IDs than business IDs. In this way we can use the cutoff value 10M (10000000) to check if an ID is associated to a user. To sum up using integer number as ID (such as 3234) instead of a raw string (eg. ""G8qH6TbfEhoYmS9KZM2Hfg"") has several advantages:Splitting business data by cities has the following advantages:We want to recommend the most highlyrated businesses to users. There are mainly two types of algorithms used for recommendation systems:Both algorithms have pros and cons. For simplicity and fast prototype we explored the first one in Yelper. If we have more time it is not difficult to integrate the second one into our system which will be discussed in the final section.For the last step above persisting the citywise models to disk (or cloud) is a good choice because we can compress transfer distribute or cache those models.We are particularly interested in analyzing the userbusiness interaction and gaining citylevel insights. Why do we want to do this? Below are two assumptions:We build citywise userbusiness network graphs in the following steps:As an example the dynamic userbusiness network graph of the city Madison (US) is shown below. Each node is either a user (in green color) or a business (in blue color). If a user u rated a business b then there is an edge from u to b namely u > b. For the city of Madison there are in total more than 10K edges and unfortunately the library D3.js cannot handle and render such a large number of nodes and edges. Thus we randomly selected only a small number of edges to be rendered in Chrome.What can we gain from this dynamic network? Well just from a single network we can gain the topological relationship between users and businesses. The density of edges (in red) reflects to some extent how the users rate all the businesses in a city. If we generated those dynamic graphs for all cities we may find that each city's network is distinct. We can further distill indepth information just from the networks themselves such as in/out degree clustering page rank analysis min cut community discovery etc.We implemented a web server to allow the user to interact with Yelper to get recommendations. This web server was built using:Below are several features of the frontend:As an example we show the user interaction of Yelper through the image below. The user with ID ""10081786"" requested recommendations for the keywords ""restaurants"" ""book store"" ""library"" and ""ice cream"" in the city of Charlotte. The Yelper returned the ""topK"" recommended results and showed the businesses locations via Google Map API.Those are just a few factors out of many that drive the need for a strong recommendation infrastructure. A detailed enumeration is definitely out of the scope of this post.One neat solution is to view all incoming request from users across many cities as a nonstopping stream and use Apache Kafka as a message broker to redirect all requests to Kafka then let the Spark Streaming handle anything that is piped from Kafka into Spark streaming in a faulttolerant and scalable manner. The advantages are:We thus simulate the realtime handling of a large amount of users' recommendation requests locally () as shown in the figure below. On the left side of the terminal window we run the Spark job to handle the requests in Spark Streaming at a frequency of 1.5 seconds. Note the text message showing the time stamp such as """". The Yelper logo is shown before each of the recommended results.On the right side of the terminal we run a python script to generate users' recommendation requests at a random time interval. Those requests were piped into Apache Kafka such that Spark Streaming can consume those requests.We build static userbusiness network graph using a Python library called graphtool () (). Now we show briefly what it looks like for the citywise userbusiness network. Again if a user rated a business there will be an edge connecting the user node and the business node in the network.In terms of visualization those edges there is a dilemma. The more nodes and edges that are included in the graph the more precise the graph will be but the visualization will become a mess since it is nearly impossible for many graph layout algorithms to elegantly visualize clearly the graph details when the edge number is above say 0.1 million. For this reason we adopted two simple strategies: out of all the edges in a city we either randomly selected 1% (or 2%) of all the edges or sequentially selected the first 1% (or 2%) of all edges.It is interesting to mention that even if we randomly select only 2% of the entire edges the generated networks still have the power to reveal many insights about a city if we dig deeper. Let us now briefly go over three cities in the US: Charlotte Las Vegas and Pittsburgh.We  selected 3312 edges which is only 1% of the total edges in this city.We  selected 11547 edges which is only 1% of the total edges in this city.What if we  select 23095 edges (2%)?Instead of randomly what if we  select23095 edges out of all the edge list?What about Pittsburgh? We  choose 2230 edges (2%) out of all edges lists.Again similar to the dynamic network mentioned in the previous section a lot of insights can be revealed from those static graphs if we delve deeper using graphbased algorithms and analytics techniques. For example as we all already know the economy of Las Vegas is higher than Pittsburgh. From the network there are clearly more nodes with a high number of incoming edges in Las Vegas than in Pittsburgh.If I have more time below are several potential directions that I can think of to dig deeper into those networks:",NA
Deciphering the tau tau decay of Higgs Boson,51,https://nycdatascience.com/blog/student-works/deciphering-tau-tau-decay-higgs-boson/,The theoretical discovery and experimental observation of the tau tau decay of Higgs boson is a milestone in particle physics that “contributes to our understanding of the origin of mass of subatomic particles”. Its importance is acknowledged by the Nobel Prize in Physics of 2013.The Kaggle community also paid huge attention and hosted a data science competition with nearly 1800 teams participating in 2014. The task was to analyze a set of simulated particle collision data containing features characterizing events detected by the Large Hadron Collider at CERN. The objective was to classify events as either a signal indicating the tau tau decay of Higgs boson or a background noise.  Although this completion was finished almost two years ago we were still very interested in the Higgs boson Kaggle dataset and wanted to apply our newly obtained machine learning knowledge to analyze it.   The initial explanatory data analysis helped us identify 11 out of the 30 features have missing values.  For convenience we renamed all the 30 features with serial name from var1 to var30.  As shown in the table below of all the 800000 observations from the training and test set there seems to have three systematic types of missingness: 71% have missing values in var5 var6 var7 var13 va27 var28 and var29(Type I); 40%  have missing values in var24 var25 and var26(Type II); 15% have missing values in var1(Type III).   As shown in the correlation plot below there are three clusters of correlated variables that are of interest: var4 var10 var12 var20 var22 var24 var27 and var30 seem to correlate with each other; var5 var6 var7 and var13 are correlated; var1 va3 and var8 are correlated.   With a further look into the scatterplot with var1 and var3 it seems that there is a strong positive correlation between the two features.Then we checked the distribution of signal (in cyan) and background (in red) with regard to the two features. It looks like that the two labels have similar distribution regarding the two features.The two graphs below show there are strong positive correlation between var24 and var30 and between var27 and var30 respectively.  So why we were interested in analyzing those correlations plots above? It’s because we aimed to utilize those correlation to impute the data. Type III missingness only accounts for var1 and exists in 15% of all observations; Type II missingness shows in 40% of all observations with three features including var24; Type I missingness is available in 70% of all observations and involve 7 features including var27. We would lose much information if we just remove all incomplete features. We used simple linear regression to predict those missing values of the the 11 incomplete features from the 19 complete features such as var3 and var30. Since three of our models were tree based we were not much afraid of increasing collinearity due to the linear regression imputed features.A combination of those three types of missingness could result in any of the six missing patterns as shown in the table of variables with missing values. If missingness itself is information that could be utilized for predicting the signal then we should integrate the missingness for modeling. As such we created three dummy variables (0 for missing 1 for nonmissing) to represent the three missing types. Since they are categorical features their combination could well represent the six missing patterns.  Since this is a binary classification problem logistic regression model was first employed due to its efficient computation capability.  We built three logistic regression models with different features. The first model uses var1 imputed by a timeconsuming Multivariate Imputation by Chained Equations(MICE) method and the 19 complete features. The second model  has all the features from the first model and the other ten features which were imputed by linear regression. The third model has all features in the second model and the three additional dummy variables . AIC and BIC were calculated to evaluate the performance of the three models. As the model complexity increases with more features both AIC and BIC become smaller. It indicates that more features would result in better model performance. The Approximate Median Significance(AMS) score which is used by Kaggle to measure the model accuracy also favors the more complex model with all the 30 features and the 3 dummy variables. In the following modeling we would use all the 33 features. Random Forest was an efficient ensembled supervised learning for classification. It operates by constructing many decision trees for training and outputting the class that is the mode of the classification of the the individual trees. It has two important tuning parameters: number of variables randomly sampled at each split(mtry) and number of trees to grow(ntree) to get the best model performance.  According to rule of thumbs square root of the number of variables (5.74)might be likely to be the best parameter of ‘mtry’ so we searched for the best ‘mtry’ around 5.74. We used 5fold cross validation with different ‘mtry’ values from 1 to 9 and ‘ntree’ values of 100 500 and 1000.As shown in the graph above the highest AMS value appears when ‘mtry’ is 2 and ‘ntree’ is 500. Extreme Gradient Boosting(XGBoost) is very similar to the regular gradient boosting framework but it’s more efficient due to the fact that it can perform parallel computation on a single machine as well as the fact that it uses a greedy algorithm. XGboost supports classification.Before running XGboost we must set three types of parameters: general parameters booster parameters and task parameters.  parameters relates to which booster we are using to do boosting commonly tree or linear model. Whereas the  parameters depend on which booster you have chosen and lastly the the parameters that decides on the learning scenario. In our model we relied on default parameters in the  section of tuning. We focused mainly on tuning our  parameters. XGBoost comes with a wide array of different parameters to tune. We focused mainly on tuning those parameters that are generally considered the most important to tune. The 5 parameters we tuned are the following: eta ntrees maxdepth scale_pos_weight and gamma.Gamma : minimum loss reduction required to make a further partition on a leaf node of the tree. the larger the more conservative the algorithm will be.Max Depth : maximum depth of a tree increase this value will make model more complex / likely to be overfitting.Scale Positive Weight : Control the balance of positive and negative weights useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases)While we did perform cross validation in our analysis it’s important to note that one of the special feature of xgb.train is the capacity to follow the progress of the learning after each round. Because of the way boosting works there is a time when having too many rounds lead to an overfitting. You can see this feature as a cousin of crossvalidation method. XGBoost allows user to run a crossvalidation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. This is unlike GBM where we have to run a gridsearch and only a limited values can be tested. The XGBoost gave us our best results. We obtained an AMS score of 3.67 and placed 197 on the leaderboard. This was in the top 10% of all submissions.Treebased model would also allow us to draw the feature importance plot based on comparison of information gain of all the features. We drew feature importance plot below and it seems variable var1_d is an important feature which ranks number 7 of all 33 features indicating the missingness of var1 might be important in predicting the signal.  Ensemble modeling combining multiple predictions generated by different algorithms would normally provide robust predictions as compared to prediction by a single model which only has one individual rule.  It’s expected that the diversification and independent nature of each model would improve the predictive accuracy of the ensemble model.  From those models discussed above the XgBoost model seems to perform way better than the other three. When doing ensemble it’s reasonable to give a better model with more weight. We rank the model by their AMS score on the test set and assign weight 4 to the best model (XgBoost) 3 to the second best( Random Forest) 2 to the third(GBM) and 1 to the fourth(Logistic Model). All the probability predictions on the test set have their rank. The higher the rank is the higher probability it is signal; the lower the rank is the higher probability it is background. We could utilize the weighted average to ensemble those four predictive results on the test set and then reorder the averaged result by rank. Based on our best model the top 14% in rank are predicted to be signal and the rest are background. For the ensemble we use the same threshold to classify signal and background. The ensemble gives an AMS score of 3.4 which is worse than the XgBoost model which yields 3.6.  ,NA," Gradient Boosting Machine(GBM) is another efficient way of treebased supervised learning for classification problems which produces a prediction model in the form of an ensemble of weak prediction models. It builds the model in a stagewise fashion like other boosting methods do and it generalizes them by allowing optimization of an arbitrary differentiable loss function. Instead of building many decision trees (bag of trees) at a time the boosting procedure generates trees in a sequential manner starting from a shallow tree and then continuously extracting information from the previously grown tree to build a new tree.  Since the parameter tuning of gradient boosting is very computationally expensive we used the default parameter and obtained a AMS score 2.087 in the test set. We used all 33 features as mentioned above  to fit the model.
Here are some parameters worth considering when using GBM: The total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion.
 The maximum depth of variable interactions. 1 implies an additive model 2 implies a model with up to 2way interactions etc.
 A shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or stepsize reduction.
 minimum number of observations in the trees terminal nodes. Note that this is the actual number of observations not the total weight.Eta : step size shrinkage used in update to prevents overfitting. After each boosting step we can directly get the weights of new features. and eta actually shrinks the feature weights to make the boosting process more conservative.We performed a 5fold crossvalidation using the tuning grid.The Higgs Boson Challenge was a really challenging yet enjoyable machine learning task. All things considered we are happy with our best AMS score that gave us a top 10% final ranking. We explored various advanced machine learning methods including Xgboost as well as exploring advanced feature extraction methods like PCA.
The fact that we only made use of limited computing power definitely held us back. With a lack of proper computing power we were unable to do extensive parameter tuning — especially when it came to algorithms like neural networks. Another limitations was out lack of domain knowledge required to understand and evaluate the features given. This reduced our ability to select the best preprocessing methods to clean and select data to train the model.
As mentioned above we would like to extend our work and incorporate more effective computing power. This will enable us to use neural networks and stack them with other machine learning algorithm like random forests and boosted trees.",NA
Decoding the God Particle,51,https://nycdatascience.com/blog/student-works/decoding-god-particle/,"The discovery of the Higgs Boson marks one of the greatest days in science – the standard model had been confirmed. For decades scientists have searched for the particle that can explain how things have mass and only recently did ATLAS and CMS experiment begin the building of CERN that lead to this discovery. Following these events CERN put out a kaggle challenge to test the public’s capabilities in understanding and solving their simulated data.The ultimate goal of the Kaggle competition was to find out machine learning classification methods to improve the statistical significance of the Higgs Boson experiment.  The challenge was designed to develop a model that classified events based on if Higgs boson decays into tau tau channel (signal) or background noise. For this kaggle challenge team Quark was tasked with creating a two class classification algorithm to determine whether or not an event’s signal is indicative of a Higgs boson’s. Our algorithm was scored based on a metric called the Approximate Median of Significance (AMS).The AMS score provided above is a localized formula of the Gaussian significance of discovery  which in its basest form can be simplified to the ratio of true positives to false positives. Scientists need at least a significance of 5 to make claim to a new discovery which is equivalent to p  2.87 x 107­­. The Higgs Boson kaggle dataset was used in this analysis. Exploratory data analyses and algorithm development was performed in R. Our main goal was to maximize the AMS score   the higher the score the better our model was able to detect the tau tau decay signal of Higgs boson from known background noise.During the exploratory analysis of data we noticed that the distribution of variable “weights” which only appears in the training set is quite special. When we ranked the weights from large to small in a descending order we were amazed by seeing that the response variables in the training set could be directly inferred from the values of weights. These findings made us curious about the role of weights in the dataset. It turned out to reflect the true nature of simulation from which the “signal” only amount to a very tiny fraction of the “background”. The physicist provided more “signal” observation to train the model otherwise what the machine would learn would be basically the pattern of “background” and would not distinguish “signal” and “background” accurately. Therefore the “signals” typically have lower weight and should be given less emphasis in the model training phase. It seems the weights are a representation of the domain knowledge of the physicist in controlling the price to pay of in the false positive and false negative cases. Based on the data provided the model is scored higher by predicting “background” correctly than predicting “signal” correctly.The sheer amount of missing values also caught our attention. We discovered that seven data fields have missing values up to 70% and three data fields have missing values up to 40%. One data field(DER_mass_MMC) has 15.24% missing values. A closer inspection at the pattern of the missingness led us to discover that all missing features are related to PRI jet num except except DER mass MMC. The technical documentation for this competition justified our finding by explicitly stating that some of the features associated with ""PRI jet num""(recorded as 0 1 2 and 3) are undefined.    The figure above shows the pattern of the missingness (highlighted in red). We noted from the combinations plot (right side) that there were 6 patterns in our dataset  Only one combination was complete and the remaining five had some missing columns.For the observations where PRI_jet_num  0 there were 11 columns with missing values of which 10 columns were completely missing and “DER_mass_MMC"" (The estimated mass of the Higgs boson candidate) occasionally had missing values. Similarly for observations where PRI_jet_num  1 we found a pattern where there were 7 out of 8 columns values were completely missing and  ""DER_mass_MMC"" again had occasionally missing values. For the observation with PRI_jet_num  2 or 3 there were no completely missing columns. The observations all had complete data except for those which likewise observed ""DER_mass_MMC"" occasionally missing. It was obvious the features that were completely missing when associated with values of PRI_jet_num are not missing at random (MAR). While we were able to identify the 6 patterns of missingness in our data we had to ask the question: What is this missingness trying to tell us? Should we be imputing or removing the missing data? Are these columns even necessary? We then investigated the reason that accounts for the missingness of “DER_mass_MMC”. The visualization of both of the training set and test set helps us to be convinced that the division of observation based on “PRI_jet_num” and missing value pattern could be generalized to the test set. We noticed that the proportion of “DER_mass_MMC” missing value are different across the columns This tells us that that the % missingness of “DER_mass_MMC” is also not completely missing at random  if it were we would see equal proportions of missingness across all groups. Interestingly the proportion of missingness in groups PRI_jet_num in 2 and 3 are similar suggesting that the mechanism to which their missingness is derived might be the same. One benefit associated with it was that we had more observations to train a single model for PRI_jet_num 2 and 3. Based on our assumption that PRI_jet_num strongly impacts the nature of missingness and should not be treated as a whole we decided to divide the training and test dataset based on the number of PRI_jet_num values. Given the analysis above the whole dataset was divided into 3 groups:Group 1 – Observations with PRI_jet_num value  0Group 2 – Observations with PRI_jet_num value  1Group 3– Observations with PRI_jet_num value  2 and 3After dividing the data into 3 groups we removed the columns which were completely missing in each group. The documentation provides compelling proof about the reason behind the complete missingness in columns  contextually. It did not make sense for these features to have values; some were specific to multiple jet particles e.g. variables that ended in _jet_jet meant it was a derived calculation between at least two jet particles. As a result it was explicitly stated that for PRI_jet_num values below the required value could not be calculated and as a result undefined. Therefore it is justifiable to remove the columns that were completely missing in each group. After removing all columns that were completely missing in each group the only variable that still had missingness was ""DER_mass_MMC"". The missing data for DER_mass_MMC was unrelated to number of jets due to the topology of the event being too different from the expected topology. As we earlier determined “DER_mass_MMC” is also not missing completely at random (MCAR) and therefore would not be wise to just remove. In order to ensure we were not removing an important feature “DER_mass_MMC"" required imputation..The missing data in “DER_mass_MMC"" for each group was imputed using the Multivariate Imputation by Chained Equations (MICE) function from the MICE package. The function used a Multiple Imputation (MI) approach which is based on repeated simulation; it has been commonly used for complex missing value problems. The function returned a set of complete datasets (by default 5) that is generated from an existing dataset containing missing values. In our case 5 complete datasets were created by imputing values for “DER_mass_MMC” by interpreting missing values from all other variables in the dataset for each group using multivariate linear regression. Standard statistical methods were applied to each of the simulated datasets by this function. The final step in imputation for each group was the average of the 5 datasets that were created by the function.We used different machines learning approaches to build models classifying the tau tau decay signal of the Higgs boson from background noise. The models selected were Logistic Regression Random Forest and Gradient Boosting Machine  the reasoning will be described in the following sections. The models were trained on the training dataset and scored on testing dataset to obtain a final prediction. We then checked the performance of each model by comparing AMS scores evaluated through kaggle submissions.The first model we decided to try to model the events was Logistic Regression. Logistic regression’s main strengths are its quick training and computation as well as clear interpretability. Although being a supervised modeling method it also contains descriptive information regarding the feature selection. Logistic regression is a nonlinear log transformation on the traditional linear regression to the odds ratio. As a result it is a simple and effective method to model classification problems. The assumptions made in linear regression do not transfer over: The results above compare regressions between two models within one group: with and without weights. We wanted to observe the effect of weights. The default threshold value is 0.5 here.The model in the left with weights taken into account has a significantly lower (64% vs 71%) accuracy nonetheless the highest AMS. It’s reasonable because the evaluation is based on AMS with weights rather than just accuracy. As a result of weights not equally assigned for “signal” and “background accuracy could be compromised in order to get a higher AMS score in the model training. We could see the prediction of the model favored the “background” which was consistent with the conclusion we made before.  The model suggest that DER_mass_MMC is not actually important  the most important being DER_mass_transverse_met_lep DER_mass_vis DER_deltar_tau_lep and DER_pt_ratio_lep_tau. However based on the previous analysis DER_mass_MMC is what we expected to be important in determining the response variable. It might be the case that DER_mass_MMC and probabilities for the classification are not linear. We need more proof from treebased method which are a group of nonlinear methods.After submitting and scoring against the test set we got a result of 2.034Random Forest (RF) is an ensemble learning method for classification and regression. By creating a large number of decision trees it collates individual trees’ votes in order to predict the class; Sometimes that may be the mode of the classes (classification) or mean prediction (regression) of the individual trees.We needed to tune the number of randomly selected features (mtry) used in a given decision tree. For classification problems it is recommended in “Applied Predictive Modelling” book by author Max Kuhn and Kjell Johnson to start with an mtry values around the square root of the number of predictors. The book also suggested to start with an ensemble of 1000 trees and to increase that number if performance is not yet close to a plateau to get optimal results. The tuning parameters set for the random forest model for all groups are mentioned in the table below.The final mtry values for our groupings of PRI_jet_num  0  1 > 1 were 5 5 and 4 respectively.",NA," For our Random Forest and Boosting model we performed “Leave group out Cross Validation(LGOCV)”  or “Monte Carlo crossvalidation"" to cross validate our training model. Cross validation is an important method to evaluate our training model before it is used against a test set. Traditionally data sets are split ~70% training: 30% testing and scored. However not all of the data is used in training and as a result may not improve accuracy. Kfold cross validation is a method where the whole data set is split among K sets and K models are trained across K1 training sets and 1 testing set. It is the simplest method to use but it only explores a few possible ways that our data could have been partitioned. Kfold cross validation is a good unbiased estimator of the algorithm’s performance but with high variance. LGOCV allows for further exploration by randomly sampling our dataset multiple times for training sets (25 resamples for our model). LGOCV reduces accuracy variation between the models at the cost of some bias.The ROC curves give us information about the threshold value to choose to have the best true positive rate and minimize the false positive rate. For the dataset where PRIjetnum  0 we found that the model is able to predict the training observation perfectly when selecting a threshold of 0.495.The variable importance graph above for the RF model shows the features that were important for this model. From the plot we can see that DERdeltartaulep and DERmasstransversemetlep were the two most important features for the model. It was interesting to see DERmassMMC ranked 6 in the variable importance.For the dataset where PRIjetnum  1 we found that the model is able to predict the training set perfectly at a threshold of 0.501.From the plot we can see that DERmassMMC was the most important variable followed by DERmasstransversemetlep and Dermetphicentrality.For the dataset where PRIjetnum > 1 we found that the model is able to predict the training observation perfectly at a threshold of 0.502.From the plot we can see that DERmassMMC was the most important variable followed by DERdeltaetajetjet and DERdeltartaulep.  DERmasstransversemetlep moved further down in the importance ranking.Analyzing variance importance plot for each group we found that variable importance varied for each group. The dataset that had PRIjetnum 1 and PRIjetnum >1 had DERmassMMC as the most important variable. However this was not the case for the dataset having PRIjetnum  0; DERmassMMC ranked at 6variable importance plot. Therefore this suggests that it is good to have different random forest models for each group because the importance of the variables differed. In all Random forest models mass was an important feature and is in agreement with the science behind Higgs Boson detection.Each model and threshold value was used to predict the test dataset for each corresponding group. After predicting the label for the test data set  from each model we combined data sets for each jet number into one complete dataset and tested for AMS score as instructed in the Kaggle website. The final AMS score achieved by Random Forest model was 2.328 outside the top 50% of the Kaggle leaderboard.After looking at the results from random forest the next model that naturally followed was boosting. Random forest is a bagging method while Boosting works by ensembling many weak learners (shallow decision trees) accounting for small subsets of the total observations. Boosting generally has more predictive power than traditional prediction methods  however it is not without its flaws: it is significantly more computationally expensive than random forest there are more parameters to tune and a higher risk of overfitting.We trained our boosting model with the following parameters:Shrinkage is a parameter where the lower the better  it is the learning rate where the errors of one split is understood by the following. We chose a value of 0.001 as it is a decently low value while not being too computationally expensive.Interaction depth is the number of splits per tree. The maximum interaction depth is defined by the square root of the total number of features (in our case 32). To be thorough we tested 1 3 and 5.For the number of trees we continued using the heuristic from random forest i.e. 1000 trees for consistency. However in hindsight this is a parameter we should have also tuned with a range. Boosting is much more prone to overfitting and by plotting the accuracy vs. number of trees we would have been able to see the point where more trees no longer gave us better results.Min Observations is the number of observations taken into account per split. If improperly tuned this parameter is often what results in overfitting.The above ROC curves detail not only the respective cut off values we chose but also the resulting tuning parameters for each PRIjetnum subset. The min obs chose for each model was 100 and the highest interaction depth (5). The only exception was the boosting model where PRIjetnum  0. It instead chose only an interaction depth of 1 and perhaps we could see what had happened in the variable importance graph:The most noticeable thing is how the varImp graph for PRIjetnum  0 hardly took any other variable into account other than DERmasstranvsersemetlep. This makes sense when in context of an interaction depth of 1 (each tree only made a split on one variable that would reduce the gini impurity). Because boosting is greedy it kept reusing the same variable. This implies that perhaps boosting is not the best method for this subset of data. The final AMS score is a terrible 1.4 which is to be expected.Imputation is just another layer of modeling and should be treated very carefully. It’s like a regression model inside a classification model in this case. If inappropriately treated it might introduce significant bias in the model and add uncertainty. To make a less biased imputation it’s critical to understand why the data is missing and if the pattern of missing data is related to the outcome. We were doing well in this regard. Based on our assumption the whole dataset was divided into three groups and imputed creating separate machine learning algorithms for each. The predictive models were also scored  separately based on “PRI jet num”.It’s a bitter lesson to know the AMS scores were not desirable for each algorithm  especially when compared with the results of our peers. It’s evident that the predictive power of models trained on separate “PRI jet num” groupings were no match for complex boosting algorithms treating the dataset as whole. One possible explanation is that by dividing the dataset into three groups we essentially cut off the intertwining relationships between the three subset of data and hence lost some degree of predictive power. We assumed that the  “PRI jet num” variable would be so important as to consider the effect of it to the response variable separately. Even worse after the division the patterned missing values for the two groups ( “PRI jet num”   0 or 1) became impossible to be kept in the vector space and were removed.  Again it resulted in loss of information because our separate models had fewer observations to train to be  generalized  and to have fewer predictors predicting the outcome accurately.We finally realized that those patterned missing values though meaningless to impute in the setting of physics are instructional on their own. Ironically it turned out the missing value pattern is exactly one of the most important pieces of information to infer the response variables. One better practice is to potentially code those patterned missing value 0 indicating the fact that they are undefined while keeping the pattern of these reasonably imputed values in the whole dataset. Then each of the distinct classification algorithms like logistic regression random forestetc. could be applied on the whole dataset to learn the patterns namely the explicit missingness that sophisticated physicists left there. Without domain knowledge it’s really important to not let the machine to learn our bias which was regarding “PRI jet num” in this project. However we wouldn’t know the result of the prediction unless we tried. Trial and error that’s how the machine learns and also how we learn.Link to the code : ",NA
Higgs Boson Signal Detection,51,https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-competition/,As a part of exploratory data analysis we employed Principal Component Analysis an unsupervised method to determine whether it is practical to reduce dimensions. The first principal component explained only 23% of variance. Based on the result of scree plot we reduced dimensions to 10 principal components but even then only 75% variance is explained. PCA showed us that it would be difficult to eliminate variables without sacrificing prediction accuracy. Before building the model we subset the training dataset provided on Kaggle into our own subtraining and subtest. Consider an extremely unbalanced ratio of signal and background counts we utilized the sample.split() function in R to ensure that both subtraning and subtest would have same signal/background ratio. In other words we avoided a scenario where subtraining might have 85% background while subtest contained 99% background.The xgboost model yielded a pretty good result with default parameters. We then constructed our own crossvalidation function to grid search the best tuning parameters for high accuracy and AMS score. This submission ranked us to top 100. The rationale behind this performance advancement was that we improved each individual tree by increasing the maximum depth of a tree and eased the overfitting issue by decreasing number of rounds for boosting.,NA,Discovery of the long awaited Higgs boson was announced July 4 2012 and confirmed six months later. 2013 saw a number of prestigious awards. But for physicists the discovery of a new particle means the beginning of a long and difficult quest to measure its characteristics and determine if it fits the current model of nature which is a daunting endeavor. Therefore CERN the European Organization for Nuclear Research in collaboration with the data science competition site Kaggle initialized an open source challenge entitled the Higgs Boson Machine Learning Challenge to explore the potential of advanced machine learning methods to improve the discovery significance of the experiment. Our team took on this challenge and with limited backgorund in particle physics we devised a systematic approach to first understand the dataset test individual machine learning methods and then expand on our findings from there.Complete cases in the training data is 68114 out of 250000 observations or only 27.2% of the dataset. This is a red flag for handling missing data with imputation because of the overwhelming number of missing data. Since we do not have a very good handle on advanced physics this meant that we had to rely on using methods that handle missing data well such as XGBoost.The correlation plot shows strong relationships among some of the variables. We can see high correlations between some of the DER(derived variables) with PRI(primary) variables.The table plot was designed to handle large data sets and is very a powerful visual tool to understand distinguishing factors within each variable and across variables.The table plot divides the observations in 100 equal groups which are plotted as rows. The mean for each group of observations is displayed as a line and the accompanying standard deviation is displayed as a dark blue bar surrounding the mean.In the table plot below the results are sorted by the outcome (“s” or “b”). One can immediately see that in some of the variables the means and/or standard deviations are quite different. The table plot directs us to variables that appear to be the most discriminating in terms of distinguishing whether observations are “signal” or “background”.Based on the table plot and supported by calculations of mean differentials and standard deviation differentials we selected a reduced set of 10 variables that we deemed to be the most important predictors and ran them using our model. It is worthy to note that 9 of the 10 variables chosen were DER variables.Approximate Median Significance (AMS) is the metric by which Kaggle evaluates entries for the Higgs Boson Machine Learning Challenge. In a nutshell through weights the AMS gives positive points for True positives (predicting “s” when outcome is really an “s”) and gives negative points for False positives (predicting “s” when outcome is really a “b”) except that the negative points for False positives are roughly twice in magnitude than the positive points for True positive. This is the reason why the optimal results for the models use a threshold of around 15%  the threshold being the percentage of “s” predictions out of the 550000 total predictions submitted to Kaggle for evaluation. Compare 15% to the 34% “s” in the training data and you can see the effect of double penalty for False positives.The first individual model we tested was a random forest as this model is typically quite effective on large datasets can work with many features and quickly gives a sense of which variables are most important.  We trained this model under the following conditions:As expected the variable importance plot below clearly demonstrates the relevance of the mass related variables particularly the “DERmassMMC” feature.  This is consistent with findings in the previous section.Unfortunately the best AMS score produced on the test data set was 2.10 which is well outside of the top 50% of the Kaggle leaderboard.  Given the large gap between the random forest model performance and the top models in the competition we decided to move forward with other individual models rather than continuing to tune the random forest.After using random forest we then attempted a more complex model using a neural network. One of the biggest problems with neural networks and most black box methods of prediction is that tuning parameters is often difficult because accuracy does not necessarily improve on a linear scale with tuning. Thus when constructing the neural network for this data the choice of network topology and complexity was a pressing concern and we sought to achieve a more systematic approach towards finding the optimal network topology and probability threshold for choosing signal over noise to achieve the highest possible AMS score.To achieve this we built a function containing a simple forloop that creates a neural network model with one hidden layer and an increasing number of hidden nodes for that layer with each iteration. Nested within each one of these nodedetermining iterations is another loop that then finds the threshold value at intervals of .025 that results in the highest AMS score given the topology set in the parent forloop. Through this methodology we were thus able to ascertain the optimal number of hidden nodes on a single hidden layer and its threshold that yielded the highest AMS score.When testing this function on sample sizes of 10000 and below this process worked flawlessly and with each increasingly complex network topology the function would pick a different threshold that maximized the AMS score on the sampled training and validation set. However when using the method on the entire training data with every iteration of increasing hidden nodes on the single hidden layer the method would choose the same optimal threshold of 1 and deciding to classify every observation in validation set as background which produced a poor AMS score of .599. So why was this happening? Lets investigate some possibilities...If you remember back to the section above where we sought to understand the data we discovered thatThrough these observations on the data we can hypothesize that there may be complex dependency between highly correlated features that results in classification of finite signal over background and this would thus give an explanation as to why the neural network we engineered performed poorly and always chose to classify the entire data set as background. Because of the potential interdependency between the features theorized above it is possible that given a single hidden layer topology the neural network will never encapsulate this complex relationship and therefore always yield a poor AMS score. To remedy this we may want to in the future try not only optimizing on network for threshold and the depth of a single layer but through the number of layers chosen in the network topology. Perhaps a wider topology would allow the network to better encompass and tune itself to the interdependent relationship between features and thus be a better model of the data and yield a higher AMS score. The next complex model we chose to train on the data was XGBoost.The error of a model can be an effect of bias and variance. We got a mediocre result from random forest which has relatively low bias and high variance due to fully growing the decisions tress in parallel thus we moved on with boosting model where we would have higher bias and lower variance by growing sequential trees.After running 1050 models we found the parameters associated with the highest training AMS score are eta  0.1 maxdepth  9 and nrounds  85.The ranking on Kaggle leader board of this submission wasn’t as high as we expected indicating potential overfitting. By slightly tuning the parameters into eta  0.1 maxdepth  10 and nrounds  75 we improved the AMS score to almost 3.7.,NA
Kaggle  Higgs Boson Machine Learning Challenge,51,https://nycdatascience.com/blog/student-works/kaggle-higgs-boson-machine-learning-challenge/,"This blog encompassesin this case is binary:  either there is a Higgs Signal 3.53929.A descriptionThe data mainly consists ofThe snapshot below shows this information as it is displayed on the website. peculiar aspect of both the training and test data is thatThe data’s description statesOne can analyze the nature and extent of these 999.0 entries by replacing them with ""NA's"" and doing this missing data analysis. We star The plot to the leftin thefigure it is evident thatwhere one or no jet events are undefined. FurthermoretheThere are alsoIn conclusionin terms of whether or not the Higgs mass is defined and correspondingly if the event resulted in  onejet  more than one jet  or no jet production at all.  (2 x3) . We  add two new features to incorporate this information .AMS istheparticularfor the competitionThe red region corresponds to high AMS scores which are linked to low false positive and high true positive rates That is expectedone such model would be identifying just 25 % of Higgs signal correctly keeping the false positive rate to 3%and still have the same AMS score of 4.in the previous plotproduced by a perfect prediction of the training dataTheblue line below indicates AMSevery prediction is said to be aFor each of the data subgroups we calculate the AMS score by assigning either signal or background noisetoall the events in that subgroup and using correct predictions for the rest of the data .The figure above shows that performance on data where the Higgs is not defined has almost no effect on the AMS score if everything is classified as background signal for these subgroups.  This behavior occurs because there is very small signal in the data where the Higgs is not defined.Moreoverthe weight of the signals is low and the background noise has higher weights. These three factors make  identifying everything as background noise is as good as identifying everything correctly.Thusclassifying all events where the Higgs is not defined as backgroundin terms of thethe subgroupsThis is a pA densityWe first look at theThe order here is {210 }. Upper triangle which corresponds to DER (derived) features in all three seems to be correlated .  These are not observations but features engineered by CERN group using particle physics. The lowercontainsExamining the variance explained by theroom for the For examplefifteenWe do this forLet's look at the density plot of the last 9 whitewashed features of subgroup 2. We see they share a few common characteristics .As pointed earlier they  are all angle features for directions of particles and jets.In the case of the 5 phi angle features they have uniform and are identically distributedover the range for both the signal and background. This is true to some extent for eta features also but for phi it is strikingly true. Conceptually it does make sense as the particles and jet would scatter off in all directions whether or not they are signals or background. Thusthe variableswillfollow auniform distribution.The plot below contrasts this uniform distribution aspect of the least influential features to the density plots of the first 9 most influential features.The lastwith respect to which features are least influential in explaining the variance. But this particular Higgs Kaggle competition's success is determinedmaximizing the AMS score. ThusBut we can still use the above insights as a guideapproach.Let’s evaluate the approach outlined above. Using only the data which matters will reduce computation power and time needed.  This model will be more accurate as it will reduce noise.  Moreover less data to deal with means that one can try more computationally expensive models like Neural networks and SVM's and try to get lucky with automatic feature engineering .  To quantify  this benefit we  plot the amount of data used at each modeling iteration.We fit an xgboost decision tree model to our training data using the insights above . I chose xgboost here due to its having  low variance  low bias high speed and more accuracy.  We will follow the ""Third Iteration"" schema  from section 7 .  Let's prepare the training and testing sets by dropping the phi variables and assigning the background noise label to data where Higgs is not defined and splitting the data where Higgs is defined into 3 subgroups.""AUC"" is the metric of choice here as it responds well to misclassification errors.  The optimal number of trees to maximize the AUC score will be found by cross validation. We fit the whole training data to  the optimal number of trees for each dataset and make predictions for the test data to submit to Kaggle. The Private AMS score for this model is  . That's satisfactory for me at the moment considering we didn't tune any hyperparameters except for the number of treesand set the same threshold for all three subgroups. One need to do a grid search to find the three different thresholds.",NA,"a comprehensive exploratory data analysis of or Background signal. In addition the other goal is to find if certain properties of data can help us decide which models are more appropriate for this problem and what should be our choice of parameters for those models. I implement all insights learned to get an AMS score of  of the data is available from  website for the competition .  a training and a test file to make predictions and submit for evaluation. One  many values are 999.0 .  that those entries are meaningless or cannot be computed and  −999.0 is placed for those entries to indicate that .by plotting a fraction of training data which is  999.0 for different features and the combinations of features that occurs in the dataset shows there are 11 columns where values are 999.0 with three subgroups of 1 3 and 7 columns .  The combinations of features indicates there are 6 such combinations . Doing  same analysis on submission data gives exact same plot.  This indicates that original data can be subdivided into 6 groups in terms of number of features having 999.0 .Investigating the names of features with 999.0 shows that name of columns with 999.0 are   [""DERmassMMC""  ]  [""PRIjetleadingpt""     ""PRIjetleadingeta""    ""PRIjetleadingphi"" ]   [ ""DERdeltaetajetjet""    ""DERmassjetjet""       ""DERprodetajetjet""   ""DERlepetacentrality"" ""PRIjetsubleadingpt""  ""PRIjetsubleadingeta"" ""PRIjetsubleadingphi"" .Diving further into the group with 7 features is associated with two jets group of 3 features is associated with atleast one jet production which is not defined in events when there is no jet .  certain observations where Higgs mass is not defined and this is not jet dependent .  the original data can be subdivided into six groups . It's dependent on signal  background and weights associated with them in a peculiar manner as below.A plot of AMS for complete training data range for  s and b is shown below . AMS color is saturated for score above 4 as top leaderboard score is less than 4 and we want to concentrate on what makes those models good.but the peculiar aspect in the figure above is that there is a range of models which can achieve top ranked score of 4 and Next we investigate how much AMS score on training is influenced by performance in 6 data subgroups we identified in the last section. The red line  is .if  signal . will perform equally well  AMS score . This reduces  of data for predictive modeling to 3 that is Higgs is defined and event resulted in either one jet or more than one jet or no jet at all.We prepare our four subgroups of data each for training and test  and remove all the features which have NA's and one's which have become redundant and scale the data . Finally we are left with only 60% of original data which is useful for predictive modeling. Doing same for submission data also leaves with 60% of predictive modeling data further cementing the idea to drop NA's  keep aside Higgs mass undefined data and splitting original data into three subgroups.We explore response of Higgs Kaggle  which is named Label in the dataset . Plotting histogram of the weights associated with the Labels indicate Label and weights are dependent on each other . The signals exclusively have lower weights assigned to them than the background and there is no intermingling between the two Labels . retty good reason for Kaggle to not provide weights for test set ! plot of background signal does show some distinction between three subgroups but not obvious ones so we would leave it at that and come back if needed.Histogram of signal shows that there are three weights or channels in which Higgs is being sought after. There are more ""no jet"" events with higher weights than ""two or more jets"" events which are larger in number but have lower weights . correlation among features for the three data subgroups .  right triangles which  all PRI (primitive) features  are not correlated . It could be a good idea to drop all the engineered correlated DER features .principal components used indicates there is a reduction of features in all three subgroups .  for subgroup 2 components out of 30 can explain 90 % of variance in the data and 24 components can explain 99% of variance . Subgroup 1  {909599} %  {11 1315}                             Subgroup 0   {909599}%  {9 1013}So our next challenge is to identify which original features do not have any influence or have very little influence in explaining the data . We start by multiplying the PCA eigenvalues and corresponding PCA eigenvectors and plotting the projections on a heat map. The product calculated in the previous step represents the transformed variance along the PCA directions . We then sorted this along the  horizontal axis  by the PCA eigenvalues'  starting  with  lowest on the left to the highest on the right . As is evident from the figure below the lower eigenvalue transformed variance (starting from F30 and going towards F1) is zero which is what we would expect from the PCA plot from the last sectionWe now sort the original features along the vertical axis with respect to their contribution to the variance.  the transformed variance products in descending order.  We sum up the absolute value of the contributions and not just the contribution (as a feature can have positive or negative projection indicated by red and blue colors). At the end of this process we are left with features that contribute the least variance displayed at the bottom.The last 9 features in the above plot stand out from the rest of features as they have white blocks or zero contribution towards first four principal components. Another important observation is that they are all  phi or eta angle features.Similarly phi and eta angles in subgroup 1 and subgroup 0 show the same behavior. In next section we will see why they are least useful in explaining the data .  The same is evident for angle features for subgroup 1 and subgroup 0.section gave us plenty to think about  by discarding any features or for that sake any amount of data may not be a wise decision .  I propose the following We use ""AUC"" as our choice of metric as it responds well to misclassification errors.  We use cross validation to find numbers of trees which maximizes AUC .We fit the whole training data to most optimal number of trees for each dataset and make predictions for test data and prepare file for submission on Kaggle.Private AMS score for this model is  . That's satisfactory for me at the moment considering we didn't tune any hyperparameters except number of trees  here .  Plus we set same threshold for all three subgroups. One need to do a grid search to find the three different thresholds.",NA
"Pump it up, Drill it down: an Analysis of Water Projects in Tanzania",51,https://nycdatascience.com/blog/student-works/linlin_cheng_proj_5/,projectprojectsprojectsAn effective algorithm would increase the probability that funds are directed to those who can used them best and need them the most.,NA,"There is a water crisis in Tanzania: safe water source is scarce and waterborne diseases are prevalent. Thousands of individuals and agencies have stepped in to build water points to help but how effective are they? This  combines machine learning techniques with data visualization to point out potential causes of malfunctioning  identify possible success of potential  and redirect funds to the places where they are in dire need and can be spent most efficiently.Located on the East coast of Africa Tanzania is home to 51.82 Million people 68% of whom are living under the $1.25 poverty line and lack access to basic water supply. Except for the luckier population who reside closerby to the great lakes the rest had no choice but to rely on either ground or surface water extraction.However as the drainage system is poorly constructed  the leakage into the ground water system becomes a major source for contamination. Yet those who switch to draw from surface water face the same problem in regard to the safety of water and the fact that they have to walk miles to get there.What makes it even more serious is that this daily cumbersome task falls often into the hands of young girls who should otherwise be at schools. Therefore a water crisis is not only represented by the presence of water borne diseases it is also aggravated by the long term decrease of robustness in the future generations.Having witnessed the severity of the situation many NGOs central governments and local communities have stepped in to build individual improved water points. But how effective are they? This project aims at approaching this question and beyond using machine learning algorithms and exploratory data analysis. The dataset is available on  originally compiled from  and Tanzanian Ministry of Water.In Figure 2 each colored dot represents a project constructed during 19602013 the available time range for the dataset. It is observable that the number of projects are quite sparse around the central areas and the still functioning projects are not exactly evenly spread out: there is a higher concentration of red dots in the southwest the functioning ones and higher concentration of blue dots in the southeast the malfunctioning ones.Figure 3. shows an exponential growth trend for the number of projects constructed. By dividing the height of the blue bars to the height of the red bars we may see the ratio of functioning projects decreases almost steadily as the further back in time we look. And so are the heights for the green bars the projects that are still working but need some repair. And why is it the case? The lack of maintenance.Figure 4. shows that local communities perform the best in terms of the percentage of functioning projects before the Government of Tanzania foreign government and the UN agencies.  Furthermore we can group local community and the Government of Tanzania into the Searchers who have better knowledge of the specific logistics as well as the actual need from the people but unfortunately are not so well rounded in terms of money. Planners which include the foreign governments and the UN agencies in this case on the contrary are the ones that are financially sufficient but relatively not so cognizant of the best approach. Therefore when planners come into the country with aid money the institution that comes with them are not necessarily fit into the local situation.For instance Figure 5 depicts the breakdown of the functionality of projects by payment types. By assigning the types with certain kinds of payment as ""Market"" and the rest ""NonMarket"" the difference of functioning rates between groups is quite observable. Payment however minimal establishes a reward mechanism so that the people who provide such service have direct incentives to properly maintain the functioning status of the water points. And in case of a bleach the people who manages the water points are easily held accountable for their misconduct.Free goods are always exploited as in the case of ""Nonmarket"". If anything is free they are often over consumed and not applied to the scenarios where their potential utilities are maximized.  Therefore a properly designed internal market that stimulates natural growth are more efficient and the searchers are the ones to be engaged in the functionality of the mechanism. Where do we start?There is a long way to a before we reach autonomy and the current projects still heavily rely on the foreign aids.  The modeling approach in this section aims at assessing the potential success of future projects. The original dataset involved 41 features with 59400 observations covering information on the geographical locations water source types funders etc. The objective of the model is to classify functionality of the proposed projects into three categories: functioning functioning but needs some repair and nonfunctioning.  Methodologies involved include data cleaning feature engineering and two rounds of tuning for parameters  (see the pipeline described below):The final model had an accuracy of 81.11% on the subsetted test data. The variable importance plot(Figure. 6) shows that beside geographical indicators extraction type construction year funder as well as payment type play significant roles in the model.In conclusion what is needed isn't more money but better spending. There is still a lot of room for improvement for international donors.  A proper investigation of the actual needs as well as establishing proper incentives are the keys to an improvement in efficiency.Teaching a man to fish isn't always the solution. We also need to teach a man to raise his fish.Appendix: Project Code
Part One. Main Script:Part Two. Model Tuning Script:",NA
Team DataUniversalis' Higgs Boson Kaggle Competition,51,https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-competition-2/,"Contributed by Shuo Zhang Chuan Sun Bin Fang and Miaozhi Yu . They are currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between July 5th to September 23rd 2016.Discovery of the long awaited Higgs boson was announced July 4 2012 and confirmed six months later. But for physicists the discovery of a new particle means the beginning of a long and difficult quest to measure its characteristics and determine if it fits the current model of nature.  The ATLAS experiment has recently observed a signal of the Higgs boson decaying into two tau particles but this decay is a small signal buried in background noise.  The goal of the Higgs Boson Machine Learning Challenge is to explore the potential of advanced machine learning methods to improve the discovery significance of the experiment. Using simulated data with features characterizing events detected by ATLAS our task is to classify events into ""tau tau decay of a Higgs boson"" versus ""background.""We have tried a number of different machine learning methods. Below is a quick review of the methods with a general description their pros and cons.Let us have a look at our first treebased model Random Forest(abbreviated as RF below). There are two parameters we need to tune for RF: ntree and mtry. Ntree controls how many trees to grow and mtry controls how many variables to draw each time. Due to the large size of the data my first try was very ambitious: ntree  [200050008000] and mtry  [3456]. However it turned out that the computation was tremendously large and led to crush of R. So my second try was less ambitious with parameters ntree[5008001000] and mtry  [345]. From the below graphs we can see that mtry  3 is the optimal parameter for each subset of data set.However the AUC curve looks abnormal because the graphs showed that the RF predicts every single observation correct. Why is that?In our case since weight is estimated from a probability space. In order to do prediction RF model bisect on the probability space continuously until reach certain criterion (eg. no more than 5 observation in each subregion) and use the mean of each subregion to predict. However our probability space is very sensitive and ntree  1000 is too large thus making the RF too accurate. What is more by looking at only 3 variables (especially the traversemass variable) we can clearly tell whether the label is going to be a 'l' or a 'b'. So the first few split of the tree model is already sufficient to make the prediction. By making ntree equal 2 or 10 we can get a 'normal' graph. However doing so is not very meaningful because it simply made a fine model less finer.To improve RF next step is to apply gradient boosting. There are 4 tuning parameters: interaction.depth n.trees shrinkage and n.minobsinnode. Interaction.depth controls maximum nodes per tree  number of splits it has to perform on a tree (starting from a single node). For example interaction.depth  1 : additive model and interaction.depth  2 : twoway interactions. N.trees means the number of trees (the number of gradient boosting iteration) and increasing N reduces the error on training set but setting it too high may lead to overfitting. Shrinkage is considered as a learning rate which is used for reducing or shrinking the impact of each additional fitted baselearner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. The intuition behind this technique is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous its negative impact can be easily corrected in subsequent steps. N.minobsinnode controls the minimum number of observations in trees' terminal nodes which is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances. Imposing this limit helps to reduce variance in predictions at leaves. We can conclude the 4 tuning parameters are related with each other and change in one parameter has impact on the others.Let's take subset 1(jet_number equal to 1) for instance. Due to the large size of data and computational cost our first try is to use small trees (200 500 800) with lower interaction depth (3 4 5) a wide range of shrinkage (0.1 0.05 0.01) and small values of n.minobsinnode (10 50 100). The best optimal parameters with the highest AMS value is 800 n.trees 5 interaction.depth 0.1 learning rate and 100 n.minobsinnode which lead to AUC equal to 0.88(as shown in the graph). Therefore our second try is to fix n.trees and increase interaction.depth to (5710). The best optimal parameters with the highest AMS is 10 interaction.depth but it produces a lower AUC value equal to 0.86. The possible reason is overfitting by introducing too much interaction between the features. So our third try is to fix interaction.depth to 5 increase n.trees to (800 2000 5000) and lower shrinkage to 0.01 in order to reduce overfitting. And 5000 trees results in a higher AUC value (0.91). So our last try is to increase n.trees to (5000 7500 10000) and fix interaction.depth and shrinkage to 5 and 0.01. Our best optical parameters (interaction.depth:5 shrinkage: 0.01 n.trees: 10000 n.minobsinnode: 100) give us the best result with AUC equal to 0.91.The same process of tuning parameters is applied to the other 2 subsets.For subset 0(jet_number equal to 0) the best tuning parameters are interaction.depth: 5 shrinkage: 0.01 n.trees: 10000 and n.minobsinnode: 100 and result in AUC equal to 0.91.For subset 2(jet_number equal to 2 and 3) the best tuning parameters are interaction.depth: 5 shrinkage: 0.05 n.trees: 800 and n.minobsinnode: 100 and result in AUC equal to 0.91.There are 6 parameters to tune for Xgboost.Our tuning strategy for xgboost is as follows: We combined the training results from the three split datasets into one submission file as illustrated in the handdrawn picture below.After the split each subset corresponds to its own optimal tuned parameter. Then we were facing two strategies to combine the three vectors of probabilities:We tried both strategies multiple times and submitted our results to Kaggle. However the rankings fluctuated around 1000 in the private leaderboard which did not meet our expectations. Indeed at the submission stage it is possible for us to further tune a ""magic"" cutoff value say the value ""C"" in strategy 1 above to obtain a great submission file that achieving relatively higher AMS score or ranking in the leaderboard by submitting perhaps hundreds of times to Kaggle.However we didn't step into this direction because we don't think repeatedly submitting results to Kaggle to achieve high ranking was an elegant solution. .We sit back and started to figure out the root cause. Soon our team realized that this combining process is actually mathematically flimsy.Let's again look at the examples in the handdrawn graph. Although P03  0.36 in the subset jet #0 and P12  0.36 in the subset jet #1 the two probabilities 0.36 has totally different meaning:In other words the predicted probabilities in each subset were all conditional probabilities a.k.a conditioned on jet numbers. Thus two identical probability values from two subsets mean different things and they were not necessarily comparable with each other because we had no prior knowledge about the internal distributions of each subset. Even if we do do we know the signal distribution of the testing dataset? Not at all. The testing dataset could contain 15% observations with jet #0 or completely no jet #0.To sum up splitting the dataset based on jet number is indeed a possible way to achieve relative acceptable ranking or AMS score. However based on our analysis it also has drawbacks:",NA,"All R codes can be found here:
The previous research on the Higgs Boson revealed that the derived mass is an important indicator to determine its presence. By analyzing the distribution pattern of the variable DERmassMMC by label we could see that the mass observations of “s” label has narrower distribution and higher peak value than “b” label. Mass observations of the two label groups have similar mean values.We noticed that only about a quarter of the dataset are complete cases. 11 variables out of 33 have missing records which are designated as 999.  10 out of the 11 variables are related to jets as well as the number of the jets.By carefully examining the missingness through the jet numbers it can be found that the variables aforementioned above corresponding to jet number 0 are all missing while they are partially missing in jet number 1. There is no missingness detected in jet number 2.      After imputing all 999 values for each jet number group we can find high positive or negative correlations between a few variables especially the variables with a prefix “DER” and such correlations differ in different jet number groups. It can be inferred that these variables may derive from same origin.With the help of principal component analysis (PCA) we can better understand importance and relationship among all variables. From the PCA scree plot it can be known that first component has contained most of information and the data dimension can be reduced down to 8 variables. The PCA correlation result shows that some variables with prefixes “PRIjet” or “DER” have relatively higher loading values in the first two component PC1 and PC2 which may indicate the importance.After splitting data to 3 parts based on jetnumber and dropping missing columns that have no physical meaning for each subset there is still one important feature (""DERmassMMC"") which has missing values for subsets with jetnumber equal to 0 and 1. We applied KNN to impute missing values by choosing parameters ksqrt(nrow(subset)) and distance2.The data set was split into three categories to do the neural network (NN) training. We tested several neural network tools under “Caret” library and chose “nnet” as we thought one hidden layer would be appropriate for training. Two tuning parameters are available for controlling prediction accuracy: number of hidden units which connects inputs and outputs and can feed into the output layer; weight decay which is a parameter to control growing rate of the weights after each update. We assumed that small weight decay and similar number of hidden units as number of variables would generate accurate predictions. We tried different numbers of hidden units ranging between 622 with an increment of 2 as well different number of decay weights between 00.1.Below is the architecture of neural network when applies 1 hidden layer and 20 hidden nodes where blue lines indicate positive relationships while red lines indicate negative relationships.Relative importance was acquired from neural network training using label as response variable. It can be noticed several variables with “DER” as prefix have higher importance ranging 0.60.9 to label than the other variables. This shows consistency to the result of PCA analysis.The result of NN parameter tuning on the data set when PRIjetnum  0 demonstrates that the NN training performance which is reflected by AMS score is quite fluctuated as hidden node units increase. Great weight decay values (0.1 0.01) have better AMS score than small values as well as clearer increasing trend of AMS can be observed. The AMS almost stays at 0 when weight decay is set as 0.0001. We found the best combination for training this dataset when hidden size of nodes  10 weight decay  0.01 and accuracy prediction  82.54%.After the first attempt of parameter tuning it can be concluded that higher weight decay may positively contribute to AMS score and consequently a new range between 0.0010.15 of weight decay was applied for NN parameter tuning on the data set when PRIjetnum  2/3. From the tuning result it can be found that most of AMS score line monotonically increase as more hidden units of nodes involved in NN training only except when weight decay  0.01. Generally NN training performs better for weight decay  0.1 or 0.15 than smaller weight decay values. The best set for training this data set is found when hidden size 22 weight decay  0.1 and corresponding accuracy of prediction  80.4%.  Let's briefly revisit what made us want to split the dataset:This means that splitting the dataset into 3 subsets was actually not a good strategy. Knowing this was very important since it made us think even deeper into our decision process as we navigated the project.",NA
Exploring the Anatomy of a Successful TED talk,52,https://nycdatascience.com/blog/student-works/exploring-anatomy-successful-ted-talk/,From Pericles to Barack Obama some of the greatest orators of our time have committed themselves to spreading an idea that will not only inform but also inspire.  TED (Technology Entertainment and Design) continues this tradition as a nonprofit that serves as a medium for successful and unique individuals to share their stories passions and innovations. As a global community TED strives to engage people on and off the internet through videos of conference talks. Driven solely by the question “How can we best spread great ideas?” TED has become a wellestablished clearinghouse for knowledge inspiration and wisdom. But what drives viewership? What makes one video more popular or rated more beautiful than another? For this project I sought to explore these questions by extracting data from the TED website:1.Speaker Name2.Speech Transcript3.Views4.Title5.Rating6.Month uploaded7.Duration8.Speaker GenderUsing ScraPy 1874 transcripts were collected ranging in time from 2006 to 2016. You can see the code used in the development of the spider and collection of the data here:Sometimes it may not be the diction tone or length that controls views – in fact it may sometimes be an audiences hidden biases. One of the major things that can be seen from doing a simple numerical exploratory data analysis is the difference in male vs. female speakers. The ratio is almost 2.2 males for every 1 female speaker which may imply that men are given more opportunity than women to speak at the conference. Take a look at the following Rating Proportion graph: What you can see here is that women are more likely to be rated ‘Beautiful’ much more likely to be rated ‘Courageous’ and ‘Inspiring’. Men on the other hand are more likely to be ‘Fascinating’ and ‘Ingenious.’ A Chi squared test shows that at least one of the categories is dependent on another with a significant pvalue of 2.2e16. This leads me to believe that women are generally recruited to speak about social topics such as feminism. Despite the difference in rating proportion Women and men garner about the same amount of views: Diction is also considered a huge factor in determining a speech’s success. For each transcript the top ten words that were used were collated then summarized by frequency. Comparing the frequency tables of words among different quartiles:  It seems that the top ten words are similar across all quartiles: ‘can’ ‘one’ ‘like’ ‘people’ ‘just’ ‘now’ ‘know’ ‘actually’ ‘see’ ‘really’ ‘world’. The rating proportions are visualized in the form of a word cloud – what can be seen here is the same prevalence of words shown above:   Finally to understand if any of the continuous data collected had any effect on views I ran a linear regression against views with the duration of the talk the gender of the speaker and the date it was uploaded. The following are the initial results: The initial QQ plot looks terrible above 1 quartile. I tried to linearize this relationship with a boxcox transformation (lambda  18/99):  The QQ plot looks much better and the result is a significant (pvalue  2.2e16) equation. The significant coefficients are the duration and the date – the latter being much more significant. The equation however only explains 7.3 % of the variance. Date is also correlated with views because the longer a video has been uploaded for the more views it is likely to have. As a result this linear model is inconclusive. With more time further relationships may be investigated with text mining machine learning algorithms such as Naïve Bayes classifiers. What can be noted is the discrepancy with male and female speakers in proportion and numbers. While TED is a great organization that warms our hearts and inspires our thoughts it may have some legwork to do before becoming fully egalitarian.,NA,[https://gist.github.com/LostMailman/58a033566d326748f62b561b159171e8#filetalkspiderpy]Analysis Code:[https://gist.github.com/LostMailman/58a033566d326748f62b561b159171e8#filetedanalysisr] ,NA
Higgs Boson Machine Learning Challenge,52,https://nycdatascience.com/blog/student-works/secretepipeline-higgs-boson-machine-learning-challenge/,Contributed by (Emma) Jielei Zhu Le Wei Shuheng Li Shuye Han and Yunrou Gong. They are currently in the NYC Data Science Academy 12week full time Data Science Bootcamp program taking place from July 5th to September 23rd 2016. This post is based on their fourth project  Machine Learning (due on the 8th week of the program). ,NA,"The discovery of Higgs Boson is crucial as is the final ingredient of the Standard Model of particle physics a field filled with physicists dedicated to reducing our complicated universe to its most basic building blocks. The discovery of Higgs Boson was announced on July 4 2012 and was acknowledged by the 2013 Nobel prize in physics. However discovery is only the very first step. We still need to understand how it behaves. As it turns out it is quite difficult to measure the new particle's characteristics and determine if it fits the current model of nature. A key finding from  experiment is that the Higgs boson decays into tau tau channel but this decay is a small signal buried in background noise.The goal of this Machine Learning Challenge is to improve on the discovery significance of the tau tau decay signal. Using simulated data with features characterizing events detected by ATLAS our task is to classify events into ""tau tau decay of a Higgs boson"" or ""background"".The data contains a training set with signal/background labels and with weights a test set (without labels and weights) and a formal evaluation metric representing an approximation of the median significance (AMS) of the counting test. The performance of the classification result is thus evaluated by the following formula:In sum the goal of this project is to maximum the AMS score to improve the discovery significance of the tau tau decay signal of Higgs Boson from background noise.We first tried unsupervised learning methods to analyze the features of training and test dataset. Secondly we used different supervised Machine Learning approaches to build models to classify events into either the tau tau decay signal of Higgs Boson or background noise.To understand each feature of this dataset we visualized univariate distributions for each feature. We found that the features whose names end with 'phi' have a uniform pattern as shown from the graph above (plot shown from 'PRItaiphi'). The distributions for each class ('signal' and 'background') are extremely alike––nearly completely on top of each other. What this means is that the univariate distribution of the variables whose name ends with 'phi' do not contain much information to help distinguish 'signal' from 'background noise'.From the bot plot of  'PRItaiphi' (see below) we can further understand that the 'phi' type variable is an angle basically range from 180 degree(3.14) to 180 degree(3.14) . The distribution is perfectly symmetric and no outliers and no difference by the labels.However the identical distributions should never be the reason for excluding these 'phi' features from applying to the machine learning models later on because they may potentially has great influence for classifying the signal when they combine with other features.Another important finding from the correlation matrix graphs in each of the subsets is that some of the features are strongly positive correlated to other variables which may incur multicollinearity problems
especially for conducting logistic regression models to classify the labels. Thus we will not perform logistic regression in the following modeling part.Additionally 99% of the values of  'DERpttot' are the same as of 'DERpth' in subset data of jet number 0  which indicates that for further modeling part we could remove one of the features.In our reduced model part we dropped the variable 'DERpttot'.From the Principle Component Analysis(PCA) scree plot for 'jet0' subset we found that the first dimension explains about 22.1% of total variance the second dimension explains about 14.3% and the third dimension explains about 141%. This means the first 3 dimensions together explains only about 50% of the total variance which is quite low. Additionally we don't see any sharp drop off in the percentage of variance explained from this scree plot suggesting no natural cut off point in keeping certain dimensions and discarding others.Looking at the scree plot for 'jet1' subset we see a very similar pattern––low variance explained for the first few dimensions and no sharp drop off for the later dimensions.Again the same pattern for the 'jet2/3' subset.From the variables factor mapPCA for trainjet0 we can see that DERsumpt DermassMMC Dermassvis and PRIleppt contribute almost 80% of their variance on the PCA 1 and also contributed some variance on dimension 2 but not as much as the dimension 1. For dimension 2 we can see that Dermasstransversemetlep contribute almost 80% of their variance on dimension 2 and also some portion of variance on Dimension 1.From the Variables factor map PCA for trainjet1 the Dersump contribute a lot of variance on Dim1. For dimension 2 DermassMMC and Dermassvis contribute a lot on dim 2 and also contribute part of variance on Dimension 1.From the variables Factors mapPCA for trainjet 2.3. The Dermassjetjet and DER deltaetajetjet have large portion of variance contributed to dimension one. The variable Derpth PRijetloadingR have large variance contributed on Dim2.From the Exploratory Data Analysis on the previous three PCA we found out that DERmassMMC contributed variance a lot on both trainjet0 and trainjet1 so we may assume that this variable has large impact on the total data. But we may use later test to see if this factor is truly important.We tried Random Forest as our first algorithm. We wanted to set a benchmark score and look at which variables Random Forest thinks are important and which are not. Additionally we wanted to compare and contrast the list of variables PCA thinks are important against the list Random Forest thinks are important. We believed that this analysis would not only let us understand the variables better it would also help us eliminate some unnecessary variables that contribute little to the AMS score. There were a few variables that we specifically paid attention to including “DERmasslep”—the only variable that contained missing values after segregating data by jet number and variables that contained the term ’phi’— variables that explain little variance according to PCA.Our entire modeling process with Random Forest came in 3 stages with increasing AMS score for each stage (3.28 > 3.42)In the first stage we fitted a Random Forest model for each of the 3 subsets using the ‘rf’ method in the caret package. The best models were selected based on absolute AMS scores which happened when ‘mtry’ was 20 for each of the subsets (20 was the maximum number tested because . What that means is that the best models all wanted the maximum number of variables for consideration at each splitting point of the tree. The models looked fine but when we plotted the ReceiverOperating Curve (ROC) we saw the predictions were 100% correct on the training set. This is a clear sign that we were probably overfitting our training data.To solve or at least alleviate the overfitting problem instead of choosing the models that outputted the highest AMS score we went after the simplest models that were within 1 standard error of the best empirical models which brought us to stage 2 of fitting Random Forest models. Using this new selection metric we picked models that either had ‘mtry’ equals 1 or ‘mtry’ equals 2. When we plotted the ROC again sadly we still see a perfect classification rate.We first fitted a Random Forest model with varying 'mtry' values. The ones tested initially included 1 2 5 20. Out of these values 20 gave the best AMS scores on each subset although with marginal differences. We visualized the performance of the best models by plotting ROC and calculating the area under the curve (AUC) score. What we found was a plot that claimed the model was able to predict the training observations perfectly.",NA
Muse: a Better Music Recommendation Application,52,https://nycdatascience.com/blog/student-works/muse-automated-music-playlisting-recommendation-application-using-weighted-clustering/,song's title and artist name will then be used to query nd return and then play the top related video.,NA,(LINK SOON)The average American spends 4 hours a day listening to music (SPIN) and 93% of Americans listen to music daily in some capacity (EDISON). Almost half of that listening is via AM/FM radio but music streaming services are gaining a greater and greater market share and have even turned around the declining revenues of the  music industry.This success breeds competition among streaming providers to increase customer acquisition and retention by creating music recommendation algorithms that have just the right ingredients to serve up the tunes their customers crave.Let’s first take a look at one of the pioneers in the music streaming service world: Pandora. Pandora was established in 2001 and has had a decreasing market share among newer streaming competitors.Why is this? I personally believe it is because of the nature of the recommendations on Pandora. Pandora only allows users to create playlists based on one artist or song which often doesn’t encapsulate the original mood the listener was looking for and yields poor suggestions that often are repeated after as little as 10 songs later. On the other end of the streaming service spectrum are platforms like 8 tracks which do suggest playlists that a user may be interested in listening but all of these playlists have to be manually created by users. Services like 8 tracks have more dynamic playlists than Pandora but again these services rely on users to actively create playlists and then suggesting the correct playlists to users. Somewhere in the middle of the human editorial and algorithmically generated playlist spectrum of music streaming services is Spotify. This compromise is one of the main reasons Spotify has the largest market share as of March 2016.Spotify uses a combination of collaborative filtering and other playlist statistics to suggest very poignant songs to users that feel like that music guru friend you trust is making the suggestion. Spotify's collaborative filtering works by analyzing a users' playlist and finding other users' playlists that have the same songs but maybe a a few more and then will suggest those additional songs to the original user. Spotify not only uses this method for song suggestion but also weights songs based on whether a user favorited it and listened to it many times following the initial like or even when a user is suggested a song and skips it within the first minute. Because of this combination of collaborative filtering and certain statistics tracking Spotify can suggest extremely accurate song recommendations that feel eerily familiar. As great as Spotify's recommendation engine is what if there was way to build upon its already impressive algorithm and to suggest songs that are even more playlist specific.The general concept behind Muse is simple. Where Pandora suggests songs based on a single seed artist or song Muse takes into account an entire playlist the attributes of each song within a playlist and the play counts of each song to form a better query on the new Spotify API endpoint for recommendations.In early 2014 Spotify acquired EchoNest; the industry’s leading music intelligence company providing developers with the deepest understanding of music content and music fans. Through the use of Spotify's API which now allows access to these EchoNest services developers can provide seed information and target attributes in a query and the API will send back recommended songs in its response. This EchoNest API endpoint is the backbone of many suggestive services like Shazam Pandora etc.. As you can probably imagine choosing the best query parameters is thus very important and indeed this is what determines the accuracy and relevance os the  recommendations the API responds with. As stated previously Muse optimizes these query parameters to be as representative of an entire playlist as possible and therefore bring back recommendations that are more relevant. So how does Muse do it?Upon login via Spotify oAuth users automatically see their local iTunes playlists and Spotify playlists on the lefthand column. If for some reason their iTunes Library XML file is not at a standard location on their computer users can specify the correct path for Muse to look for the file.From here a user can click on a playlist which is where the magic of Muse takes place through the following algorithmic process which I have appended a diagram below followed by bullet points to explain each step in greater detail:Once the Muse algorithm has completed users can then click on any recommended song. The chosen  Users can also click the play button which will play both playlist tracks and recommendations in order by relevance score.Users can then drag recommendations they like into their playlist which changes the composition of their playlist thus triggering the Muse algorithm to rerun and get new relevance scores and recommendations based on the playlist with the newly added song.Although this feature is not entirely implemented yet it is easy to imagine that a form could be put in place that would prompt a user to select a playlist how many new songs or how many additional minutes of music they are looking for as well as how much variability in the new recommendations they would like and Muse could automatically add the number of recommendations needed to meet the expanded playlist criteria. For example if the user wanted to add 5 new songs to the playlist that were most similar to the original playlist (by specifying low variability) then with each added recommendation sorted by relevance the variance of the playlists' attributes would tighten as illustrated below. On the other hand if a user wanted to add 5 new songs that were similar but not too similar Muse would add recommendations with lower relevance to the original playlist which would still approach a low variance minimum for playlist attributes as illustrated below again but at a slower rate which in turn would expand the variability of the new playlist's songs.However a more interesting algorithm than this would be to let Muse run as it is described in the previous flow chart where with the addition of each recommendation Muse would recompile the playlist's phantom average track based on the playlist that now contains an additional song and add a then fetch potentially completely different recommendations. This process would then continue recursively until an entire playlist with new recommendation was reached with specified duration or threshold level. This can be best visualized using a similar workflow as the one described previously but with the addition of a final recursive step.The application of this recursive playlist generation is very interesting for its potential to create varied suggestions through a unique decision path. For example if the user wanted to to expand a current playlist by 10 songs that were as similar to the original playlist as possible by specifying a low recommendation relevance then Muse would add the top recommendation with each recursive iteration and approach a playlist attribute variance minimum as fast as possible until the playlist duration or number of intended songs was achieved.Yet if a low enough recommendation relevance was set by the user (say 70%) then with each recursive iteration Muse would add the recommendation with a relevance score closest to 70% which would mean that playlist generation may take a slower more circuitous route towards the playlist attribute variance minimum.The best way to visualize this process is to imagine falling down a whirlpool. The initial width of whirlpool may be wide similar to a playlist you may have containing songs with varying attributes but as you fall further into the whirlpool its width tightens and the playlist's variance of song attributes approaches a minimum. It is important to note though that like how a whirlpool twists and turns away from the center as it approaches its minimum so too does the variability of recommended songs with each recursive step meaning that each added recommendation brings you closer to the bottom of the whirlpool but may pull you in different directions throughout its descent depending on the variability of the recommendations. Finally the pace at which you are pulled down the whirlpool creating a tighter playlist variance is defined by the recommendation relevance level initially set in the playlist generation form.One final unique feature of Muse is that it allows users to create a playlist based of the Billboard Top 100 for any days since Billboard's inception. A user can click on the top bottom left of the screen to change the date and Muse will use Selenium to inject the specified date into  and then webscrape the top 100 results to create a new playlist. Muse will then weight these songs by the number of weeks the spent on the charts and find recommended songs that sound like the top hits from that day but could be from any era.onMuse was created using:,NA
Predict Movie Rating,52,https://nycdatascience.com/blog/student-works/web-scraping/movie-rating-prediction/,"How can we tell the greatness of a movie before it is released in cinema? This question puzzled me for a long time since there is no universal way to claim the goodness of movies. Many people rely on critics to gauge the quality of a film while others use their instincts. But it takes the time to obtain a reasonable amount of critics review after a movie is released. And human instinct sometimes is unreliable.Given that thousands of movies were produced each year is there a better way for us to tell the greatness of movie without relying on critics or our own instincts?The tools I used for scraping all 5000+ movies is a Python library called ""scrapy"". Below are some brief steps. The source codes and documentations can be found in github page .Many important movie information were considered and scraped from IMDB website. For example movie title director name cast list genres etc.The scraping process took 2 hours to finish. The scraping of movie posters took a little longer than pure text data. In the end I was able to obtain all needed variables for 5043 movies and 4096 posters. Overall they span across 100 years in 66 countries. There are 2399 unique director names and 30K actors/actresses.The image below shows all the 28 variables that I scraped. Roughly speaking half of the variables is directly related to movies themselves such as title year duration etc. Another half is related to the people who involved in the production of the movies eg director names director facebook popularity movie rating from critics etc.I am especially interested in knowing the answer to this question: Will the number of human faces in movie poster correlate with the movie rating? Movie poster is an important way make public aware of the movie before its release. It is quite common to see faces in movie posters. Below are the the movie posters from 8 movies that are not so great in terms of IMDB rating score (below 5). They tend to have many faces.It should be pointed out that it is unfair to rate movie solely based on the number of human faces in poster because there are great movies whose posters have many faces. For example the poster of the movie ""(500) Days of summer"" has 43 faces all from the same actress. But remember that having large face number (> 10) in poster and simultaneously being a great movie is uncommon based on my findings. Interestingly many posters made my face recognition algorithm fail to work such as:Overall nearly 95% of all the 4096 posters have less than 5 faces. BesidesOut of the 28 variables I am especially interested in know how does the IMDB rating score correlate with other variables. From the 3D grosscountryrating plot below we can see that United States produced the largest amount of movies across the past 100 years (19052015). The sheer amount dwarfs other countries in the number of produced movies. The points at the top corner of the plot denote the movies having the highest gross in the movie history. Many countries produced great movies but still there were quite a few bad movies.Movies having rating larger than 8.0 are listed in the IMDB top 250 and they are truly great movies from many perspective. Movies with rating from 7.0 to 8.0 are probably still good movies. Viewers can gain something from them. Movies with rating from 1 to 5 are sometimes considered as ones that ""sucks"" in one way or the other. One should avoid those movies unless they have to. Life is short. USA and UK are the two countries that produced the most number of movies in the past century including a large amount of bad movies. The median IMDB scores for both USA and UK are however not the highest among all countries. Some developing countries such as Libya Iran Brazil and Afghanistan produced a small amount of movies with high median IMDB scores.In the last century it seems that the number of movies produced annually largely increased since 1960. This is understandable since the development of filming industry goes hand in hand with the development of science and technology. But we should be aware that along with the boom of movie industry since 2000 there are many movies with low IMDB score. The social network is a good way to estimate the popularity of certain phenomena. Therefore it is interesting to know how does the IMDB score correlate with the movie popularity in the social network. From the scatter plot below we can find that overall the movies that have very high facebook likes tend to be the ones that have IMDB scores around 8.0. As we know IMDB scores of higher than 8.0 are considered as the greatest movies in the IMDB top 250 list. It is interesting to see that those greatest movies do not have the highest facebook popularity. I highlighted several movies to illustrate this finding. The movie ""Mad Max"" and ""Batman vs Superman"" both have very high facebook likes but their IMDB scores are slightly above 8.0. The movie ""The Godfather"" is deemed as one of the greatest movies but its facebook popularity is hugely dwarfed by that of the ""Interstellar"".It is plausible to believe that the greatness of a movie is highly affected by its director. How does the movie IMDB scores compare with the director facebook popularity? From the plot below it can be seen that the directors who directed movies of rating higher than 6.0 tend to have more facebook popularity than the ones who directed movies of rating lower than 6.0. And I listed the top four directors who have the most number of facebook popularity (Christopher Nolan David Fincher Martin Scorsese and Quentin Tarantino) along with their four representative movies.Great actors/actresses make a movie great. They are the souls of movies. How does their facebook popularity look like?For a given movie I scraped all the available cast members in the IMDB movie page. After retrieved the number of facebook likes for all cast members I ranked the numbers in descending order and picked the top 3 actors/actresses. This is based on a simple assumption: leading actor/actress tends to have more facebook popularity than supporting actor/actress; and no matter how great a movie is there will be no more than 2 leading actors/actresses. For notation purpose I named the facebook popularity for the top 3 actor/actress as ""actor_1_facebook_likes"" ""actor_2_facebook_likes"" and ""actor_3_facebook_likes"". Note also that the variable ""cast_total_facebook_likes"" is calculated by summing up the facebook popularity of all the available cast members.The assumption indeed matches with the plotted graph below. The top first actor/actress has the most number of facebook popularity while the second and the third actor/actress have much lower popularity. But it can also be shown that high facebook popularity of the leading actor/actress does not mean that a movie is of high rating.The prediction of movie ratings in this article is based on the following assumptions: With those 28 variables available for all scraped movies can we predict movie rating? Before we begin it is necessary to investigate the correlation of those variables.Choosing 15 continuous variables I plotted the correlation matrix below. Note that ""imdb_score"" in the matrix denote the IMDB rating score of a movie. The matrix reveals that:Surprisingly there are some pairwise correlations that are perhaps counterintuitive:The threedimensional PCA plot shown below reveals more information than the correlation matrix. For the 15 continuous variables we can see their relationship with the three principal components in space. The colorful points denotes all the movies. We can see that some variable vectors tend to cluster and point at similar directions meaning that those 15 variables have multicollinearity between some variable pairs. This may lead to problem when we want to fit linear regression model to predict movie rating.Although initially I scraped 28 variables from IMDB website many variables are not applicable to predict movie rating. I will therefore only select several critical variables.Both the correlation matrix and the 3D PCA plot show that multicollinearity exists in the 15 continuous variables. When fitting a multiple linear regression model to predict movie rating we need to further remove some variables to reduce multicollinearity. Therefore I remove the following variables: ""gross"" ""cast_total_facebook_likes"" ""num_critic_for_reviews"" ""num_voted_users"" and ""movie_facebook_likes"". Some variables are not applicable for prediction such as ""num_voted_users"" and ""movie_facebook_likes"" because these numbers will be unavailable before a movie is released.The plot of the fitted multiple linear regression is illustrated below. From the ""Normal QQ"" plot we find that the normality assumption of regression is somewhat violated. Thus I apply the boxcox transformation and refit the model. Although the model became uninterpretable the assumptions of multiple linear regression namely no multicollinearity normality constant variability and independence are wellsatisfied. From the detailed information of the fitted model we find that the model is significant since the pvalue 2.2e16 is very small. The ""title_year"" and ""facenumber_in_poster"" has negative weight. The ""actor_3_facebook_likes"" variable was not included in the model at all meaning that the social network popularity of the third actor in the cast member is not significant to predict the movie rating. This model has multiple Rsquared score of 0.201 meaning that around 20% of the variability can be explained by this model.Random Forest model was fitted to predict movie rating using the following variables:The movie dataset was divided into two parts 80% of the movies were treated as the training set and the rest 20% belonged to the testing set. Up to 4000 trees were generated to fit the random forest. The number of variables tried at each split of the decision tree is 2. The mean of squared residuals is  and the percentage of variable explained is 27.21% better than that of multiple linear regression.From the fitted random forest model the variable importance can be revealed in the graph below. It is interesting to see that duration is the most important variables followed by the budget and the director facebook popularity. Different from the multiple linear regression model above the ""actor_3_facebook_likes"" is considered as an important variable even slightly more important than the ""actor_1_facebook_likes"". Since the fitted Random Forest model explains more variability than that of multiple linear regression I will use the results from Random Forest to explain the insights found so far:",NA,"It should be pointed out that most movies have more than one posters. Some may argue it is unreliable to detect faces only from one poster. Well it is indeed true. However just like a great book usually having a single cover I believe a great movie needs to have a ""main"" poster the one that the director likes most or longremembered by viewers. I have no way to tell which posters are the ""main"" posters. I assume the poster that I webscraped from IMDB main page of a movie is the ""main"" poster.Below are the movie posters from 8 great movies (IMDB rating scores are above 7.5). They all have only one human face.",NA
Secret to Winning a League of Legends Game,52,https://nycdatascience.com/blog/student-works/secret-of-lol/,ddict boyfriend at 5:00 and it takes 1 hour to be the restaurant. She tells the function the date time and destination. At 3:20 my function helps her find that her boyfriend still in game so an email will be sent as follows:,NA,As a League of Legends fan and data scientist I never give up combining those 2 things I love together. In this project League of Legends game data was collected with a wellstructured scraping framework to support the further analysis and exploration and about 35000 rows match data of more than 400 players were scraped the dataset is consisting of original data and features created with feature engineering based on my gaming experience covering data of player's information her or his game performance statistics and so on. With the this pretty informative dataset I not only the made game result predictive model but also made pregame strategy analysis function and a  dating reminder function for helping users built desired romantic relationshipThere are 10 players in each game and they form 2 teams to fight each other. The website gives almost all information on player level for each hame she or he had in 2 months.Figure 1  game information sample from na.op.ggDon't panic if you are not familiar with those game terms. Please assume that we are predicting the result of a fight between 2 teams of people. For each person in this fight let's say we exam her or his following features (speed strength and intelligence):Figure 2  table of a fighter's testsIf the number of tests is large enough we are confident to use the average value to indicate the 'fight power' of each person and then get the 'fight power' of teams and finally utilize forecasting models to predict the result of a League of Legends game.For scraping the website Python package  was used because the website require visitor to click the 'game details' bottom to display full information of a game. By using this package we could simulate real users' behavior so that we can access to some contents otherwise could not be scarped with other packages like beautiful soup.The scrapping process starts from visiting the page of a player and:1) Refresh the information by clicking the 'renew' button.2) Choose the game type 'normal' by clicking the dropdown list.3) Collapse all game tabs all information in the table will be scraped.4) Open another web page on which we can scrap information of each player's champion preference.Figure 3  the tricky parts of scrapingFor each player about 80 game records will be collected for estimating her or his game level and for a 10player game it takes about 35 to scrape after I optimizing script to make it welldesigned for collecting data effectively and flexible enough to overcome problems resulted from the complex html structure of this website.To make better prediction four features were made from original dataset based on the understanding of this game they are as follows:: This feature is the player's win rate when she or he plays a certain champion which brings the analysis to the level on champion.: The website has a list for each player indicating how well a player can do with a certain champion by reordering the list I got a feature that precisely depict to which extent a player is good at a certain champion.: How many games does one player played during a period. This feature was made because the belief that a player have to play a champion to keep she or he familiar with it like you have to keep programming to ensure that you are good at it.: How many games does one player played with a certain champion during a period.Figure 4  the summary of logistics regression modelFigure 4 illustrates the importance of each feature we could see that the  is the most significant feature for predicting the game result which beats all original feature. Another created feature is also with strong predictive power. We can draw a conclusion that predict the game result on champion level (drill down features onto champion level) will offer a great model. What's more by applying this model to test data we get 86.1% accuracy.With the model we know that what's important to win a League of Legends normal game and we are able to get opponents' information before the real game starts with scraping. So we will know on which opponent we should focus and an indeed wiser decision could be made if we know enemies better.It is noteworthy that there is a button named 'Living Game' next the refresh button which allows user to check the status of a player and we scrapers can play with it.By using the Python googlemaps package we could get how long does it take to go from place to place. So we could automatically remind our date with another package which will sends email for you.Let's say a girl is going to have a dinner with her game aWe can have tons of fun with scraping can't we?,NA
Sentiment Analysis Of Yelp User Review Data,52,https://nycdatascience.com/blog/student-works/sentiment-analysis-yelp-user-review-data/,Social data provides important realtime insights on consumer opinion – on lifestyle habits brands and preferences. Because these opinions are unsolicited they provide genuine insight into consumer feelings and as such they should be valued. Yelp provides restaurant details including name price rating address and reviews. The ratings given by the users say how good the restaurant is but do you really think that the ratings alone is sufficient to give the correct information? No because people who really hated a restaurant would comment on their experience.  The same goes for athe good experience. SoThus one would expect thatperforming sentiment analysis would give give a better insight about judging ainto the masses’ opinions of restaurants.Web Scraping the yelp.com to scrape the restaurant data. The data I scraped Restaurant data was scraped from Yelp using the python package BeautifulSoup. The data consists of information such as restaurant name rating price number of reviews address and user reviews.  I split the web scraping module into two tasks. The first one is toscraped the  restaurant name rating price number of reviews and address. The second one is toscraped the restaurant name and user reviews. The data sets were then merged. Finally we merge the two datasets. We use BeautifulSoup to scrape the data.My scraping was restricted to the restaurants in a 2 mile radius around Times Square. I scraped the restaurants in and around Times Square. To be precise I scraped the restaurants placed in a 2 mile radius.Most of the restaurantsin the first 20 pages of the yelp data hashave a rating of 4.The distribution shows that the majority of the restaurant reviewsrange from 0 to 1000.Sentiment Analysis was performed using the Natural Language Toolkit. The name of the specific package used is called Vader Sentiment. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rulebased sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. The code for Sentiment analysis is as follows:It works on the word levelbyclassifyingsplitting each word into either positive negative or neutral. I want toWe concentrate on the positive and negative words as neutral words doesn't add value. The plot of the sentiment analysis is as followsA restaurant with a rating of 4 has an equal mixture of negative and positive  words. It’s safe to say that these reviews are mixed So the restaurant has mixed reviews.The algorithm can be combined with the the text mining so that the dish name specified in the reviews can be combined withincorporated into the sentiment analysis algorithm to give an output saying thatsay whether or not a particular dish is associated withhas a positive sentiment or negative sentiment and a overall score can be specified.,NA,The code used to scrape the first model is as followsThe code used to scrape the second model is as followsThe distribution of the ratings is as followsThe distribution of the number of reviews is as followsThere are few interesting observations showing that  reviews and ratings contradict. The plot of those observations are as followsThe restaurant has a rating of 4 but the sentiment analysis says that the restaraunt has more negative reviews than positive reviews.,NA
Starbucks Collectors on Fredorange.com,52,https://nycdatascience.com/blog/student-works/starbucks_collector_world/,The five categories are selfexplanatory except One cannot easily label those mugs with few traders seekers and owners because there are two possible explanations. They might be new products which just have not started trading yet or they could simply be unpopular. ,NA,"Contributed by . Amy is currently in the  12 week full time Data Science Bootcamp program taking place between July 5th to September 23rd 2016. This post is based on her third class project  Web Scraping (due on the 6th week of the program).You may also explore this project via  and .Lots of us have hobbies that outsiders could not easily understand. Poster collectors tirelessly hunt for vintage posters on eBay and at local antique shops. Sneaker collectors camp outside of Footlocker overnight just to get a pair of limited edition basketball shoes. The outsiders call these addicted collectors crazy for spending so much time and money on chasing after incomprehensible collectibles. However collectors need not care about judgments from outsiders because they often have formed their own community oftentimes online where they make friends with collectors who share the same passion and insider language.This project focuses on using web scraping data visualization and Kmeans clustering (an unsupervised method) to understand a special group of collectors: Starbucks collectors. These collectors often refer to themselves as ""muggers"" and are usually active on Facebook eBay and other online social platforms. Muggers actively exchange or purchase secondhand Starbucks products from other collectors around the world.One of the biggest online platforms for the Starbucks collector community is a usercontributed website called Fredorange. An Austrian Starbucks collector created the site as a virtual platform for muggers to share and contribute information about Starbucks products. Muggers can also use this site to set up deals. Fredorange.com has been the goto website whenever old and new muggers want to find out latest Starbucks product release or to simply showcase their collection.For this project I am interested in creating a database for Starbucks collectibles and collector profiles in order to find patterns in supply and demand of mugs. I decided to scrape the following attributes(in red box) from the website.Using the BeautifulSoup and pandas packages in Python I scraped the desired attributes from Fredorange.com. The following is the code for scraping user profiles. You can find codes for scraping mug profiles .Data cleaning was difficult because all data were contributed by users. Fredorange does not give users options of city and country when they sign up for an account. As a result there were many phrases with typos or in different spellings or foreign languages in city and country attributes. Also some users chose to only indicate their city but leave their country blank so it was another tedious task to figure out users' country from their city. The code below reflects a small part of this data cleaning process. is a Shiny app I created for noncollectors to understand how Starbucks muggers communicate and for collectors to visualize current supply and demand of Starbucks secondhand products. The website consists of four parts. The first tab shows the geographic distribution of Starbucks products and the collector community. The second tab shows a ranking system of product values based on Kmeans clustering result. The third tab shows Scarcity vs. Popularity graph for collectible mug editions. The last tab shows Scarcity vs. Popularity graph for popular collectable countries of origin.After data cleaning it was possible to visualize the geographic distribution of Starbucks products and collectors. Unsurprisingly United States has the largest numbers of products and collectors. One can visualize the geographic distributions of collectors and products outside of United States by selecting maps that say ""... excluding USA"".This is how the geographic distribution map of collectors outside of USA looks on the . Other than United States collecting Starbucks product is a popular activity in Canada parts of western Europe and parts of East Asia.Using Kmeans clustering algorithm with numbers of owners seekers and traders as inputs I separated all mugs into 5 distinct groups. The choice of k was based on examination of withincluster variances of different k's. The parameterwas set to 100 so that the algorithm was run 100 times before selecting the lowest withincluster variance.The Kmeans clustering result clearly separate all mugs into 5 groups with distinct characteristics which I labeled into five categories:  (Purple)(Red)(Yellow)(Green)and(Blue)The screenshot below shows how the ranking system based on Kmeans clustering looks on the . Note that Kmeans clustering is an unsupervised algorithm so the ranking system was not a classification model. However the clustering result does allow us to effectively have an idea about the value of each mug in the secondhand market. Four editions and six countries were included in the last two tabs of the Shiny app because they have relatively large numbers of ""high difficulty mugs"". Users can select only editions or country of interest to visualize each mug's supply and demand in the secondhand market. Details about each mug will show up when users hover over each data point.This is how the Scarcity vs. Popularity graph looks on the . Two new variables derived from seekers and owners are used to help visualize supply and demand of mugs from popular editions and counties of origin. Popularity is an index ranges from 01 using the formula Seeker/max(Seeker). Scarcity is another index ranges from 01 using the formula |Owner/max(Owner)1|. For example mugs with high popularity and high scarcity are most likely Similarly mugs with low popularity and low scarcity are most likely View Complete R Codes .View Shiny Application ",NA
Analysis of  Olympic Games by web scraping,53,https://nycdatascience.com/blog/student-works/web-scraping-olympics-games/,"Contributed by Shuo Zhang. She is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between  July 5th  to September 23rd 2016.I really like to watch the summer Olympics.  It’s simply breathtaking to watch the world's best athletes compete in the various sports!  I also love the Olympics because of the plethora of data available. From judging to timing to preliminary rounds to finals to the the various Olympic records there is data for every sport and country at the Olympic Games. Most of it especially more recent data is free and easy to find. The Rio Olympics is almost over but we can be confident of one thing:  the US United Kingdom and China will top the medals table when it's all over. One question we might want to ask is why these countries are so successful. Has the Olympic games achieve gender equality in competitors?  Does age impact the number of medals the athletes can get? Data can help answer these questions.Past summer Olympics data can be found at: .The overall website is organized and structured very well. I wrote a web scraper in Python using the package ""Beautiful Soup"" and extracted the following data for the further analysis：Here's an example of the code used to do the scraping:Analyzing the number of Olympic medals won by geographic region by year reveals the true impact and extent of medal diversification.  For example whilst more countries are winning Olympic medals how many medals are they capturing compared to traditionally strong Olympic nations? Is their success fairly minor or more pronounced?  What are the possible contributing factors to their success in the Olympics? I listed the history of total medals won by the top 13 leading countries.This graph shows that since the first modern Olympic games the landscape of medalwinning nations has markedly changed. Before World War II Olympic success was dominated by the United States and Europe. Afterwards more African and Asian countries begin to participate in the Olympics and the medal standings are marked by the arrival and growth of many regions including Japan South Korea China and Hungary. Here are three factors that affect medal standings revealed by analysis: Medals won in the past can be seen as an indicator of a ""sports culture"".  The United States for example always perform quite well. Sporting prowess is important to them so  many people take part. The United States hosted the 1904 Olympics and won 231 medals compared to 48 at the previous games. The phenomena occurs again and again.  For instance  China hosted the 2008 Olympics and collected 100 medals compared to 63 at the previous Olympics. This is a recognized pattern. Performing in front of a home crowd combined with extra investment in sport  gives the host country a medals boost. Australia won 27 medals in 1992 followed by 41 medals four years later. This was probably due to increased investment in sport in the runup to the 2000 Sydney Games. The UK as another example increased its medal haul from 30 to 47 between 2004 and 2008 prior to hosting the 2012 Games.This graph also illustrates some national indicators such as GDP population GDP growth and life expectancy also possibly play roles in the medalwinning battle of the Olympics: Countries with a high GDP like Germany or the USA can afford to invest in sports facilities and their populations have enough leisure time and money to take part in sports. This may not be the case in poorer countries. A big population means a big talent pool to choose athletes from  in China's case 1.36 billion people.These countries with a high GDP growth  tend to invest more in sport because they value the prestige that sporting success brings. China is a good example.Countries with a high life expectancy have a big healthy pool to choose athletes from such as Japan.T0 take a closer look I extracted the data from the 2012 Olympics  including the medal winning record of all the participating countries  and analyzed the relationship between total winningmedals won by each country and its population GDP GDP growth and life expectancy.This correlation graph demonstrates the total winningmedals is primarily correlated with population and also other variables such as GDP and life expectancy affect the total winningmedals. And the scatterplots illustrates cluster pattern between the five variables. Thus a Kmeans clustering model is applied to find the underlying pattern.KMeans clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted and the algorithm just tries to find patterns in the data. The key of Kmeans clustering is to determine the number of clusters K. I used the average silhouette method to determine K.The graph indicates that the optimal number of clusters is 5. I choose two dimensions that cover the most of data and plot the data to see the clusters.The distribution of the 5 clusters by different variables is illustrated below and we can see cluster 5 has highest average total winningmedals a relatively high average population a relatively high average GDP per capita a relatively low average GDP growth and a relatively long average life expectancy. Cluster 4 has the lowest average total winningmedals a relatively small average population the lowest average GDP per capita the highest average GDP growth and the shortest average life expectancy. Which countries belong to these clusters?To get a clear view of the country distribution I only labeled the top 5 leading countries in the total medals standings in cluster 2 3 4 in the following graph.  We can see the Kmeans clustering well separates the countries to developing and developed countries. While developed countries win the most medals China as a developingcountry catches our eye for its successful performance in the Olympics.By applying the same Kmeans clustering to the data of 2008 Olympics we can see the same pattern in the below graph.I created a breakdown of which countries the data shows will excel in which sports by investigating its past performance in 28 summer Olympics given the total medal values– where a gold medal is worth three points silver two and bronze one.The three graphs shows that United States Russia and Jamaica are likely to dominate the athletics events the United States Australia and China excel in swimming and China the United States and Russia athletes are good at gymnastics.This graph shows that more women participate in the Olympics. My focus is on the popular Olympic sports. What differences remain between the ways that male and female athletes are involved in Olympic competitions? I analyze all of the men’s and women’s events at the London 2012 Olympics to identify gender differences in the structure and rules of the sports and in the opportunities for male and female athletes.There were 132 women’s events and 162 men’s events at the London 2012 Olympics. Of these 57 events on the program are gender exclusive (i.e. there were medal opportunities for men but not for women and vice versa): 42 events are open only to men (25.9% of men’s events); and 15 events are open only to women (11.4% of women’s events). Together these exclusive events constituted 18.9% of the Olympic programs.This graph shows sport distribution of these exclusive events. For example there are more men's events than women's in the wrestling and canoeing sports while there are more women's events than men's in synchronized swimming and rhythmic gymnastics.This graph shows a total of 10903 athletes competed in the 302 medal events – 6068 men and 4835 women so there are 1233 more men than women competing in London. And male athletes compete for 991 medals while female athletes compete for 872 medals. In conclusion the data shows that there is still some way to go to achieve gender equality.I wanted to dig a little deeper so I created density plots of the distribution of athletes’ age by 4 types: all athletes athletes who won gold medals athletes who won silver medals and athletes who won bronze medals.You can see that very young medalists (early teens) and older medalists (late 30sover 40) tend to win fewer medals than medalists within the “sweet spot” of late teensearly 30s.Some sports like equestrianism have older athletes winning medals whereas a sport like gymnastics has a peak agerange of earlytolate teens  and early 20s.We can see which sports produce the most medals.  Athletics is number one followed by swimming rowing and football. So if you’re an aspiring Olympianbut you’re not quite sure what sport to train for you’ll increase your chances of medaling if you choose athletics.If I had more time  I would apply Kmeans clustering to all the data from 19602004 and investigate whether the same pattern exists and if there is difference what is the reason for this difference.",NA," The analysis can be found in the following link:
The analysis  can be found in the following link:The analysis can be found in the following link:",NA
Finding a dream house?  Data help!,53,https://nycdatascience.com/blog/student-works/web-scraping/finding-dream-house-data-help/,The Portland Oregon metro area has a lot to offer. A 90 minute drive west brings you to the beach. In the opposite direction there's a snowcapped mountain. It has a thriving food and wine scene a burgeoning tech sector and temperate weather. An increasing number of people would like to move to this metro area. Perhaps the greatest concern for these new residents is finding housing. Data can help.Records for house sales on  over the past three years in the neighboring cities of Portland OR and Vancouver WA were scraped by using the Python packages BeautifulSoup and Selenium. Around 20000 sold house records with 15 variables are extracted including zipcode address location selling price days on redfin beds and baths size price per sqft and etc.  The public school ratings were gathered by scraping the website  according to zip code. A screenshot is below.We see that in the Portland metro area the median house price per square foot increased 35% from Aug 2013 to Aug 2016.  However the house transaction volume was increasing during the past three years.The house transaction volume is strongly seasonally dependent  it is highest in summer and lowest in winter. The new school year starts in September so the real estate market is influenced by people looking to move within a desirable school district. Since offers need one or two months to be processed the data shown here is delayed. Agreements for house sales usually happen in the Spring. Therefore this is the hottest season for real estate market.Vancouver WA and Portland OR are the twin cities in the metro area. However which of these is the best place to live?  In general the living cost in Portland is more expensive than Vancouver as shown in the boxplot. The red boxes show the price of each zip code area in Portland and the green boxes show those in Vancouver. In seven out of ten zip code areas in Portland the median price per unit area is above $200/sqft. The median price per unit in all the areas of Vancouver is lower.During the past three years the price growth rate per unit area demonstrates the potential appreciation capability. The top 10 zip codes are in Portland and the bottom 10 zip codes are in Vancouver. In this regard some areas in Portland such as 972029720697220 have gradually increasing house prices. Buying a house in these areas would conceivably be more rewarding in the long run. In comparison the price appreciation in Vancouver is relatively weak.The public school ratings for elementary schools middle schools and high schools were webscraped. The ratings were averaged by zip code since the house data was segmented in this way. The school ratings are positively correlated as shown in the plots.  Especially for elementary and middle school ratings the correlation coefficient is 0.9083869 which is very high. Usually good elementary middle and high schools are bound together and vice versa.As shown in the plots the housing cost in Portland is higher than that in Vancouver for the same education quality. Also as school ratings increase so do house prices. Now you know when and where to narrow your target for house hunting based on price location and school district. What specific features do you care about in a house? The following plot shows the correlation relationships among those numerical features. As previously explained school ratings among elementary middle and high schools are highly correlated. The price house size (sqft) number of bedrooms and number of bathrooms are positively correlated. Larger houses more bedrooms and more bathrooms lead to higher house prices. In general one would expect smaller houses to be cheaper than larger houses. However when one takes into consideration the price per square foot this relationship doesn’t really hold.As observed for the house in one zip code and within a short time period (August 2015 to August 2016)  house price is positively correlated with house size (sqft). However the price per unit square foot shows a complex trend. When the size is less than 2000sqft the price per unit square foot drops. It is constant from 2000 to 5000sqft until it slightly decreases after 5000sqft. Therefore buying a house smaller than 2000sqft means you pay more for each square foot. The combination of house price and house size should be optimized before submitting an offer.,NA,As we see the analyses of housing historical records are able to help residents find a desired house fully with trust of data.,NA
Landing my dream job by scraping Glassdoor.com,53,https://nycdatascience.com/blog/student-works/web-scraping/glassdoor-web-scraping/,Data scientists are tech savvy. We do not like manually browsing through job postings. We would rather our profile and skills be matched with the best available positions. Aren't there dozens of companies providing similar services? Sure but you have to sign into multiple websites share your personal infos set an alert check the company reputation filter out useless recommendations ... Way too manually intensive!,NA,"In this post I am going to use web scraping to define a simple recommendation system for data scientists looking for new employment. The idea is using to use the resources published on Glassdoor.com to create a dataset addressing the US job market for data scientists and data analysts. A web application developed in R Shiny allows the user to upload a resumé and receive a list of recommended job posts depending on the candidate skills education and work experience.
Data scientists are also creative. While it may be difficult and time consuming to match a resumé with multiple sources it is possible to reverse the approach and create a single job posting database combining the results of multiple queries for a specific position. The database is then matched to a resumé and the best recommendations are provided on the basis of a given set of skills.For this project I will limit the scope to Glassdoor.com a California based company founded in 2007 providing a database of millions of company reviews salary reports and interview reviews. About 1300 job posts for data scientist/ data analyst published within the last month in US were collected and analyzed. The resulting database shows an expected concentration of job vacancies in the well known hubs for data science San Francisco and New York.Let's now have a look at the candidate profile: as shown in figures 13 the ideal ""Data Scientist"" holds a Master/PhD title and is proficient in a number of tools for scripting database management visualization and parallel computing. While Python and R are by far the preferred scripting languages the more ""traditional"" programming languages such as Java and C++ remain popular. The historical competition between Matlab and Mathematica seems to be definitely won by the first although its popularity is well below R. As for data manipulation and modeling tools SAS appears to be the most frequently used software followed by Excel and SPSS.Once the keywords are defined for the prospective data scientist in terms of skills education and experience it is possible to move to the next step and match a resumé with any given job post. Before looking into the technical details it is important to realize that both records typically lack a uniform structure. CVs follow a variety of templates and they may or may not be complete; similarly job posts are found to vary quite significantly in terms of length and structure. Despite the template defined by the service provider (Glassdoor in this case) even the most basic parameters (i.e. company name location rating) are often inconsistent.
Under these conditions I chose to treat both resumé and job posts as unstructured text filtered by a set of common keywords and then compared according to their similarity. Among the various measures of similarity available in literature I will be using the so called ""Jaccard similarity"" defined as the ratio between the intersection and the union of two ""bags of words"" respectively. This metric is meant to reward a good match of keywords while penalizing those CVs (or job posts) containing a very broad description. In these cases we will expect the union of the two ""bags of words"" to increase more than the intersection decreasing therefore the value of similarity.As mentioned in the first paragraph the job search is deployed as a Shiny app (see the screenshot below) receiving a resumé as input processing the text and producing a list of recommendations with the relative link to the website. The user can filter the list by location company or company rating and select the preferred link to the job post. The reliability of the system is closely related to the number of keywords (currently over 50) used for the match.The prototype application presented in the post allows one to get relevant recommendations for most resumés tested among bootcamp participants at NYC Data Science Academy. Its limit relies mostly on the number of jobs scraped and on the limited bag of words chosen as as filter. In order to become a valuable tool for job search the app will need to include an ""update"" function that allows the user to rebuild the database with the latest job posts published within 23 weeks. Furthermore the keywords currently hard coded in the search functions (see helperP3.py below) should evolve into a dynamic list of skills obtained from the same database. This will allow me to improve the similarity measure and to reflect the evolution of the labor market.Developed in Python. Deployed using R Shiny.
View Github: Packages used:Although Glassdoor.com provides an API to retrieve informations on job posts the project requires a manual web scraping. For such task I chose Python Selenium which allows one to browse through a website mimicking the behavior of Chrome. Despite the relatively simple structure of the page the script requires a few tricks as Glassdoor tends to throw a number of popup and CAPTCHA messages intended to limit if not to avoid the presence of bots. The scraping is performed in two stages at first recording a brief description of the post and the relative link then browsing through the list of links and retrieving the post description. These two operations are achieved by the same functiontuning the parameter  for the short description and  for the complete post. The function is reported below.Once the scraping is complete the dataset needs to be processed in order to obtain a list of relevant features for each post. The process is done using by several functions displayed below in the  script. The script also contains a few functions used for scraping calculating the Jaccard similarity checking data consistency and I/O. For data cleaning and building the keyword dictionary I found the work presented by  particularly useful.In the third part the function  is used to produce the data frames used for visualization. Depending on the number of job posting found for a given category the function returns the top 10 locations top employers most frequently requested skills education and languages (besides english). The data frames can be plotted as a word cloud by calling the  function.In the final part of the project a simple Shiny app is built to match the users' resumé and the database. The interface between R and Python is managed by the  package allowing one to load the  script in R. The python function    accepts the input text (uploaded in pdf format) and converts it into a single string by means of . As a result the function produces a csv file containing the recommendations which is read by R and displayed dynamically by Shiny.Contributed by Diego De Lazzari. He attended the NYC Data Science Academy 12week full time Data Science Bootcamp program taking place between July 5th to September 23rd 2016. This post is based on his third class project  Python Web Scraping (due on the 6th week of the program). The R code can be found on .",NA
Seed Accelerators and Social Media: What made VCs Fund These Startups?,53,https://nycdatascience.com/blog/student-works/seed-accelerators-social-media-made-vc-fund-startups/,The success of a startup depends on many factors such as the founders funding and theenvironment of the industry in which it is established. Startups never stop searching for achance to improve their probability of success. It’s the same for venture capitalists(VC’s). VC’s work hard to select the best target to invest with  to maximize theirprofit.Navigating the early part of its existence well is crucial toa startup’s success. A good seed accelerator can provide enough mentorship and funding support for startups. Mentorship help founders clearly understand what they want and what they should focus on. This iswhy some successful startups are usually born in the same seed accelerator.After a seed accelerator VC’s play an important role in helping startups to become stronger. However it’s difficult for VC’s to know whether a young startup will succeed or fail. It’s common to use the Discounted Cash Flow method to a public company but this method can’t be applied to a startup. In fact most startups don’t have clear financial records and formal financial reports. Therefore Relative Valuation is a choice for evaluating fastgrowing startups. A critical part in the Relative Valuation for online companies is finding a related company and assessing whether the two or similar or not based on the number of users. We can also explore how startups behave on  social media to indirectly assess its  number of  users.Thisproject focuses on webscraping data from  and . The first contains data about seed accelerators while the latter serves as the source forsocial media data of startups.I first took out the top ten seed accelerators with the most past funding. 'Y Combinator' dominates the feed in this respect. This is partly due to its being older. According to Wikipedia it is the first seed accelerator. Y Combinator’s creation was followed by TechStars (2006) and Seedcamp (2007). The bar chart to the left cements the importance of age when it comes to seed accelerators.The top ten startups from companies with a valuation greater than 1 million dollars are ordered by their total amount of funding. Most of them are very popular today.  All of them are online companies which proves the importance of the number of users in the valuation of startups.The data scraped from twitter contains some interesting insight. Friends_num statuses_num and favourites_num are more correlated with each other than with followers_num but these three variables are less correlated with funding (total amount of funding) than followers_num. This means that followers_num has greater direct influence on how much funding a startup can get. It's reallya reasonable projection since the number followers on Twitter depends on how popular the startup is and people who follow the company’s Twitter account are more likely to be users of itsbusiness. However the other three variables is not direct indexes of how popular the business of the startup is because a startup can write as many statuses as possible on Twitter even though it has only a few followers.The analyses above serve as a guide on how to apply multiple linear regression to this problem. The initial form of the model lies below.However assumptions such as multicollinearity need to be checked before building an effective regression model. It is also possible that the variance explained by the model might be small due to the variables not having enough predictive power.,NA,Contributed by Shu Liu. Shu is currently in the  12 week fulltime Data Science Bootcamp program taking place between July 5th to September 23rd 2016. This post is based on his third class project  Web Scraping (due on the 6th week of the program).You may also explore this project via R Python Codes and Data on Github.name website number of followers number of friends number of statuses amount of funding rounds of fundingname address established year website amount exited amount funded number of startups exited number of startups fundedAmount of funding/rounds of fundingSeed factor (year state amount funded(number of startups funded) )  &Users factor (number of followers),NA
What is Christie's selling? A web-scraping project,53,https://nycdatascience.com/blog/student-works/much-christies-making/,Like many other firms Christie's went through a period of ups and downs in the late 90’s and again in 2008.  In both cases however Christie’s recovered and has shown relatively steady growth. Aside from classical auctions the company has also extended its business into private transactions and a wider category of art. See Figure 2. for a word cloud of Christie's sales event titles.This project focuses on analyzing the sales data from Christie's Auction house’s using a dataset built from .  Since the dataset is not publicly available this project also presents the webscraping code that I used.As a transaction intermediary with a fine reputation Christie's Auction House slices off a significant amount of money as commission: it charges 25 percent for the first $75000; 20 percent on the next $75001 to $1.5 million and 12 percent for revenue above $1.5 million. Christie's global sales revenue corresponds well with the overall trend of the global economy: when the economy was booming in early 2000s the art business grew steadily.When the subprime crisis occurred in 2008 Christie's sales witnessed a plummet of around 50%.   As stated above it resumed healthy growth subsequently.Despite a drop in the number of events in its main office in London in more recent years Christie's offices in the two London offices remain the busiest venues among the rest in terms of total number of events held. In addition the auction house signed a 30year lease for its Rockefeller office in New York and opened a number of subbranches to accommodate the ever growing demand from the American market.,NA,The following code presents a method to construct a database for Christie's event list:After scraping through the website I used the following code to transfer the list of dictionaries into a .csv file that can be easily processed in R:The output comes in a neat format with 10090 observations of events with related ID Name of Events Location Date Total Sales and urls that directly link to the auctioned item lists for each of the events.In the third step I used Selenium to imitate a click to loop through urls for each of the events to access another set of urls for each of the items:In the last step the following code proposes a way to build a dictionary set of each of the auctioned items  similar of the one obtained in Step 1:,NA
What makes a crowdfunding campaign a success ?,53,https://nycdatascience.com/blog/student-works/makes-fund-raising-campaign-success/,"The website’s front page has a very simple layout with previews of a few campaigns and of categories on the left side of the page. I scraped the first 100 ads from the Medical Memorials Emergencies Volunteer Charity Animals Sports  and Education categories.The first campaign from the snapshot above provides a general template of the data available. It has a customary title monetary goal  current amount creation date number of contributors the creator’s name the creator’s location the campaign’s category under which it was set up  number of likes  number of Facebook shares story section describing the fundraising cause photo  name and time someone donated and comments left by people .Due to time limitations I decided to scrape only the numerical data for this project so the processing and analysis could be completed quickly. I used   for crawling web sites and extracting the data. I started with setting up containers to collect the items.The most time intensive portion of creating the scraping framework wasfinding the relevant xpaths for the data . Once that was done the Scrapy spider collected data from 100 campaigns across eight categories which was stored in a csv file. It took me a while to get a handle of finding xpaths and making scrapy spider but ultimately tweaking   proved to be sufficient for the task in hand.I cleaned and processed the data in Python. The cleaning process mostly involved encoding strings and removing punctuation and numerical abbreviations.  I then created two new features: ""days""  for the number of days the ad has been up since the creation date and percentage of monetary target met. Then i focused on averages of the downloaded and new features across the eight categories.I started with plotting  the distribution for number of days an ad was up for different categories . Unsurprisingly urgent ads such as those for memorial services  emergencies and medical needs were the shortest (a median of 3 days). Personal categories such as sports volunteering and charity were up there for longer time (a median of about 10 days) . To my surprise the animals category also had a low median (about 4 days) Americans love animals !From the perspective of  campaign targets  the medical category seems to have high values  with a few statistical outliers with targets of half a million.  Another stark outlier is in Education where a campaign had a target of one million dollars. I personally would like to know what that person is studying !not influenced by categories.If we look at current funding statuses with respect to the percentage of the target amount garnered there is a uniformity across all categories of about ~ 60% ! I am  impressed that if one sets up a $1000 campaign they can raise on an average at least $600.My next investigation looked at the average number of people who contributed to a campaign and the average number of social media shares per category. Campaigns with urgent needs are more active in this regard while medical needs have more people contributing on average. Memorials havethe highestnumber of Facebook shares.The correlation plot of all the features shows two strong correlations. One seemed a bit trivial in that is the money raised being proportional to the number of contributors. The other one between money raised and the number of Facebook shares is more interesting. It means even if a person contributes or doesn't contribute it always helps to spread the word and pass the ad around on social media.The major focus of this project was to scrape and analyze data from Go Fund Me. I believe that there is more to learn from the data I collected and even more potential insights in the uncollected data. Some of the things I want to explore in the future are:",NA,"The third project of the bootcamp was geared towards web scraping data. I decided to scrape a crowdfunding platform which allows people to raise money for events from accidents to trips. My preliminary goal for this project was to explore patterns to find factors which influence a fund raising campaign  quantify  ""success""  of a campaign and build a model to predict it.",NA
How expensive are the real estate properties in NYC?,54,https://nycdatascience.com/blog/student-works/how-expensive-are-the/,Every year people come to New York City from all over the world to pursue careers chase dreams and seek success. They might even want to buy a home. In that case these questions may come to mind: What are the prices of the properties in NYC? What kind of property will I be able to buy? What is the average price in my neighborhood?  This shiny project seeks to provide answers to these questions.The data comes from a real estate company website City Realty and covers sales history from 2003 onward. The shiny app focuses solely on the year 2016.  The original data contains the following variables:For preprocessing the geocode() function turned addresses into coordinates to facilitate plotting on a map. Sometimes this process produced errors such as putting a NYC property in Australia.  To solve this the area of note was restricted to NYC. The addition of a variable called Area was created which assigns boroughs to each property.  Finally the Price.FT2 and Price variables were transformed from strings to numbers and renamed  Cost.FT2 and Cost.   The default map where all the sales are plotted appears first in the Interactive Map panel.On the side bar you can choose which neighborhood you want to have a look at the price range and the property type(2b2b eg.). Let's say you chose Brooklyn the map will give all the location of the sales and number of sales in each area.If you are particularly interested in one of the properties you can simply zoom in and click on that property. A popup will appear and show you the address and a website linkto this property.In the Explore data session I calculated the average price per square feet for each Area so that you can see the price trend over time.Of course you can compare prices in multiple Areas as well.Thank you for reading this blog post. Please feel free to give any comments!,NA, In the Reference panel you can explore the original data and do some search also.For example you might want to know the most expensive property listed in this shiny app. By looking at the slider in the interactive map we know that the maximum is about 59M. Thus we simply type in 59 and the result will come out.,NA
Thinking about Starting Your Own Business? Check GEM First!,54,https://nycdatascience.com/blog/student-works/thinking-starting-business-check-gem-first/,"As one of the most dynamic markets in the world my homeland China is now undergoing a huge economic transformation.  A friend once told me that almost everyone born after 95s’ in China is now thinking about starting his own business. Putting aside the credibility of this claim his wordsindeed incited my interest in investigating the entrepreneurship environment in China. In searching for the answer I came across Global Entrepreneurship Monitor (“GEM”). They've been investigating this topic for more than 15 years! Looking at their public datasets I initiated the idea of visualizing it hoping this App will aid the future businessmen across the world when they do their initial researches about entrepreneurship.For example in this screenshot of the app I selected the tab ""Global Overview"" and then ""Perceived Opportunities"" and the App shows a corresponding map with darker color representing higher perceived opportunities. If you click on the map the country name and specific statistic will pop out. (Note that countries colored grey are those ones outside the scope of  GEM 2015 survey)Ok. Now you have an overall idea about where are the hottest zones in terms of starting a new business but you may now start to wonder can I really do this in my country? Will the government support me? Can I find a place to build the factory? How will people think about me if I really quit my job and start a small business?From a GEM point of view these factors are called Entrepreneurial Framework Conditions (EFCs). They are the conditions in an Economy that would either enhance or hinder the cultivation of new businesses. I've put them all on a radar chart you can choose whatever countries of your interest to plot. By observing the shape of the polygons you can clearly see which countries' are advantaged and which are disadvantaged with respect to entrepreneurship.Rank rank rank. It's not a valid infographic if it does not contain ranks. So here I provide the choices for you to create your own rank! Simply slide to choose the top n's you want to show and choose 2 ratios of your interest you'll get 2 colorful bar charts showing the ranks that you want to examine.To create this App I used 2 datasets downloaded from GEM they are the National Expert Survey(NES) 2015 and Adult Population Survey(APS) 2015 for convenience of the users I embedded the two data tables inside the App.  You can scroll search any data you want and if you are really interested you can always go to .",NA,This shiny APP is made possible by   –  the world's foremost study of entrepreneurship who generously made their  accessible to the public online.If you are interested in entrepreneurship the first thing you may want to know is is there high opportunity to start a business in a certain country (probably your own country)? You can check the global overview tab there I selected 4 most import ratios for you to visualize.,NA
Visualizing Global LPG Trade,54,https://nycdatascience.com/blog/student-works/tracking-global-lpg-trade/,95% of the world's goods are transported via the ocean including a variety of cargo and essential energy resources. Liquid petroleum gas commonly known as LPG or referred to as propane is just one of these energy resources. LPG is mixture of flammable gases often used for heating cooling appliances and cars in developing countries who lack the energy infrastructure for more common types of fuel. As these developing countries grow the volume of LPG transported has followed to meet these developing countries increasing energy demands. The volume of LPG transported worldwide has increased drastically over the past 3 years and the order book for new vessels will account for 35% of all vessels on the water when completed by 2019. These trends substantiate the importance of this growing commodity type and this application was develop to better map global LPG transportation at a country and vessel level in order to assess the transformative patterns that have occurred over the past few years and dramatically visualize this data which has existed mainly in only private excel sheets. Furthermore this project lays the foundation for a dashboard that could not only visualizes historical data as it does in its current state but could merge this import/export data with automated production and ship position data to predict future over and undersupply of LPG in various regions (see next steps section for more) and give ship owners an edge positioning their open vessels in this increasingly competitive market. Probably the most noticeable trend in the last five years in global LPG transportation is the the dramatic change in which countries export the most LPG. The Far East region including primarily China Japan South Korea and India have remained consistent as the largest importers of LPG year after year with an increase in Indian imports and a decrease in Japanese imports over the last few years. Although the volume of imported LPG has increased among these countries the countries have remained fairly consistent. The same however cannot be said about countries which export LPG. Over the last five years the greatest export countries of LPG have shifted dramatically. Five years ago countries within the Arabian gulf region including Qatar UAE and Saudi Arabia dominated the global exports while now the U.S. has become the single largest exporter of LPG (up from nearly zero exports in 2011). For those in the industry this trend is common knowledge but again this known trend has probably not been visualized in such a dramatic mapped fashion or through  flow charts that adjust dynamically over time. The real power of this application exists not in   but through the potential it has if  further data sets are compared against this preexisting data. If I were to get access to APIs that automatically feed in the real time data for the preexisting data sources the current positions of all global VLGCs with their ballast or laden (empty or full) statuses worldwide and LPG  inventory and production data at a country level it would not be very difficult to assess present and future over or under supply of LPG globally. Comparing these data resources against the flow data the application already presents would allow a user to assess the number of VLGCs in each region or country compared to the inventories and production levels for that country both historically and forward looking. Predicting future over or under supply of VLGC and inventories has amazing potential because ship owners could then make more accurate speculations on where to send their open vessels.  ,NA,When starting this project I knew I wanted to plot LPG trade data from IHS Connect () which has lifting information indicating the volume traded as well as the source and destination countries of every voyage where a vessel was carrying LPG.  This was a great start but the first hurdle was how to obtain coordinate data for countries in order to then plot the arcs between countries you see on the map tab. To do this I merged this lifting data with the Google countries dataset ()  both on the source country and the destination country columns to assign a latitudinal and longitudinal center for each country.With this accomplished I then created a reactive function to filter this lifting data based on user input from the date slider. This was done using observeEvents to update reactiveValues that could then be accessed within other reactive elements in the server.R file. The filterData reactive function filters the reactiveValue for data  based on the country dropdown selectors which then updates the map flow chart and data table reactive components. However because this filterData function doesn't change the reactiveValue the list of all countries still within that date range is still accessible and can be used as the selection options in the dropdown country selector .The most interesting part of developing this application from a technical standpoint was probably the implementation of the threejs package for R within a shiny dashboard application. The threejs package for R exposes a few threejs widgets including the globejs module which I used to create the main component for map tab of the application. Although threejs is a powerful javascriptbased graphics framework this package really limits the amount of customization allowed on the exposed components. Given these limitations I used the filterData function to update the arcs positioning thickness and colors based on adjustments to the reactive date slider.The rigidity of the adjusting Javascript and CSS from within a shiny R application was probably the most substantial hurdle I faced when developing this application. Although I was able to add script tags to the dashboard body to set up jQuery event handling adjusting the threejs package from within the server.R was less successful given the time constraints of this project. Writing custom CSS was accomplished by injected a style tag at the top of the dashboard body component of the UI.R file like so:For more information read the inline comments within the files of the github repoWritten in R using R studio. Deployed using ShinyIO.Packages used: ,NA
H-1B VISA Explorer Shiny App,55,https://nycdatascience.com/blog/student-works/h1-b-visa-explorer/,Unlike Apu who  didn't want to go home after finishing education and had to live in US illegally for US not having an H1B visa program at that time  the author wanted to explore  the chances of an application getting approved  how long it takes for an application to be processed  which profession  job title  employer has high volume for applications and the success rate for applications.The US H1B visa is a nonimmigrant visa that allows US companies to employ foreign workers in specialty occupations that require theoretical or technical expertise in specialized fields such as in architecture engineering mathematics science and medicine. The job must meet certain criteria to qualify as a specialty occupation. Under this visa a US company can employ a foreign worker for up to six years. Individuals are not able to apply for an H1B visa to allow them to work in the US. The employer must make the petition for the employee. H1B visas are subject to annual numerical limits. Current immigration law allows for a total of 85000 new H1B visas to be made available each year. This number includes 65000 new H1B visas issued for overseas workers in professional or specialty occupation positions and an additional 20000 visas available for those with an advanced degree from a US academic institution.For details you can follow the link below:The case disclosure file covers VISA determinations issued between October 1 2015 through June 30 2016.Link to the data is below.The web app's basic layout is designed with a principle of being simplistic intuitive and easy to use. It provides three types of data exploration: “Temporal” “Statewise” and “Top of the Bunch”.  The Temporal analysis explores thequestion of how long it takes for applications to be processed as well as how many applications are submitted on a daily basis.   and time series of applications submitted and processed . Statewise analysis allows the user a to choose a number of different visualizations to dig intostatewise data.  Top of the bunch answers which states  jobs   job titles and employers have the most  applications and highest salaries.In a typical work week two thousands applications are submitted and processed. The data shows apretty big spike in the month of March for both submitted and processed applications. As it turns out April 1st is the last day to submit and process applications for year 2016 H1B visa cap.Most applications seem to be processed within a week's time frame as is evident from the plot with the mode being on day 6.The app allows different visualizations for the data . Below is a way to look at total certified applications on a pie chart. The number of applications approved and denied are proportional to applications submitted so this pie chart for applications approved follows a similar trend as the map where California and Texas had the most applications submitted.Approval percentage for applications is quite high with a minimum of 88 % to as much as 98 % in a few states. It indicates that once an application gets selected via lottery it has a very high chance of being approved in all states.The app lets the user explore up to top 20 results of by  states occupations job titles and  employers in terms of number of applications being approved for H1B visa and salary ( Average salary for top states) . Below are some interesting features .Bar chart below shows California  Texas  New Jersey and Illinois have the most applications approved .IT and consultancy companies from India are the top beneficiaries of the the program. Google and Microsoft are among the US companies that stand out.Job titles are related to occupation category so the same result of software professionals and financial analysts getting most approved applications still shows here.The app also lets users explore top states occupations  job titles and employers in term of salary .  Though number of applications are dominated by software professionals .  Top salary occupation and job titles are ones belonging to medical field .If one is interested in knowing which specific employer offers the highest salaries here is a graph.Surprisingly North Dakota  Washington and Maine are the top three states in terms of average salary.,NA,"This article is not a social commentary on US H1B immigration visa as the featured image of the article might prelude .The image is an ""attempt"" at humor about concerns of outsourcing jobs among some sections of US population to Indians by the author who himself is of Indian origin and like the  character  """"  from Simpsons in the image came to US from India on a student visa to do his PhD and as it would happen to be made this app to showcase skillsets to potential future US employers.California  Texas  NY  NJ area dominate in term of bulk of applications submitted .Average salary for H1B Visa is more or less uniformly distributed with 45 states within 60k80k range.IT jobs and Financial analysts jobs account for bulk of H1B visas approved .",NA
Tracking migration patterns through Eastern and Southern Europe with Shiny,55,https://nycdatascience.com/blog/r/shiny-tracking-migration-patterns-eastern-europe/,Over the last 8 years more than 1.7 million migrants reached Southern Europe either through Turkey or crossing the Mediterranean Sea. At the same time another 2.8 millions registered Syrian refugees are currently located in Turkey. ,NA,"This project attempts to visualize the migration patterns followed by over a million migrants in the last 18 months by means of an  developed in ""Shiny"". While offering a dynamic picture of the migration flow through Eastern Europe from Greece through the Balkans up to Austria the project aims at analyzing its composition in terms of country of origin and gender.According to the United Nations High Commissioner for Refugees () the total number of refugees globally accounted for in 2016 is estimated to 14.5 million people. When internal displacements and ""stateless"" individuals are considered the total population of concern reaches a shocking 58 millions largely located (~75%) in Africa and Asia. While it is hard to conceive such a huge flux of people fleeing war poverty or persecution in their country of origin the consequences of such displacements have recently become a central topic of debate within European Union. The UN refugee agency is continuously collecting the daily arrivals per country  allowing a precise mapping of the migration flow and therefore supporting the emergency response plan. While the project focuses on the arrivals gender and origin recorded between October 2015 and June 2016 the complete database can be found .
As shown in Fig. 1 the Shiny application appears as a dashboard where the sidebar is used for the navigation while contents are displayed in the main tab.The balkan route represents the daily arrivals combining the visualization of the daily arrivals on a map (either as single frame or as animation) with a time series for each country. As expected the flow is rather discontinuous with a number of ""spikes"" propagating from Greece to Macedonia (FYROM) Serbia Croatia and Austria.  The flow of migrants splits between Slovenia and Hungary up to mid 2015 when the latter closes the border forbidding any further access. A similar policy is applied on Albania and Montenegro. Despite such limitations the flow does not seem to be stopped. A comparison between the 6 countries  involved shows basically the same trend.As mentioned in the previous section the second and most dangerous route towards Europe crosses central Africa and the Mediterranean Sea. If the balkan route is quite well defined both geographically and ethnically the latter is much more complex as it entails most north African countries from Morocco to Egypt and multiple destinations such as Spain Malta and Italy. In the last 18 months about 100.000 migrants reached the Italian coast mostly from Algeria Libya and Egypt. Surprisingly only 25% of the arrivals are refugees while the majority comes from Nigeria Eritrea Gambia Cote d'Ivoire and several other countries. The difficulties and risks associated with the African route have a clear effect on gender distribution: women and children account only for 26% of the total arrivals in Italy against the 48% estimated in Greece. Overall both in 2015 and 2016 the number of registered minors resulted larger than the number of women for a total of 300.000 arrivals.Due to the time constraints of the project the application is mostly focusing on the Balkan route and on ""hosting countries"". Furthermore data are dowloaded and processed directly without exploiting the flexibility offered by the UNHCR API. In the next future the app will be completed by merging the two routes in one single map and updating the underlying statistics in real time. Furthermore I would like to develop a similar map for the ""countries of origin"" in order to provide a complete migration pattern from the country of origin to the actual destination.In the final section I will briefly describe the essential steps taken during  the development of the web application.  As anticipated in the title I used the Shiny Dashboard framework for ROne of the first steps in the development of the app was to build a reactive table depending on two inputs: a chosen dataset and a given number of columns (allowing multiple choices). As the first input influences the available choices for the second input I used a In contrast with the usual reactive expression (using lazy evaluation) observers execute their content as soon as their dependencies change (i.e. they use eager evaluation).Once the datasets is available and processed the main map was created. In order to achieve that I used the package  for the world map and  for the time series. The latter allows to compare arrivals over time or to visualize a single country over a certain number of periods. The sample frequency (i.e. the smoothness of the curves) can be set by the user.Written in R using R studio. Deployed using ShinyIO.Packages used:Contributed by Diego De Lazzari. He is currently in the NYC Data Science Academy 12week full time Data Science Bootcamp program taking place between July 5th to September 23rd 2016. This post is based on his second project  R Shiny (due on 4th week of the program). The R code can be found on   while the App is stored on .",NA
What are the most popular smartphones and apps in China?,55,https://nycdatascience.com/blog/student-works/popular-phones-apps-china/,"Contributed by Shuo Zhang. She is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between  July 5th  to September 23rd 2016. The shiny website: China is the 4th largest country in the world with a population of 1 401 586 000 (and still counting). There are 980.6 million smartphone users in the country as of today. China is as great as their Walls and is considered a giant when it comes to mass production of goods electronics furniture toys and now it is dominating the advanced world of smartphones. You can easily spot a ‘China Phone’ with the huge display screen vivid display colors HD camera features and the plastic (most likely) outer case.Apps are revolutionizing the way we travel and nowhere is this more true than China. Downloading these apps before you get to China can make everything a whole lot easier as it alleviates problems with the language and can help you get your bearings.When you travel to China and you want to buy a phone the first question that comes to mind is “what are other people like me buying”. There are so many phone brands making it difficult to pick one. Supposing that you finally make a choice and you are ready to install apps the second thought is “what are the most popular and useful apps?” From the phone manufacturers' perspective who will win the phone battle and occupy the China market? Who are their target customers? How can they improve sales volume and increase market share? From another point of view when you walk into a store and you use your app to make a payment how much information does the store manager receive about who you are (i.e. gender and age) based on the phone and app you are using? The shiny app described in this blog can help answer these questions. I hope you enjoy your tour of exploration.TalkingData China's largest thirdparty mobile data platform releases a weekly report detailing every android phone and app used in China (i.e. excludes the iPhone). Phones with the Android system dominate the Chinese smartphone market representing 89.9% of the total number of phones sold in China according to research from iiMedia. The dataset contains a wide variety of variables about the location gender and age group of the users involved. We'll focus on those variables for the shiny. The data schema can be represented in the following chart:From this main dataset I created two separate datasets. One is for device_id related to phone brand users' gender age and group and location ; the other is for device_id plus app_id related to phone brand users' gender age group  location  app categories whether apps are installed or not  and whether apps are active or not. For data analysis please refer to the link for R codes: From this shiny you will get answers to the following questions:China’s phone market is dominated by Xiaomi and Huawei. But Samsung Vivo and OPPO also catch our eyes for their good performance. Also I list the top 4 popular phones in 2015 provided by IDC. By comparison  Samsung's dramatic increase in 2016 is most evident and Samsung now ranks third in the market.Furthermore the shiny app provides a way to identify the trendiest phones in the top 3 brands.We can take a look at the possible reasonswhy the market is preoccupied with Xiaomi Huawei and Samsung.First let’s look at the Xiaomi Redmi Note2.This smartphone has broken all the selling records in China. It sold over 800000 units within 12 hours. It is one of the few pocketfriendly phones in China and with its 13MP camera and a powerful Octacore MediaTek chip. It also offers a userfriendly interface and a lively display .As Huawei’s flagship phone the Ascend Mate 7 makes the enthusiasts silent with its display size of 6 inches advanced processor 13MP of a rear camera and its long battery life.Samsung S6 and S6 Edge offer a better performance than the previous generations of the Samsung Sseries. It has a powerful Exynos 7420 processor built on a 14nm process which makes it the most advanced among all the other smartphones offered in the local tech market including the latest Android 5.0.1 Lollipop.Xiaomi and Huawei are popular local smartphone brands in China. Yet different cities  in China may have a preference toward different smartphone brands. To get a clear view of their geographical distribution I separate the map to two parts.Consumers in different tier cities have different concerns with respect to smartphones when purchasing new sets. The graph shows a distribution of the smartphone users by gender. Men have more smartphones than women in China with 70% of men owning these devices and 30% of women on average in May 2016. Additionally different phone brands have different distributions of female and male users. Huawei and Samsung seem to have an advantage in attracting males while OPPO and Vivo are more attractive to females. This is not surprising.It suggeststhat females prefer fashion phones such as OPPO and Vivo while males like functional phones such as Huawei and Samsung. Xiaomi is the king here (of course Apple is excluded).Let’s take the top 5 phone brands for example. To get a clear view of the age distribution I categorize the ages into  6  groups: below 27 (the post90s) 2736 (the post80s) 3746 (the post70s) 4756 (the post60s) above 56 (the post50s).First let's investigate the general distribution of each brand.The youngest users are between 16 and 18 years old  the oldest users are between 65 and 82 years old and the users' average age is between 27 and 32 depending on the phone brand.Secondly let's dig deeper to the age distribution by different age category.Differences exist in the purchase of smartphones by age. The post90s consumers are more likely to buy Xiaomi Huawei and Vivo. However consumers in the post80s post70s post60s and post50s are more inclined to buy Xiaomi Huawei and Samsung. All graphs reveal the same information that the young generation prefer to buy the fashion and more affordable phones such as Vivo and OPPO while senior people are more in favor of Huawei and Samsung. Again Xiaomi is the dominant.The further analysis of age group can be performed by the following code:  The graph shows the rank of  the top 10 most popular app categories in terms of installed apps. In May 2016 industry tag is the firstmost popular category  the secondmost downloaded app category is property industry 2.0. Also property industry 1.0 and services 1 hold places in the apps market.Let's take industry tag and video for example.Types of installed apps in smartphones have a significant discrepancy in different cities. For industry tag Shanghai Shenzhen Beijing Sichuan and Henan occupy the largest market share. While for video Shenzhen Anhui Shanghai Sichuan and Beijing have the largest market share.Let's take Xiaomi Huawei and Vivo for example.Different phone brands have different distributions of APP categories. Industry tag and property industry 2.0 seem to have an advantage in attracting Xiaomi and Vivo phone users while property industry 1.0 and services 1 are more attractive to Huawei phone users.The graph shows a distribution of the app categories by gender. Men download more apps than women in China with 75% of men installing these apps and 25% of women by average in May 2016 and different phone brand shares a similar distribution.Let’s take the top 5 app categories for example.The youngest users are between 16 and 17 years old  the oldest users are between 68 and 80 years old and the users' average age is between 28 and 32 dependent on each app category. While many people think of apps as things that younger people use there are older people who download and use apps. After all there are all kinds of senior citizens who are very techsavvy and are using computers and mobile devices every day.How about the preferences of different age groups?Differences exist in the downloading of apps by category and age group.  The post90s and post80s consumers are more likely to install  services 1 and video. However consumers in the post70s post60s and post50s are more inclined to download services 1 and p2p net load. Again industry tag and property industry 2.0 are the dominant.The further analysis of age group distribution can be performed by the following code:
Surprisingly there are more inactive downloaded app categories than there are active ones which means most people install a lot of apps but they actually do not use them frequently.In the shiny you can customize your own word cloud by selecting your gender and age.For example I am female and 30 years old.From the graph I find out the top 5 bestselling phones are Xiaomi Huawei OPPO  Samsung and Vivo and the top 4 mostdownloaded apps are industry tag property industry 1.0 property industry 2.0 and services 1.I think you can find your own answers now:If I learn more machine learning skills  I could b",NA,"global codes:
ui codes:
server codes:
There are 6 pages in the shiny. In each page you  have different choices by selecting items in the sidebar. I will show you my findings when I explore the shiny app.Let's take a look at the top 10 popular smartphones that are trending now in China.The best 10 Android phones on the market  have similar features with more advanced cameras faster processors and fresh apps and some of them are coming in at cheaper prices. Let's take the top 2  phone brands for example.Their geographical distribution is similar to each other in the 3 firsttier cities: Beijing Shanghai and Shenzhen but they have different sale volumes in the other provinces. For Xiaomi users in Sichuan Fujian and Jiangxi account for the most market share. While for Huawei users in Shandong Fujian and Hunan take the most market volume. China's cities can be categorized to four tiers based on economic factor (i.e. the firsttire cities have highest percapita income and the fourthtire cities have lowest percapita income).  From CIW report China’s thirdtier and fourthtier cities account for 58% of the country's smartphone market while China’s firsttier cities only account for 13% and secondtier cities 28%. We can get the same conclusion when the market share of the 3 firsttier cities is calculated. For Xiaomi the 3 firsttier cities only account for 10.6% and for Huawei this percentage is 12.2%. Consumers in firsttier cities value product evaluation reviews. Secondtier cities consumers are more concerned about related information on additional features and performance metrics. And thirdtier and fourthtier cities consumers are more inclined to focus on price of smartphones.                                                                                                                                                               ",NA
"WHERE, WHEN, WHAT – How to live safely in Los Angeles?",55,https://nycdatascience.com/blog/student-works/live-safely-los-angeles/,In this interactive interface users are free to check crime and collision types during different time periods and different seasons. The temperature in Los Angeles is really stable all the year round so a year is only divided into two seasons hot and warm according to the historical record. Hot season is from May to October (6 months) and warm season consists of other months (6 months).In addition I also divided a day into seven periods for deeper exploration:  • Early morning: 4am  8am • morning: 8am  11am • Noon: 11am  1pm • afternoon: 1pm  5pm • evening: 5pm  8 pm • Night: 8pm  11am • Midnight 11pm  4am. Generally speaking 'Hot' season (May to Oct.) has slightly more crime and collision than 'Warm' season (Nov. to Apr.) in all periods of a day. As to crime and collision types 'Traffic' related issues are the major part and burglary and theft related issues also worth notice.,NA, 2011~2014 Crime and Collision in Los AngelesContributed by Shu Liu. He is currently in the NYC Data Science Academy 12 week fulltime Data Science Bootcamp program taking place between July 5th to September 23rd 2016. This post is based on his second class project  Shiny (due on the 4th week of the program).Crime and collision have always been issues that concern people especially for those who are living in Los Angeles or who are going to live in this area. As a big city with a large population Los Angeles has problems such as traffic accidents burglary and robbery. This Shiny Visualization project is designed for people who care about their safety in Los Angeles.Three features of crime and collision were investigated: geographical distribution crimes time and crimes types. The shiny app described here includes detailed information about crimes type location time and date of incidents.  The database covers the period from 2011 to 2014. Readers are welcome to explore inside and know more about crime and collision in Los Angeles.You can explore this project via . (It may take you several seconds to load the app.)You can check the R code on .The shiny app allows users to customize the area and year they’d like to pay more attention to and know every detail about every crime and collision that happened in the past four years. On another page “Crimes Area” visitors can explore more about crime and collision trends or drill down to learn more about specific incidents.LAPD oversees 21 areas in Los Angeles. The bar chart shows the number of crime and collision incidents based on how the LAPD defines different areas.   According to the right graph the Hollenbeck has the fewest incidents and the most occur in  77th Street. This rank is based on the total volume of crime and collision instead of crime occurrence density.The shiny app allows one to check the historical trends in crime and collision not just the current amount.  In some areas for example Hollenback a low volume of incidents can be misleading.  Despite its looking apparently safe because of the minimal volume of crime and collision it has an increasing trend during the four years.Dividing total crime and collision into seven days of a week it’s interesting to find that Friday has the most crime and collision while Sunday has the least. I cannot give a proper explanation without professional knowledge in criminology but this graph does tell us a pattern of crime in a week.In addition there are more noteworthy trends in this threedimension graph. This graph can tell us more information about how days and a specific time period together influence the crime and collision occurrence . From the right angle it’s obvious that crimes are much more frequent after noon than before noon. During 5:00  6:00am there are the fewest crimes compared with other times in the day.After transferring to another side we focus on the 0:00pm (midnight) of every day. Crime and Collision are more likely to happen at 0:00pm on Saturday and Sunday and the afternoon and mornings of weekends are relatively safer than weekdays ,NA
"Consumerism, the Original Sin of Global Warming?",56,https://nycdatascience.com/blog/student-works/consumerism-original-sin-global-warming/,Global warming is happening at least 95% of the scientists believe so. And the results are quite severe: the sea level is rising animals are dying and we are witnessing more extreme weather conditions in recent decades than ever. This blog walks through some of the evidence for the existence of global warming a possible cause and a call to action.  Figure 1 plots the anomalies of world average temperature relative to the Jan 1951Dec 1980 average.Despite of the variations in late 18c the deviations away from zero has a clear trend increasing since the Second Industrial Revolution.On the global level Figure 2 captures snapshots of North America and Antarctica for the daytime temperature in the week of June 17 2016. Areas that are hotter than the average temperatures for the same week or month from 20012010 are marked in red while areas that are colder are colored in blue. In the north where it is summer in June there is a higher degree of concentration of red dots showing that the summer is hotter while the opposite happens in the south where the winter is colder.What is the cause of such changes? An immediate answer goes to the accumulation of  that tends to absorb and emit radiation in the atmosphere. But since GHG has always existed on the planet earth there must be an external reason for this the series of changes. And that reason most likely is us humans.As conquerors and consumers of the material world humans emit an excessive amount of GHG to generate power for direct consumption and to produce goods and services.  There are two ways to quantify the emission of GHG: CO2 equivalent by Production per capita and CO2 equivalent by Consumption per capita; the two terms represent different metrics to quantify the impact of GHG on the atmosphere.The first method measures the per capita level of total carbon emission during the production process within a geographic border. It is the most commonly used measurement. However measuring emission based on production ignores the fact that in many cases producers and consumers are geographically spread out in a highly globalized world.Therefore by reallocating the amount of carbon emission during the production process to the final users of the goods and services the second method provides better measurement as well as additional insights on the subject of Consumerism and Global Warming.And as a matter of fact we do see a clearer positive relationship between Carbon Emission (per capita) and National Income (GDP per capita a proxy for consumption) if we select CO2 based on final demand rather than CO2 based on production. (See Figure 3 and Figure 4)Similarly as the economy grows for countries around the globe over time we can also observe a higher rate of emission per capita as brown countries (high emission per capita countries) become more brown and green countries (low emission per capita countries) become a darker green.It is interesting to point out that the relation becomes less clear if we switch to the first method of Carbon Emission measurement.Finally if you are still reading this blog post and wonder if there is anything that you can do to help slow down the anthropogenic tragedy from occurring so fast on a global level the truth is: Yes You Can.Consumption is a necessary driving force of economic growth and social development the purpose of this article is not to argue for zero consumption.  Rather unnecessary purchase behaviors as well as the habit of wasting can be improved. In addition the pursuit of efficiency is highly advised whether it be the purchase of vehicles with best Fuel Economies or the usage of public transportation overall. At last you can always be the one to spread the word!,NA,This post is prepared to illustrate my Shiny App: Consumption Production and Global Warming available  (content slightly altered due to display restrictions).,NA
United States: Fury Road?,56,https://nycdatascience.com/blog/student-works/united-states-fury-road/,Driving fatalities have clearly decreased in the past 10 years with the overall numbers decreasing around the time of the financial recession and continuing to stay low even into the recovery. It's unclear exactly why this decrease happened however I imagine it was a combination of safer cars combined with lower levels of driving. Drunk driving distracted driving and speeding all seemed to follow a similar trend suggesting that the rate of these dangerous driving hasn't decreased significantly in the past ten years.Finally I included a simple tool to allow you to compare states directly based on any rate that you wish to see. In the above example you can see that my home state of Connecticut has about one fifth of the driving fatality rate that Wyoming has. ,NA,"When I was researching crime rates for my visualization project (check it out !) I discovered that more than 16000 people died last year as a result of homicide around 11000 of which were from guns. While both of those numbers are incredibly high I was shocked to discover that they are overshadowed by another number: driving fatalities. 32675 Americans lost their life last year as a result of a car accident making it the number one accidental cause of death in this country. Unless you are a BASE jumping enthusiast or an amateur deep sea diver it is very likely that driving is the most dangerous thing you do every day.Despite these statistics driving is very rarely treated as a dangerous activity. Many people still drink and drive despite the growing popularity of Uber and Lyft. And who among us can say they've never glanced at their phone while behind the wheel? In terms of policy however driving receives a disproportionately low amount of attention considering the amount of destruction it causes. There are countless politicians who have built their careers as being tough on crime yet I don't know of a single successful politician who has run on the platform of being a safe driving enthusiast.This problem is obviously far more complex than the scope of this project. The goal of this app is to simply facilitate a better understanding of the driving problem in the United States. In which states is drunk driving the worst? Where is there the most distracted driving? How much does speeding play a role in these accidents? I hope these questions and more can be answered through investigation of this data. I hope you enjoy it and please remember to drive safely!

The National Highway Traffic Safety Administration (NHSTA) releases a yearly report detailing every fatal accident in the US. The report contains a wide variety of variables about the mechanics of the crash the location and the passengers involved. It also contains data regarding the blood alcohol content of the driver whether the driver was distracted and whether either party was speeding. We'll focus on those variables for this app.The data was in the SAS format which I read in to R using the sas7bdat package. The data after 2012 contained two extra columns which needed to be dropped. I cycled through each year of data and appended them together into one large dataset.From this main dataset I created separate datasets for fatalities related to distracted driving drunk driving and speeding. The key variables were also broken out using different numeric systems so I normalized them to be either zeros or ones.The NHSTA dataset didn't contain any population data so that needed to be merged in. I used the state level population data from the Uniform Crime Reports that I had previously downloaded and calculated the rate of driving fatalities in each state.
I used tabs to organize the graphs and used the shiny theme 'spacelab'. Here is one example of a panel you can view the rest on my Github .The outputed data was based on the selector in the UI which I adjusted using a series of logicals. I then used Plotly to create the graphs.Wyoming is the clear ""leader"" in fatality rate with over 30 people per 100000 dying in a car accident. Montana and Mississippi have also seen very high rates of driving fatalities. It seems that states with large urban populations tend to have lower rates of drunk driving. While it's not immediately evident why this trend has developed I would guess it mainly has to do with the fact that people tend to drive at lower speeds in large cities (compared with states that are very highway heavy). It could also be linked to the better sources of public transportation in cities.In recent times driving fatalities in North Dakota have increased dramatically likely as a result of the shall gas boom. States dominated by cities continue to be lower than other states. These are only a few of the many insights that can be gleaned from this map and I encourage you to play around with different combinations of filters and year ranges.Deaths from traffic accidents are clearly a tremendous issue facing the country. There is some hope on the horizon though as selfdriving cars have the potential to be a huge step forward in terms of safety. The technology is still several years off though and have countless regulatory hurdles to pass. It's still unclear how politicians will treat this new technology however it seems unlikely that a majority of Congress will act quickly to help the technology develop. This will be especially true if we don't start treating this problem with the concern that it deserves. ",NA
Visualizing Global Terrorism Trends,56,https://nycdatascience.com/blog/student-works/visualizing-global-terrorism-trends/,It wasn’t the sound a firecracker would make. However this is an artifact (and a limitation) of the data.  What is captured here is a trend in improved data collection in this increasingly digitized age.  ,NA,"I want to start with a short story. Back on March 12th 1993 I was sitting in class and I heard a loud explosion. This was a very visceral feeling. I felt it in my gut. It turns out that a bomb had gone off a couple of miles down the street at the Bombay Stock Exchange. This was one of 13 bombs that had gone off simultaneously in Bombay my hometown. This wasn't the only time we've had to experience such a thing.So the topic of global terrorism trends hits very close to home. As such I wanted to do something to visualize publicly available data on global terrorism with the main purpose of helping people be more informed about how terrorism trends are changing on:
A. Attacks that are internationally motivated
B. The targets of all attacks
C. The number of attacks over the past 45 years in the U.S. as well as in India.I used the Global Terrorism Database which is managed by an organization called START headquartered out of the University of Maryland. The data as of July 21st contained close to 157000 records. START has done a great job building such a compendium of information that is available in the public domain and their efforts should be commended.The data spans from 1970 to 2015  and as can be seen from the below plot skews very greatly in terms of volume in recent years.One question I wanted to answer was what proportion of attacks have been internationally motivated? There are two main variables that represented this information:
Logistically international attacks  which looks at cases where the perpetrators' nationality differed from the location of the attack and
Ideologically international attacks  which looks at cases where the perpetrators' nationality differed from the victims' nationality. There is likely a fair amount of overlap between these variables but as can be seen from the chart below (which shows the proportion of attacks that were logistically or ideologically international across the globe) that such trends are reducing in proportion over time.A second interesting trend is one involving targets of attacks. As can be seen in the chart below businesses seemed to be a more frequent target when compared to regular citizens back in the 70s and 80s. The ""citizens"" category refers to people being targeted but not for any affiliations they may have with let's say government military police business or any other organization or group. As can be seen in the chart below regular citizens are becoming a target for a higher proportion of events as the years go by.The bar charts below shows this same increasing trend in regular citizens being targeted by comparing this category to all other target types between 1970 and 2000. In the 70's and 80's the proportion of cases of citizens being targeted in relation to other targets was fairly low but the change in trend between the two years shown in the charts below is very apparent. This may be one factor that additionally contributes to the perception of a lack of safety around the world. Anyone can be targeted.The last trend I'd like to focus on is the location of attacks. How are terrorist events spread across countries and how are these changing over time? The chart below visualizes the proportion of attacks by country for any specific period of time. In the tree map below we're looking at all years since 1970 and it presents an expected picture of asian and middle eastern countries experiencing more trouble than others.Let's look at specific years. 1970 shows a very surprising difference. One reason for the high proportion of terrorism in the U.S. in 1970 could be the lack of information from foreign sources during the 70s. But the two screenshots included below is definitely suggesting a reduction of terrorist activity in the U.S. and a relative increase in activity in Asia over the last 40 odd years.The maps below corroborate the finding about the reduction of terrorist activities in the states over the years. Despite an increasing number of sources of information (especially digitized information) the number of terrorist activities recorded is trending down in the U.S.In comparison the picture of terrorism events in India (2015) gives a glimpse into the alternate trend that is underway in Asia. While Northern India has always had trouble from separatist movements it is particularly surprising to see the heavy cluster of events on the east coast of India.To conclude I have focused on just three overall trends here but ones that are very relevant to the conversations we are having today about global safety.  There is a lot more great information in the Global Terrorism Database such as the differences by region in the proportion of attacks on abortion clinics the types of weapons being used the fact that explosives seems to be the biggest problem in terms of terrorist attacks around the world among others. My next step is to create a web app to help you visualize such trends yourself and uncover the story that you may find the most relevant. Stay tuned.",NA
Analysis of Hygiene of The Restaurant in Manhattan,57,https://nycdatascience.com/blog/data-science-news-and-sharing/restaurant-in-manhattan/,containing the results of restaurant inspections that are conducted once a year. The whole data set contains 450781 historical inspection records of 24765 restaurants. Every year each restaurant is assigned a score and grade according to that score which reflects the health and hygiene of that restaurant.Since 75% restaurants are graded AThe center of A restaurants is located nearby Times Square and extends to cover the upper east side of Manhattan and the Columbia University areaThe C group heatmap is greatly different from the previous two. The center is further south and there is a separate group that covers northwest Manhattan.From three heatmaps above we can draw two conclusions:  that we’d better go midtown to north if we are seeking a great restaurant even though that area is basically full of restaurants from every group; and people in Harlem are suffering from food sanitary problems.Indeed while the distribution matters so do changes in grades. The scatterplot below was generated based on a comparison between “overall grade” and latest grade from each restaurant. This captures the trend of how restaurants’ sanitation condition changes over time. Overall grade was then compared to most recent grade.The red dots indicate the restaurant grade is getting worse while theAs the boss of a restaurant you should think twice before every business decision. With the result of this analysis the runner of a restaurant can also take the grade into consideration. For example if the owner is running a restaurant in Harlem the grade would not trouble him or her due to the average poor sanitation condition.   The restaurant owner could save the money rather than trying to improve his grade.Want to have a healthy dinner? Look at the map before looking at the menu.,NA,The food quality issue is getting more and more important; can how we eat healthier in a second? We do it with data. With the data analysis and visualization we are more than ever capable of precisely finding the right way to a good restaurant.The dataset  released by New York City Department of Health and Mental HygieneThe following features of the original dataset were picked up to draw a picture of New York City’s restaurant sanitation and the two main tricks were played onto the dataset: the was converted into time format for getting the latest grade and the  and  were combined and then used to fetch geographical coordinates of each restaurant with an R package named 'Rgooglemaps'.Pretty good! This bar plot indicates that most of the restaurant are graded as A for all boroughs.  and it seems that we are quite safe when having dinner in New York City.History means a lot. We are definitely unwilling to have dinner at a restaurant that was once infamous. To fully analyze the overall sanitation of restaurants the historical records should be taken into consideration and here comes the result.By applying the grading rules of DOHMH the overall score was used to add a new variable called  into the original dataset supporting the analysis of grade change.The scatterplot shows the distribution of different groups of the restaurants. There are many blue and green dots which represent A and B restaurants and the green army occupies a huge area which means that most restaurants have an issue on food security in the past so there are so many restaurants graded as A but has a B overall grade.The following are three heatmaps that drilldown the analysis on a group level  based on their overall sanitary conditions..For B group the center obviously moves toward the southwest. There are two highdensity areas of B restaurants located on the upper east and Greenwich which means the upper east Manhattan has more B restaurant. green means improvement. We can see that most all restaurants remain the same grade over time but almost all restaurants whose grade changed are getting worse what a pity!,NA
Round 7 Of The Yelp Dataset Challenge,57,https://nycdatascience.com/blog/r/12868/," variety of servicesFor every year sinceSince 2013 YELP have been organizing the ""Yelp Dataset Challenge""Possible research directions such asThe data is available on 's website across descriptionincludes a concise look at the data’s contents.This maiden experience of working with json data in R was difficult. However with some research the data’s loading and transformation was eventually complete. I first explored the distribution of business ratings. MostA similar pattern occurs when the microscope is turned onhe average number of reviews for different ratingsOn an average businesses with ratings between 3.54.5 starshave more reviews.I suspect that this is due to a positive feedback loop. People will look for highly rated business on YELP to visit and then themselves leave high ratings themselves Going deeper in the data a cursory glance at the state level revealed that Arizona has theAt the citylevel took the crown for most businesses in Yelp. This is unsurprisingly given the city’s many casinos  hotels and restaurants.The Las Vegas effect is perhaps responsible for Nevada’s havingthe highest number of reviews even though Arizona has more Yelp business listings. Again Ithink this is a knockon effect of  its the service industry .When it comes to the businesses themselves the field is dominated by restaurantsThe distribution of the various restaurantsubcategories is more uniform.",NA,"In this blog i am documenting my explorations in the data from ""The Yelp Dataset Challenge"". In particular i am interested in finding peculiar patterns and some interesting insights the data could lend us. is a crowdsourced review platform for local businesses where people can rate and write a review about a. where YELP provide some of the data collected from it's website and also offer prizes for data science projects deemed good.   etc are listed on the challenge's website. five different  json files under business  users  checkin tips and reviews categories. Yelp'st. increasing number of reviews .Las Vegas This is some of my preliminary analysis of this data. There is definitely room for a deeper look at its intricacies which will reveal some of America’s attitudes towards various businesses. Here are some things that I learned and possible directions for future analysis:",NA
What Do Americans Do for Fun?,57,https://nycdatascience.com/blog/student-works/what-do-americans-do-for-fun/,Working in the marketing industry I've always had consistent interest in observing people’s lifestyle: How do they use media? How do they spend money? And of course how do they spend their time? With my deep curiosity about this particular topic and passion for data analysis I discovered ATUS the American Time Use Survey.  is sponsored by the Bureau of Labor Statistics and is conducted by the U.S. Census Bureau. The major purpose of it is to develop nationally representative estimates of how people spend their time. ATUS respondents are selected from the Current Population Survey(CPS) samples and they are interviewed only one time about how they spent their time on the previous day where they were and whom they were with.After data cleaning the raw dataset came down to 1118532 observations of 33 variables.Speaking of Time Use the first thing I looked at was the trend along atimeline.  The bar chart below shows the top 10 activities over a five year period from 20112015. I initially hypothesized that certain healthy activities like working out would show a gain in popularity year after year but unfortunately the answer was NO.In this chart I summarized and ranked the total duration time of each independent activity and divided it by the unique respondent number of that year. This gives us. We can see of course the number one activity was sleeping. One interesting finding is that on average At first glance I could not believe my eyes and went back to check the code.What I foundis that the survey blends weekdays and weekends and does notexclude the unemployed. But anyway this finding is valid enough to indicate the irreplaceable status TV and movies havein American culture and the impact on people’s life.That’s partly why we have the word “couch potato” which isn’t a favorable description of a person at all.  To discover more in this realm I decided to drill down into recreational activities specifically and find out what are the other things Americans do for fun.Using the same method I listed the top 10 recreational activities that Americans spend most time on. Note that a missing bar means that particular activity appears in the top 10 list at least once but is replaced by another activity in that year. The pattern stays consistent and . But I was even more surprised to see that no sports activities made it into the top ten as I tend to think of Americans as active sports lovers.  The data told me that it's just my stereotype and it seems most Americans prefer a sedentary lifestyle.Next I do acomparison between people with different backgrounds and locations: The first thing I need to do is to .I found that together the top 10 recreational activities possessed nearly all of the time people spend on leisure so it is fair enough to look at only the top 10s since they are representative enough.I divided them into 2 categories: Beneficial & Non Beneficial.are:  are: This is for sure a disputable classification but there is barely any standard criteria for this so I made the classifications based on my judgement.  Applying the definition above I calculated a new variable which is:I call it  and .The graph above clearly shows the relationship between Region Family Income and Beneficial Fun Time Ratio.  There are several interesting conclusions we can draw:On the map above The  is very interesting and sort of . I was supposing with sunshine and ocean Californians and Floridians would have a very high ratio.  On Youtube videos is looks like they surf and bike all day long.  I even went back to check my code and data manipulation process but unfortunately in the world of data visualization counterintuitive things happen all the time. The only reason I can think of is that maybe the geeks in California spend too much time playing games.I also want to examine the possible relationship between lifestyle and education level. Thus I plotted the graph below and I also added a new continuous variable personal weekly income as another factor.The first thing everybody will notice is that people who did not finish college have obviously less income than those who did.  But that's not the purpose of this graph.  but that never happens. It seems that some doctors can possibly be couch potatoes and some people with lower degrees can maintain a healthy lifestyle too! So my conclusion here is Given the dataset I can also explore the relationship between different job titles industries and the Beneficial Fun Time Rate. I also need to drill down to each state's micro data to explain the map I plotted previously. In all this is such a fun data exploration journey and if you read till here I would like to thank you for your patience and you've probably been sitting for too long now you may need to get up and go for a walk!,NA,(the code of this project can be found on Github: )The original ATUS data comes with 6 linked files:However it is simply not enough to look at only the region what about states? Can I rank the states by Beneficial Fun Time Ratio?,NA
Impact of HbA1c test on diabetes treatment from 1999 to 2008,58,https://nycdatascience.com/blog/student-works/impact-hba1c-test-diabetes-treatment-1999-2008-2/,Diabetes mellitus is a chronic disorder associated with disturbances in carbohydrate fat and protein metabolism and characterized by hyperglycemia. It is one of the most prevalent diseases affecting approximately 24 million individuals in the United States.The motivation of this study is to use data science to help focus treatment development on more effective forms of treatment. In this work I use the Health Facts database (Cerner Corporation Kansas City MO) a national data warehouse that collects comprehensive clinical records across hospitals throughout the United States. [1]  The data is an extract representing 10 years (1999–2008) of clinical care at 130 hospitals and integrated delivery networks throughout the United States. The final data includes 69051 patients’ distinct visits with 50 variables.  The plot below shows that patient numbers dramatically increase with age and reaches a peak in the range of 8090 years old. Within the diabetic population the highest count is Caucasian then African American Hispanic and Asian respectively. Because the data is collected in the USA the result strongly depends on the specific demographics of this country.  In our dataset each observation corresponds to a unique patient diagnosed with diabetes. However not every patient had diabetes as a primary diagnosis.  In many cases other diseases were more dominant in terms of symptoms and so even people with diabetes might have a different primary diagnosis.  The plot shows the breakdown by primary diagnosis of patients who also have diabetes.  Circulatory disease ranks first  followed by respiratory disease neoplasms and digestive conditions. Diabetes as a primary diagnosis ranks 5th.  Since different diseases receive different medications I categorized the data by primary diagnosis.A subset of the original 69051 diabetics was chosen for further study.  The selection was based on including only those patients whose initial diagnosis was performed using the HBA1c test as this is test is unique in being able to identify how well the disease is being controlled.  12000 subjects all diagnosed with HBA1c remained in this study for further investigation.These 12000 subjects were further broken out into three groups: those whose diabetes is  under control (“normal”) and two groups whose diabetes is not under control.  In one instance (“high without change”) the doctor did not change medication despite the high occurrence of diabetic symptoms and in the third group (“high with change”) the doctor changed medication in response to the diabetic symptoms.The objective of this study is to understand the relation of HbA1c test result and the medication numbers.The distinct medication number with HbA1C test  is analyzed in box plot. With primary diagnosis of diabetes the median medication numbers are lower  than that of circulatory by HbA1c test result respectively. Among the top ranked primary diagnosis such as circulatory respiratory neoplasms and digestive patients with high HbA1C test result and medical changed received more distinct medications compared with those with normal test result. Oppositely those with high test result but no medical change received less distinct medications.  This information combined with readmitted ratio back to hospital proves the treatment results according to HbA1C test.4One of the features to demonstrate an effective treatment is the readmission rate of patients back to the hospital within 30 days. This bar plots show the relation of HbA1c test and readmitted ratio by primary diagnoses.Circulatory patients overall have higher readmission rates among all the patients. The diabetic patients with high test results show reduced readmitted rates only after being treated.  The respiratory injury and musculoskeletal patients with normal HbA1c test result show a relatively lower possibility of going back to the hospital compared those with high test result. We assume the patients with high test results are more serious than those with normal results. For the patients with primary diagnosis of diabetes it appears that treatment reduces the rate at which they return to the hospital.  It is the opposite for  circulatory respiratory digestive diabetes musculoskeletal and genitourinary diagnoses.  In these cases patients appear to do better if there is no change in their medication.  For neoplasms and injury patients with high test result the changed medication reflects a lower readmitted ratio than the previous medication. The readmitted ratio by medical specialty is plotted in the bar chart.  Patients with hematology/oncology oncology  and vascular surgery ranked among the highest readmitted rates.  This suggests that these diabetic patients are very likely to return to the hospital in short time. On the bottom of the chart the patients with obstetrics/gynecology and otolaryngology issues show very low readmission rates.This study looked at diabetic patients from 1999 to 2008. First we found that the diabetic population is increasing with age.  The rates vary for different ethnic populations from Caucasian (the highest) to  African American Hispanic and then Asian respectively. Second we found that many patients with diabetes have other primary diagnoses such as circulatory respiratory neoplasms digestive  injury musculoskeletal and genitourinary in sequence by population. Third compared with normal test results when HbA1c high test with medication change the number of medications increased. However for those high test result without medication change the number of medications is decreased. Fourth we looked at the HbA1c test impact readmitted ratio. We found strong evidence of when there is a need to change medication based on the diagnosis.  Fifth the diabetic patients  with different primary diagnoses show different readmission rates.I believe that for the data from 1999 to 2008 people realize how important of HbA1c test is for the diabetic patient’s treatment. Further data analysis for diabetic patients from 2008 to present with large amount of HbA1c test is ongoing.References:,NA,"Five factors are investigated for revealing the information of diabetic patients and the relation of HbA1c testing result and patients’ readmission ratio back to hospital.
356",NA
Higgs Boson Kaggle Case Studies,59,https://nycdatascience.com/blog/student-works/higgs-boson-kaggle-case-studies/,With the 42 variables we currently have we were able to reduce the dimension down to the 15 synthetic variables.Next step is to reconstruct the given variables in terms of newly created Principle Components and train logistic regression on the newly constructed variable dimensions.  Using 5 folds crossvalidation and repeated 5 times on the sample data.Note that the metric to optimize is accuracy in this model.  We didn't tune the model in terms of AMS. The AMS submission on the final test data is about 1.61.  Takeaways from this mode:Third model we trained was gradient boosting model with 500 tree  learning rate of 0.1 inter depth of 10.  We performed a 2 fold crossvalidation twice on the entire training data.  After doing some research we make a cutoff on the probability prediction and call the upper 14% of events as signal.ptimize this threshold to the AMS. used a testing grid. After running several tests around the best cutoff is the top 14% (0.86).Neural networks have been widely applied to different areas of industries including stock market prediction credit rating fraud detection property appraisal and medical diagnosis. For the Higgs boson data where the actual Higgs boson events are much rarer than the other background events. This shares the same character as the data shown in fraud detection where the multilayer neural networks were successfully applied. With the limited computational resources instead we only consider one layer of neurons and use neural number as the tuning variables to learn about our data. For the prediction accuracy we use 20 principle components from the scree plot of PCA analysis. We split our training data into 80 percent for machine learning and 20 percent for test set validation. We observe the single layer possess 80 percent training prediction precision almost comparable to the result from test set without overfitting. The prediction accuracies are almost unchanged when the number of neurons are above 4. For the AMS measure the test set prediction (around 1.2) is much worse than the training set result 2.3.To dramatically improving the learning an adaptive boosting may be applied to the single layer neural network to boost the weight of the importance for the rare events. The outcome can be beneficial when one face with limited computational resources.Why we care about Higgs bosons? Bosonic particles in nature such as photons and phonons are all massless. The interesting aspect of bosons are due to the exchange symmetry which leads to the enhancement of probability for observing another bosonic particle when a boson particle exists a priori. The Higgs bosons are special due to its bosonic nature with the mass originated from Higgs mechanism from the standard model. The Higgs mechanism provides an explanation on the origin for fundamental particles with mass.,NA,Contributed by by and . They are currently in theprogram taking place between January 11th to April 1st 2016. This post is based on their fourth class project  Machine learning(due on the 8th week of the ).The particle physicists explore the fundamental force and mass of the universe. They use particle accelerator to break the atoms of the energetic particles to detect subparticles (smaller particles than atom).  The ATLAS detector at CERN's  was built to search the mysterious Higgs boson responsible for generating the masses. The Higgs boson is named after particle physicist  who with other five physicists predicted the existence of such a particle in 1964.On 4 July 2012 the ATLAS and CMS team announced they had each observed a new particle in the mass region around 126 GeV. This particle is consistent with the Higgs Boson predicted by the Standard Model. The 2013 Noel in physics was awarded jointly to François Englert and Peter Higgs for their theoretical discovery of Higgs boson the origin of mass of subatomic particles.The production of a Higgs particle is an extremely rare event. A Higgs is produced every few trillion collisions. Therefore it takes years to find this particle. It is challenge for physicists to analyze such massive data with extremely weak signal.Machine learning is proposed for this competition to explore the potential improvement of the significance of the signal. This may potentially speedup the discovery of true signals and save enormous hours for particle physicists.Variables are indicated as “may be undefined” when it can happen that they are meaningless or cannot be computed. In this case their value is − 999.0 which is outside the normal range of all variables. Both background and true signal has large missing values. Seven features have missing up to 70%. Three features miss up to 40%.  We treat the missing values as NA in the tree based models.  We imputed the missing values for the dimension reduction models.The jets number (PRIjetnum) has only 3 values:  0123 and therefore can be separated into 4 categories.From physics point view the crosssection of Higgs particle generation strongly depends on the energy. This is true for other particles. Therefore it important to keep all the energy related features such as momentum. We add 16 momentum as new features in our model to leverage the sensitivity to the energy. The azimuth angles have less information. We keep them in our study.We utilized principle components to reduce dimensions so that we can obtain the most information out of the features as well as eliminate correlations between variables.  We used random forest as our second model.  In this attempt we try to fix the mistakes we made with previous model.   The threshold is the key in this dataset.  Because the dataset is not a perfect 5050 balanced of signal and background picking the threshold to assign signal and background is very important to tune for a higher AMS.  Below si the importance of features ranked by random forest model.After tuning with 500 trees and mtry of 7 and also tuning the threshold manually we were able to achieve a 2.68 AMS.  Note that because running time we only run the model on 10% of the training data.  In order to get a better result we should expand the training size and crossvalidation.Using GBM we were able to achieve 3. 51 ranked 732 on the Kaggle.  We could have done cross validation in more fold to and finer grid to achieve a better score.The XGBoost is type of adaptive boost the training error of the last step is used to train the next tree. This mechanism make it very powerful to find the best solution. CrossValidation is used to tuning the parameters.   The optimized parameters are: the maximum tree depth is 4; the number of iterations is 170; the learning rate is 0.1. The auc score is 0.94 and AMS score is 3.60 with Kaggle rank 584.  The best score is 3.66 from the crossvalidation.Our attempt to predict signal using the given features and various model gave us a chance to examine the pros and cons of different models.  We wish to do stacking and ensemble as the last step to combine couple model's results.,NA
Why did she get an A while I got a D?,59,https://nycdatascience.com/blog/student-works/get-got-d/,Education is a key factor for achieving longterm economic growth. Determinants of students’ performance have been the subject of ongoing debate among educators academics and policy makers.This study focuses on secondary education in Portugal.  During the last decades the Portuguese education level has improved. In the secondary schools the core classes of Mathematics and Portuguese (the native language) is the most important since they provide fundamental knowledge for the success in the remaining school subjects (e.g. physics or history). The data of student performance in Mathematics and Portuguese holds valuable information and can be used to improve decision making by parents and schools and to optimize student success. Modeling student performance is an important tool for both educators parents and students.  It can help us better understand this phenomenon and ultimately improve it.This data set provides information about student achievement in two Portuguese secondary schools. The data attributes include student grades demographic social and school related features.  It was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). The target is to investigate the contributing factors associated with G3 (the final year grade).The raw data contains 382 observations and 53 variables. The target variables are G3.x (the final year grade of Math) and G3.y (the final year grade in Portuguese). The contributing factors will be presented in 3 categories: schoolrelated (i.e. school extra education support) studentrelated (i.e. past course performance age study time desire to pursue higher education) and familyrelated (i.e. parents' status quality of family relationship parents' education and job). I analyzed most variables and listed the top contributing factors in the following illustration.The boxplots of final year grade distribution show the difference in student performance by school. In this graph we can see that for the GP school the median final year grade for Math is C and the median final year grade for Portuguese is B while for the GP school the median final year grade for Math is D and the median final year grade for Portuguese is C. We can conclude that the GP school has better student performance. In the following analysis I will separate the plots based on school.The next question we want to assess is whether G1 and G2 will significantly influence G3. Let's take math performance for example.There are a couple interesting facts that show up in these graphs. First we notice the data trend can be categorized into two relationships: the cluster with 0 grade (students who dropped a course) and a strong correlation between G3 and  G2 G1 (students who did not drop a course ). So the analysis is divided to two parts based on the trend.The figure shows a linear relationship between the current grade and the past grade which means the better you did in the first and second grade the higher final year grade you would get.Upon further inspection of the data it becomes obvious that the group with 0 grade most likely belongs to students who dropped the course. There are a couple of interesting facts that show up on the previous graph. First it has G1 and/or G2 grades but final grades of 0. Second there are no G1s of 0 but there are G2s with 0 value.The graph shows the fact that past class failure plays a role in current student performance and we can summarize that successful students tend to have a history of success.From this graph we can conclude that the age of the students also plays a factor in the final year grade. The older the student is the lower the final year grade he is likely to achieve.In terms of study motivation the student with a desire to pursue higher education has a higher probability of achieving success.The graph shows an association between study time and the final year grade; the  successful students tend to spend more time on coursework.It is hard to conclude a relationship between number of school absences and final year grade. To get a better understanding of the plot I grouped the number of school absences into 4 categories: 09 1019 2029 30+.The boxplots at the left of the graph indicate that students with working mothers tend to have better course performance than those with homestaying mothers. Also upon further investigation of mother job types the boxplots at the right of the graph  demonstrate that students whose mothers have a higher education level are most likely to achieve success in their courses. Furthermore plotting mother's job with education allows us to understand how the job distribution varies among education levels. Here we see the working mother has a greater portion of higher education level and especially the mother who works as a teacher has the most advanced  education degree on average. In conclusion the student who has a working and welleducated mother tends to be more successful.From the graph we can see that the first consideration is the quality of school course and the second is whether the school is close to home.I have addressed the data visualization of secondary student grades of two core classes (Mathematics and Portuguese) by using past school grades (first and second periods) demographic and school related data. In conclusion the student achievement is highly affected by previous performances. Also there are other relevant factors that contribute tostudent performance such as: school related demographic (e.g. student’s age study time desire to pursue higher education parent’s job and education). The conclusion is summarized:,NA,"Contributed by Shuo Zhang. She is currently in the NYC Data Science Academy 12 week full time Data Science Bootcamp program taking place between  July 5th  to September 23rd 2016 . Please refer to the following link for R codes:
The graph shows that the students who dropped G3 failed both at G1 and G2. The further investigation of the data displays that 13 students dropped G2 39 students dropped G3 and all of students dropped G3 also dropped G2.The new graph presents that successful students tend to have less school absences.Let's take mother's education and job for example.If more data can be provided about the student performance from more schools from the same community and more subjects (i.e. history)  the analysis can be more accurate. ",NA
Higgs Boson Kaggle Case Study,60,https://nycdatascience.com/blog/student-works/11652/,With the 42 variables we currently have we were able to reduce the dimension down to the 15 synthetic variables.Next step is to reconstruct the given variables in terms of newly created Principle Components and train logistic regression on the newly constructed variable dimensions.  Using 5 folds crossvalidation and repeated 5 times on the sample data.Note that the metric to optimize is accuracy in this model.  We didn't tune the model in terms of AMS. The AMS submission on the final test data is about 1.61.  Takeaways from this mode:Third model we trained was gradient boosting model with 500 tree  learning rate of 0.1 inter depth of 10.  We performed a 2 fold crossvalidation twice on the entire training data.  After doing some research we make a cutoff on the probability prediction and call the upper 14% of events as signal.ptimize this threshold to the AMS. used a testing grid. After running several tests around the best cutoff is the top 14% (0.86).Neural networks have been widely applied to different areas of industries including stock market prediction credit rating fraud detection property appraisal and medical diagnosis. For the Higgs boson data where the actual Higgs boson events are much rarer than the other background events. This shares the same character as the data shown in fraud detection where the multilayer neural networks were successfully applied. With the limited computational resources instead we only consider one layer of neurons and use neural number as the tuning variables to learn about our data. For the prediction accuracy we use 20 principle components from the scree plot of PCA analysis. We split our training data into 80 percent for machine learning and 20 percent for test set validation. We observe the single layer possess 80 percent training prediction precision almost comparable to the result from test set without overfitting. The prediction accuracies are almost unchanged when the number of neurons are above 4. For the AMS measure the test set prediction (around 1.2) is much worse than the training set result 2.3.To dramatically improving the learning an adaptive boosting may be applied to the single layer neural network to boost the weight of the importance for the rare events. The outcome can be beneficial when one face with limited computational resources.Why we care about Higgs bosons? Bosonic particles in nature such as photons and phonons are all massless. The interesting aspect of bosons are due to the exchange symmetry which leads to the enhancement of probability for observing another bosonic particle when a boson particle exists a priori. The Higgs bosons are special due to its bosonic nature with the mass originated from Higgs mechanism from the standard model. The Higgs mechanism provides an explanation on the origin for fundamental particles with mass.,NA,Contributed by by and . They are currently in theprogram taking place between April 11th to July 1st 2016. This post is based on their fourth class project  Machine learning(due on the 8th week of the ).The particle physicists explore the fundamental force and mass of the universe. They use particle accelerator to break the atoms of the energetic particles to detect subparticles (smaller particles than atom).  The ATLAS detector at CERN's  was built to search the mysterious Higgs boson responsible for generating the masses. The Higgs boson is named after particle physicist  who with other five physicists predicted the existence of such a particle in 1964.On 4 July 2012 the ATLAS and CMS team announced they had each observed a new particle in the mass region around 126 GeV. This particle is consistent with the Higgs Boson predicted by the Standard Model. The 2013 Noel in physics was awarded jointly to François Englert and Peter Higgs for their theoretical discovery of Higgs boson the origin of mass of subatomic particles.The production of a Higgs particle is an extremely rare event. A Higgs is produced every few trillion collisions. Therefore it takes years to find this particle. It is challenge for physicists to analyze such massive data with extremely weak signal.Machine learning is proposed for this competition to explore the potential improvement of the significance of the signal. This may potentially speedup the discovery of true signals and save enormous hours for particle physicists.Variables are indicated as “may be undefined” when it can happen that they are meaningless or cannot be computed. In this case their value is − 999.0 which is outside the normal range of all variables. Both background and true signal has large missing values. Seven features have missing up to 70%. Three features miss up to 40%.  We treat the missing values as NA in the tree based models.  We imputed the missing values for the dimension reduction models.The jets number (PRIjetnum) has only 3 values:  0123 and therefore can be separated into 4 categories.From physics point view the crosssection of Higgs particle generation strongly depends on the energy. This is true for other particles. Therefore it important to keep all the energy related features such as momentum. We add 16 momentum as new features in our model to leverage the sensitivity to the energy. The azimuth angles have less information. We keep them in our study.We utilized principle components to reduce dimensions so that we can obtain the most information out of the features as well as eliminate correlations between variables.  We used random forest as our second model.  In this attempt we try to fix the mistakes we made with previous model.   The threshold is the key in this dataset.  Because the dataset is not a perfect 5050 balanced of signal and background picking the threshold to assign signal and background is very important to tune for a higher AMS.  Below si the importance of features ranked by random forest model.After tuning with 500 trees and mtry of 7 and also tuning the threshold manually we were able to achieve a 2.68 AMS.  Note that because running time we only run the model on 10% of the training data.  In order to get a better result we should expand the training size and crossvalidation.Using GBM we were able to achieve 3. 51 ranked 732 on the Kaggle.  We could have done cross validation in more fold to and finer grid to achieve a better score.The XGBoost is type of adaptive boost the training error of the last step is used to train the next tree. This mechanism make it very powerful to find the best solution. CrossValidation is used to tuning the parameters.   The optimized parameters are: the maximum tree depth is 4; the number of iterations is 170; the learning rate is 0.1. The auc score is 0.94 and AMS score is 3.60 with Kaggle rank 584.  The best score is 3.66 from the crossvalidation.Our attempt to predict signal using the given features and various model gave us a chance to examine the pros and cons of different models.  We wish to do stacking and ensemble as the last step to combine couple model's results.,NA
Higgs Boson Machine Learning Challenge,60,https://nycdatascience.com/blog/student-works/higgs-boson-machine-learning-challenge-3/,The DataMissingnessElasticnet RegressionMachine Learning AlgorithmFull Model vs Reduced ModelPrediction ComparisonConclusionImprovements,NA,Contributed by by  and . They are currently in theprogram taking place between January 11th to April 1st 2016. This post is based on their fourth class project  Machine learning(due on the 8th week of the ).The training data for this project consisted of a single data frame with 33 columns and 250000 rows. 30 of the columns were independent variables with numerical values: 29 floating points 1 integer. One of the columns EventId was an integer observation index not to be used for prediction. Another column Weight was a floating point artifact from simulation the data originated from. Finally the single dependent variable Label was a twolevel factor 'b' and 's' for background and signal. For a more complete description of the data the physics motivation for the problem and the accuracy metric used please visit https://www.kaggle.com/c/higgsboson. The test data consisted of same variables as the training data except for Weight and Label and had 550000 rows. Our objective was to predict whether each row of the test data corresponded to a 'b' (background) or 's' (signal).Before deciding on the type of predictive model to apply to the decay data we examined the data's missingness. We found six missingness combinations in the 33 columns and 250000 rows. We found exactly the same six missingness combinations in the test data. This indicated to us that the missingness was not due to random instrumentation but rather systematic. To understand why the missingness might follow the pattern we observed we studied the particle physics background. We found two sources of missingness that explained all the missing data: topology and number of jets.A topology of a particular particle decay is the collection of decay channels that result in particular endstage particles that are pickedup by the detector. For example the topology of the Higgs boson decay signal that corresponds to the 's' Label in our data set corresponded to: one Higgs boson decaying into two tau particles; one tau decaying into a lepton and two neutrinos; and the other tau decaying into a hadronic tau and one neutrino.For certain topologies the phycisists could generate estimates for the mass of the candidate Higgs boson.For observations where a topology was recognized a floating point value for DERmassMMC was present. When the topology was unexpected the value for DERmassMMC was missing.Jets are pseudoparticles that result from top quark decay one of the three expected background decay topologies. 12 of the 30 columns of independent data were either directed measurements of jet quantities or quantities derived from those measurements. As a result when for instance no jets were observed (not all decay topologies involve jets) the data in the jetrelated columns were missing.The six categories of missingness therefore corresponded to 1) either a familiar or unfamiliar topology; 2) 0 1 or 2+ observed jets. 2*3  6: the number of missingness categories in the data. Once we understood that the missingness in the data corresponded to different physical paradigms we decided to model each of the six missingnesscategories as it's own data set. We analyzed each category in isolation from the others both in our elasticnet regression and in our SVC construction.To get some insight into which of the 30 independent variables might be particularly influential for this problem we used elastic net regression. Elasticnet regression is a weighted combination of ridge and lasso regression designed to minimize the sum of the regressions residual sum of squares (RSS) and a measure of the model complexity. The idea is that by minimizing this quantity we can obtain the simplest model necessary to explain the data well enough. The way this method simplifies the model is by shrinking coefficients of variables that do not contribute to effectively predicting the dependent variable in a regression paradigm. We tuned a unique elasticnet regression to each of the six data categories. In each regression values for alpha (controls the relative weights of the ridge and lasso model complexity penalties) and lambda (controls influence of model complexity penalty vs. RSS) are selected. After we tuned regressions for each of the six categories we examined the variables that still had large coefficients. Before the individual elasticnet regressions each of the six categories had 1730 variables. After the regression each of the columns had 26 variables.Interestingly in all six regressions on different data subsets two variables appeared important: DERdeltarleptau and DERptratioleptau. DERdeltarleptau is a measure of the angular difference between the emitted lepton and tau particles. DERptratioleptau is a measure of the relative transverse momenta of the emitted lepton and tau particles. Since these two variables survived the elasticnet regression pruning process in all six cases it seemed to us like they may have relatively large predictive power. These results were only subjective however and would need to be confirmed by SVM modelling of the reduced variables.Our group decided to experiment with fitting a Support Vector Machine (SVM) to each of the six subsets of our data in order to classify both signal and background noise. SVMs are designed to cut a hyperplane between data points in high dimensional spaces for classification purposes. Considering the number of variables in each our our subsets we believed this was an especially appropriate algorithm for this data set.There were three attributes of our model which we had to consider: feature selection (addressed using our Elastic Net Regression) kernel selection and the parameters for our kernel (cost and lambda). For our feature selection we decided to tune an SVM to each of our subsets with only the variables that we found to be significant for the respective data frame (as per the Elastic Net regression). We concurrently tuned another model with the full data frame including all variables for each subset with the goal of deciphering whether the reduced data sets had comparable predictive power to the full data sets.Code for tuning our SVM model on our first data frame:Function to choose the right model to predict each instance of the test set:We tuned our models using 21% of our data and 2fold cross validation (that computation time to tune our model using 5 or 10fold cross validation would have prohibited us from finishing the tuning process in time to submit our predictions. We see in the chart below that using Elastic Net regression we were able to eliminate upwards of 80% of our data and maintain a high degree of accuracy in our training setWe first submitted our predictions using the models tuned on our reduced data set and received a Kaggle score of 1.218 (rank of 1633 at the time of submission). We then submitted our predictions using the model we fit to the full data set and scored just over 3.02 (a rank of approximately 1023 on the leaderboard).Variables that were eliminated by Elastic Net Regession were still useful in tuning our SVM model. Even though the coefficients of many of our variables had been pushed to zero they still provided meaningful the basis for Support Vectors to create additional hyperplanes with which our model was able to significantly improve the classification process (as evidenced by the improvement of our Kaggle score from 1.218 to 3.02). We had expected that the full model would perform moderately better than the fitted model but we were surprised by exactly how inferior the fitted model performed. It was evident that our SVM models performed best with no variable elimination no matter the significance of those variables in Elastic Net regression.The SVM was very expensive computationally (one model could take upwards of 3 hours to tune given 5fold cross validation) but seemed to do well as a predictor of classification (especially compared to other teams that used Random Forests).,NA
Predicting a New User's First Travel Destination on AirBnB (Capstone Project),60,https://nycdatascience.com/blog/student-works/predicting-new-users-first-travel-destination-airbnb-capstone-project/,Our ultimate project goal was to complete the task assigned by the  That task was to predict the country of a new user’s first destination out of 12 possible outcomes for the destination country: United States France Canada Great Britain Spain Italy Portugal Netherlands Germany Australia other or no destination found (NDF).  you can find a Shiny app that gives basic information on the possible destination countries. The code for this project can be found .The competition allowed up to 5 predictions for each new user in the test dataset. For example: United States France Australia Italy and Portugal. The metric used to grade these predictions was  (NDCG) which measures the performance of a recommendation system based on the relevance of the recommended entries. It varies from 0.0 to 1.0 with 1.0 representing the ideal ranking of the entities. This metric is commonly used in information retrieval and to evaluate the performance of web search engines.The Airbnb Kaggle dataset consisted of: With respect to our workflow we created the engineered features using Random Forests and Gradient Boosting Machines. We then decided to train and run predictive models using XGBoost and AdaBoost. We ran XGBoost models on the unstacked data. We ran XGBoost models and AdaBoost models on the stacked data.UnstackedIn both the unstacked and stacked models both missing age/gender are some of the most important features to prediction. This is consistent with our exploratory data analysis which appears to show associations between both (i) missing age/gender and a country of first destination and (ii) missing age/gender and NDF (the latter of which we assume contributed more to the models). Counts and sum_sec_elapsed were also found to be important and are from the users' sessions data.As a team we:Steps to improve our predictions:,NA,Contributed by . They recently graduated from theprogram that took place between April 11th to July 1st 2016. This post is based on their final class project  the Capstone Project due on the 12th week of the .Stacked,NA
Predicting demand from historical sales data- Grupo Bimbo Kaggle Competition,60,https://nycdatascience.com/blog/student-works/predicting-demand-historical-sales-data-grupo-bimbo-kaggle-competition/,regular gradient boosting. Xgboost can do cross validation regression classification and ranking. Since it can also give feature importance it is a very good model to use in competitions such as Kaggle.,NA,"For the capstone project we chose to work on Kaggle’s competition on Grupo Bimbo forecasting the demand for products from previous sales data. Before delving into the project explanation it will be good to give some brief information about the global baking industry.The global baking industry is a US$461 billion industry. The product shares in the industry can be seen in the charft below:

As can be seen from the chart the largest category with 53% is fresh and frozen bread and bread rolls. Although most of these products are consumed on a daily basis it is still a growing industry and it is forecast that the annual growth rate over the next five years will be 3%:The global baking industry is highly competitive with more than 277000 companies operating worldwide. There are various types of stores providing baked goods however small bakeries dominate the industry. Besides small bakeries there are also regional family owned bakeries as well as supermarkets and grocery stores with instore bakeries.Although it is a competitive industry there are several major players who compete with the highest market shares. The four major players are Grupo Bimbo Mondelez International Yamazaki Baking Company and the Kellogg Company. These four companies account for 10% of the market and Grupo Bimbo is the largest one of these with a 4% market share. The overall market share percentages can be seen from the graph below:Grupo Bimbo has over 1 million stores and 45000 routes in Mexico.  Its annual sales is US$14.1 billion and it operates in 22 countries. Since the global baking industry is still a growing market it is important to meet the demands of the customers.  Grupo Bimbo's current inventory calculations are performed by their direct delivery sales employees. They singlehandedly predict the forces of supply demand and hunger based on their personal experiences in each store. For the products which have a shelf life of one week the error margin is very small regarding inventory planning.The main goal of the project is to develop a model to accurately forecast inventory demand based on the historical sales data. Five datasets are provided by Kaggle:To understand the correlation plot below let us first go through what all the variables are that we were given in the train set. The first variable Semana is the week of the sales/demand. Agencia ID is the Sales Depot. Canal ID is the Channel. RutaSAK is the route. Cliente is the client. Producto is the product. Ventauni are the units sold. Venta are the sales in pesos. Devuni are the number of returns and the final variabledemandis what we will be predicting. So out of all the variables we are given only the last 5 are numeric while the rest are categorical. None of the numeric variables are in the test set of the data so the problem here is predicting the demand with only 6 categorical features. As we can see in the correlation plot below demand is correlated highly with sales. We would expect this as demand is equal to the sales minus the returns. Let us look further into how these numeric features are related to each other and their distributions.
Below are the scatter plots of all the numeric variables provided in the train data. We can see in these charts two things: first sales almost perfectly match demand and second where demand is high returns are low. We see a similar relationship with sales. Low sales mean high returns.
Below are the distributions of the numeric variables which explain why predicting demand will be challenging in this competition. First off let us look at the distribution of demand. The demand for each client product pair has a minimum of 0 and a maximum of 4777. What makes this so difficult to predict is that the majority of the demand lies between 0 and 6 which means the data has a lot of variance. The standard deviation of demand is 25 even though most of the data is between 0 and 6. As was explained earlier demand is equal to the sales minus the returns.  This complicates things because almost all of the returns are 0 but the maximum value is up to 500. Both of these factors makes the demand very hard to predict.The last bit of exploratory analysis that we did looked at the demand for products each week. We noticed there may be some monthly seasonality in the data. In other words we found that week 3 looks like week 7 and week 5 looks like week 9. Why this may be important is that the test data are for weeks 10 and 11. This means that week 6 would correspond most closely with one of the test weeks. As can be seen in the graphs below week 6 is the most different from all the other weeks. If this seasonality is correct then that particular week will be very important in predicting the test data.
The first model we tried was random forests. This model when we tried running it with all of the data kept running into memory issues making it so that the model could not run. After reducing the size of the train data we were using to just one week it was able to run but as soon as we tried to test any feature engineering the model would never stop running. This continued to happen even after reducing the number of trees and increasing the maximum node. Since the random forest would not run do to the size of the data and the limits of our computational power we decided to try other models.The next model we tried was Gradient Boosting or GBM. We ran the GBM model with a learning rate of 0.05 and 100 trees which took 6 hours in total to run. Doing this plus local cross validation we were able to get a rmsle score of 0.64. This however is a much worse score than the public script on Kaggle which takes the medians of demand in an intelligent way. With the bad score and the amount of time it would take to run and test features we decided to try other models.In the end all of us decided to use xgboost as our model. We did this so we could concentrate on feature engineering and could share both our train sets and have the same baseline for testing our features. We chose xgboost as it both ran quickly so we could test new features and had very good predictive power.So why xgboost? Xgboost is extreme gradient boosting. It is a more efficient version of GBM that has both linear model solver and tree learning algorithms. It is fast  as it has the capacity to do parallel computation on a single machine. This makes it 10 times faster than We crossvalidated our results in order to optimize our parameters and test features by using one week out cross fold validation. One of the most important and impactful features that we first implemented was creating a lag of 3 weeks. We did this by creating product client pairs for each week and then imputing the median of these product client pairs for each similar pairing in the following weeks. So for example given week 9 we took the median value of demand from the product client pairs from week 8 and merged them into week 9 creating a feature called Lag1. If the product client pair did not exist in the previous week it was impute with 0 to indicate that that client had not bought that product in the previous week. By doing this we reduced our dataset from ~74 million observations to ~40 million observations. Each week that we would have included before week 6 would contain a column of lag with a constant 0 value because we would not have had data for said week (Given week 5 we would not be able to have a lag of 3 weeks since we did not have data for week 2) so we removed them from the dataset.Due to the amount of time it would take to do a full 4 fold 1 week out cross validation (roughly 2 hours) we decided that it would be more efficient to simply replicate the competition environment via training on weeks 68 and testing on week 9. We implemented this validation strategy for the testing of all of our features. Additionally our 3week lag features provided the benchmark validation set score through which we determined the effectiveness of the new variables we introduced.

After attempting our numerous models we changed strategies and just tried engineering and testing features en masse. We got together and brainstormed about 30 different features to try divided up the work between the three of us and got to work. The features and their descriptions that I Kyle tried are as follows (With features providing an improvement in bold):The importance of many of the features that we implemented above is shown below through using xgboost’s feature importance plotting. As one can see the lag features have a large impacted on the predictions of the model. What’s a bit strange is how far down the list the product ID feature is. Also strange as mentioned above is how important the town and client count per town features are.
Below is our scatterplot matrix for our different added numeric features against each other and the Demand target variable. As one can see the lag features seem to be pretty linearly related to the Demand. Again we see some strangeness with the count variable. There appears to be some sort of pattern there that may make it easier for a decision tree to split on the different values of count.

We chose to implement relatively simple ensembling methods to create our current top submission. We turned down the learning rate on our XGBoost from 0.1 to 0.05 and used our validation strategy of training on weeks 68 and testing on week 9 to find the optimal number of rounds for this learning rate. We then used 1 week out bagging training 4 XGBoost models each time with a different subset of 3 of the 4 weeks of training data. We then took these predictions on weeks 10 and 11 of the test set and averaged them. This gave pretty good results producing an rmsle of 0.47393. The current top Kaggle script uses the smart merging of previous weeks medians and also does a great job of it producing an rmsle of 0.49834. This script is quite impressive as it runs in a very short amount of time and does not require the training of any models. We used up almost 2 days worth of submissions plotting the weighted average rmsle scores for 1 5 10 and 15 percent top Kaggle script with our corresponding bagged XGBoost predictions. We then fit a polynomial line to this curve and found the minima to be at around 30% Kaggle script 70% bagged XGBoost predictions. This gave us our current best rmsle score of 0.46293.
For our next steps we’d really like to try out more features. We still have plenty of ideas left so at this point it’s just implementing them and finding the ones that work. Some of our ideas currently in the testing pipeline are creating principal components from our numeric features and then using these as additional variables using monthly clustering.  As we saw in one of our previous graphs there may be some similarities between weeks 5 and 9 and so on. The strangest week distribution wise is week 6 which also give the closest cross validation score to week 10. Another feature to try is biweekly clustering meaning clustering weeks 6 8 and 10 together.As noted above our current implementation of weighted average is likely overfitting the leaderboard. Although this provides a large decrease in our score we need to find the most generalizable weighted averaging and will do so using cross validation. We would also like to do some more sophisticated ensembling once we are able to get some of our other models up and running. Lastly we would really like to get back to first place.",NA
Forecasting the Higgs Boson Signal,61,https://nycdatascience.com/blog/student-works/forecasting-the-higgs-boson-signal/,"The Higgs Boson is a type of unstable subatomic particle that breaks down very quickly.  Scientist studies the decay of the collision and works backward. To assist scientist in differentiating the background noise from the signal we offer some machine learning algorithms to better predict the Higgs Boson.Like all data science projects we began with some exploratory analysis of the variables. We first used a correlation plot to inspect the different relationships going on between variables. As indicated by the dark blue and dark red points there seems to be a high correlation among many of the variables. We noticed the PRI_jet variables for example have a lot of blue dots in relation to the variables DER_deltaeta_jet_jet DER_mass_jet_jet and DER_prodeta_jet_jet. This is likely since according to the documentation for the challenge  those DER variables are derived quantities computed from the PRI or primitive quantities measured from the particles.Next we wanted to zoom in on the correlation plot and see some scatterplots of select variables.Here we looked at scatterplots between PRI_jet_leading_pt with the three variables PRI_jet_leading_eta PRI_jet_leading_phi and PRI_jet_all_pt. The orange circles represent events that were classified as signal while the blue circles were those classified as background. We noticed some variables seemed to have a linear relationship such as PRI_jet_leading_pt and PRI_jet_all_pt while others did not have any obvious form.Looking at scatter plots amongst select DER variables we saw a linear relationship to still be present. Between DER_deltaeta_jet_jet and DER_prodeta_jet_jet for example there was a negative linear relationship.Finally we had a look at some scatter plots between a DER variable and a few PRI variables. We found it interesting to see the plot of a DER variable and its related PRI variable from which it was calculated from. For instance the variable DER_deltaeta_jet_jet and PRI_jet_subleading_eta had a somewhat vshape as it is derived from the absolute of the difference between that primitive variable and another. 
Classification tree models are great for descriptive purposes.  They produce relatively easy regions to trace the process of the model. Although they tend to have a lower prediction accuracy.
Our first basic tree without pruning produced an accuracy of 72% with 3
For a more robust model with greater prediction we attempted a Random Forest.  A Random Forest is the average of a collection of trees resulting in an ensemble with greater predictions. Although unlike a single tree you lose descriptive abilities.  We ran the model with all the variables on the righthand side is the variable importance plot returning an AMS score of 1.781. Impressive just changing the model type the AMS increased by over 0.5 points.We then decided to try a model with
only the top 14 variables along with two columns created to focus on the missing values for DER_mass_transverse_met_lep and DER_deltaeta_jet_jet.  We found these columns to individually reduced the errors at most and together worked great. Resulting in an AMS
score of 2.795 with an accuracy of 83.5Finally turning to a more predictivebased model we attempted using the support vector machine algorithm. Using the scatterplots earlier and seeing as how there was much overlap between signal and background we chose to use a radial kernel as the problem did not look linearly separable. The pros and cons of using this algorithm are as follows:ProsConsDue to the model being computationally costly with the size of the dataset we considered tweaking different parameters in order to get it running without consuming too much time. We chose to train it on 1% of the data and used 5fold crossvalidation to find the best estimates for cost and gamma. Next we used the results from the random forest model and reduced the data set to the 14 most significant variables. Finally with trial and error we found the best threshold for the model to be 0.7. Using all these led to an accuracy of 0.8056 and an AMS score of 2.72036. Considering the number of models we ran for the Higgs boson challenge we recommend the random forest model since it has the greatest balance of predictability and interpretability. The random forest model gave us the highest AMS score of 2.795 and was able to be trained on all the data. We noted that in terms of AMS score the support vector machine model was second however we choose to recommend the random forest model as the latter is less complex to train has less parameters to tweak and is not computationally inefficient as the former. In addition should the main purpose be for description we recommend the basic classification tree with some pruning.",NA,"Contributed by and . They are currently in the program taking place between April 11th to July 1st 2016. This post is based on their fourth class project  Machine Learning (due on the 8th week of the ).398 terminal nodes and an AMS score of 1.132.  The righthand chart displays the number of misclassified observations by terminal nodes suggesting a tree pruning of about five terminal nodes (where the errors remain flat as the terminal nodes increase).  The pruned tree with five terminal nodes returned a lower AMS score of 1.038.  After increasing the nodes to 10 the AMS score increased to 1.24.Taking a look at the tree we see the Derived Lep Eta Centrality variable at the very top indicating its importance in determining background noise or a signal of the Higgs Boson. If we were to get a new observation the model would work as follows beginning at the very top ask is data point is greater or less than 0.0005? If less go left if greater go right.  Moving on to the next terminal node and so on.  Very easy to interpret but if you are interested in higher predictability this may not be the best option.% selecting four features at a time at random creating 500 trees and taking the ave
rage.  This seems like a great model.When people think about reducing dimensions they may think about reducing the amount of information which would result in a less precise model. This is not always the case and it can sometimes improve models. Advantages of reducing dimensions include reducing the time and storage space required making it easier to visualize data and remove multicollinearity which would improve the model.Because there are columns composed of other columns in the dataset we decided to perform least absolute shrinkage and selection operator (LASSO) to remove possibly related predictors and focus on the ones that would give us a clearer model. After performing LASSO on the variables we have scaled down many of them down to 0 leaving 12 that we will use.By applying Gradient Boosting Model (GBM) with certain parameters we were able to get an AUC score of 0.8525 with a threshold of 0.002 yielding an AMS score of 2.28471.After focusing our model on the 12 variables returned by LASSO we attained a better score. We got an AUC score of 0.8525 with the same threshold of 0.002. This gave an AMS score of 2.30291 which was higher than the previous model run with all the variables. This shows us that we were able to attain a better predicting model by reducing the number of variables which probably removed related variables.Because the AMS score gained was not much higher we decided to move onto another machine learning method that may yield better results. ",NA
Higgs Boson Machine Learning Challenge,61,https://nycdatascience.com/blog/student-works/higgs-boson-machine-learning-challenge-2/,The DataMissingnessElasticnet RegressionMachine Learning AlgorithmFull Model vs Reduced ModelPrediction ComparisonConclusionImprovements,NA,Contributed by by  and . They are currently in theprogram taking place between January 11th to April 1st 2016. This post is based on their fourth class project  Machine learning(due on the 8th week of the ).The training data for this project consisted of a single data frame with 33 columns and 250000 rows. 30 of the columns were independent variables with numerical values: 29 floating points 1 integer. One of the columns EventId was an integer observation index not to be used for prediction. Another column Weight was a floating point artifact from simulation the data originated from. Finally the single dependent variable Label was a twolevel factor 'b' and 's' for background and signal. For a more complete description of the data the physics motivation for the problem and the accuracy metric used please visit https://www.kaggle.com/c/higgsboson. The test data consisted of same variables as the training data except for Weight and Label and had 550000 rows. Our objective was to predict whether each row of the test data corresponded to a 'b' (background) or 's' (signal).Before deciding on the type of predictive model to apply to the decay data we examined the data's missingness. We found six missingness combinations in the 33 columns and 250000 rows. We found exactly the same six missingness combinations in the test data. This indicated to us that the missingness was not due to random instrumentation but rather systematic. To understand why the missingness might follow the pattern we observed we studied the particle physics background. We found two sources of missingness that explained all the missing data: topology and number of jets.A topology of a particular particle decay is the collection of decay channels that result in particular endstage particles that are pickedup by the detector. For example the topology of the Higgs boson decay signal that corresponds to the 's' Label in our data set corresponded to: one Higgs boson decaying into two tau particles; one tau decaying into a lepton and two neutrinos; and the other tau decaying into a hadronic tau and one neutrino.For certain topologies the phycisists could generate estimates for the mass of the candidate Higgs boson.For observations where a topology was recognized a floating point value for DERmassMMC was present. When the topology was unexpected the value for DERmassMMC was missing.Jets are pseudoparticles that result from top quark decay one of the three expected background decay topologies. 12 of the 30 columns of independent data were either directed measurements of jet quantities or quantities derived from those measurements. As a result when for instance no jets were observed (not all decay topologies involve jets) the data in the jetrelated columns were missing.The six categories of missingness therefore corresponded to 1) either a familiar or unfamiliar topology; 2) 0 1 or 2+ observed jets. 2*3  6: the number of missingness categories in the data. Once we understood that the missingness in the data corresponded to different physical paradigms we decided to model each of the six missingnesscategories as it's own data set. We analyzed each category in isolation from the others both in our elasticnet regression and in our SVC construction.To get some insight into which of the 30 independent variables might be particularly influential for this problem we used elastic net regression. Elasticnet regression is a weighted combination of ridge and lasso regression designed to minimize the sum of the regressions residual sum of squares (RSS) and a measure of the model complexity. The idea is that by minimizing this quantity we can obtain the simplest model necessary to explain the data well enough. The way this method simplifies the model is by shrinking coefficients of variables that do not contribute to effectively predicting the dependent variable in a regression paradigm. We tuned a unique elasticnet regression to each of the six data categories. In each regression values for alpha (controls the relative weights of the ridge and lasso model complexity penalties) and lambda (controls influence of model complexity penalty vs. RSS) are selected. After we tuned regressions for each of the six categories we examined the variables that still had large coefficients. Before the individual elasticnet regressions each of the six categories had 1730 variables. After the regression each of the columns had 26 variables.Interestingly in all six regressions on different data subsets two variables appeared important: DERdeltarleptau and DERptratioleptau. DERdeltarleptau is a measure of the angular difference between the emitted lepton and tau particles. DERptratioleptau is a measure of the relative transverse momenta of the emitted lepton and tau particles. Since these two variables the elasticnet regression pruning process in all six cases it seemed to us like they may have relatively large predictive power. These results were only subjective however and would need to be confirmed by SVM modelling of the reduced variables.Our group decided to experiment with fitting a Support Vector Machine (SVM) to each of the six subsets of our data in order to classify both signal and background noise. SVMs are designed to cut a hyperplane between data points in high dimensional spaces for classification purposes. Considering the number of variables in each our our subsets we believed this was an especially appropriate algorithm for this data set.There were three attributes of our model which we had to consider: feature selection (addressed using our Elastic Net Regression) kernel selection and the parameters for our kernel (cost and lambda). For our feature selection we decided to tune an SVM to each of our subsets with only the variables that we found to be significant for the respective data frame (as per the Elastic Net regression). We concurrently tuned another model with the full data frame including all variables for each subset with the goal of deciphering whether the reduced data sets had comparable predictive power to the full data sets.Code for tuning our SVM model on our first data frame:Function to choose the right model to predict each instance of the test set:We tuned our models using 21% of our data and 2fold cross validation (that computation time to tune our model using 5 or 10fold cross validation would have prohibited us from finishing the tuning process in time to submit our predictions. We see in the chart below that using Elastic Net regression we were able to eliminate upwards of 80% of our data and maintain a high degree of accuracy in our training setWe first submitted our predictions using the models tuned on our reduced data set and received a Kaggle score of 1.218 (rank of 1633 at the time of submission). We then submitted our predictions using the model we fit to the full data set and scored just over 3.02 (a rank of approximately 1023 on the leaderboard).Variables that were eliminated by Elastic Net Regession were still useful in tuning our SVM model. Even though the coefficients of many of our variables had been pushed to zero they still provided meaningful the basis for Support Vectors to create additional hyperplanes with which our model was able to significantly improve the classification process (as evidenced by the improvement of our Kaggle score from 1.218 to 3.02). We had expected that the full model would perform moderately better than the fitted model but we were surprised by exactly how inferior the fitted model performed. It was evident that our SVM models performed best with no variable elimination no matter the significance of those variables in Elastic Net regression.The SVM was very expensive computationally (one model could take upwards of 3 hours to tune given 5fold cross validation) but seemed to do well as a predictor of classification (especially compared to other teams that used Random Forests).,NA
Sneak Peek of Panama Papers (Offshore Companies) Data Leak,62,https://nycdatascience.com/blog/student-works/sneak-peek-panama-papers-offshore-companies-data-leak/, company is not liable to taxation in its country of incorporation it will be taxed in the country where it carries on its business.  The two main purpose of having the offshore companies are first to have tax benefit and the other is to hide the final beneficial owners from this structure.  Again the purpose of this App is to unveil this offshore world.  There are many complicated structures within itself and governed by different jurisdiction laws. ,NA,Contributed by . She is currently in thefull time Data Science Bootcamp program taking place between April 11th to July 1st 2016. This post is based on her second class project  Shiny (due on the 4th week of the program)The Shiny App can be found by clicking: On May 9th 2016 the International Consortium of Investigative Journalists released a searchable database with information on more than 200000 offshore entities that are part of the Panama Papers investigation.  The data comes from the Panamanian law firm Mossack Fonseca one of the top players in the offshore world and includes information about companies trusts foundations and funds incorporated in 21 tax havens from Hong Kong to Nevada in the United States. It links to people in more than 200 countries and territories.The purpose of this App is to give the users the access to look at the geographic development of this secretive offshore world from 1975 to 2015.  By definition an offshore entity is that is incorporated in a lowtax jurisdiction.  TheLet's start with an example.  U.S. has been a significant player in the world of all the offshore companies.  That means these companies actually operate in the U.S. while having the jurisdiction somewhere off the shore.  First let's see where the money is from: the intermediaries (law firms banks and accounting firms) facilitating the set up of the entities.  The map below shows the participating middleman's country.  You can see most of the world countries actively participating. They move the money in or out of U.S. in this case.And where the money goes in the end?  the final jurisdiction is where the company is incorporated and need to fulfill the tax duties. As you can see on the graph it's very much concentrated in Central America and Europe and some countries in SouthEast Asia.The next tab is show the geographic distribution of the offshore entities.  It covers all the top countries with the most offshore entities.  The table below gives you the exact statistics by country.  As we can see Hong Kong and Switzerland are the traditional offshoreonshore countries that offer not only low corporate tax but safety and good reputation for the entities.The entities status shows the purpose of having these entities.  The majority of the entities are no longer active.  26% of the all entities are still available.  That means most of them are not set up for the operational purpose.  This could explain why people have the presumption that offshore entities are not a typical corporation.  There are multiple purpose of establishing offshore entities.,NA
New York City Street Trees in Shiny,63,https://nycdatascience.com/blog/student-works/new-york-city-street-trees-in-shiny/,     After deploying the Shiny app it became clear that missing values for species was due to the tree condition (#6 and 7) in which they were categorized as Stumps or Empty Tree Pits.  This makes sense as it is difficult to identify a tree if it is missing or just a stump. ,NA,Contributed by . She is currently in the full time Data Science Bootcamp program taking place between April 11th to July 1st 2016. This post is based on her second class project  Shiny (due on the 4th week of the program).For our second project building a Shiny app I chose to continue with the subject addressed in our first project because I felt there was more to be learned though an interface like Shiny. Also as an urban designer I am fascinated by ways in which coding and data analytics can inform the design process. For this project I will be expand beyond the scope of Manhattan to include all 5 boroughs.The dataset was downloaded from  and was collected as part of  a street tree census maintained by the .The firs census was collected in 1995 and has been conducted by trained volunteers every 10 years since its inception.Similar to the  project the data had to be cleaned and parsed. There were missing values for species community districts tree condition and geospatial information required for mapping. Observations with missing values in the species community district or tree condition fields were not dropped from the analysis but grouped and represented accordingly. Observations with missing latitude or longitude were removed only in the dataset used to create the map because Leaflet would not map any points after it encountered its first observation with missing geospatial data.When designing this app I imagined it being a tool for fellow design colleagues who rely on visualization of spatial relationships to inform design. In addition the capability of exploring the dataset itself was also important.  or Having the data accessible through an interactive interface like Shiny allowed for a more fluid analysis of the data.I could quickly and easily produce graphs visualizing different parameters and then compare them to the map for spatial analysis.Inspecting the graph of tree count per community district showed some CDs in the Bronx had little to no trees.  Toggling back to the map revealed that this made sense since those CDs with no street trees were predominantly parkland.During the Exploratory Visualization project it was unclear why there are so many trees with missing values for species.It was also helpful to have the ability to normalize the data at the check of a box.  In the example below we see the tree count per community district by tree condition and then the same data normalized.This was a fun project to design and produce.  Although there are aspects that need to be worked out it is very clear how powerful Shiny can be in communicating ideas through accessible data visualization. Mapping and visualization are integral in the urban design field yet a lot of designers lack the access to robust software like GIS.But with a little coding experience Shiny makes its possible to visualize the data which informs the design process.Aspects I'd like to work on:,NA
A Comparison of Supervised Learning Algorithm,66,https://nycdatascience.com/blog/student-works/a-comparison-of-supervised-learning-algorithm/,Which supervised learning algorithm is the best? For people who just start their machine learning journey this question always comes to their mind.,NA,"Contributed by . She is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on her own class note and previous machine learning research. She posted this research on the 8th week of the .To answer this questionwe used 4 different types of data sets (One for regression problem and the other 4 for binary classification problem) to test 6 supervised learning algorithms: SVMs (linear and kernel) neural networks logistic regression gradient boosting random forests decision trees bagged trees boosted trees linear ridge regression. And also applied model averaging to improve the models.For the parts of the methods Boston housing is trained twice once with original attributes and once with scaled data. Also to test the performance of logistic regression Boston housing has been converted to a binary problem by treating the value of “MEDV” larger than its median as positive and the rest as negative.Hypothyroid has lots of missing values and 18 of 34 predictors are binary. The information on missing values and how many unique values there are for each variable are shown as below (only shows the results of training data):[code lang""r""]
sapply(hypfunction(x) sum(is.na(x)))
V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26
0 277 44 0 0 0 0 0 0 0 0 0 0 0 0 278 0 429 0 152 0 151 0 150 0 1840
sapply(hyp function(x) length(unique(x)))
V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26
2 89 3 2 2 2 2 2 2 2 2 2 2 2 2 202 2 67 2 245 2 151 2 244 2 44
[/code]A visual take on the missing values in this data set might be helpful:We replaced the missing values with the mean in the continuous variables. Since there are few missing values in the binary variable “Sex” and replacing by the variable’s median or mean will cause bias we deleted these 73 records. (training:44 test:29)For the ionosphere dataset which contains 34 predictors we have eliminated the two first features because the first one had the same value in one of the classes and the second feature assumes the value equals to 0 for all observations.I tuned the parameter by using 70/30 splits for training data and compared different models’ crossvalidation. In addition this report also used the Caret package in R for tuning the gradient boosting and random forest model. This section summarizes the tuning procedures and the results of each variable. By interpreting these results I will choose the appropriate parameters for each algorithm.The tuning result shows that whether data is scaled doesn’t affect the gradient boosting model. When treedepth5 the cross validation is the smallest.By using caret package we confirm that the best combination is: shrinkage0.001 number of trees300 n.minobsinnode  10 and treedepth5. Part of the results are shown as below: [code language""r""]
gbmFit1
 Gradient Boosting

13 predictor
No preprocessing
Resampling: Repeated Train/Test Splits Estimated (25 reps 0.7%)
: 180 180 180 180 180 180 ...
Resampling results across tuning parameters:
shrinkage interaction.depth n.minobsinnode n.trees RMSE Rsquared RMSE SD Rsquared SD
0.001 1 10 15 8.337091357 0.6640256183 0.7941402762 0.09248626694
0.001 1 10 30 8.265578098 0.6722929163 0.7915189328 0.08870257422
0.001 1 10 45 8.195669943 0.6778771101 0.7895308048 0.08435275865
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees  300 interaction.depth  5 shrinkage  0.01 and n.minobsinnode  10.
[/code]The tuning result shows the crossvalidation and Rsquared value with different numbers of independent variables used to include in a tree construction. When mtry6 the cross validation error is the smallest.The results of caret package confirmed the choice.[sourcecode language""r""]
rfFit1
Random Forest253 samples
13 predictorNo preprocessing
Resampling: Repeated Train/Test Splits Estimated (25 reps 0.7%)
Summary of sample sizes: 180 180 180 180 180 180 ...
Resampling results across tuning parameters:mtry RMSE Rsquared RMSE SD Rsquared SD
2 3.290959961 0.8765870076 0.5079010701 0.01964500327
4 2.914160845 0.8946978414 0.3797723734 0.02067581818
6 2.826270709 0.8963652917 0.3505325664 0.02565374865
8 2.828542294 0.8932305921 0.3384923951 0.02829620363
10 2.856976760 0.8893013207 0.3504001443 0.02988188614
12 2.909261177 0.8842750778 0.3543389148 0.03184843971
14 2.964247468 0.8795746376 0.3713993793 0.03205890656
16 2.957650138 0.8801113129 0.3717238723 0.03240662814
18 2.955660322 0.8802977349 0.3745751188 0.03249307098
20 2.955327699 0.8802633254 0.3646805880 0.03206477136RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry  6.
[/sourcecode]In this section we used both original and scaled data to find the difference. The scaled data’s cross validation error is smaller. So we chose the neural networks with resilient back propagation and the number of neurons in the hidden layer for the training process should be 6.Since all the kernel methods are based on distance we need to scale the data set before applying to SVM model. The tuning result shows the crossvalidation error with different kernel methods. Since SVM will overfit training data easily we will select the linear SVM with the cost3.2.For linear ridge regression the result shows that we should choose lambda0.01. We can reduce this error by choosing the features so rerun the same tuning process by choosing features[ 6 11 8 10 13 ]For those 4 classification problems we also did the same processes as we mentioned above. Instead of going over the details we would only show the model selection result of the four classification problems as following:To improve the performances of linear ridge regression and logistic regression we used the R packages glmulti and MuMIn for model averaging. For each data set we chose the best 3 or 5 models to do model averaging to see if it will improve the performance of linear ridge regression (for regression) and logistic regression (for classification).We selected the top 5 ridge regression models based on their AIC scores:By averaging the top 5 models the results shows the variables {CRIM RM AGE DIS TAX PTRATIO B}are all significant in both fullaveraged model and conditional averaged model2.Classification ProblemFor the classification problem we choose 3 logistic model based on their AIC scores and do model averaging.  The results are shown as below:The tables and plots below show the estimate accuracy based on the holdout test data set. The best performing model for each data set is  while the worst one is .Below shows the MSE of test data set for Boston Housing data the Random Forest model has the lowest MSE 32.64723. The ridge regression performs the worst with MSE is 303.57214. But we can improve the ridge regression model by using model averaging (with MSE is 264.95846) and feature selection (with MSE is 55.51494).In this table we show the GINI of test data sets for classification problems random forests performs the best on both Wdbc and Hypothyroid data sets; while the best model on Ionosphere is SVM.Overall the gradient boosting has the best average performance. In addition the linear ridge regression has the poorest performance due to it's not quite suitable for classification problems. Also the logistic regression model has a very bad average performance but it performs very well on Hypothyroid. Just as in the No Free Lunch Theorem there is no best learning algorithm for all data sets.The averaging models of Wdbc and Hypothyroid have improved the performance over the original logistic regression while the model for Ionosphere didn't.For the 3 classification problems SVM random forests and gradient boosting have excellent performance on the area under the ROC. While Logistic regression and ridge regression performs the poorest. Comparing these plots Wdbc’s models tend to have larger areas under the ROC curve than those for Ionosphere. These can be also shown by the GINI of test data (in Table 5).With the excellent performance on all 4 data sets gradient boosting and random forests are the best algorithms overall.For regression problems the traditional linear ridge regression can be improved with model selection and model averaging. While using logistic regression to classify the response is not a good idea. For classification problems SVM gradient boosting and random forests perform very well. And the model averaging doesn’t improve all the logistic regression performance in each data set.Just as Rich Caruana and  Alexandru NiculescuMizilEven mentioned in their great :",NA
Space Oddities – Where do Satellites Come From?,67,https://nycdatascience.com/blog/student-works/visualizing-artificial-satellite-data/,For my data visualization project I looked at artificial satellite data from the . It concerns the 1305 operational satellites orbiting Earth as of the 31st of August 2015. While there have been fascinating visualizations of space debris done by Google Earth and others the UCS data contain satellite origin locations and ownership information useful for analyzing the overall industry.The data is collected from multiple sources. First the Convention on Registration of Objects Launched into Outer Space requires that countries report satellite information to the United Nations Office of Outer Space Affairs. Even so some countries neglect to report launches of covert satellites. However other countries act as watchdogs reporting when they detect a foreign satellite launch. Amateur astronomers also fulfill this watchdog role.Next I examined the launching state of these satellites. While 25% of satellite origin countries are labeled “NR” (Not Reported) many of these are new satellites that are not yet labeled. These values are therefore missing at random (MAR) with respect to time and do not severely undermine the analysis. Using  and  I grouped the most active satellite origin countries and plot them on the world map:Source: Source: The majority of geosynchronous orbiting satellites are ± 1 km of the equilibrium distance of 35786 km. Even though the UCS dataset claims not to give enough information to find a single satellite’s exact location it does contain the longitudes of geosynchronous satellites. Since geosynchronous satellites orbit above the equator at the same rate as the Earth’s rotation I was able to plot the locations of geosynchronous satellites based on their longitudes. Positive longitudes correspond to “degrees east” and negative longitudes correspond to “degrees west”.,NA,Contributed by Thomas Kolasa. He is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on his first class project  R visualization (due on the second week of the ).The launch site with the most satellites currently in orbit is the Baikonur Cosmodrome in Kazakhstan one of the main launch sites of the Russian space program. The second busiest launch site is in French Guiana where the European Space Agency performs most of its launches.And here are launch locations on the world map:There are currently 26 working satellites in orbit launched from Odyssey a mobile drilling rig turned launch pad.Four other sea launches took place. While the dataset lacked more details submarines are also capable of launching small satellites into orbit. Since a submarine can launch from nearly anywhere in the open ocean this is currently the most likely way that a completely anonymous satellite can enter orbit.One current satellite was launched from an L1011 Aircraft as shown here:24 satellites were launched from the International Space Station. While I originally thought this classification was due to human error small research satellites do launch from the ISS (shown below). The data show they are research satellites of approximately 4kg of mass each and are in polar low earth orbits.[youtube http://www.youtube.com/watch?vAdtiVFwlXdw&w560&h315]The dataset groups the uses of satellites into civil (academic or amateur) commercial government (meteorological scientific etc.) or military. I next looked at these purposes by the three largest satellite launching countries and others. I also looked at the “Not Reported” country of origin of satellites to identify any trends.The types of artificial satellite orbit are Low Earth Orbit (LEO) under 2000 km in altitude Medium Earth Orbit (MEO) between 2000 km and 35786 km in altitude Geosynchronous Orbit (GEO) at 35786 km in altitude and Elliptic Orbit which deviates from the previous nearcircular orbits. The majority of working satellites are in low Earth orbit but they are also the most likely to first burn up in the atmosphere. With the exception of the Apollo missions astronauts have only flown low Earth orbits.I estimated the distance of a satellite from the Earth to be the mean of its Perigee and Apogee. While excel has conditioned many of us (including me) to simply plot group means on bar charts for comparison the following plot presents more facets about the data. By presenting a scatter plot with jittering it visualizes each orbit type’s mean distance from Earth and its distance variance.My next exploration of the data tried out the JavaScript library D3.js. The following visualization once again shows satellite distance from Earth randomly jittered along the xaxis this time with detailed satellite information on hovering. Selecting specific countries allows the user to investigate specifics about a country’s satellites including a satellite name country of origin overall use and detailed purpose.Although I formatted the plot for an R Markdown WordPress does not support JavaScript. You can see the scatter plot  and the corresponding code below.See the R data cleaning and visualization code in its See the D3 code's,NA
Predicting Steps from Wearable activity tracking device,68,https://nycdatascience.com/blog/student-works/capstone/predicting-steps-wearable-activity-tracking-device/,Nowadays there are wide selections and different price options for wearable activity tracking devices or “wearables”. Most of these tracking devices harness a 3axis accelerometer to understand the user’s motions. By analyzing acceleration data the trackers provide information about frequency duration intensity and patterns of movement to determine a multitude of health metrics such as steps taken distance traveled calories burned and sleep quality. Another advantage of owning this device for data and fitness enthusiasts is the ability to log their food activities and weight over time with the possibility to set daily and weekly goals for themselves for steps calories burned and distance walked. One of the most popular wearable tracking devices is Fitbit. Like most wearable activity tracking devices  there is the option to synchronize your data to your user account via Fitbit Connect. In this work we consider Steps as our health and fitness goal that we wish to predict.Fitbit users can extract their data by logging into their Fitbit Connect account as shown in Fig.1 below where data for one year of activities and sleep could be extracted in CSV format.Fig. 1 : Data Extraction for Fitbit user’s webpageAfter gathering a one year activities and sleep data an exploratory data analysis is achieved using Python in order to determine the assess the relationship or determine the patterns between different type of measurements from May 8 2015 to May 7 2016. For each day the extracted data has the following features (columns of our datasets):First we take a look to the distribution of our raw data as shown in Fig.2 below.Based on the raw data from Fig.2 we can see the following:In this dataset we distinguish 3 types of health or fitness goals: Steps Calories burned Hours of Sleep and Sleep efficiency. To assess the sleep quality for a specific day we define Sleep efficiency as the percentage of Minutes of Sleep over Length Of Rest In Minutes.In the following section these goals are represented over days of the week months and grouped by work days (from Monday to Friday) and weekends (Saturday and Sunday).Most of the global recommendations on physical activity for health are targeted toward meeting certain daily goals. Fig.3 below shows the daily achieved goals.Although there are no global recommendations on physical activity for health it is interesting to observe the evolution of goals parameters by month. We can see that October was the least active month with few hours of sleep despite having the highest sleep efficiency. From Fig.4 we can also observe that the Fitbit user is more active during Spring and Summer months with higher hours of sleep but low sleep efficiency.A new feature is created to group days by workdays (from Monday to Friday) and Weekends (Saturday and Sunday). Fig.5 shows the achieved goals parameters. During the workdays the Fitbit users improve all the health goals comparing to weekends.In the following section the relationship between several features in the dataset is observed. Along with the feature that distinguishes between workdays and weekends other features are added to the datasets such as: Yesterday sleep hours and Yesterday's sleep efficiency in order to assess the effects of a previous night sleep on the goals. The following features: Days Months and Weekdays have been encoded.In this work we consider Steps as our health and fitness goal that we wish to predict.First we look at the trend shared by predictors i.e the features that will be used to predict Steps. We compute the correlation matrix shown in Fig.6.From Fig.6 we can observe some strong correlation between some sleep predictors.  Distance is strongly correlated to Floors and both are intercorrelated to Minutes very active.Fig.8 shows the relationship between Steps and other predictors. We can see that there is a strong linear relationship between Steps and the predictors. Distance according to Fitbit is calculated by multiplying your walking steps by your walking stride length.In order to predict the Steps we split our data into train (75% of the dataset) and test (25% of the dataset). The train data has the following predictors:We fit different machine learning algorithms on our train dataset and make corresponding predictions on our test dataset. First we assess variable importance using linear regression and test p values for each predictor. Results are shown in Table.IFrom Table II simple linear regression seems to be well suited for the nature of this prediction problem as it has a lower value of RMSE in this case we have an error of 3437.32 steps. No improvement is observed using regularization such as Lasso or Ridge.Considering the nature of our data we have some multicollinearity and also a strong linear relationship between some features and our response variable Steps. Multicollinearity is important in regression analysis as it may cause unstable estimated coefficients and a loss of the model interpretability as it depends on the data used to train our model. In order to have a constrained model we introduce regularization of the linear regression using Ridge and Lasso methods to predict the Steps based predominantly on the most important features. Regularization in this case shrinks the least important coefficients (Ridge) or can eliminate them completely (Lasso) in order to improve generalization of the model. For this model regularization doesn't improve interpretability or the predictive power of the model.As for the Random Forest Regressor it fails in predicting Steps especially for small values as observed in Fig.9. In the exploratory analysis the mean daily Steps is between 8000 and 12000. The poor results of Random Forest Regressor are due to the nature of the algorithm being a nonlinear algorithm. In our case our data is very linear and thus we need a lot of branches per tree to get a good approximation.In this work the relationship between the measured Fitbit parameters was explored. We choose to predict the number of steps based on multiple features. Despite an apparent multicollinearity simple linear regression seems to give better results predicting the number of steps.,NA,Fig. 2 : Raw DataFig. 3 : Daily goalsFig. 4 : Monthly goalsFig. 5 : Workdays and Weekend goalsFig. 6 : Correlation matrixFig. 8 : Relationship between Steps and predictorsTable I : Feature importance and pvalueIn order to measure models accuracy we compute the RMSE and R2 as shown in Table.II.Table II : Prediction Scores for different Machine LearningFig.9 shows the comparison between Predicted Steps and Steps from the test dataset.Fig. 9 : Comparison between models Predicted Steps and Test Steps ,NA
Using R Shiny App to visualize gender pay gap in the United States,68,https://nycdatascience.com/blog/student-works/using-r-shiny-app-to-visualize-gender-pay-gap-in-the-united-states/,Use the App below to explore changes of income and gender pay gap over the past 30 years and see how different factors influence the pay gap.,NA,Contributed by . She is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on her second project  R Shiny.(due on 2nd week)How to use the App:All data are collected from From exploring the data above here are some of my observations:Overall the good news is that gender pay gap has shrunk for the past few decades. However the decrease in the pay gap has stopped for at least 5 years. To date women are still making about 20% less then men. Gender pay gap is a continuous problem in the United States we should first seek an understanding of stereotypical gender roles we will then be at a better position to fix gender pay gap.,NA
Using web scraper and K-means to find the colors in movie posters,68,https://nycdatascience.com/blog/student-works/using-python-and-k-means-to-find-the-colors-in-movie-posters/,To answer this question we need analysis movie posters of different movies. First of all we need to build a training dataset of movie posters. So I used the  Image search engine for this. And the part of images I got looks like this:To extract the color of images we need to represent them in an easier and numerical way: Converting one image to a matrix of RGB pixels. For example for a 200x200 px image we convert it to a series of 40000 RGB pixels.To maintain the size of the dataset I resized the images down to 200x200.Then I used kmeans clustering to do image segmentation separate the pixels into different clusters based on their colors. Here is an example of image segmentation and compression from Bishop’s book I used python to do the kmeans analysis the code is shown as below:I tried k3 k5 and k10 since lots of the movie posters used black letters and frames k3 and k5 didn’t capture the main colors of posters. I chose k10 to do kmeans to all these 112 images. Finally I got 1120 colors of 112 movie postersPart of the result shown as blow:,NA,Contributed by Amy(Yujing) Ma. She is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on her third class project – Web scraping(due on the 6th week of the ).Each movie has their own posters. Even in today’s alwaysonline climate the movie poster remains a powerful form of advertising. Every movie poster has its own color scheme based on the movie’s type content and tone. Best movie posters should catch people’s eyes. So what kinds of colors are more likely to be used in different types of movies?To play with some plots in this post please go to the related post on .By using python to scrape all the movie posters from the website I finally got 112 photos for 4 types of movies (horror comedy animation and action movies).This result shows one of the weaknesses of kmeans clustering it’s sensitive to initialization which leads to unwanted solutions. In this poster Kmeans couldn’t capture any gold or orange colors.For each type of movies I plot its movie posters’ colors into a 3D scatterplot based on RGB values. Each point represents a poster color. By comparing these 4 plots I found most of the horror movies use darker and red colors in their own posters while the comedy and the animation movies would choose various colors to correspond to their themes.1120 colors contribute to many to discover the specific patterns so we need to reduce the number of colors and make them into several categories. For this we can transform the colors into  and use  and  package in python to calculate the visual difference between poster colors and some basic colors.Then I categorized these colors into 17 colors by choosing the smallest distance value in each row.This is a part of the dataset of comedy movie posters:After transforming the data I counted a number of each basic color within each movie type.,NA
Interactive Exploration of NBA Lineup Data,69,https://nycdatascience.com/blog/student-works/interactive-exploration-of-nba-lineup-data/,My  explored the relationship between the performance of NBA lineups (5man combinations of players) and the 5 players who made up those lineups. A full breakdown of how I put together this dataset can be found at the link above. However this initial investigation really created more questions than answers.,NA,"Contributed by . He is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on his second class project  R Shiny (due on the 4th week of the ).For my second project I created  that would allow a user to try to answer some of these questions themselves. With so many possible variables to explore the aim was to pass control to the user.The data set consists of both  and  pulled from . The  are the statistical output of specific 5man combinations. An example is . You can hover over the columns for definitions of the stats and click over to other tabs to see more of them and play with the various filters. An example  would be that the lineup of Bismack Biyombo Cory Joseph Kyle Lowry Patrick Patterson and Terrence Ross makes 11.6 3pointers per 100 plays. Their  is 11.6.For each lineup I pulled in the fullseason stats of the 5 individuals within that lineup and averaged them. For our example lineup Bismack Biyombo makes zero 3pointers per 100 plays (he has other skills) Cory Joseph makes 0.5 Kyle Lowry makes 3.4 Patrick Patterson makes 2.5 and Terrence Ross makes 3.3. Their  is 1.94.In basketball there's only 1 ball so for offensive stats it's usually more relevant to look at usageweighted values. We scale the offensive stats based upon how likely each player is to be the one using the possession. Biyombo uses only 11.7% of his team's possessions when on the floor Joseph uses 16.8% Lowry uses 25.8% Patterson uses 13.1% and Ross uses 17.0%. That only adds up to 84.5% of available possessions and this lineup doesn't simply suffer shotclock violations on the remaining 15.5% of their possessions. So we rescale their stats and their  is calculated as 2.19 ().On the first tab of  we can explore the relationships between the  and the  in either raw or usageweighted form. It's very important to choose appropriate settings (basketball knowledge required) or else the results will likely be meaningless or at least very difficult to interpret. For example usageweighting is not particularly applicable to defensive rebounding.The initial view displays  on the Y axis and  on the X axis. We see an increasing slope which makes sense (we would expect lineups of higher net rating players to be have stronger net ratings). There is some explanatory text which can be collapsed once you've read it. Below that to the right we see the size of our filtered data set and the results of a linear fit on this data. Please note that no numerical EDA has been done so the validity of the fit is questionable. This should be seen merely as a starting point for further analysis rather than a statement of statistical fact. To the left we can change our  on the X axis and our  on the Y axis. There is also a checkbox to switch between raw and usageweighted Player Stats. Below a slider allows us to filter our data set based upon the number of minutes played by a lineup.Let's explore the relationship between the 3 pointers made by a lineup and the 3 pointers made by the players in that lineup. We'll set the  to  and the  to  and click the  toggle to use usageweighted stats. Surprisingly there doesn't seem to be any relationship. Maybe it's just the lineups with very small numbers of minutes throwing our data off so we can shift the left side of the slider over to 100 and see that while this has significantly reduced our number of data points we still don't see a relationship.When we think about this this makes some sense. 3 point attempts (and hence makes) may be the result of ball movement and freeflowing offense rather than the particular proclivity for and talent at shooting of the individuals in that lineup. So let's explore a few more possibilities.Most 3 pointers seem to be assisted so let's change the  to . We now see a slight downward trend (which may or may not be significant... personally I hope it isn't). The data seems to suggest that there probably isn't a positive correlation. This seems quite surprising though because we might have expected that players who generate lots of assists would also generate lots of 3 pointers for their teammates. However our data doesn't seem to bear this out.Another hypothesis is that 3 point opportunities come as a result of the defense collapsing due to penetration. Let's see what happens when we set  to  (percentage of points in the paint). We finally see a possible positive relationship although more investigation is needed to determine if it stands up to statistical scrutiny. though this suggests that having players who score their points in the paint seems to lead to 3 point attempts (surprisingly thoughdoesn't seem to have this relationship... more investigation is definitely needed).You can use this tab to explore many similar questions.Our next tab is inspired by some of . The layout is similar to the previous tab but our plots are slightly different for investigating diminishing returns.Our initial plot looks at true shooting percentage(). On the Y axis we see the difference between the lineup's true shooting percentage and the ""null hypothesis"" true shooting percentage (what we would expect in the absence of diminishing returns). On the X axis we see the extent by which the players in that lineup have increased or reduced their usage. The increasing slope here suggests that as players are forced to decrease their usage their true shooting percentage increases.If we want to look at offensive rebounding() usageweighting no longer makes sense. Let's select  as the  uncheck the  checkbox and check the  checkbox. Our null hypothesis is now that a lineup's offensive rebounding percentage should be equal to the sum of the offensive rebounding percentages of its players. The Y axis now display the difference between reality and this expectation. The X axis now displays the null hypothesis offensive rebounding as well. What we see is that the more offensive rebounding we have in our lineup the greater the diminishing returns.The final tab allows for visual inspection of the distributions of our data. It's useful when you see a strange shape in the plots on the other two tabs and wonder where there's actually an interesting shape in the data or if you're just looking at random noise due to small numbers of data points.",NA
M3 - How to fund your startup,70,https://nycdatascience.com/blog/student-works/m3-how-to-fund-your-startup/,"| All well and good but what remains largely unaddressed is the question of funding. The general process is known  line up your investors hone your pitch and be prepared for rejection. But that first crucial step: how do you find wouldbe investors? There are resources available but it often still feels like a stab in the dark. Perhaps we can do better.This is M3  a tool meant to aid in exactly this process. What does it do?These are firms that have funded startups like yours in the past and will fund similar ones in the future. In short  we're a shortcut for finding the right venture capital firm. How exactly this fingerprint is generated and used will be discussed in more detail below.Lots of infrastructure. This was a challenging app for us to get up and running in just under two weeks  at times we were much closer to being fullstack engineers rather than data scientists!An aside: Shakespeare scholars often discuss issues of authorship and authenticity in Shakespeare's oeuvre. Example: the scene involving the Greek goddess Hecate in MacBeth is often attributed to fellow English playwright Thomas Middleton. There are historical arguments to be made about its role as song and dance interlude but there are also stylistic and syntactic arguments. It just doesn't read quite like the rest of Shakespeare.If you'll permit a slight stretch these scholars are doing what we're doing. Semantic fingerprinting is a technique that maps bodies of text into comparable analyzable ""fingerprints""  a fingerprint being simply a large sparse vector otherwise known as a Sparsely Distributed Representation (SDR). Once you have an SDR for two bodies of text whether they be Shakespeare or company descriptions comparisons become simultaneously insightful and simple.The underlying theory of the semantic fingerprinting technique used here comes courtesy of Cortical.io an AI company led by researcher and author Jeff Hawkins. Cortical.io is seeking to create a new kind of artificial intelligence by taking cues from the most powerful intelligence engine we know of  namely the human brain. In brief Hawkins and researchers at Cortical.io believe that the human neocortex (the largest and most evolutionarilyrecent area of the brain) is underpinned by one universal learning algorithm. The stands in opposition to a more compartmentalized understanding of intelligence in the brain (this area corresponds to language here music here math) but modern research supports the idea. A 2009 article from Scientific American discusses technology allowing   a strong example of the brain’s ability to adapt by employing a common learning algorithm across all senses and experiences.A full explanation of Hawkin’s theory is beyond the scope of this blog post  interested readers are enthusiastically directed towards his excellent 2004 book  which has remained relevant despite a full decade of progress in AI. Two shorter but more complex reads are available on the  and a white paper on .As outlined in the mathematical properties paper above two vectorized corpora are related to each other in a simple way. The core of it is overlapping bits  take the union of two SDRs and the more bits they share the more closely related they are. It’s not quite so simple of course  some common distance metrics employed are:",NA,"Contributed by  and . They took  program between Sept 23 to Dec 18 2015. The post was based on theirThe startup world is not for the faint of heart. Behind every success story lies a dozen failures and the odds can still turn for even the most robust startup. Countless resources have been poured into aiding wouldbe founders  websites like CrunchBase or AngelList new podcasts like StartUp not to mention countless howto guides and videos.On the technical side here's what you're looking at:Let's take a second to talk algorithms though  what are we actually using to pair startups with VCs?We can outline the basic pieces fairly quickly however:Last but certainly not least  how are we generating these fingerprints of a given input text? That's where Cortical.io's API comes in. Play around with this   cortical.io provides API keys upon request and the API provides everything you need to begin fingerprinting and comparing. The default base corpus is a generic English language corpus  this was the corpus used for all fingerprinting in our project.So our procedure is now clear: gather text data on startups and venture capital firm generate fingerprints using cortical.io's API and given a startup select the best match. This best match is the venture capital firm whose text indicates the closest semantic similarity to the text of your own product  the a priori assumption here being that a venture capital firm who describes themselves similarly to your startup (OR: a venture capital firm whose portfolio includes similar startups to yours) is a venture capital firm that is more likely to fund your startup.This procedure was broken into several stages:Stage 1  how do we get this data? We scrape. Lots and lots and lot of websites. 5000+ websites to be more specific. We do this with lots of infrastructure and a powerful enough scraper / crawler to take care of most of the dirty work. Simultaneously we begin to analyze the text that comes in. Jumping straight into it our workflow for stage 1:Without lingering too long the gist is: Scrapy goes out and scrapes lots of pages on lots of websites. We used Goose for text extraction as well as some generic XPath / CSS selector manipulations. A few cleaning functions were defined and pages were consolidated into one body of text per website. A Scrapy pipeline was built to dump into a MySQL database hosted on Google Cloud Services.Meanwhile as text was being dumped a simultaneous process was running to analyze the extracted text per site. We used a few different APIs for analyzing  cortical.io primarily but also TextBlob and OpenCalais just so we could play a bit.This scraping/analyzing cycle went on for several days. We launched 10+ spiders simultaneously on an AWS instance all scraping and dumping different websites. We ran these processes in the background handled connectivity issues checked for bad data and so on. It was a lot of spinning cogs but it came together and we had our data set.What sorts of issues did we have? How good was our data? Can we do better? Here are some issues / thoughts we had while in stage 1.This last bullet is worth lingering on. Not all text on a website is meaningful  in fact most of it probably isn't. What we're actually interested in how a VC / startup describes itself its projects its mission. So how do we get there? Well one way could be with an algorithm called tfidf (textfrequency inversedocument frequency) which allows you to properly weight the importance of word based on a ratio of its frequency in one document vs a set of documents. You could also do TextRank an algorithm based on PageRank that uses a graph approach to discover important words and phrases unsupervised. Or perhaps we generate a fingerprint using cortical.io of just boilerplate finance / legal and subtract out that fingerprint from our VCs and startups.All of these are worth pondering and investigating and so will be in future iterations of M3. They're especially worthwhile because right now the keywords for some websites are nothing more than ""legal agreement finance disclosure"" and so on. Not particularly meaningful. Stay tuned!Now we need our queryable frontend. For this we chose Flask a fairly lightweight Python framework meant for fullstack development. Flask provides interactivity between Python and the web dev side using a language called Jinja so the challenge was established. Figure out Flask figure out Jinja figure out enough web dev to get by and put it all together in a speedy frontend.For the visual component we used   specifically the simple but elegant Cover template freely available. Bootstrap comes with a little setup required especially when working with Flask. There are a handful of default components including the Bootstrap minimum CSS / Javascript files plus a few other things here and there. Mostly though Bootstrap was delightful. The entire power of the internet is at your fingertips with a little elbow grease.Flask and Jinja was another story. Flask is compact compared to Django but a fullstack framework is complex regardless. Many  were read many long hours debugging silly web dev problems and so on. A few main points:To be honest there's simply too much code to go into here. You're welcome to explore the code on GitHub  we're continually making efforts to increase commentary modularization etc. The total code output is well over 1000 lines of Python and that doesn't begin to touch the HTML CSS and Javascript that was customized for the app.How it works in the end is simple  enter your website OR a text description of your product and we'll find the top 3 best matches based on a variety of metrics and return them to you That's it. We'll be working on a brief video tour of our product in the near future.Does it work? Yes (mostly). The algorithm works very well and the data is all there. We're working on getting better representative texts as discussed above  subtract out boilerplate scrape supplementary sites and so on. One success story is the company kaplancleantech.com  enter in that site and your best bets for VC funding all come from VCs with a history of clean tech entrepreneurship.This technology is incredibly powerful if used well. We've used it but we too have a lot of work to do before the algorithm is truly robust. But once it is this matching algorithm could be used in any industry on any two bodies of text for any time there are buyers and sellers. Site content creation medical community fiction  the sky's the limit.This project was an exercise is putting together a very complex workflow from scratch  we started with nothing more than an API and an idea and now we have Python Flask Bootstrap Scrapy MySQL and more all woven together into a presentable product. There is certainly much more work to be done but we're just getting started.",NA
On Visualizing Hollywood BoxOffice Revenue,70,https://nycdatascience.com/blog/student-works/on-visualizing-hollywood-boxoffice-revenue/,My goal was to analyze the accuracy of news headlines relating to Hollywood; including but not limited to the changes in domestic versus overseas BoxOffice revenue and the marketability of different genres overseas. Specifically I focused on articles that had little or no visualizations but drew clear conclusions based on general trends.,NA,"Contributed by  He is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on his first class project  R visualization (due on the 2th week of the ).I utilized two different websites for my analysis: IMDB and BoxOfficMojo. IMDB datasets were used to aggregate movie ratings and the BoxOfficeMojo dataset was used for movie finance analysis. These datasets were cleaned and joined to create a single dataset containing movie ratings and revenues.There was an alternative IMDB dataset which contained aggregate movie ratings and finances but the IMDB datasets I chose had a subset of 668 IMDB users who all reviewed the of movies. I considered this a more robust dataset since all 668 individual users reviewed the same set of movies making their ratings more comparable  as opposed to the alternate dataset where each movie had a varying number of users that rated it.before              after It was important to clean the years in both datasets because multiple movies had the same name and incorrect matching would occur without joining by year  title.“Where Americans once were the only game in town for Hollywood U.S. audiences are taking a back seat to moviegoers across the globe — particularly in Asia.”“And foreign markets are getting the industry's highestprofile films first. Battleship opened in Asia and Europe more than a month before it reached the USA last May.Conclusion: Foreign revenue has accounted for an increasing percentage of total revenue every year since 1992 as shown by the increasing slope values for the regression lines.“Big noisy spectacle travels best. Jason Statham the closecropped star of many a mindlessly violent film is a particular Russian favourite. Films based on wellknown literature (including cartoon books) and myths may also fare well.”“Comedy travels badly: Will Ferrell and Adam Sandler provoke guffaws at home but incomprehension abroad""Conclusion: The overseas density plot confirms that Drama and Comedy genres perform worse overseas when compared to domestically. A majority of Drama/Comedy movies generate less than 50 mil overseas while Action and Animation genres show a much wider distribution of revenues (overseas).“...little effort is being made to deliver sophisticated storytelling ... movies are crafted mainly to provoke visceral  as opposed to intellectual response”Conclusion: The rating sweet spot that generates the most revenue is between 3.5 and 3.8. Movies that score greater than 4 show a sharp decline in revenues. This could be due to the fact that the average movie goer more easily appreciates an average movie (cough cough ** Michael Bay movies).Conclusion: There seems to be a linear trend between the number of movies a studio produces and it total domestic revenue. This doesn’t have to be the case; for example Lionsgate produced 5 of the top 100 movies in 2015 and could have generated 100mil in revenue (~50mil actual). This leads me to believe that movie studios do equally well selecting which movies to produce.Conclusion: There seems to be a linear trend between how much revenue a movie made on its opening weekend and its lifetime domestic revenue. Hollywood considers opening weekend numbers as a good predictor of how well the movie will perform and this plot supports that theory.Conclusion: The Highest Grossing Movie per year accounted for a decreasing percentage of total BoxOffice Revenue. This could suggest that studio's are either making more money per movie producing more movies or a combination of the two. Further analysis is required from different datasets.My data visualizations confirmed many of the conclusions drawn in the news articles. What I found most interesting was how good a predictor opening weekend turns out to be for overall performance and that movie studios are evenly matched in terms of how well they select movies. Since audience reception is such a complex factor to predict it's surprising that the studios are consistently able to make good decisions. ",NA
Project 1: Exploratory visualizations of Yelp academic dataset,70,https://nycdatascience.com/blog/student-works/project-1-exploratory-visualizations-of-yelp-academic-dataset-draft/,The dataset visualizations and result outputs in this post are NOT representative for any types of overall business users reviews in Yelp,NA,"Contributed by . He is currently in theprogram taking place between January 11th to April 1st 2016. This post is based on his first class project  R Visualization (due on the 4th week of the ).NOTE: .
* Main purpose of this post is to utilize several visualization techniques with a large dataset.These table and map indicates that average review ratings are between 3.36 and 3.84. The rows in the table are sorted in descending order by average review scores. The color of a circle in the leaflet map corresponds to average rating scores shown in the bottom left corner in the map.In these two plots North Carolina is the state with the highest rating score (3.84) while Maryland has the lowest rating scores (3.36). Very differently from my initial expectation New Jersey and New York were not ranked within top 5 but worst 2 and 3. To get more details I created another plot a distribution grid of average review rating scores at a deeper level.Basically the average review rating scores in each state was reclassified from 1.0 to 5.0 by 0.5 increase. For visualization purpose percentage of rating score is weighted.In this distribution grid of average rating score each column indicates a proportion of each score by state while each rows indicates a proportion at a specific review score in each state. For example in the first column California state has 17.8% of average rating score 5.0 14.1% of average rating score 4.5 and 19.2% of average rating score 4.0. In the top row California marked 17.8% of average rating score Georgia marked 10.6% and Illinois marked 8.8% of average rating score 5.0.From this grid we can see the pattern that most states have businesses whose average rating scores are between 3.5 and 4.0. Interestingly although California has the biggest proportion (17.8%) of average score of 5.0 North Carolina seems to have more businesses with highest average rating scores (11.8% of 5.0 20.4% of 4.5 24.8% of 4.0 and 22.6% of 3.5 respectively). Unfortunately New Jersey does not seem to be different from in the previous plot. All proportion are rather evenly distributed.I found another request link to download the Yelp Academic dataset. (The link to request an access is currently missing. I'll update a link once I found). This dataset is quite big (around 1.9GB in csv format) and contains 5 tables:The first question is about whether having more followers at Yelp will show a tendency to rate more frequently? To see more direct effect I show visualizations of the overall users elite users and normal users separately.:The first visualization boxplots shows slowly but gradually an increase pattern until some points around 1000 then decreases. My hasty assumption is that Yelp users will aggressively provide reviews proportionate to the number of fans. However once they have enough number of fans (around 1000 in this case) they would less likely to show similar frequency of review activity.:When we see the elite group alone in this boxplot group the pattern is similar to the group of overall users. Notably the number of review counts by outliers in early stages (when they have fewer fans) is quite higher and bigger in frequency than the next stage. It seems that elite users at the early stages provide more reviews to get more followers while elite users in the later stages seem to participate in more activities such as Yelp events for elite group.:The second question is about whether having more followers at Yelp is related to higher rate scores? This analysis is done similarly to the previous question.
:In the boxplots of overall users we can see a pattern that the average rating scores gets increased as the number of fans increases.
:The boxplot graph also shows the similar pattern as the previous one. Here I'm a little bit curious about the elite group especially having more than 1000 followers whether they try to show themselves as nice reviewers providing higher rate scores and whether they were invited better businesses more often as the number of fans increases.
:This boxplot for normal users does not tell much about the pattern except for that 50% of average ratings are between 3.5 and 4.0.",NA
"Diabetes, Obesity, and Income",71,https://nycdatascience.com/blog/student-works/diabetes-obesity-and-income/,This was my first data science project associated with the bootcamp.  The goal of this project was a visualization and explorations of New York state diabetic incidence data.  I wanted to look at data sets where there existed previously established relationships.  I chose a public health issue of diabetic rates and obesity it has been shown that there is a strong link between these two issues.  I thought it would be of interest to see how this relationship played out in New York state.  The data used were part of the 1250 freely available data sets on the  website an amazing resource for public health issues.  Four different sets were utilized: an income data set; a diabetic rate set; a diabetic mortality rate and an obesity rate data set(all from 2009).  All of these data sets provided granularity at the  county level.  The inclusion of the last three data sets are very straight forward.  Income data was included in this analysis for two reasons first there is a long established relationship between income and diabetic rates.  Secondly New York is the only state where the poorest county(Bronx) is directly adjacent to the most affluent county(Manhattan/New York County).  I was curious to see what relationships existed alongside this economic disparity.  New York state contains fiftytwo counties so I wanted to find a way to represent this data that made comparisons between counties easy and also retained spacial relations.   Choropleth maps were ideal for this task.  These maps use color intensity to represent aggregated data within a geographic region either intensive or extensive.  Luckily for me the R programming language had several libraries to aid in this mapping choroplethR had the shallowest learning curve.  Rather than just bury the reader in plots I will present only the most interesting plots in the post.  But here is a link to a Shiny application that I created that will allow you to explore with all the data.  Before my data could be mapped with choroplethR library it required some manipulation.  Thankfully the data was complete if anything there was more data than I required.  Several of the data sets contained information years beyond 2009 my year of interest.  Some data sets included aggregations of regions that required exclusion.  Lastly all of the data sets contained formating that required adjusting so they joined with the FIPs dataset required by choroplethR.  Here is the code for the for my loading and data munging.After the data sets had been joined to the FIPs county data set they could be visualized with choroplethR.  The code for the plots is listed here.The data was also plotted in scatterplots to better visualize some of the nonspacial relationships within the data sets.  The code for the scatterplots is listed below.Conclusion:The data substantiated the already established relationships between income obesity and diabetes.  Counties with the highest average incomes had the lowest incidence of diabetes.  The link between obesity and diabetes was also shown.  Interestingly counties with high incomes and high obesity rates had low rates of diabetes.  The link between income and diabetes may be as strong as the relationship between diabetes and obesity.  I haven't included my Shiny app code for the sake of space put it can be found on my Github here.,NA,"Contributed by . William took  program between Sept 23 to Dec 18 2015. The post was based on his
Here is the choropleth map of diabetic rates for all the counties in New York state in 2009.  The values represent all the percent of the population that has been diagnosed by a doctor as being diabetic.Here is a choropleth map of all the average incomes within the five New York City counties.  The disparity between Manhattan and the other four counties is striking.Here is a scatterplot exploring the relationship between the diabetic rates for each county and their corresponding obesity rate.  It's worth noting that this data is inherently normalized  due of it's ratio nature (incidence per 100000 residents).",NA
On The Rise of Data Science Startups,71,https://nycdatascience.com/blog/student-works/on-the-rise-of-data-science-startups/,"Our project is centralized around the development of an open source workbench that is focused on providing data scientists with automated tools for exploratory analysis and model selection.  The full stack design is made in R a statistical programming language. Before getting into the lowlevel details let's take a step back and think about the trending term ""Data Science.”It seems that everywhere you turn these days there’s someone starting a “Data Science” company.  Are you a PhD Dropout from Berkeley? Start a Data Science company. Are you a programmer that knows how to use MongoDB? Start a Data Science company.  Did you study english at Yale? That’s right  Data Science.  The good people cbinsights.com made a chart about Venture Investment in AI over the last 5 years:What gives and why now?There’s been a lot of attention on data science platforms and workbenches that attempt to improve the data scientist’s workflow or allow non datascientists to perform data science through an immersive user interface.  We’re going to show you how to build your very own open source machine learning workbench in R.  Please steal it. Everyone has seen some form of the below chart with Computer Processing power rising exponentially.  While the fast CPU has driven many innovations the inexpensive CPU is not the critical factor in Machine Learning.It’s not just CPU’s that are dropping in price but .  Distributed Machine Learning algorithms depend as much on memory network speed and to a smaller degree hard disk speed as much as they rely on CPU speed.  It’s the aggregation of multiple exponential trends that is democratizing access.  Many people think despite these price drops that tools like AWS are still “too expensive.”  This couldn’t be further from the truth I want to explore the AWS pricing.Here’s an AWS pricing list as of 11/20/15.  The critical factor here is the $0.126 Per Hour Pricing.Assuming that you live in the NorthEastern corridor California or the midwest and assuming that your computer draws at least 1 kWh renting server space from amazon is less expensive than just paying for computer electricity in your home state.  I live in NYC pay my own electricity and I was able to save money by moving my computation demand onto AWS.With AWS you can get a server with up to 244 GB of main memory and up to 40 CPUs; no more limitations by hardware and computation time for your Rbased analyses.  While hardware price reductions are nice we will see that Machine Learning software prices have collapsed even further. Open Source machine learning libraries has been a revolution for the machine learning community.  The once obscure and specialized topics of machine learning and statistical learning can now be leveraged by a much larger demographic globally. Machine learning workbenches have been implemented in academia and industry.  The following is a short list of many popular platforms. ($140000 for the first year) ($51000 per user) ($58500 per user)Dataiku DSS & etc (~$10000 per user per year)You get the idea.The worst part about the SAS/IBM/H2o prices is that much of the software that runs these machine learning libraries is open source to begin with.  These companies have a business model of taking freely available open source tools building a GUI on top of the system and charging tens of thousands of dollars per year for support.Weka’s software design is centralized around java. Dataiku DSS uses primarily Java Python and a kernel design compatible with multiple languages including R.  H2o is Java and R.  IBM’s software is built on SPSS an older programming language that was . Most of these expensive products have proprietary model formats and data cleaning requirements making interoperability and portability of code a near impossibility.  Python libraries for Machine Learning (Free)  Java libraries for Machine Learning (Free) Open Source Machine Learning (Free) Open Source Machine Learning (Free)R Python Spark Hadoop Caret (Free)Model Class Package Caret Function SyntaxldaMASS predict(obj) (no options needed)glm stats predict(obj type  ""response"")gbmgbm predict(obj type  ""response"" n.trees)mademda predict(obj type  ""posterior"")rpart rpart predict(obj type  ""prob"")Weka RWeka predict(obj type  ""probability"")LogitBoost caTools predict(obj type  ""raw"" nIter)Shiny is not the only new tool for computer visualizations but is a fully functional web app development package that can streamline R code directly into an interactive frame without the need to know know javascript or html.  The Shiny package is compatible with many other interfaces including Google Viz Tableau matplotlib bokeh (and a ton of others).  With R and Shiny you can setup a webserver and provide visualization tools to your BI teams in realtime.  Did we mention this tool is free?   We’re broke students and our classroom is at WeWork where we get free coffee and beer.  Shiny and R fit right in.Let’s now delve into the app we made.  As we mentioned before we wanted to make an opensource application in theme with the growth of data science startups.  Using a combination of Shiny caret and other great open source tools we made a fairly workable platform that can perform basic data analysis preprocessing modeling and validation.  We spent only three days developing this app and there will be surely many bugs and glitches with the code.  Keep in mind that our main intention was to create a functional prototype to showcase a small fraction of creative possibilities available to us through the open source community.  We used R Studio as the main IDE for our app.  R console works fine as well.  This blog will give a general overview of our development process.  The code is available here if you wish to play around with it and learn more about our full stack.To begin we created two blank r files: ui.r and server.r within a new project directory or folder.  You can name this folder anything.  For the ui.r file you will need to install and load the following packages below.In the server.r file you will need to install and load the following packages as well. In order to make a visually appealing and straightforward interface design we implemented the shiny dashboard package.  This package had a very straightforward documentation that can be found here.  The shiny package comes with a large pool of high quality icons css themes and other bootstrap quality elements.  I recommend following the  and then cross reference your learning with our code to get the most out of this blog post.  The server.r code was tricker than the UI for a couple of reasons reactive shiny features is a must for creating an interactive shiny app.  The main shiny blog does a fantastic job explaining dynamic and reactive scripting in R so I will leave the explanation to them in .  Essentially reactive functions in shiny means that you are creating smaller “pseudofunctions” that automatically receives user input when interacting with features such as a checkbox or slider.  We wanted reactive functionality to allow the users to customize their tuning parameters for the modeling part of the app.  The second part is the analysis feature.  The analysis features contains three subfeatures: preprocessing feature graphing and modeling.  For the preprocessing subfeature we kept the options minimal and included cross validation IPA and ICA options that can be activated through shiny widgets.  The feature graphing is a simple graphing application that produces a lattice plot of all data features in the data set.  For the modeling subfeature we made four algorithms available for the user to choose and tune: KNN logit boost gradient boosting method and neural networks.  All of the modeling algorithms were from the caret package and is a fantastic package to familiarize yourself with if you want to pursue more machine learning application in R.   The demo video above provides a visual overview on how to operate the shiny application.  Feel free to reach out to us if you have any questions or comments about the code.  Again this is completely open source so please take it and play around with it!",NA,"Contributed by  and . They took  program between Sept 23 to Dec 18 2015. The post was based on theirThe Machine Learning servers and tools that used to be exclusively the domain of Hedge Funds Fortune 1000 companies and large drug manufacturers are now accessible to anyone. The data science workbench tool that we built over a week is meant to illustrate how easy it is to duplicate the features of the more expensive institutional packages using completely free software.  Our code is also hundreds of lines of code something that should be relatively easy to maintain by an enterprise.Our project is focused on the full stack implementation of R in order to not only explore its computational nuances in a data science setting but also explore how R’s UI capabilities can contribute to a positive workflow and user experience.Old design paradigms for Machine Learning required a developer to learn many different modeling packages usually written by different people with inconsistencies in how models are specified and predictions are made.In the past each row of the below table was a completely different workflow.  Each Model Class had its own input format and tuning parameters. Running a single data set through multiple models used to require hundreds of lines of code.  However there is an Rbased Open Source Alternative  Caret which has Standardized model tuning.  We’ve been calling this the “ScikitLearn” for R.  The Caret function Syntax is a dream to work with and anyone can create tune and compare the results from multiple models with ease.  Caret is 100% free.Once these packages are loaded and declared in their respective files we can proceed with the UI phase.For those new to Shiny and R we recommend playing around with these tutorials to provide introduction to Shiny to gain a better understanding of the relationship between the ui.R and server.R files.  You can find the  to the main shiny page.Due to the size of the project we won't be going into the details of the code.  However if enough requests are made we may consider creating a tutorial blog post to go more in depth in the development of our app.  You will find the main files in this blogpost below.  Again feel free to access our  if you wish to play around with our app and source code.
Our app has three main parts.  The first part is the data preparation feature.  You can upload almost any csv file and the app will automatically perform missing analysis.  We mainly used the iris.csv data set as the main test set.The third part of the app is the results feature.  After selecting the models to use for the uploaded data set you can see and compare results from the different models on one page.We are living in an exciting era of data science.  Looking into different perspectives of the open source resources available to any data enthusiast it's easy to see how so many startups are gaining traction in industry.  Everything you need to start a data science startup are only a few keystrokes away.  For fun we loaded our app onto an AWS instance and compared its computational runtime with a local instance of the app.  We found that our app was x10 faster on an AWS instance than my local machine (Macbook Air 8 GB ram 512 SSD).  This only shows that anyone can create a budget friendly data science startup as long as you are creative and determined to see it through.  Thank you for reading our blog post and as a bonus please enjoy our deep learning art below!WE HAD SO MUCH EXTRA AWS HORSEPOWER WE MADE DEEP LEARNING ART USING PICTURES OF JOE’S CAT:",NA
"The Great Escape: Health, Wealth, and the Origins of Inequality",71,https://nycdatascience.com/blog/student-works/the-great-escape-health-wealth-and-the-origins-of-inequality/,: Build a companion interactive presentation for a book : Build an interactive presentation with embedded  apps:   and  winner of the 2015 Nobel Prize in economics is a Professor of Economics and International Affairs in the Woodrow Wilson School of Public and International Affairs and the Economics Department at Princeton University.In  Angus Deaton–one of the foremost experts on economic development and on poverty–tells the remarkable story of how beginning 250 years ago some parts of the world experienced sustained progress opening up gaps and setting the stage for today's disproportionately unequal world.,NA,Contributed by . He took  program between Sept 23 to Dec 18 2015. The post was based on hisThe interactive presentation with the Shiny apps embedded in it is live at .A screen capture of the presentation is below:[youtube http://www.youtube.com/watch?vqElqGyCSB9k?rel0&w560&h315]I embedded several GoogleVis Motion Charts in my interactive presentation. I went over how I created a GoogleVis motion chart using R and R Markdown in a previous blog post Gapminder Data Visualization using GoogleVis and R.The specific steps involved were:An article can also be embedded using an iframe:The interactive presentation with the Shiny apps embedded in it is live at .More information about creating interactive presentations using R R Markdown Shiny and Knitr can be found at:,NA
Lending Club investment simulator,72,https://nycdatascience.com/blog/student-works/lending-club-investment-simulator/,We build 2 mains tools to explore and run simulations on the data provided quarterly by Lending Club.,NA,Contributed by . Jean took  program between Sept 23 to Dec 18 2015. The post was based on hisPlease see the app  You can also find the code of the app  (LC) is a peer to peer online lending platform. It is the world’s largest marketplace connecting borrowers and investors where consumers and small business owners lower the cost of their credit and enjoy a better experience than traditional bank lending and investors earn attractive riskadjusted returns.Here is the link to more  about Lending Club.For our first project we already did some analysis on this data.You can find the blog post  and the full R publication The first tool's main focus is to allow users to explore the data both visually and with data frames summaries. For the visuals we decided to use bubble graphs as they have the advantages of enabling the visualisation of 4 dimensions in a user friendly and accessible way:One discrete dimensions or groups represented by each bubble and three continuous variables for the abscissa the ordinate and the size of the bubbles.The groups available are:The continuous variables available are:Finally the user can generate these graphs on different subgroups of Lending Club's data:Using these options the user can create this type of visualisation:For the user's convenience we added 2 visualizations:One visualisation summarizing the total amount of loans issued for the group selected by the user to put things in perspective especially if the groups differ greatly:And an other visualization showing the number of loans issued in each group selected by the user. This is particularly useful to detect if there is a bias of size i.e. if loan in a given group tend to be bigger. For example here we can see that higher the income the bigger the loans are: indeed up to a factor of 3 between the lowest quantile and the top quantile:Finally we added a summary in the form of a data frame to have access to the basic statistics of each group:The second tool we built is the investment simulator. It allows the user to run investment scenarios based on LC's historical data. The user can tune and filter the loans in order to select a subset of loans with specific properties that the user suspect thanks to his previous analysis using the first tool  will outperform Lending Club's rating system and hence improve the performance of their portfolio.After running a simulation the user has access to the following outputs:This table contains statistics one almost all feature of the loans and borrowers in the portfolio. This table can be transposed to see more details by checking the box 'transpose summary' just above the 'invest!' button:Regular/compact summary:Transposed summary:Finally the user has full access to his portfolio and can choose to display all the loans or filter them in anyway he/she may want:We manage to improve the returns from an average of ~6% with no selection to ~12% with our best strategy! This is a huge increase and well overperforms the market standard in fixed income.These are example of funds boasting their superior investment performances:As we see the performance achievable on Lending Club's platform exceeds these industry averages on portfolios of great diversity. Indeed one of our ~12% performing strategy ended with 584 loans!For our next steps we wish to implement machine learning algorithms to build strategies automatically but with taking great precautions in avoiding overfit to our data.Here is the hall of fame:The code is included in the app for the curious mind:You can also find the code of the app The code for the investment simulation is split over 2 functions:Please see the app  You can also find the code of the app,NA
What is Hadoop?,73,https://nycdatascience.com/blog/data-science-news-and-sharing/hadoop-3/,"The Hadoop system now hosted at Apache Software Foundation (hadoop.apache.org) consists of about a dozen toplevel projects.It began in 2006 as an effort to create an opensource implementation of MapReduce Google's system for exploiting parallelism in their large data centers.HadoopMapReduce is still at the center of the Hadoop ""ecosystem"" but compared to the Hadoop of 2006 today's Hadoop is more general more efficient and easier to use for a variety of applications.Among the systems in the ecosystem are Hive which provides an SQLlike database querying facility for massive distributed data; Pig which uses its own language to simplify the construction of MapReduce ""pipelines;"" and Spark which leverages the Hadoop ecosystem to provide MapReducelike parallelism at greatly reduce cost.Today Hadoop is used by countless companies and universities either in their own datacenters or on commercial systems like Amazon's EC2 or Google's GCP.Financial companies use Hadoop to analyze stock purchases; drug companies use it to explore drug interactions computationally; scientists at NASA use it to process massive streams of data from satellites.Its popularity comes from its simplicity and from the growing suite of tools supporting not only basic parallel processing but also applications like data mining and data visualization.Increasingly Python and R are being deployed within the Hadoop parallelism framework; the simplicity of using dynamic languages and the efficiency of parallel computers makes this a great solution for processing huge data sets.",NA,"Hadoop is the most popular system for doing ""Big Data"": analyzing large data sets using multiple communicating computers in a datacenter.  Really a suite of related programs Hadoop has become popular because it hides the complexities of parallel processing making it easy for programmers to harness the power of thousands of computers. Hadoop includes systems to implement basic technologies like distributed data storage and job scheduling and a growing roster of userlevel systems to help programmers analyze their data more easily in a variety of programming languages.  Even as it evolves Hadoop continues to be the world's most popular system for doing Big Data.",NA
Learn from the Kaggle Master: Owen Zhang Discusses Winning Solution,74,https://nycdatascience.com/blog/data-science-news-and-sharing/learn-from-master-owen-zhang-1-on-kaggle-com/, Owen explains how he utilized “ ’s solution was written entirely in R. Owen used’s’s Interview below.,NA,Owen Zhang #1 on Kaggle.com recently released his winning solution for the . As the Kaggle Team notes in .In his Winner’s Interviewprevious competition experience domain knowledge and a fondness for XGBoost” to finish ahead of 455 other data scientists.Notably Owen packages including  and .What has Owen taken away from this competition? (1) With a good computer R can process “big data.” (2) Always write data processing code with scalability in mind. (3) When in doubt use xgboost!Owen gave the following pieces of advice to budding Data Scientists: (1) don’t be afraid to try things and ask questions (2) get the fastest computer you can afford and (3) try to understand the problem/domain and don’t build models “blindly” unless you have to.Read Owen complete Winner ,NA
Targeting Twitter Influencers Through Social Network Analysis,74,https://nycdatascience.com/blog/student-works/find-twitter-influencers-through-social-network-analysis/,’ll probably find’s’s ’s InfluenceFlow Scoreainly focused on identifying and classifying superfans and d,NA,"





This project designs a program for charting the influencers and patterns of the Twitter community. My findings will be able to help online media companies identify their most important influencers on Twitter and understand these “superfans’” interests patterns and behaviors. Furthermore recommendation systems can be built based on natural language processing of influencers’ Twitter timelines to suggest content that more compellingly attracts influencers’ attention.My  below showcases the analysis result of the top 20 retweets in the @thisisfusion timeline:“Twitter Influencers” (1)“Twitter Influencers” (2)“Twitter Influencers” (3)Recommendation systems have become widely used by news media companies. As evidenced by the “” section of NYTimes.com or the “” plugin on HuffPost Social News personalized placement of articles on apps and websites is now being used to guide users to posts they most interesting.At the core of news media recommendation systems is the tripartite ability to recommend the right content to the right people at the right times. Understanding who views what when makes possible recommendations in real time. A few recommendation algorithms are already in this marketcontentbased methods for instance or collaborative filtering ones.Fusion a joint venture between Univision Communications Inc. and Disney/ABC is a multiplatform media company that champions a young diverse and inclusive world. Social media referrals have contributed to over 90% of the web traffic of its online presence (see: ).To better understand Fusion social media market segments two main Fusion data sources were used in this project: (1) Twitter API (tweets information related to @thisisfusion); (2) WordPress API (news article information with related tweet). This is all open source data making the project reproducible. In the future the model we developed could be modified to analyze Facebook for instance as well.Working with Fusion data sources my “Twitter Influencers” Shiny app shifts the market focus of recommendation systems from news media outlets to social media networks. The project specifically targets those most influential users of social networks like Twitter.As is true in real life Twitter users have varying levels of influence on other people. Different users get different amounts of attention and reactions. Based on trends like that I wondered how I might quantify the influence of Twitter users in general. Developing a model for doing so would allow me to identify those users who bear most influence for Fusion via @thisisfusion in particular.Two sets of metrics were implemented in this project:. Based on retweet history in a time series and the following relationship between retweet users a simple influence graph can be drawn as follows:In the context of a single retweet network a given userCentrality Score indicates how important that user is within the network.. In the context of a given Twitter community a particular user indicates how influential that user is within the community.Not limited to a specific tweet InfluenceFlow Scores capture overall information flow in particular Twitter communities. Within the Fusion community (signified by @thisisfusion) a certain user’s InfluenceFlow Score is calculated by the product of that user’s number of followers and number of times they mentioned @thisisfusion in their most recent 400 tweets.Having gathered the InfluenceFlow Scores for all retweet users one quick way to categorize users is to associate retweets with categorical information consisting references to sections classes categories etc.culled from articles and hashtags alike. By grouping users according to their interests editors can easily find the influencers of any topic or milieuany category any community any interest.The model for collecting superfan data from every tweet runs as follows:To make my recommendation system more robust two questions need to be answered properly in the future.First is there a better way to predict users’ future interests?I used users’ past retweets as a basis to understand their interests. But the limited retweet records from one user may mislead the algorithm to give unbalanced weights to different topics. A more robust method would involve analyzing users’ own timelines using natural language processing (NLP) tools.Analyzing users’ timelines would give our recommendation system the full range of user behavior. The system would be able to get to know a user’s interest from the full set of their past behaviors as expressed in natural language.As an experiment toward this end we used the NLP tool  to extract both scores and interests from user timelines.Second question: is there an algorithm to suggest personalized recommendations to superfans?Again utilizing the prevalence of natural language on Twitter will help. A contentbased recommendation system could be built in the following steps:(1) Mine text on all superfans’ timeline content
(2) Cluster texts on vectors from AlchemyAPI
(3) Observe distributions of superfan scores and interests in different clusters
(4) Match vectorized user timeline text and article text using cosine similarity to give contentbased recommendations.Because of time and data limitations results from this project may be subject to certain biases. Within our threeweek timeline I midn’t include time analyses. And Twitter API limitations meant rate and query limits and translated to a lack of infrastructural support. It took one to two days to get basic data needed for 20 tweets and I couldn’t retrieve more than 3200 tweets and 100 retweets which sacrificed the flexibility of popularity analyses and meant that data consistency issues occurred a lot.Despite constrains I was able to envision frameworks for many recommendation system features. Social media networks continue to provide a rewarding avenue for doing data science.",NA
Become a Master Event-Hunter (and Coolest Kid on the Block) at the Click of a Cursor: My NYC Event Finder R-Shiny app,75,https://nycdatascience.com/blog/student-works/event-hunting-in-nyc/,Fridays and Saturdays have the most events (191 and 192 respectively) although Monday albeit being the leasteventy isn’t too shabby with 105 options. This is New York City after all.,NA,"New York is a veritable paradise for people who enjoy cultural events. Many websites like  and  list a variety of events for potential eventgoers to explore. However browsing through hundreds of listings on websites like these can be boring and quite timeconsuming. We settle for simply as much as we've searched for.Imagine: you're planning to go to a Broadway show tonight. You'd love to explore some other arts events along the way toward the theater before curtaintime. What you  want to do is waste half your day browsing event listings and trying to manage logistics. Before my RShiny app you'd have settled for easiesttofind events or events you already knew about. Or you might've given up searching entirely. Now however you have a third better optionone that will instantly get you into the know!By way of a beautifully verdantcolored interactive map NYC Event Finder displays search results for up to 10 categories of cultural events (ranging from ""Classical & Opera"" to ""For Children""). Narrow down results by category location or keyword(s) at the toggle of a button. With an interactive map at your fingertips NYC Event Finder helps you find where you  to go.Adjacent to the (that interactive list to the left of the screen allowing you to modify searches by event type) the events that are ""nearby"" a given location are displayed on a Google map.Search content for the  input panel (near the bottomleft corner of the screen) could include an address an intersection or even a building's name. Besides ""nearby"" locations you can also narrow searches based on category borough and even keywords.In the example below the location is Central Park; nearby ≤ 2mi; and ""dance"" is an additional keyword parameter.Each red pin icon on the map indicates that one or more events are to occur at that location. Click on the icon and a popup window displays venue links for all available events.No need to keep tabs on your own either! You can save events to the backend database by clicking the  button on the bottomleft corner of the screen.Quite often people are not only interested in finding an event but also want to know optimal times and places to go for certain kinds of events. With all the data retrieved from the API (""application programming interface""in this case the ' events listing API) it's possible to do some simple analysis to be able to speak to these concerns. The following are two types of analyses that can be done.On the Event Analysis page below the number of events I've searched for so far is 316. A simple graphed count by days of the week the results of which are displayed below shows that the number of events increases by day over the course of a given week. The chart below meanwhile indicates that Art and Theater together making up 40% of all listings are the two most common event . Filtering results by "" picks"" these two categories account for a whopping 67% of all picks.Bringing it all together NYC Event Finder provides a heat map that indicates when (day of the week) and where (e.g. by borough) you might want to be focusing on for a given event category.For those who care about the backend My NYC Event Finder RShiny app utilizes the  events listing API to retrieve events information. All event data is stored in a SQLite database. Visualizations have been implemented with the GoogleVis API and d3heatmap. Location search is done through Google Maps Geolocation API.",NA
Meetup: Scikit-Learn Workshop by Andreas Mueller,77,https://nycdatascience.com/blog/meetup/meetup-scikit-learn-workshop-by-andreas-mueller/,Andreas currently is a Research Scientist at the NYU Center for Data Science a research group dedicating to open source software for data science. Previously he worked as a Machine Learning Scientist at Amazon focusing on computer vision and forecasting problems.,NA,"This past Monday over 70 data scientists and open dataers went up to Microsoft for the ScikitLearn Workshop Meetup. The event was held through the NYC Data Science Academy and NYC Open Data. Andreas Mueller one of the core developers gave a presentation about some of the most advanced features of ScikitLearn.ScikitLearn is machine learning library for Python. It has become a powerful tool for many data science practitioners. The talk introduced some of the exciting aspects of scikitlearn such as building complex machine learning pipelines model evaluation parameter search and outofcore learning.For those who missed the meetup here is the video!
Read ScikitLearn Workshop slides.",NA
Featured Talk: #1 Kaggle Data Scientist Owen Zhang,78,https://nycdatascience.com/blog/meetup/featured-talk-1-kaggle-data-scientist-owen-zhang/,Featured Talk: #1 Kaggle Data Scientist Owen ZhangThank you very much for coming and thanks Vivian for the generous introduction. I’m sorry to say this but it sounds much more impressive than it actually is. I want to tell you a little bit of background. I used to work in the insurance industry. I worked several years in Travelers and several years at AIG. So one thing that I learned from my corporate career is that it’s very very important to manage your expectations. We didn’t do a very good job so I’m trying right now. So I’m going to give about one hour of talk. We’ll cover a few topics and I’ll cover a few topics including my personal take on how to do well in Kaggle competitions. If you have any questions please wait until the end. We have about a half hour of open Q&A session so everybody will have a chance to ask questions.Today I’m going to cover these few topics. First I’m going to give a brief introduction on how data science competitions are structured at least how Kaggle does it. I’m going to cover a few pointers on how I do it: philosophy strategy and also techniques. Then I will have two example competitions so two that did I relatively well. I just want to use that example to show what I actually do and to show that each competition is quite different. The last I want to cover is the things that we learn from a competition are they useful in the real world or not. I do want to answer that question at the end because actually I get asked that question quite often.How is a data science competition structured? At least how is a supervised learning competition structured? Usually the host will have all of the data and then they will split the data into three pieces. There’s the training piece which we can all download and we have both the predictors and the target. And then there is the public leaderboard piece. We havetraining for all three pieces the difference is what we know about the target. So we know all the targets exactly for training. Then we get a summary metric on the target for the public leaderboard.After you build a model on the training data you apply the model to the public leaderboard data and make predictions. Then you upload the predictions. Because the host has [the third piece on] the server they have all the target variables they compare your predictions with the actuals to see how well your prediction predicts it. If I tell you summary score that’s where the public leaderboard is. Throughout the competition it will last a few months you will get constant feedback of how well your model performs on that piece of data compared to all the others. So you can get some form of idea if you’re doing the right thing or not. So at the end of the competition what really determines the outcome is the private leaderboard. So you’ll have some sort of idea if you’re doing the right thing or not. But at the end of the competition what really determines the outcome is actually the outcome is actually the private leaderboard ranking. Which you only get to see once. That’s after the deadline passes then they will switch over from the public to the private. It depends on the competition. It’s more often than not [the public and private leaderboards] are the same. Sometimes the public leaderboard and private leaderboard are very different. So there’s quite a bit of uncertainty involved.Why is the competition structured that way? Because this forces us to do what the predictive model is supposed to do: that is to predict what we haven't seen before. So the private leaderboard matters most because we have absolutely no knowledge about the target variables in the private leaderboard data. That is how our model would be used in the real world. So whenever we build a supervised learning model the purpose is to use the model to predict something we haven't known yet. So if we know something already there is no reason to predict it we can just take a look at it. So it is very important to evaluate the model on data that the model does not have knowledge of.So a little philosophy: I am actually quite fond of philosophy because when it comes time to predict something you have no knowledge of there is usually nothing to go by so you have to fall back on philosophy.So I'm sure that most of you have certain data science background and we all know that one of the biggest pitfalls in building a model is overfitting. Overfitting means that you have a training data set and you build a model on the training data set you can throw a lot of factors at it and try all kind of different things. You can build a model that fits the training data set very well however the model may not generalize. So you can have a model that does exceptionally on the training data set but if you build a model that predicts very well on things you already know but it doesn't predict well on things you don't then that would be a model that is overfitting.In this case in a lot of Kaggle competitions overfitting is the biggest risk. So what I learned through the years is that there are many ways to overfit. The easiest way to overfit I think we all know is to have too many parameters in your model. So if you have one hundred data points in your data set and you have one million predictors in there you can probably predict it exactly because you have so many degrees of freedom. But that is only the most common way of overfitting. There are actually many other ways.One very subtle way of overfitting is that you keep building different models and you test each model against the answer which is your public leader board and then you you see we have a model works well you keep it and whenever it doesn't work well then you can drop it. You can keep doing this. Actually in a lot of situations you can keep doing this and you will find that your model's performance on the public leaderboard just keeps growing. And there's no end to it. But the reality is that you are just overfitting the public leaderboard. I think that happens quite often.In statistics (ok I'm not a statistician so let me know if there is a statistician in the audience) but in statistics we always use a five percent significance test. So if something is given a null hypothesis you have less than a five percent chance of observing what you have observed then you have to say that this is significant. But we run multiple comparisons very very often when doing models its better if you try very hard. You randomly try one hundred things then a few of them might turn out to be statistically significant but that is because you tried so many things so here the philosophy is This is the is the secrets slide. These are the secrets. The longer version is on the left and the shorter is on the right. The slides will be posted on Vivian's website or on Slideshare so there's no need to take notes on the slides' content.The short version is that (1) you need to be disciplined. The temptation to overfit public leaderboard is very strong. Sometimes psychologically I cannot control myself just to overfit the public leaderboard because that makes me feel good. We are all human. We need to be disciplined so we don't look embarrassed after the private leaderboard comes out and we had been doing so well on the public leaderboard then it flips and you say 'ahhh' you can't take pride in yourself anymore. It doesn't look good.The next is to work hard. The Kaggle system rewards effort. You never lose anything by participating. So you can rank absolutely last in a contest and you still relatively speaking do better than not participating. Participating more gets you better results. Even within competition trying to work hard gets you better. We will talk on that how to properly work hard later.The next one is learn from everyone. This also is a little bit against human nature. We all have our own pride right? Occasionally I would think ok this time I just try to do everything myself I don’t want to look how other people do it. There is a little bit of like hubris in there. But on the other hand people other than you always know better than you individually right? Because there so many of them there is only one you so it’s pretty hard to compete against everybody. I always feel that really learning from others that you can really improve. There is really no pride in doing everything by yourself otherwise. Actually I’m not sure how many people can really invent all the mathematics that learned in elementary school. And I think that takes more than one lifetime to invent all that. So there is really no “oh I learned this myself.”The next one is luck. So we are predicting very noisy data and things we haven’t seen yet. And there is really no absolute to differentiator between noise and the signal. There is none. So the only way to really differentiate them is that you have more data. Which if data science competitions there is none. You have everyone’s data there is a signal to noise ration that’s pretty determined. So there is always nonzero luck. So if you participate in something and you don’t do very well in a particular competition that’s probably just bad luck but if you do well that’s all your effort. []I always get reminded before I go to give a presentation to make sure that I bring something that’s concrete that people can take away. The rest of the presentation is something that’s more concrete. But I personally feel it’s how you approach the problem. The philosophy and strategy are more important than specific techniques. So I don’t want to say just learn all the techniques and you’ll do well. So a lot of times it’s not about what you do there are a lot of specific things you know is necessary to build a good model. The key there is actually how to allocate your effort. We all have a time budget. There is a deadline and then we have to do our day job and feed our kids if you have kids. Turns out there is limited number of hours per day. It’s actually worth it to think about how much you want to allocate to doing feature engineering how much time you want to allocate into trying different forms of models and how much time you want to allocate into hyperparameter tuning. So actually I do think about that consciously.Now to talk about some technical tricks. Gradient Boosting Machines. I use GBM on everything that I can use GBM on. There are things that you can’t GBM. One example would be very large data. If you have 100 million observations you probably don’t want to write GBM. But for anything that can use GBM GBM. I probably overuse GBM. So why is GBM so good? GBM automatically captures to a very large degree nonlinear transformations and subtle and deep interactions in your features. GBM also treats missing values gracefully.I use the R implementation of GBM that is what I work with so often. There are at least four very good GBM implementation today. Two years ago there was only R that was good. But nowadays you can use either the R package you can use scikitlearn package its very good. And if you want go parallel there is H2o package and xgboost. They all are very good implementations. Each has their own quirks. But they are all able to do nonlinear transformation interactions. Those two things are what people spend most of their time on when we’re building linear models or generalized linear models. In the days when we had logistic regression or when we had only Poisson regression in the industry in the insurance like we were. Whenever we are building models we are not really  models because we always run the  models like Gamma regression. So what we are doing is looking at all the outliers all the transformations turning it to Ushapes Wshaped hockey stick shaped whatever shaped. If you use GBM it will take care of this for you automatically thats why I use it so much. If you haven’t tried it please do try it.GBM like all the other modern machine learning algorithms has a few tuning parameters. When your data set is small you can just do a grid search. But actually GBM has three separate pretty orthogonal tuning parameters. If you want to do grid search you actually do need a pretty big range. I usually use some ruleofthumb kind of tuning so it’s mostly for saving time. So if you do a very smart grid search you can probably do better than the ruleofthumb. But rule of thumb is very time saving.The first set of tuning parameters is how many trees you want to build and the learning rate. They are reciprocal to each other. A Higher learning rate will get you to use fewer trees. I want to save time right? Usually I just target 5001000 and trees and tune the learning rate. The next one is just a common tuning parameter for decision trees (GBM is based on decision trees). The number of observations in the leaf node. You can look at the data and get feel of how many observation you’ll need to get mean estimate. So if your data is very noisy then you make more and if the data is stable you probably need less. Interaction depth is a very interesting for the R version of the GBM. The R version of interaction depth describe how many splits it does; so it’s not interaction depth as in the  depth. If you have interaction depth of 11 that means you have above 10 leaf notes. It doesn’t mean that you will have a 1000 leaf nodes. Don’t be afraid to use 10; I use 10 very often.There is one thing that GBM does not doactually there is one thing that all the treebased algorithms do not do well which is dealing with higher cardinality categorical features. So if you have feature which has a categorical variable that has many many levels throwing them into any tree based is basically a bad idea because the tree will either run super slowly if you are encoding all of them or just overuse that feature because it’s overfitted. So for high cardinality featuresif you want to use them in the treeyou need to somehow encode them as numerical features.There are few different ways of encoding. High cardinality features are very common. We often see things like zip code or in medical data you see diagnosis code (there are tens of thousands of ICD9’s) or text features. One approach for encoding them is to build ridge regression. Starting a logistic regression is very simple you can add regression based on your categorical features and then use prediction itself that will be numerical as an input to your GBM. That’s what people usually call stacking. Basically you build a stack of different models–subsequent stages of models will use the previous stages of models’ output as their input. As I described if you have text features or numerical features you get all the text features into your ridge regression model then you make a prediction. Then you put all your predictions side by side with all of your raw numerical features and fit this in a GBM. And this usually works pretty well. If you haven’t tried it you can replace these by a categorical features. And for a lot of Kaggle competitions you just do this and you reach the top 25%. So people that run bottom half of kaggle probably aren’t using GBM. [] Because it is quite easy to rank in the top half. There are many beginners that really get into this on the bottom.The one particular risk here is that if you use the same data to build the first stage model and then use the same data again on Model 2 very often the prediction from Model 1 will be overfitting in Model 2 because you already used the targeted variable in Prediction 1. Prediction 1 has a leakage in there. Basically it has leaked the information from the actual target. Whenever you put the Prediction 1 like that with numerical features Prediction 1 will be given too much weight.The more you overfit Model 1 the more weight will be given to Prediction 1. So it can be quite bad. What you want to do here is use different data for Model 1 and Model 2. So you can split your data in half. Use half of the data for Model 1 and half for Model 2. This will give you a better model than using all the data for Model 1 and all the data for Model 2. You can swap them you can split the data half for A half for B using all of the data in this way. This will usually give good results. If you’re not trying to win competitions i.e. for practical purposes I feel that this kind of model is already good enough for a lot of practical applications.You can take this one step further if you have smaller data where even half and half might be limiting your data too much you can do something like crossvalidation–you can split your data tenfold and do 10 different Model 1’s each one using nine to predict the other ones. You can then concatenate the models together so that each record will have an outofsample prediction from Model 1. Then you can avoid the overfitting problem in Model 2. So that’s how you deal with categorical features.Once you can deal with the categorical features GBM is good for 95% of general predictive modeling cases. You can make a GBM model a little bit better (i.e. for marginally boosting Kaggle ranking it may or may not be worth it for you).The reason is GBM can only approximate interactions and nonlinear transformations. So when GBM is approximating the transformations or interactions it can’t differentiate noise versus signal so inevitably you pick up noise. So if you know there are strong interactions it’s better that you explicitly code them into the GBM. GBM’s feature engineering is just a greedy space search so that it can’t really find very specific transformations. So for example a lot of times when we build a sales forecasting model the best sales forecaster is actually the previous time period’s sales. If you want a GBM to automatically discover that I think actually that’s pretty difficult so you should code that definitelyThe second most often used tool by me is linear models like generalized linear models or regularized generalized linear models. I used to use glmnet very often it’s also a very popular R package. So from a methodology perspective glmnet is actually all the generalized models are kind of opposite of a GBM. So all the generalized linear models are global models assuming everything is linear and all the tree models are local models assuming everything is staircase shaped. So they compliment each other very well.So linear models become quite important when your data is very very small or when your data is very very big for different reasons. When your data is very very small you just don’t have enough signal to support a nonlinear relationships for interactions. It’s not that there isn’t; there always are nonlinear relationships in interactions. But if you don’t have enough data to detect them it’s better to stick with a simpler one so that’s when the linear model works well. On the other end of the spectrum and you really have billions and billions of observations only linear models are fast enough for you so everything else will never finish in our lifetime so that’s why go back to linear models when the data is very large. So the linear models when you do a model blend (GBM and glmnet complement each other well) you get a very nice boost in performance. The downside of glmnet or anything similar to this kind of model is that it requires a lot of work. All the things GBM does for me automatically I have to deal with them myself–I have to deal with all the missing values all the outliers all the transformations and interactions. It really takes a lot of time.And the last thing I want to cover is regularization. My personal bias is this: In this day and age if you are building linear models without any regularization you must be really special. Always regularize. It’s required. There are two very popular regularization approaches one is L1 one is L2 so basically L1 give you sparse models L2 just makes everything’s parameter a little bit smaller. The sparsity assumption is a very good assumption. The book  describes why that is the case. If your problem is sparse in nature assuming sparsity will get you a much better model than not assuming it. If your problem is  sparse in nature then assuming it will not hurt that much. So the lesson is always assume sparsity. Unless its a problem you  is not sparse.So there are actually problems that we know is not sparse like text mining or if you’re doing zip code. If you think ‘what is sparse?’ like there are three zip codes that are special they have some kind of parameters and everything else is the same. That’s the same thing in text mining if you assume that 500 words is very special and all the other 500 thousand are not special. So in that case when you’re doing text mining or doing high cardinality categoricals that’s where you  assume sparsity. But other than that it’s simple assume sparsity.First I admit that I’m not a very good text miner if that’s a word. I Google things and I read whatever other people write and try to follow their examples. My approach to text mining is the simpler stuff usually works better. As far as we have come in Machine Learning the Ngram and Naive Bayesbased approaches actually work surprisingly well and a lot of times actually it’s quite hard to do better. Here on text mining my view is make sure you get the basics. The basics such as Ngrams and also the trivial features such as how long the text is how many words are in the text and if there is any punctuation those kinds of features are actually very important.The last one is: a lot of problems that are heavy on text are not necessarily driven by text. This year’s KDD Cup is an interesting example. The KDD Cup it’s a problem it’s actually sponsored by a New Yorkbased company. They have a website where you can donate to your local school’s teacher’s projects. So if you’re a teacher in an elementary school or a high school you can post credits say “I”m doing this reading club with my students I need 100 dollars to buy this carpet.” People really host those! If you sympathize with that effort you will say I’ll give you five dollars I’ll give you ten people can add together doner will chose. They setup the competition to see which project is more likely to get attention from people and more likely to be funded.Teachers will put up essays and summaries of their projects. So it’s very nicely written—some people write very nice essays not everybody. All text data are provided also with the cost and type of project. Whenever you have unstructured data the text always is way bigger than the structured data so that I always feel that I’m obligated to use the text a lot. But it turns out that usually some other features are much more important. For example projects that cost less are much more likely to be funded. That doesn’t need the essay to tell you. If you ask people for one thousand dollars for a laptop no essay will save you. So it’s much easier to ask people for math books. That’s what the data tells us.If you do a competition on Kaggle then you will learn this. It’s almost never that a single model will win a competition. People always build a variety of models and blend them. Blending is just a fancy word for a weighted average of models. You can do a little bit fancier than weighted average but not that much. I usually just use weighted average. My rationalization for why blending works is because all of our models are wrong. As George Box said “All models are wrong but some are useful.” I would like to think all my models are wrong but  useful. “It’s very hard to make predictions especially about the future.”When we study regression analysis or something similar in school we always start or models with Independent Identical Distribution (IID) Gaussian errors. In my entire life I have  seen a real data set with that distribution. It never happens. I don’t know where it should happen but it never happens. So our assumptions are never right. So whatever you assume unless you’re doing a simulated data set everything is wrong. But the hope is that our models are wrong in different ways so that when you average them their wrongness kind of cancels each other’s out.There are a few things to keep in mind. One is that simpler is usually better when things are uncertain. If you are not sure just add Model 1 Model 2 Model 3 and divide by three. That’s usually better than taking the parameters unless you have a lot of data. The next one is a very easy and useable strategy which is to intentionally overfit the public leaderboard. And if you do it carefully it might give you a nice boost. So what you do with this you build many different models and you submit them to the public leaderboard individually and then based on their score you keep the better ones and then average them and that’s your blend. So based on the public leaderboard that might give you a nice boost. This model will work better than any individual model submitted. So either that will work better on the private leaderboard or not depend on how much data you have on the public leaderboard. If you have a lot of data on the public leaderboard this will help you because when you’re doing this you are actually implicitly using the public leaderboard data as your training data. So you actually have the advantage over people who don’t do this. A real advantage if the public leaderboard data is large enough. But if it’s  large enough you will do terribly bad.One thing that’s very useful when doing blending is that you are not going after the strongest individual model. You always want a model that works but you want a model that’s different. Sometimes I will intentionally build a weak model. They help a lot when you add them to a strong model. So the key is the diversity—I’m sure HR people will be very happy to hear this we need diversity in model building. This is true it’s actually scientifically proven it’s good.There are many ways to build diverse models. You can use different tools. You use different model structures. You can use different subsets of features. You can use different subsampled observations. You can build a model so it’s weighted. You can build both weighted and unweighted numbers. Usually the unweighted one will work worse. But if you can take 90% of the weighted one and a bit of the unweighted one it will work better than just the weighted. But here is try and build those models more or less blindly; try not to look at the answer. You know the public leaderboard is always there and you always want to test them. Try not to use that as a guide to filter out the models unless you are confident that the dataset is large enough. It’s a total judgement call based on the noise and the size of the data.Let me try to address this question objectively although it’s not possible—obviously I like competitions. But sometimes people ask me “other than being a fun game is there anything really useful from competitions?” First of all let’s acknowledge that it’s a fun game. The fun game itself is entertaining so we shouldn’t dismiss a fun game as something not useful. But beyond that there are two ways of looking at this.The model in a competition only covers an actually very small portion of the necessary work to make the data science very useful in this work. So to make the model really useful we need at least another three pieces. One is that we need to select the right problem to solve. So if you find a problem that’s interesting only abstractly but there is no realworld application there is no immediate value. The next one is we need to have good data. The model is certainly a garbage in garbage out problem. If you don’t have good data you cannot expect to have good models. The third one is we need to make sure the models are used the right way. A lot of the time it’s possible to build very good models but then they are implemented wrong. It happened to me in the past; you have a good pricing model in insurance and when they swap the parameters like x1 parameter goes to x2. And then you figure out this doesn’t work and then say. “oh they swapped it.” And I never asked why they swapped it but they do. With all these three plus the right generalizable model and then you’ll have the right solution. So the fun part of building a model in Kaggle is only a small part we all need to keep that in mind in reality.But aside from that a competition helps us in very many ways. So I have two ways it helps the sponsor. Iif you have a company either a startup company or enterprise. Building a computation for your own data set is very helpful. There are two ways this is very helpful. One is to measure to a degree the signal versus noise in the data so if you put up reasonable prize money needs you can request 99% valid signal and that will be squeezed out by other people on Kaggle. Kaggle people are quite good at that. The second way is [finding out] if there are any flaws in your data so a lot of times it is because of the data collection process issues. You may have predictors that are not real predictors. Such as if a particular field is missing you always have a yes or no as an answer. So if you have anything like that the Kaggle crowd will find it for you so you can go back to fix it. The model is not useful if you have that problem but at least you can fix it.But as a participant I learned two major things on Kaggle one is that I have to build a generalizable mode—just predicting what I know is useless. That discipline is a hard discipline to learn. The next one is to fully realize day to day that there are other approaches and there are other people with better ideas. So this always keeps me on my toes and learning new things. Otherwise it’s really easy for me to sit in my corner and think “Oh I do really nice modelling work.”So let me just give two examples of two competitions I did okay in. One is Amazon’s user access competition. This is one of the most popular competitions. It used to be the most popular competition but recently the Higgs competition has more participants. This one has about 1700 teams participating. I got second place on this one. This is one of the most interesting experience for me. I was the no. 1 in the public leaderboard. But this time I didn’t try to overfit the public leaderboard. Sometimes I do but this time I didn’t. I still lost somehow to another team. Never figured out why.The [challenge was] to use anonymized features to predict if employee access request would be granted or denied. If you work you know for a large company this is quite often; if I need access that folder someone needs to say ‘yes’ or ‘no.’ And all the features are categorical: resource IDs manager IDs user IDs and department IDs. Many features have many many levels. But I wanted to use GBM right. This is how I converted all the categorical data into numerical. There are two ways I used to encode categorical features into numerical. One is how many that levels appear in the data. You can do this for all your categorical features and all the interactions for categorical features. And another one is using the average response—the average y for that level as a predictor. So here you have to do something slightly more complex than doing a straight average because a straight average will lead to overfitting on certain levels.Beyond those encodings the final model is a linear combination of three different kinds of trees plus the glmnet and other features plus two subset features base trees. So this is a blend that I tuned manually. At that time I did not fully understand the online learners like a Vowpal Wabbit. So I didn’t use it. In retrospect if I had used them I would have derived a better model because they have a very different algorithm. This competition had a particular requirement that everybody had to publish their code. So my code is up there on the Github. Do  evaluate my software engineer skills when you read the code.So here is something that is very easy to do actually for encoding categorical feature by doing the mean response. This is a very simple data set we have a categorical feature the UserID And for the level A1 we have six observations. Four of them are in the training data and two of them are in the test data. For the training data you have the response variable then 0 1 1 0 and in the test data you wouldn’t have the response variable. So here I show how to encode this into a numerical. So what you do it to calculate for the training data the average response for everything  that observation. So for the first one it’s 0. For this particular observation there are 3 other observations in the same level there’s number 2 3 4. And there’s two out of three (that’s why it’s 0.667). The second one it also has 3 other observations but it’s 1 0 0 (so it’s 0.333). Do not use itself. If you use it itself then you will be overfitting the same data.Sometimes it also helps to add random noise on to your training set data. It helps you smooth over very frequentlyoccurring values. For example if you do have this you will see that [these numbers can be thrown] into GBM GBM goes nuts because it treats them as special values. So if you add small noise on top of that it actually makes it a little more real from a data perspective. You do not need any such special treatment for the testing data. Testing data is a straight average of the response value for that level for the training. So two out of four (that’s 0.5). So the basic thing I need to do to use categorical features in GBM. This is much easier to do compared to building a separate ridge regression and I do this very very often. That’s Amazon. The amazon competition is a very simple data set. Mostly do feature engineerings on anonymized categorical features. And the response is 1 or 0.So the Allstate competition is unique in a different way in that it has very structured target variables. So it has seven correlated targets so you have A B C D E F G which represent the options people choose when people buy personal auto insurance policies from Allstate. So you can choose like you want comprehensive coverage etc. So this turned out to be a very difficult competition for two reasons. Actually a lot of people hate this competition a lot. One [reason is that] the evaluation criteria is all or nothing. So you have to predict all seven correctly to get a point. If you predict anything wrong it’s zero. So basically partial answers get no credit for this one. We had a long debate on the Kaggle forum but the Kaggle people didn’t budge. I heard this story even Allstate was upset with us so even the sponsor wasn’t happy with us.So the [challenge] is to predict the final purchase option based on earlier transactions. But whichever option they choose for their last known transaction is actually a very good predictor for the final purchase option. So the model is that if you just use the trivial “last quoted” as the predictor no change. It’s right about 53.269%. You really need a lot of decimals to see the difference that’s why they’re decimals. And then I got the third place in the competition—I was able to improve that by 0.444% and then another one’s solution improved that by 0.03%. I’m proud to say that this is statistically significantly better. I don’t know if in business it is significantly better but statistically speaking it’s significant.The challenge to this is actually mostly on the targeted variable side so that A B C D E F G are not independent of each other actually every one of them can be predicted very well. More than 90% accurate. But if you put them together they have actually very interesting structures. And then the two challenges are to capture the correlation and not lose the baseline.So I build chained models for the dependency. So I first build a freestanding F model then assuming you know F you build a G model etc. And you can actually change the order of models to put your free models first and your independent models later. And this works quite well. This is a little bit more theoretically appealing because it’s a very systematic model. So the next one is so as not to lose the baseline we build twostage models. We build one model to decide: either use the baseline or the prediction. And then you want to use your prediction then use your prediction otherwise stick with the baseline. So at least for the dummy cases you do as well as the baseline and for where you can find improvement you improve it. So I think that’s how you can improve upon very strong baseline.To finish there are kind of a few trivially useful pointers. If you really want to learn how to build generalized models participate in the competition. Just watching it doesn’t count. Form my own perspective the more frustrated I am doing it the more I learn after. You really get a good learning experience that way. If you’re just observing as a spectator you really don’t get into the problems. They also post links to papers to books. I need to admit that I have no PhD my education was in engineering related but neither in statistics or computer science. So I learned a lot just reading books and Kaggle forums. One book that really helped was Elements of Statistical Learning. It’s highly recommended. It’s like the bible of machine learning. It’s also freely available—legitimately—on the internet. Thank you.,NA,"Many thanks to Jared Hatch from ThoughtWorks for sponsoring us the wonderful space.Many thanks to Owen Zhang for giving us A GREAT TALK! It was certainly full of joy and satisfaction!
(Photo: As a tradition of the meetup we voted for the outcome of the meetup. Thumbs are up in the air means "" we are super happy and satisfied""!)Original event announcement: Owen Zhang is the Chief Product Officer at DataRobot. Owen spent most of his career in the Property & Casualty insurance industry. Most recently Owen served as Vice President of Modeling in the newly formed AIG Science team. After spending several years in IT building transactional systems for commercial insurance Owen discovered his passion in Machine Learning and started building insurance underwriting pricing and claims models. Owen has a Masters degree in Electrical Engineering from the University of Toronto and a Bachelors degree from University of Science and Technology of China. No. 1 Kaggler Owen Zhang will share a few tips on modeling competitions!!This is an opportunity to learn the tips tricks and techniques Owen employs in building worldclass predictive analytic solutions.He has competed in and won several high profile challenges and is currently ranked No. 1 out of a community of over 240000 data scientists on Kaggle.The following transcript is kindly contributed by NYC Data Science Academy Bootcamp Students from FebApril 2015 Cohort including Andrew Nicolas James Hedges Alex Alder Sundar Manoharan Malcom Hess Punam Katariya Korrigan Clark Sylvie Lardeux Philip Moy Julian Asano Jiayi (Jason) Liu Tim Schmeier.Special thanks go to James Hedges and Alex Alder for managing this transcription project with their classmates.(Photo: Owen Zhang stayed after the meetup and answered more questions from the audience.)(Photo: Owen is sharing his secret why he could win many Kaggle competitions.)(Photo:  Owen is answering questions from the audience.)(Photo: Owen is explaining the technical tricks to the audience.)(Photo: Founder of NYC Data Science Academy Vivian S. Zhang and Advisor of NYC Data Science Academy Owen Zhang took a photo after the event.)",NA
Overview of Scikit-Learn (Machine Learning in Python),78,https://nycdatascience.com/blog/meetup/overview-of-scikit-learn-machine-learning-in-python/,Regression:Classification(Discriminant Analysis):Classification(Tree based model):Classification(the others):Classification(Nearest Neighbors) :Classification(Naive Bayes):Unsupervised Learning:Feature Selection:CrossValidation:Model Selection:,NA,Vivian will go over the main categories of all the popular algorithms and introduce a few user cases of 3 mostly voted methods.The options are: linearmodel.LinearRegression linearmodel.Ridge linearmodel.Lasso linearmodel.ElasticNet lda.LDA qda.QDA tree.DecisionTreeClassifier ensemble.RandomForestClassifier linearmodel.LogisticRegression svm.SVCneighbors.KNeighborsClassifier neighbors.RadiusNeighborsClassifier naivebayes.GaussianNB naivebayes.MultinomialNB naivebayes.BernoulliNB decomposition.PCA cluster.KMeans cluster.AgglomerativeClustering featureselection.VarianceThreshold featureselection.SelectKBest featureselection.SelectPercentile crossvalidation.KFold crossvalidation.StratifiedKFold crossvalidation.crossvalscore crossvalidation.traintestsplit linearmodel.RidgeCV linearmodel.LassoCV linearmodel.ElasticNetCV gridsearch.GridSearchCV[youtube]http://youtu.be/hNwUq0Z70Zk[/youtube],NA
Speed Cameras: Revenue or Public Safety? We'll get you up to speed in a Flash!,78,https://nycdatascience.com/blog/student-works/speed-cameras-revenue-or-public-safety-well-get-you-up-to-speed-in-a-flash/,g  plot(ggmap(nycmap)+geom_point(aes(xlon ylat size  log(count) color  'red') data Cam)+theme(axis.text.xelement_blank() axis.text.yelement_blank() axis.title.xelement_blank()axis.title.yelement_blank()legend.position'none')+stat_density2d(aes(xLon yLat fill  ..level.. alpha..level..) geom'polygon' dataAccidents.lastyear))+scale_fill_gradient(limitsc(1:60))ggplot(xaes(xDateyAccidents))+geom_line()+geom_point()+ylab('Pedestrian Vehicle Accidents') +theme_bw()+xlab('')camandtixsum  function(x){ y  data.frame() for (i in 1:length(x)){ y[i1]  length(grep(x[i] Cam$Address)) y[i2]  sum(Cam$count[grep(x[i] Cam$Address)]) } y  cbind(y x) return(y)}byBorough,NA,"Then the centralTo understand if these speed cameras were distributed to increase school children's safety the camera map was overlayed with the locations of NYC grade middle and high schools (represented as black dots). As shown the highest density of schools is in Manhattan and the Bronx two of the boroughs with fewer speed cameras. Clearly the cameras were not distributed to maximize the safety of school children.Perhaps the speed cameras were deployed to increase the safety of ALL pedestrians. To explore this hypothesis the camera map was overlayed with the density of pedestrian/vehicle collisions in NYC in 2013 the year prior to camera installation. Vehicle/pedestrian collisions occur most frequently in Manhattan a borough with fewer cameras. This suggests that the largest potential impact would be made by placing the majority if not all of the speed cameras in Manhattan not the outer boroughs. In fact Brooklyn and Queens have a similar number of pedestrian/vehicle collisions as Manhattan (see below) but Brooklyn and Queens are 3 and 4X larger indicating that if speed cameras reduce pedestrian/vehicle collisions Brooklyn and Queens would require 3 and 4X more cameras than Manhattan to reduce accidents by the same number.After discovering that camera placement is not designed to maximize the safety of either schoolaged children or pedestrians a revenue focus was considered. By NYS lawspeed camera placement is only allowed within 1/4 mile from any school building. The placement of a camera outside that radius may indicate city officials are attempting to maximize revenue by issuing more tickets and betting residents will not contest the ticket's legality. To investigate the most profitable camera was selected and itsdistance to the nearest 3 schools was computed.Note the camera's location situated on an 8lane roadway with timed traffic lights just two blocks from the nearest exit on the BrooklynQueens Expressway. Visually the camera location looks close to the school on 55th St & Skillman Ave. However the computed distance is > 1/10 of a mile outside the legally allowed distance. This is a clear sign of a revenue strategy.  After examining the distribution of speed cameras safety was clearly NOT the primary goal of this program. Revenue concerns seemed to have outweighed maximizing pedestrian safety. However if cameras DO prevent pedestrian/vehicle collision they may still be useful  even though they are deployed in suboptimal intersections. This is the focus of the next section.First we will examine the effect of the speed camera program citywide and then proceed to analyze the differential effect the speed camera program has had on each borough. The data shows ~10% reduction in traffic accidents citywide after camera installation in the period from Jan 16  Jun 20 2014 over the same time period in 2013. Traffic accident data is only available on NYC Open Data as far back as Aug 2012. Pedestrian/vehicle collsions over 5 month intervals were summed from Aug 2012  Jun 2014 and visualized in a line plot (Dec 2013 was not included due to interval size). The trend is apparent pedestrian/vehicle collisions were decreasing before the installation of the speed cameras and continue to decline at the same rate after installation. Fatal accidents also declined during the same 5 month period from 57 in 2013 to 39 in 2014. This is not surprising given the decline in accidents overall.If speed cameras are associated with reduced pedestrian/vehicle collisions we would expect the frequency of collisions around a camera to decrease much more rapidly than inareas without cameras. Each camera should ""carve out"" a spot from the accident density if they significantly reduce the rate of accidents. The pedestrian/vehicle accidentdensity from 2013 (left) is shown next to the density after camera installation in 2014 (right). The shape of density profile is almost exactly the same strong evidencethat cameras do not influence the frequency of pedestrian/vehicle collisions.While there isn't any association of cameras with pedestrian accident frequency citywide perhaps the boroughs with the most cameras have experienced a reduction in accidents that is ""masked"" in aggregate by increases in accidents in boroughs with fewer speed cameras. The following script summed the number of cameras in each borough and the number of tickets from all cameras in each borough. I then created a contingency table by merging the data with accident numbers from the Accidents data frame. The contingency table of tabulations is shown below.Borough Cameras Tickets Peds.Killed2014 Peds.Injured2014 Peds.Killed2013 Peds.Injured2013 1 STATEN ISLAND 8 7505 2 112 1 153 2 BRONX 7 5216 3 646 9 728 3 MANHATTAN 8 10479 6 1140 12 1238 4 QUEENS 13 35985 10 866 16 934 5 BROOKLYN 18 25628 15 1405 13 1561Using these data the impact of the cameras on each borough was visualized. The first plot shows the change in the number of pedestrians killed in vehicle collisions byborough prior to and after camera installation. The size of each bubble in 2014 represents the number of tickets residents received. The number of cameras and ticketsappear to have no relationship with the number of fatal accidents. The Bronx had the largest %decline in fatal accidents but has the fewest cameras and tickets. Brooklynresidents received the the second highest number of tickets and have the most cameras of any borough but was the only borough to see its numbers increase in 2014. Pearson's Xsquared test provided additional evidence that cameras are not associated with the rate of fatal collisions (Xsquared  4.4056 df  4 p  0.3539).A similar analysis was conducted with collisions that were not fatal each borough shows a decline consistent with the 10% overall decline already noted as compared to2013. However the accident rates decline similarly for each borough. If the speed cameras did prevent collisions we would expect to see a much more dramatic decline inBrooklyn and Queens the two boroughs with the most speed cameras and highest number of tickets issued. Again Pearson's Xsquared test was not significant (Xsquared  3.1445 df  4 p  0.5339).In the first 6 months of operation the speed camera program has justified the transfer of 4.2 million in wealth from NYC citizens to local government coffers. The program appears to be revenue focused as the camera locations fail to conform to distributions that would optimize pedestrian safety. Additionally it appears that at least one camera operates outside the boundaries allowed by law further corroborating this conclusion. Furthermore this study has found no evidence that speed cameras reduce  collisions as suggested by the government entities and speed camera vendors. Months after the speed camera program debuted NYC lowered its speed limit by an additional 5MPH. This  has also been interpreted as a revenue maximizing strategy which may result to a transfer of wealth in excess of $12M from citizens in 2015.",NA
Steps toward recreating The Facebook IPO plot,78,https://nycdatascience.com/blog/student-works/steps-toward-recreating-the-facebook-ipo-plot/,"big white blob appeared in the center of the map. Some of the outer edges of the blob vaguely resembled the continents but it was clear that I had too much data to get interesting results just by drawing lines.""",NA,"As data scientists we are quite familiar finding and mucking through data. We merge split clean and analyze the data in order to draw our final conclusions. Our daily workflow can feel comfortingly logical at times even cut and dry. However every now and again we are reminded of the art of our craft. As  it is increasingly important to present data with visual impact. Resources like the New York Times present data in a  engaging the reader and enabling selfguided exploration.Anyone who uses R should be familiar with the graphic created by  and included in the original Facebook IPO back in 2012. This was brought up in one of the first classes of the NYCDSA bootcamp as an example of the prowess of  a popular graphics package for R. This is the kind of graphic that would inspire anyone to learn more about the features of this powerful language and associated package. In particular three things speak to me:I was challenged to reproduce the look and feel of this plot using ggplot2. There is a plethora of resources online that I made use of to do this and probably many more that the reader can find if she wants to become more familiar with the capabilities of ggplot2. In particular I draw heavily upon the tutorials by  and . If you haven't heard of them please go check them out! They are fantastic resources and were very helpful. The rest of this post will focus on some of the elements above and how to reproduce them using R.Clearly the Facebook universe of connections is vast enough to produce the plot above. In fact in his original post Butler notes his decision to plot only unique pairs of cities connected by Facebook friends rather than every every connection. ""A  Without timely access to the breadth of worldwide Facebook connections I decided to look at US airport locations and trips originating from some of the nation's busiest airports to their domestic destinations. More detailed code for how I wrangled the airport data can be found at the link to the .Having loaded the data into a table of start and end coordinates for each trip I needed to calculate great circle arcs (traces of the largest circle that can be drawn between two points on a sphere) for each trip. For this I used the geosphere package which takes two points and a step number as inputs and outputs the trace of the great circle between those two points.This would have to be done for every unique pair of combinations of airports to their domestic destinations. I used a for loop and created a path ID so that I could use ggplot2 to plot all the points.All that was left was to plot the results colorizing the paths and airport locations in a nighttime theme using some ggplot2 options:This is a small start towards the beautiful piece that Paul created. I wanted to get a little creative so I decided to take advantage of the vector output feature from R and ggplot2. I opened up Adobe Illustrator and added a subtle glow effect to each path. I even changed some of the colors which is something I could have easily done in ggplot2 of course.I don' t show these plots to highlight any shortcomings of ggplot2. Rather R output can be a beautiful thing (as Butler shows) or a great starting point for artistic effects that might aid impact.I set out to see what would be required to achieve the neat effects in Paul Butler's Facebook plot and covered a few of the basic elements that when combined with larger data sets could certainly provide a similar look and feel. While working on this project however I discovered a really nice blog post over at Spatial.lyIn it the author provides several examples of Raw R output and how his collaboration with a graphics designer enhanced the impact of the visual information without damaging the takeaways. While it is true that performing these sorts of postprocessing steps on a raw plot outside of R violates some form of reproducibility of the figure itself I feel that the embellishments discussed in the Spatial.ly article serve a higher purpose of making these informative visuals even more memorable.",NA
Achieve Real-time Results with Big Data through In-Memory Computing,79,https://nycdatascience.com/blog/meetup/achieve-real-time-results-with-big-data-through-in-memory-computing/,"Speaker Bio:
Nikita Ivanov is founder and CTO of GridGain Systems a leading inmemory computing platform. He has over 20 years of experience in software application development building HPC and middleware platforms contributing to the efforts of other startups and notable companies including Adaptec Visa and BEA Systems. Nikita was one of the pioneers in using Java technology for serverside middleware development. Hi is an active member of Java middleware community and contributor to the Java specification.",NA,"We came together for a night of networking with Big Data professionals and learning about leading edge technology that is pushing the envelope of what Big Data can do.We gave an overview  of general inmemory computing principles and the drivers behind it. Some of the topics covered but not limited to were:[youtube]http://youtu.be/hG9viqRGmvM[/youtube]
",NA
"Hack Session for NYTimes Dialect Map Visualization, Sponsored by OReilly Strata",80,https://nycdatascience.com/blog/meetup/hack-session-for-nytimes-dialect-map-visualization-sponsored-by-oreilly-strata-2/,"Speaker:
Vivian Zhang is CoFounder & CTO of Supstat Inc a Statistical Consulting firm of top Data Scientists. Vivian is a data scientist who has been devoted to the analytics industry and the development and use of data technologies for several years. You can follow her on Twitter at @ @ @ or #nycdatasciPreviously:",NA,"
Josh Katz Graphics Editor at The New York Times talked on June 10 to discuss one of his projects the ““.  He discussed how spatial statistics data visualization and information design all came together.(See )The Dialect Map project was the Times’ most popular content of the entire year of 2013. From the survey results Josh created a series of maps depicting regional dialect variation in the US which set off a sensation on the Internet and in popular media. From this he developed the New York Times’ dialect quiz turning what began as an exercise in spatial statistics and data visualization into some of the mostviewed content in NYT history.In his talk Josh delved into the background and development of the dialect maps and quiz–the story in general as well as the math behind them. He talked about how data visualization and statistics could be used to illustrate the hidden contours of our world extricating narrative from numbers and turning data into stories.Vivian will tell you all you should know about choropleth map in R to make Dialect map. We will start from the data scraping step to build the database of the survey then we will march to the shiny development including functions for map plotting. Finally we will establish a fully developed shiny application demonstrating the dialect map project.[youtube]http://youtu.be/pyBHo8euvE[/youtube]",NA
"Minority and Non-Minority Business Creation in NYC, 2005-2013",80,https://nycdatascience.com/blog/student-works/minority-and-non-minority-business-creation-in-nyc-2005-2013/,H0: Medians of MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita are equivalent.H1: Medians of MBE  Minority cap and NonMBE  NonMinority cap are NOT equivalent.H0: MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita could be representative of the same set of data.H1: MBE Incorporations per Minority capita and NonMBE Incorporations per NonMinority capita could NOT be representative of the same set of data.,NA,"Contributed by Shelby Ahern. Shelby took our Data Science with R  Beginner Level  class with Vivian Zhang in June 2014. This post was based on her final project submission.The focus of this exploration was reviewing the level of new business creation in New York City by minority and nonminority populations from 20052013. In this post I’ll review the process of preparing the data and conducting two hypothesis tests on the primary measure   the number of incorporations per capita. Data sources and definitions may be found as notes at the bottom of this post. Also the I created a CSV file for the Active Corporations and XLSX files for the MBE and Population files respectively. I then created data frames combining the data from Active Corporations MBE Directory and population files by borough and year to prep each data frame for four new additional columns of calculated measures. The calculated measures are:Here’s an example of a data frame for Manhattan:> ManhattanDataframes for  and .Immediately we see that the number of Incorporations and MBE Incorporations per year differ by at least a factor of 1000+. This is also reflected in the per capita measures. Even when compared to minority population alone the number of MBE incorporations is tens of thousands of times less than the number of NonMBE Incorporations per NonMinority capita.Graphing the per capita measures for the boroughs shows a similar disparity in the other boroughs as well  in fact the MBE per capita dimensions don’t even register within the vertical scale.Here is the code for the graph:
Other findings from the borough comparisons show:To investigate MBE incorporation in the boroughs I created a second graph of just the MBE Incorporations/capita:The glaring difference in the scale between the number of MBEs and nonMBEs lead me to consider the MBE data specifically the process and purpose of MBE certification. MBE certification is marketed by the city and the state primarily as a program that facilitates opportunity for competing for government contracts. That aim may not be relevant to the majority of businesses which is likely a significant explanation of why the MBE numbers are comparatively low. Further the certification process is comprehensive  requiring personal financial statements for example which may be a barrier to other owners pursuing the certification. In short MBE certifications do not represent the universe of businesses in NYC that are launched and operated by minorities.However without any other (as now) public data sets that include the race of the primary owner/controlling partners we’ll proceed with the analysis of new business starts by owner race with the data we’ve been using (let’s consider it an academic exercise).Q: Do the Frequency of Incorporations vary significantly between Minority and NonMinority Populations?The approach:Having determined which dimension will serve as the common measure (corp per capita; Minority vs. NonMinority) let’s take a look at the data distribution for both series. The purpose for doing so is to determine whether the data is normally distributed which is an assumption required by some standard means tests like a Z or Ttest.Here’s the code and the output for a 3 ways I utilized for reviewing the MBE per Minority capita data for normality:1. Histogram. Occurrences of MBE Incorporations per Minority capita for All Boroughs 20052013.
We can see that the data is skewed to the “left” or rather what would be considered below the where the mean would fall on a standard bell curve.2. QQPlot Occurrences of MBE Incorporations per Minority capita vs. Normal Distribution (All Boroughs 20052013)The plot shows that the points do not fall along a linear pattern which is the criteria for evaluating the normality of the data.3. ShapiroWilk Test
Lastly I conducted a ShapiroWilk test for normality and the pvalue returned was less than the test value of .1. Thus the null hypothesis  that the data is normally distributed  is rejected.Concluding that the data for MBE Incorporations per Minority capita is not normally distributed (nor was the data for NonMBE/NonMinority capita) I sought population comparison tests that did not require parametric datasets. Mood’s Median Test and the MannWhitneyWilcoxon Test were two tests that satisfied those conditions and were applicable.Mood’s Median TestA nonparametric test where the null hypothesis of the medians of the populations from which two or more samples are drawn are identical. (Wikipedia)
The null hypothesis was rejected thus we can conclude that the median(s) of the MBE Incorporations per Minority Capita data set across all boroughs and years is significantly different from the NonMinority Incorporations per NonMinority capita in the same geographies during the same period of time.MannWhitneyWilcoxon TestA nonparametric test of the null hypothesis that two populations are the same against an alternative hypothesis especially that a particular population tends to have larger values than the other. (Wikipedia)
The null hypothesis was rejected which we interpret as validation that the number of percapita incorporations per year by minority status could not be produced from the same population.There is a significant difference between the levels of new business creation by race but within this exploration is primarily attributable to how minority entrepreneurship was defined (by MBE certification). Without other ways of tracking MBE business development MBE certification serves as an inaccurate measure. Separately without broadening the objective of the MBE certification program to include other stronger incentives for prospective participants the City may not see increased rates of participation nor the MBE certification as an effective tool for encouraging minority business creation.The data include:Definitions:",NA
"Tableau Workshop IV: Beginner Level, Maximize Your Visualization Effect",80,https://nycdatascience.com/blog/meetup/tableau-workshop-iv-beginner-level-maximize-your-visualization-effect/, What language is Tableau written in? According to their own site (and Wikipedia..) it’s a new language they created called VizQL – combination of SQL and MDX for dealing with both querying and visualizing. Their conference paper: ­,NA,Many thanks go to SumAll for space sponsorship!Special thanks go to Sasha Bartashnik for giving such a great workshop!NYC Data Science Academy is offering a relative course:RSVP Click  to subscribe to our channel! !Curious about what you can do with Tableau (and what Tableau can do for you)? Learn how you can use Tableau as a tool to explore and communicate your data using effective visualizations.Sasha Bartashnik is a data scientist who focuses on understanding behavior through data analysis and compelling visualization. She has a degree in math and a masters in management science from the University of Cambridge with a side of ballroom dance. Sasha is currently teaching a beginner level class in Data Visualization with Tableau at the NYC Data Science Academy.We walked through tips and tricks to understand the data you’re working with and prepare it for analysis (outliers correlations residuals). Sasha gave a brief overview of the wide variety of tools available that enables you to use your visualization to explain and persuade (dashboards parameters filters advanced visuals). Along the way we paid attention to principles that guided best practices in creating effective data visualizations.If you want to follow along download  or have Tableau Desktop installed. And now Tableau v.8.2 is also available for Macs. Some more info on connecting to unsupported data sources through the API: ­,NA
Github Workshop II: Common Git Workflow (given by Github employee),81,https://nycdatascience.com/blog/meetup/github-workshop-ii-common-git-workflow-given-by-github-employee/,covered the common Git workflow including  publishing a project on GitHub and how to collaborate on an existing repository.,NA,"Do you want to be an open source contributor? If you're interested please follow Aidan Feldman one of the most well known hacker in NYC to get familiar with Git!Aidan Feldman is Education Hacker at GitHub by day dancer by night.  On the side he organizes  volunteers at  bikes and teaches Advanced JavaScript at NYU. In this workshop Aidan See source code here:Here are a bunch of resources for learning Git/GitHub:
In particular you could also check out Try Git to review the basics:
In addition you are welcome to !",NA
Learning R: Analyzing the English Premier League (III),81,https://nycdatascience.com/blog/student-works/learning-r-analyzing-the-english-premier-league-iii/,. This part is going to concern with the fun part of analysis as in all the data has been cleaned and we're ready to look at team performance. You can see all code examples and the sample data on my  GitHub page.daplySeasonratiosdaply.(Season Visiting)rlerleeach cbindeachGoalsOppGoalscolwiseyGoalsOppGoalsxSeason,NA,"
Contributed by Bryan Valentini. Bryan took R003 class with Vivian Zhang(Data Science by R Intensive beginner level) in MarApr 2014 and did great in class.The post was based on his week 4 Homework submission.This is a followup post to “Analyzing the English Premier League” Let's keep things simple and just use the console to explore the data we cleaned up in part 2. We want to get a sense of it before building some visualizations. We will reload the  file which has all the cleaned up data from the Arsenal perspective. Specifically we're looking at the actual Premier League matches from 2001 to 2012.Here we see the table summary which might differ somewhat if you've preexcluded other competitions when executing part 2. Otherwise we can clean focus on that subset of data really quickly by running some filters.As a warm up let's look at the calculation below of ratios of wins losses and draw. We use the  function from the  package to group the fixture results by  and then run the  function we defined over each grouping.Notice how  didn't have any losses? What a year that was!  Similarly we want to see how a team performs and outright wins at home versus away. That's pretty simple we just add another grouping criteria to the  function (see ). By summing the count of wins over the length of the input vector we get the ratio of wins to total games.If we want to look at the number of wins losses or draws in a row we can use the  function to calculate the runlength encoding of the groupings for each type of game outcome. For example if we look at the 2001 season's data and we run the  function on the $WinLoss column then we will see the following output.Runlength encoding is a simple data compression algorithm but here it is used to calculate the {WinsLossesDraws}inarow. Looking at the vector of factors above you can see the string of Ws at the end of the season. These Ws could be summarized by string '13W' instead compacting the representation of the data or in other words telling us that there was a streak of 13 won games. Turns out this is the longest win streak of the season and a good candidate to see what happened late in the season in terms of players on the field or strategies used by the the manager Arsene Wenger ('the Frenchman'). Here we see the entire encoding:Packaging this into a helper function we want to process each season and pull out the streaks for wins losses and draws respectively.A common complaint about soccer especially from Americans is that there is very little scoring. This is a fair point but only if you care about that. Watching the beautiful game is more about watching the skill of the team or the individual talents. Still the really good teams can average at least one or two goals per game and in Arsenal's case it consistently averages above 1.8 goals per game as the following analysis shows.What about fan attendance? Can we gage how consistently fans attend home versus away games? We'll use the  function from the  package to calculate the mean standard deviation max and min function. Rather than manually calculating each inside a helper function and manually  the results the  does this automatically for use.Looking at the mean data above we see that 2006 had a huge jump in attendance at home. Curious I checked Wikipedia and turns out that their new stadium opened on 22 July 2006 with a new capacity of 60338 seats. Taking a step back we see that a lot of information can be gleaned by just examining the numbers. On the other hand it's not always the ideal way to learn about data. Eventually you want to visualize it to help guide the nonobvious exploration. In the next section we'll look at few examples on how to do visual analysis.Explaining how to completely use the  package is outside the scope of this post but many tutorials can be found online and in all of the Rbased books. The main attraction of using this package is the themes that are practically ready outofthebox with the additional  package. For this entry we'll stick to one theme but you can check out the help section on the package to see the various options like the ""Economist"" theme the ""Wall Street Journal"" theme the ""Edward Tufte"" them just to name a few.Let's assume we want to look at Arsenal's mean goal performance vs all other opponents. We want to calculate the columnwise mean for Arsenal's  versus other teams . Rather than pulling out the individual columns we use the  ('column wise') functor to help us build another function that then calculates the respective means.In the code above I've introduced some spaces to make it clear how we treat each column () for all rows (). The graph this produces is:From this we can see that overall both the Arsenal defense and the offense seem to do their jobs well. Towards the later years we see a worrying trend in the opponents able to score more goals on Arsenal so there might be problems in the back line. Let's dive into this issue a bit. Let's look at which opponents are better against Arsenal. We'll plot the ratio of losses versus the goal difference to look at how often a team beats Arsenal and by how many goals.At first looking at the top right corner of the graph we see the opponents that consistently score more goals or about the same number of goals that Arsenal scores on them plotted against the opponents' winning percentage. We see the usual topofthetable teams with Manchester United and Chelsea but I almost fell out of my chair when I saw Swansea and Sheffield United. No offense to those teams they have loyal supporters but that graph didn't make sense at first. Thinking an error in my calculation I stared at the data for a bit until it dawned on me that I wasn't taking into account the EPL's relegation system. Teams that consistently perform poorly are booted from the topflight league down to the next rung and the best team(s) from that league replace the relegated teams. Without getting crazy we need to take the number of seasons played against Arsenal into account with respect to performance. I came up with a linear relationship where each loss is multiplied times 3 for heavier weighting of the opponent's win summed with each draw and all this multiplied by the total number of seasons played against that opponent. I'm not sure if this is strictly correct but it made intuitive sense.Looks good! We see that Leeds Sheffield and Swansea now drop in strength against Arsenal where one might expect. Turns out that Swansea only played against Arsenal four times over the 10+ seasons with two draws which explains Swansea's initial strength in the first chart. I glossed over some details about how I built the charts as in the text for individual data points doesn't automatically populate itself like in many other charting packages. This can be a little tedious to line up the label with the data point but it does provide some presentation flexibility. I was also pleased with how nice the charts look and flexibility to switch to a cleaner design like the Tufte theme when needed.This concludes this series of posts looking at how to clean up data and plot some basic charts using . It was really interesting to see how easy the  package makes data manipulation easy with a few lines of code. The important part to remember is that due to how terse this code can become its important to test each step and verify you're getting the results you expect. As my professors used to say ""Think before you code.""",NA
"R workshop XIX: Mixture topics of R-scrap web pages, make treemaps, predict rain",82,https://nycdatascience.com/blog/meetup/r-workshop-xix-mixture-topics-of-r-scrap-web-pages-make-treemaps-predict-rain/,"Laila showed her analyses focused on the restaurant sanitation in New York City. Several main tools were implemented in her project including craigslist pet maps R google fusion table pet finder and shiny application.
",NA,This was a series of final presentations by students of Introduction to Statistical Programming in R class by Vivian Zhang. The students had to present their final work to the audience which consisted of other students and members of NY Open Data group. Each presentation highlighted the problem that was under consideration the code created to solve it and the visualization of results mostly by using tree maps. The instructor (Vivian) introduced her students and their presentations after which a student would present and tale questions.NYC Data Science Academy is offering six relative R courses:RSVP RSVP RSVP RSVP RSVP  Restaurant Sanitation ReportSlides: Video: Laila's PresentationSouth Park Episode Popularity AnalysisJiten presented his project about south park episode popularity analysis. He retrieved the data from IMDB and Twitter and used the combined data for the further investigation.Slides: Bloomberg Global Top 500 Companies analysisAkiko scraped data from the website and did the wondful tree maps.Video:Why you should stop eating at your favorite restaurant?An analysis on the Inspection Results of New York City Restaurants using NYC Open Data and R.Video:A study of bars in NYCVideo:Video:,NA
Where are my single friends? Making the ‘OK Cupid’ Algorithm,82,https://nycdatascience.com/blog/student-works/where-are-my-single-friends-making-the-ok-cupid-algorithm/,Contributed by Harrion Alder. Harrison took R002 class with Vivian Zhang(Data Science by R Intensive beginner level) in MarApr 2014 and did great in class.,NA,The post was based on his final project submission.Harrison is also taking R004 class with Vivian Zhang(Data Science by R Intensive Intermediate level) in May 2014 and expect to do bus traffic prediction modeling and explain why bus run so slow these days.For quite some time my fiancé’s roommate has asked me if I know any single male guys in NYC for her to be set up with. Even though I had over 1000 Facebook friends the majority of which are in New York all I could do was show her a few dozen people who regularly popped up on my Facebook news feed. Scrolling through the entire friend list would be far too inefficient.During the third session of my Intensive Beginner R class (R002) I was amazed to learn that any Facebook user myself included could view and collect Facebook information about their friends using Facebook’s . By using packages in R such as RCurl and RJSONIO I could collect this information and mine it in ways that would make any app developer giddy. My first task: build a program for my friend that gets a printout of my single male friends whose current city is NYC. The query I built returned in a matter of moments a list of my 21 single male friends. This was a good start but I was ready to answer an even bigger question: what is the relationship status of the rest of my friends around the world?To undertake this larger question I collected a list of the relationship status and current city of each of my friends on Facebook only keeping those who had posted both bits of information. Using the Google Maps API I was able to then run a query where I inputted the city name and received the location’s latitude and longitude as output. I then created polar charts and maps displaying the results. Some of my findings were:This project was quite exciting to run through and present at  but I am always thinking of ways to improve it. In particular I’d like to get a more accurate representation of where in NYC my friends actually live. As depicted in the map at the top of the post my friends are centered on lower Manhattan and central Brooklyn. This is because a lookup for “New York NY” will always return the same coordinates in lower Manhattan. However by collecting the location of my friends’ last checkin I could begin to gather location data at the streetlevel. Also I’d like to fill in the missing data for many of my friends by building a method that identifies the most likely city and relationship status of my friends based on their personal network status updates and even photos.Interested in learning more about how I did it? see my codes below or check out my code on .Part I: Facebook API,NA
Authors for Open Data and Technology II: Being A Data Skeptic & Doing Data Sci,83,https://nycdatascience.com/blog/meetup/authors-for-open-data-and-technology-ii-being-a-data-skeptic-doing-data-sci/,This talk focused on the current scope and dangers of data science. The speaker also discussed her two new books.,NA,Click  to subscribe to our channel! !Cathy O'Neil was a mathematician at some point then she became a quant and now she's a data scientist blogger and activist. She recently wrote a  and  for O'Reilly.Cathy talked about things that data scientists should be paying more attention to including various biases normalization mistakes and specific civil rights issues that are resurfacing due to things like online credit scoring that use social network information.Please feel free to read Cathy's as well as her  especially .,NA
Excel Workshop II: Warm Up Session for Excel Intermediate Level Weekend Class,83,https://nycdatascience.com/blog/meetup/excel-workshop-ii-warm-up-session-for-excel-intermediate-level-weekend-class/,"Please have an installed version of Excel ready to go.
Macs: The sample files will work with MACS although the screen interface may vary from the ones displayed.",NA,"Desktop recorded:

Video Camera Recording:
This handson workshop is intended to introduce concepts for working with Excel and VBA. Familiarity with Excel is recommended and necessary to get the most from the presentation. After topics are introduced attendants have a chance to practice with them for a few minutes on a sample workbook.Mary DiPerna (Master’s in computer science) our instructor of Excel class in .More about the Speaker:""I have worked with Excel and VBA intensely for more than 10 years. I have worked with clients to develop complex Excel systems that combine lookup and summary formulas with macros to import data from sales accounting and personnel systems and produce complex data analysis.For example: one spreadsheet was used to determine pay raises and bonuses across multiple departments based on performance statistics imported from the sales and personnel systems and another was used to analysis various cost/profit alternatives based on different style and vendors options.""Topics including: Quick tour of multisheet formulas (3D and multireference) and multifile formulas Named Ranges as a technique for maintaining data integrity in multifile calculation Lookup and Logical functions Overlay and data segmentation charts Data summarization of lists with formulas and Pivot Tables Data analysis with Data tables Macros and Buttons and automation ideas.",NA
Hadoop Workshop II: Run Map Reduce Jobs on your Amazon cloud cluster,83,https://nycdatascience.com/blog/meetup/hadoop-workshop-ii-run-map-reduce-jobs-on-your-amazon-cloud-cluster/,In Hadoop workshop I and II I will walk you through the steps to configure a Hadoop cluster on Amazon EC2 and run two simple mapreduce jobs on the cluster.,NA,"
I was so happy to get many upvotes!Many thanks go to  (Conductor makes the most widely used SEO platform  empowering enterprise marketers to take control of their search performance.)Special thanks go to Caitlin Wilterdink Jon Torodash and Chris Lee (now Googler) for their assistance and for hosting us in the wonderful space!NYC Data Science Academy is offering two relative courses:
RSVP 
RSVP The Intermediate level week 1:Desktop recording:Camera recording:Vivian Zhang CTO and cofounder of SupStat Inc organizer of NYC Open Data Meetup Founder of NYC Data Science Academy(  ) She teaches R and Hadoop.Her data school hires the best working professionals to teach Python D3.js and related Data Science skills. All the courses are designed to teach you employable skills. We teach the skills and toolkits in the class and assist students to do projects of their own choices. Students will show case their projects in this meetup group at the end of their courses.You can follow along to set up your R Rjava and run mapreduce using our 2. I will go over the details:3. Preparation:1) apply for a AWS acct2) log in your acct3) create your key pair4) create EC2 instances5) configure security group6) manage your instance1) You are required to use vi editor. The basic operations are:2) generate your server rsa key for three instances3) configure ""authorizedkeys"" file for three instances4) configure ""hosts"" file for three instances5) test connections among cluster6) install Java5. Hadoop installation and configuration1) download haoop source codes2) configure the environment3) format4) start your hadoop
 ~/hadoop1.2.1/bin/startall.sh
 test whether the hadoop is running
 run ""jps"" on three instances
 on your master node you should see (the numbers will vary) 6919 NameNode 7237 JobTracker 7155 SecondaryNameNode 7445 Jps
 on two slave nodes you should see (the numbers will vary) 7653 TaskTracker 7490 DataNode 7713 Jps5) stop your hadoop
~/hadoop1.2.1/bin/stopall.sh
6. Congratulation! You have your first hadoop cluster!








",NA
Node.js Workshop I: Get Started,84,https://nycdatascience.com/blog/meetup/node-js-workshop-i-get-started-2/,Have node installed on your computer. Highly recommend using a Mac and using brew to install node however  has instructions to get up and running on any platform. If you can run 'node' and 'npm' at your command line you should be all set. ,NA,"Many thanks to Thoughtworks for offering the space and Amy Schantz for setting things up. Amy you are so so so awesome!Thanks to David Bella for giving such a wonderful presentation! Thanks to the Flatiron Group for supporting this event and showing up and hanging out after it!NYC Data Science Academy is offering a relative courses:RSVP Demonstrate what backend JavaScript is with v8Show differences between v8 and nodeSet up a simple web server with ExpressServe up some data to our front end Common tasks and pitfalls working in an asynchronous environmentDavid Bella is a computer programmer turned web application developer. He is an alum of the Flatiron School's Ruby 003 program and holds a degree in Computer Science from the University of Connecticut.node.js: eventdriven nonblocking callbacks usage as a web server with expressExpress: web server in node.js simple routing simple controllers MongoDB: NoSQL databaseClone the git repository locallyStart in the  branch and follow along1) Install NPM PackagesTry to run node server.jsUh oh! Error that you need mongodb package2) Install Mongo DB and Fire It Up!On Mac through :If you don't have brew I recommend getting it but there are manual installation instructions on the On Windows if you don't have a Mac I recommend getting one 😉 but there are installation instructions Open a new terminal window and run the server againYou should get a message from Mongo that you can ignore for now
Open your browser to http://localhost:3000 to verify we are running!3) A Quick Walk Through server.jsHere we require the Express web application framework and a local file that will act as a controller for our routesWe create an  variable to work with as our Express appThe fun stuff starts here with how Express handles routesThis will match any incoming requests to our root path such as  and run the specified functionNotice the two parameters to our functionand . These stand for request and response and reflect the HTTP request that the client sent us and the HTTP response that we as the server are preparing to send back to them.Some common things to pull out of the request are  parameters which we will get to.Some common things to set in the response is the response body or content which we have seen already.In our case we are telling the response to  with the words ""Hello Node!""As you can see there are also  and  functions on the app variable in addition to .These functions correspond to HTTP verbs that are used to implement a REST API.The functions on the  controller that are called should help explain the function of each one but here is a quick table:Notice how the HTTP verbs correspond with   Create Read Update Destroy controllers/items.jsThis is an abstraction to help us pull the logic out of  and into a separate file. Since we are keeping this app simple we are putting the database logic here as this is the only resource.First we require the  package. Then we set up a couple of helpers based off of this package. will be used to connect to an actual Mongo server in this case our local computer on port 27017 the default. will be used to create or open the specific database in Mongo.Finallywill be used to help us interact with the data format Mongo uses to store records.The next few lines setting up the server db and opening the db are fairly explanatory.Each of the lines that start with  you will notice match up to the function calls we make on the  variable in the  file.s allows you to create functions that can be used when someone s this module into their code.3. branch 'controlleractionstomongo'1) Controller ActionsWe'll start with the  function and then work our way through the REST of the REST operations heh. While we do so we will highlight any differences between the functions.Now we will start to fill in the items controller's actions to interact with the Mongo DB collection of items. findAllLet's follow along and read the  function logically.Take the database connection in variable .Run the  function on that to retrieve a collection called .Providing the following callback for when the collection has been retrievedAct upon the returned collection by calling the  function and then call  on it to make it into a JavaScript array.Once that is done send the newly created array as the response to the browser and end the connection. findByIdNew in this function is pulling the  we want to look up from the request object's parameters.We perform the same collection retrieval as last time in fact we perform this on each REST action  perhaps someone would like to abstract it or simplify it somehow?The difference is that instead of calling  we call . But in order to find a specific item we have to pass in the  we got from the request. However the way Mongo stores data is through BSON or Binary JSON. We need to convert the  given to us to BSON before we can look it up in Mongo. addItemHere we require the use of the request body which we expect to be a JSON object being sent to us by the client.In order for Express to know how to handle this we must explicitly configure it to use the ""body parser"". We can turn this on in server.js by adding the following line:Again we ask Mongo for the collection and then we insert the  which is what the client sent to us in the body of the HTTP request.Here also note that we provide an error message to the user if the insert fails for any reason. It is always good practice to inform the consumers of your API of any failures. updateItemThis is basically a combination of  and . We first accept an ID of an item to update and then use the collection update function to modify it with the new body content passed it. Note that this is entirely destructive. It will fully replace the entire item with what you have added. deleteItemThis rounds out the RESTful actions of our simple API. It closely resembles  but instead of just returning the result it deletes it and then returns it.2) Testing the APINow that we have implemented the API we should manually make sure that it works. In this section we will use two tools: one to interact directly with our backend database and one to interact directly with our RESTful Node server from the command line. Mongo Command LineLike most larger programming languages and tools Mongo comes with a built in command line program for interacting with the database.From the command line run:For convenience here is a mini Mongo cheatsheet to get up and running (I actually still use this personally whenever I work with Mongo I hope you find it helpful) curl is a wonderful utility to work with HTTP requests and responses via the command line. I often feel that GUI programs are more complicated and provide unnecessary complexity especially when the operations you are trying to perform are simple and you have a handy cheat sheet 🙂Notice how each of these URLs will hit a specific route with a specific HTTP verb. We defined all of these routes in our  file. Further we provided functions in the  controller to handle these requests namely interacting with the Mongo back end database.4. branch 'backbonecollection'Backbone.jsBackbone.js is a JavaScript library primarily targeted to the front end to help organize and display content. Technically backbone implements the ModelViewPresenter pattern. Since our backend is just a simple REST API using something like backbone on the frontend can help give our application more structure.1) Yeah but why are we using it?Why not? The great thing about Node and JavaScript everywhere is that things hook up really nicely. As we will see there isn't too much work that will go into setting up a simple backbone app.It will also provide some code to set us up with event handling which we will want for the final part of this presentation.Also just having simple reference material to work from when creating new projects can be really helpful.2) Creating index.htmlCreate a  folder and make a file called  inside of it.This file will hold our backbone template. Backbone isn't really as hard as it is made out to be and this is a good spot to start to understand what's going on.First we create a div called . Inside the div we define a . Now this isn't a script like a JavaScript file this is just an arbitrary script tag that we are going to give a type ""text/template"" to. This doesn't mean anything to HTML so it won't get displayed but it means the world to backbone which we will see soon.Inside the script we can use this  syntax to pull the names of our items attributes.If we were working earlier with  and  we would want to name things the same way here so they show up as we expect.Also note that in our HTML file we load backbone jQuery underscore.js and   which we are going to write now.3) Creating the backbone app.jsInside the  folder create a new JavaScript file called . This is pretty bad practice in general but for now we are going to just stick all our backbone code into one file.4) Item ModelThe first and most simple thing to do is to create a backbone model that will mimic the data we have on our backend in the Mongo database.We can also define some defaults here so if the backend collection doesn't have these attributes for some reason at least something will be displayed and it won't throw off our styling in the front end.5) Item Collection (Library)Next up we need to define the fact that there will be more than one of these items that we will want to display.The cool thing about this code is that we can point our front end collection to a RESTful API endpoint in this case  and backbone will automagically load the records in that collection.6) Item ViewThis is where things begin to look a little bit complicated but if we keep in mind that this whole next section of code simply  to use as our view template things aren't that difficult.It does a couple of other things first. Whenever we create an item to display it is going to create a  with class  and it is going to fill the contents of that div with the model we provides JSON attributes. These attributes will match up to the template attributes (the ones in the  syntax) and generate the html properly.7) Item Collection (Library) ViewThe last piece is a view to display our entire collection of items. This piece will wrap the collection itself into a view and provide logic to render each item in that collection.A couple of interesting things are going on here we are explicitly setting the  attribute to be a div existing in our HTML that we want to render this collection of items.Next up is the  function which sets this view's collection to be our  collection. This collection is the one that is tied to our backend API at . Now we can pull those items with the  call and finally .Also note that we have registered two events here as well the  and the  events. This allows us to run code when something happens. This is all related to that evented callback world we talk about in Node.The important call here is the  function. This function looks very confusing at first but when we step through it it is not so bad. We iterate through each model in the collection we have and run  on it.8) Create it!Up until this point we have just been doing set up. The key to it all is creating an . This will kick everything off in its  function.5. Simple Pub Sub with FayeComing Soon!6. Special Thanks To",NA
R workshop III: Data Visualization with ggplot2 (Second Time),84,https://nycdatascience.com/blog/meetup/r-workshop-iii-data-visualization-with-ggplot2-second-time/,Vivian introduced 7 basic concepts in ggplot2(mapping scalegeometic statistic coordinatelayer facet) and go over techniques to draw point barhistogramline tile and map drawing.Vivian Zhang is cofounder and CTO of a statistical consulting firmSupStat Inc. She got double Master degress in Computer Science and Statistics. She focus on business analytics and big data technologies and is a dataaholics visualization evangelist and programmer. She worked for several wellknown research institutes in the past five years and published a few papers with top scholars. The most recent publication is about effective statistical method design to reducing HIV test cost and is accepted by Journal of the American Statistical Association(JASA). Download Rstudio and ggplot2 R package.,NA,NYC Data Science Academy is offering five relative courses:RSVP RSVP RSVP RSVP RSVP RSVP 7:008:00pm Vivian demonstrated ggplot2 codes and explained in depth.8:009:00pm Hack time TAReference:<<Machine learning for hackers>>  by Drew Conway and John Myles White,NA
R Workshop X: Interactive Documents from R,84,https://nycdatascience.com/blog/meetup/r-workshop-x-interactive-documents-from-r/, Ramnath Vaidyanathan Advisory Data Scientist our rCharts/Slidfy/Shiny Instructor. Dr. Vaidyanathan is an Assistant Professor of Operations Management at McGill University.  He holds a Ph.D. in Operations Management from the Wharton School of UP and worked as a Business Analyst at McKinsey & Company before advising SupStat. He has great passion for R and has developed two R packages  and  both aiming at simplifying the creation and sharing of interactive webbased content with R.,NA,NYC Data Science Academy is offering five relative courses:RSVP RSVP RSVP RSVP RSVP RSVP Mode: Google HangoutIn this workshop we learned how to use  to create customize and share interactive web documents using R Markdown.,NA
D3.js Workshop III: Intro to Binding Data to the DOM,85,https://nycdatascience.com/blog/meetup/d3-js-workshop-iii-intro-to-binding-data-to-the-dom/,"Adam Pearce is a Data Interaction Developer for . He posts data visualizations on his  and frequently answers questions about D3 on .D3 is built on top of CSS HTML and Javascript. If you're not familiar with them check out . Before coming make sure to download these  and to have Chrome + .The creator of D3 has a several excellent tutorials that cover similar ground:


",NA,"Many thanks go to Adam Pearce for giving such a great workshop!NYC Data Science Academy is offering a relative courses:RSVP 
This will be a hands on interactive exploration of the way D3 binds data to the DOM. We'll start with the text of Pericles' Funeral Oration first displaying it with D3 then altering its appearance and finally creating graphical representations.",NA
"What I Learned From 100,000 Open Data Across 100 Open Data Portals",85,https://nycdatascience.com/blog/meetup/what-i-learned-from-100000-open-data-across-100-open-data-portals/,"He also talked a bit about brainstorming and six thinking hats. Then people did a couple of exercises.  Choose an open data catalog. Diagram how a person could manually download all of the datasets. Then change the labels in the diagram so that it describes a computer program that downloads the datasets.
 Select a guideline from one of these lists and brainstorm ways of testing it. ",NA,"Many thanks go to Thomas Levine for giving such a great workshop!Thomas Levine has downloaded 100000 datasets from 100 open data portals and this is what he learned.He talked about all aspects of how he did this and downloading was of course a big part of that. Here were two repositories that you could link to if you like. They lacked comprehensible documentation though.Playing with computers since he was young Thomas Levine eventually developed back and wrist pain so he started studying ergonomics and conducting quantitative ergonomics research. Then he realized that he’d accidentally become a data scientist. And his back and wrists now hurt less. He also has a band called CSV Soundsystem that makes music from spreadsheets.For the first half of the session he would talk about what he did and what he learned.After that he talked in more detail about how to conduct an analysis like this. The specifics depended on what interested participantsbut topics could include Planning complicated data workflows/pipelines
 Storing data
 Tricks for making things run fasterIn additionYou could try one exercise before you begin to see more details about this workshop.",NA
Policy in Practice Series: Councilwoman Brewer,86,https://nycdatascience.com/blog/meetup/policy-in-practice-series-councilwoman-brewer/,Another useful resource for what NYC is doing on technology and where there is work left to do can be found here: .,NA,"We are so honored to host the candidate for Manhattan Borough President Gale Brewer as one of our policy speakers. Special thanks go to Brian Schaitkin for helping us to initiate the series.Big Data and Tech Policy Conversation with Council Woman Gale A. BrewerGale A. Brewer has represented the Upper West Side and northern Clinton in the New York City Council since 2002. Her service in the Council is a continuation of nearly 40 years of public service to the people of New York.CM Brewer has been Chair of the Committee on Governmental Operations since 2010. It oversees governmental structure and organization with an eye toward increasing both efficiency and accountability particularly in the delivery of services and the use of technology. The Committee is also responsible for oversight of the New York City Board of Elections the New York City Campaign Finance Board the Department of Citywide Administrative Services the Department of Records and Information Services and other agencies.From 20022009 she chaired the Committee on Technology where she focused on the  use of technology to increase transparency save money improve city services and bring residents businesses and nonprofits closer to government and their communities. CM Brewer is integrating many of the best practices of the Technology Committee with Governmental Operations. Council Member Brewer’s work on tech issues includes leading the charge to acquire the dot NYC domain name as early as 2008; the passage of the most progressive Open Data legal policy ever adopted by a municipality; the creation of a Broadband Advisory Committee which examined issues of broadband adoption in all five boroughs; introduction of legislation to require all public meetings in New York City be webcast; and more. All of the City Council’s hearings are now webcast live thanks to CM Brewer’s recommendation.Other committees on which CM Brewer serves include: Aging; Finance; General Welfare; Higher Education; Housing & Buildings; Mental Health; Technology; Transportation; and Waterfronts. In addition she cochairs the Manhattan Delegation sits on the Council's Budget Negotiating Team is a member of the Rules Committee Working Group and represents the City Council on the Commission on Public Information & Communication (COPIC) the Report and Advisory Board Review Commission and the Mayor’s Management Report Update Roundtable.In addition to serving on the aforementioned committees CM Brewer has been a member of the Digital Literacy and Community Outreach Action Team (part of the New York State Universal Broadband Initiative); a member of the New York State Attorney General’s Real Estate Working Group/Subcommittee on Enforcement and Mediation; the New York Foundation Center Library Advisory Committee; and the Information Technology and Communications Committee of the National League of Cities. CM Brewer is a frequent attendee of tech events such as the NY Tech Meetup tech events and she recently served as a judge at an NYC BigApps event.She has been the sponsor of numerous laws that represent her commitment to open government health and human rights and quality of life. These include the firstinthenation Protection for Domestic Workers law (June 2003); creation of a blueribbon Broadband Advisory Committee to improve broadband services citywide (December 2005); establishment of an Electronic Death Registration system (February 2004); and legislation requiring City Publications to be made available via the Web (February 2003). Her latest initiatives signed into law include: A ban on smoking in parks and beaches (February 2011); a law to encourage the purchase of locally grown food (August 2011); the creation of an Accessible Pedestrian Signal program (April 2012); the landmark Open Data law (March 2012) requiring the city to publish most of its databases online in open machinereadable formats; and one of the most progressive pieces of legislation ever adopted in the City Council a law requiring that most businesses in the city of New York provide their workers with paid sick leave (May 2013). Opening remarks from CM Brewer (15 minutes) Question and Answer session with CM Brewer (30 minutes) Small group workshops on particular tech issues (15 minutes) Campaign office sharing general operation daily tasks and volunteering opportunities( 15 minutes) Taking suggestions from audience to run campaign about using data(15 minutes) Final presentations / discussion (15 minutes) Tech issue:
Gale's technology issue paper can be found here: . This provides some more detailed information about some of the work Gale has done on tech issues and what she would like to focus on as Borough President.
 Campaign analytic need:
Gale's team has used Google Trends but not as a big resource. Facebook ads data is also available for analytic.  She has volunteers to do phonebanking every day. Responses are coded into NGP VAN in order to determine supporters and potential volunteers. undecided voters will be followed up.Fundraising analytic need:
Gale's team primarily uses events and emails. Email usage is restricted. Dear Neighbor letters are also distributed across the borough.Call for volunteering data scientists:
Any suggestions this meetup group can make as far as other opportunities for data usage and/or analytics would be greatly appreciated.",NA
Python Workshop II: Pandas for Data Analysis (no Python basic),86,https://nycdatascience.com/blog/meetup/python-workshop-ii-pandas-for-data-analysis-no-python-basic/," We dived into using Pandas to do some analysis and visualization directly.
 We broke into groups to give everyone a chance to apply the material from the talk.Bring your favorite editor and python shell. Install pandas with 'pip install pandas'.",NA,"Many Thanks to On Deck Capital for hosting us and John Downs for sharing his experience with Pandas.This was a enhanced session of Aug 26th ""Python Workshop I: Pandas for data analysis (with python basic)"". You could find useful information from comments at previous event.John Downs our Dean and Python Instructor. As Dean he coordinates the development of programs and mentors other instructors.  He also organizes the  meetup.  His research interests include robotics with work on collaborative navigation using artificial intelligence.  He has many years of experience as a software engineer in industry and is a member of the ACM and IEEE.",NA
R Workshop I: R Basic (4th time),86,https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-4th-time/,This was a repeated session of August 29th July 8th and July 29th “R workshop I: R basic”. You could find useful information from comments at those events.Chris Whong Socrata(Vendor of NYC Open Data Portal). Individual hack time people could make sure all the R codes run on their own PC before they go.,NA," This event was coorganized by  beta NYC . Special thank to Noel Hidalgo. His organization kindly sponsored us pizza and drink.Thanks Joel Witten and his firmSquarespace for kindly sponsoring us the space!Thanks Xu Fei from United Nations to be our TA for this and last R Basic Workshops!NYC Data Science Academy is offering five relative courses:RSVP RSVP RSVP RSVP RSVP RSVP Vivian Zhang introduced how to load clean nyc open data and do basic visualization by R.Vivian S. Zhang Founder President  and our R and Hadoop Instructor. Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. She obtained expertise on data analysis and data management using various statistical analytical tools and programming languages. She cofounded SupStat founded NYC Data Science Academy and is an organizer of . Prior to taking entrepreneurial steps she worked as a Senior Financial Analyst at Memorial SloanKettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received her continued education in Statistics and Computer Science from Brown University Stanford University and Double Master Degrees from Stony Brook University and San Jose State University. She likes to portray herself as a dataholic visualization evangelist and programmer. Chris Whong gave a 15 minutes talk and introduced us to the platform.
 Vivian explained what R is what R can do how R compare to other tools you might already know and use. And She also explained some R codes.
To get the most benefit from the session before you comeplease consider
1. installing Rstudio before you come. You can download Rstudio from 
2. getting yourself a free github account so you can get some R sources right away. sign up at",NA
Kaggle Talk Series: Top 0.2% Kaggler on Amazon Employee Access Challenge,87,https://nycdatascience.com/blog/meetup/kaggle-talk-series-top-0-2-kaggler-on-amazon-employee-access-challenge/,Introduced our solution to the Amazon Employee Access Challenge.The Kaggle competition The Hewlett Foundation: Short Answer Scoring and the winners' solutions.,NA,"Many thanks go to Knewton for providing the space for this event!Special thanks go to Yibo Chen for giving such a great workshop!This is a step by step intensive 2 hours demonstration session of model building which focus on a ongoing kaggle competition.Yibo Chen is a data analyst with experience in model building such as response model in CRM and credit score card. Recently he is interested in Kaggle's competitions. After participating in some of these competitions he has learned some knowledge about the data mining and also get a score not very bad (currently 231st of 104993 data scientists). Feature engineering(extraction and selection). Modeling techniques
Use classifiers including Gradient Boosting Machine Random Forest Regularized Generalized Linear Models and Support Vector Machine) Ensembles.
Use stacking based on 5fold cv for combining predictions of the base learners. The software we use is R (2.15.1) and some addon packages including gbm randomForest glmnet kernlab and Matrix. ",NA
NYC Open Data Portal + One Mobile App Using the Data,87,https://nycdatascience.com/blog/meetup/nyc-open-data-portal-one-mobile-app-using-the-data/,Andrew introduced us to NYC OpenData () and NYC Developer Portal (). He also touched on NYC Open Data licensing problem. Kevin shared his experience using NYC open data and developing mobile app. He addressed some technical challenges he faced when building the app which were iPhone programmingspecific. His deck is .,NA,"Many thanks go to Andrew and Kevin for giving such a great workshop!Andrew Nicklin NYC DoITT’s Director of Research & Development is focused on an agenda of technology innovation open government and civic engagement. He currently manages the NYC’s OpenData portal at .Kevin Wolkober experienced web designer and developer has successful released his app ""NYC wifi"" to apple app store. His app is a WiFi hotspot locater for NYC containing a WiFi location dataset provided by NYC Open Data. ",NA
R Workshop I: R Basic (2nd time),87,https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-2nd-time/,Founder President and our R and Hadoop Instructor. Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. She obtained expertise on data analysis and data management using various statistical analytical tools and programming languages. She cofounded SupStat founded NYC Data Science Academy and is an organizer of . Prior to taking entrepreneurial steps she worked as a Senior Financial Analyst at Memorial SloanKettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received her continued education in Statistics and Computer Science from Brown University Stanford University and Double Master Degrees from Stony Brook University and San Jose State University. She likes to portray herself as a dataholic visualization evangelist and programmer.,NA,"Many thanks go to PlaceIQ for providing the space of this event!NYC Data Science Academy is offering five relative courses:RSVP RSVP RSVP RSVP RSVP RSVP This is a repeated session of July 8th  ""R workshop I: R basic"". You can find useful information from comments at previous event. Vivian Zhang introduced how to load clean nyc open data and do basic visualization by R.Vivian S. Zhang ",NA
R Workshop I: R Basic (3rd time),87,https://nycdatascience.com/blog/meetup/r-workshop-i-r-basic-3rd-time/,"sponsored us their office space for this ""R workshop I: R basic"" event.Vivian is a data scientist who has being devoted to the analytics industry and data technologies over years. She obtained expertise on data analysis and data management using various statistical analytical tools and programming languages. She cofounded SupStat founded NYC Data Science Academy and is an organizer of . Prior to taking entrepreneurial steps she worked as a Senior Financial Analyst at Memorial SloanKettering Cancer Center and Scientific Programmer at the Center of Statistics of Brown University. Vivian received her continued education in Statistics and Computer Science from Brown University Stanford University and Double Master Degrees from Stony Brook University and San Jose State University. She likes to portray herself as a dataholic visualization evangelist and programmer. Vivian explained what R is what R can do how R compare to other tools you might already know and use. And She explained some R codes.
 Individual hack time people could make sure all the R codes run on their own PC before they go. to install Rstudio before you come. You can download Rstudio from 
 to get yourself a free github account so you can get some R sources right away. sign up at",NA,"Special thanks go to Joel Witten Ilan Man and their generous employer www.Squarespace.com. Squarespace NYC Data Science Academy is offering five relative courses:RSVP RSVP RSVP RSVP RSVP RSVP This was a repeated session of July 8th and July 29th ""R workshop I: R basic"". You could find useful information from comments at those events.Vivian Zhang introduced how to load clean nyc open data and do basic visualization by R.Vivian S. Zhang Founder President and our R and Hadoop Instructor. ",NA
Visualizing DocGraph in Gephi,87,https://nycdatascience.com/blog/meetup/visualizing-docgraph-in-gephi/,Janos demonstrated how to extract a subset of doctors that are practicing in New York City and how he combined DocGraph with a modified version of the federal NPI database. He also talked about some of the challenges and pitfalls of working with DocGraph and other healthcare datasets.,NA,"Many thanks go to Dr. Janos G. Hajagos for giving such a great workshop!The doctor referral data reveals the real recommendation system among doctors. You will know how popular/good the doctors are. The goal is to help you make informative decision.The talk focused on how services are delivered to Medicare beneficiaries in New York City.Dr. Janos G. Hajagos (Stony Brook Medicine) is the lead data analyst for a unique partnership between SUNY and the New York State Department of Health. He holds a Ph.D. in Ecology and Evolutionary Biology and has published widely from risk analysis to applications of the semantic web in healthcare. He is a participant in the CTSAConnect project whose goal is to make hidden resources and expertise at medical schools discoverable.DocGraph is the brainchild of Fred Trotter author and healthcare data activist. It connects healthcare service providers that share 10 or more Medicare beneficiaries in a 30 day window. Janos joined Fred in DocGraph project and has been applying social network analysis to healthcare data for several years.
Gephi is an open source tool for manipulating and analyzing graphs in real time.Visualization the DocGraph for Wyoming Medicare Providers: Video about ""Visualizing DocGraph in Gephi"":",NA
